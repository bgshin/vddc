train_0
False
The vocabulary size is: 2665791
[w2v..]
Elapsed: 35.0521190166
[loading trn..]
Elapsed: 355.281431198
[loading dev..]
Elapsed: 39.1139750481
[loading tst..]
Elapsed: 26.3904290199
w2v is STATIC (NOT trainable)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:0)
h_pool Tensor("tower_0_dev/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:0)
h_pool_flat Tensor("tower_0_dev/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:0)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:1)
h_pool Tensor("tower_0_tst/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:1)
h_pool_flat Tensor("tower_0_tst/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:1)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:0)
h_pool Tensor("tower_0/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:0)
h_pool_flat Tensor("tower_0/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:0)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:1)
h_pool Tensor("tower_1/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:1)
h_pool_flat Tensor("tower_1/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:1)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:2)
h_pool Tensor("tower_2/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:2)
h_pool_flat Tensor("tower_2/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:2)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:3)
h_pool Tensor("tower_3/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:3)
h_pool_flat Tensor("tower_3/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:3)
2017-05-09 01:20:02.503340: step 20, loss = 1.1408, acc = 0.4940 (124.8 examples/sec; 0.513 sec/batch)
2017-05-09 01:20:08.472787: step 40, loss = 0.9910, acc = 0.5120 (216.7 examples/sec; 0.295 sec/batch)
2017-05-09 01:20:13.860382: step 60, loss = 0.8845, acc = 0.6240 (235.4 examples/sec; 0.272 sec/batch)
2017-05-09 01:20:19.236603: step 80, loss = 0.7784, acc = 0.7540 (245.7 examples/sec; 0.261 sec/batch)
2017-05-09 01:20:24.279712: step 100, loss = 0.7224, acc = 0.8220 (245.3 examples/sec; 0.261 sec/batch)
2017-05-09 01:20:29.835026: step 120, loss = 0.6644, acc = 0.8460 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 01:20:34.948848: step 140, loss = 0.6153, acc = 0.8420 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 01:20:40.169433: step 160, loss = 0.5704, acc = 0.8580 (235.4 examples/sec; 0.272 sec/batch)
2017-05-09 01:20:45.226655: step 180, loss = 0.5478, acc = 0.8380 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 01:20:50.464116: step 200, loss = 0.4904, acc = 0.8840 (245.7 examples/sec; 0.260 sec/batch)
2017-05-09 01:20:55.818217: step 220, loss = 0.4823, acc = 0.8600 (245.6 examples/sec; 0.261 sec/batch)
2017-05-09 01:21:01.050324: step 240, loss = 0.4425, acc = 0.8880 (255.6 examples/sec; 0.250 sec/batch)
2017-05-09 01:21:06.419061: step 260, loss = 0.3876, acc = 0.9120 (246.2 examples/sec; 0.260 sec/batch)
2017-05-09 01:21:11.652173: step 280, loss = 0.3889, acc = 0.8960 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 01:21:16.747521: step 300, loss = 0.3688, acc = 0.8980 (256.6 examples/sec; 0.249 sec/batch)
2017-05-09 01:21:22.125301: step 320, loss = 0.3529, acc = 0.9020 (235.0 examples/sec; 0.272 sec/batch)
2017-05-09 01:21:27.502475: step 340, loss = 0.3408, acc = 0.9100 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 01:21:32.908480: step 360, loss = 0.3609, acc = 0.8860 (223.8 examples/sec; 0.286 sec/batch)
2017-05-09 01:21:38.080333: step 380, loss = 0.3430, acc = 0.8940 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 01:21:43.667683: step 400, loss = 0.3635, acc = 0.8980 (189.2 examples/sec; 0.338 sec/batch)
2017-05-09 01:21:49.146059: step 420, loss = 0.3462, acc = 0.9000 (240.5 examples/sec; 0.266 sec/batch)
2017-05-09 01:21:54.488065: step 440, loss = 0.3354, acc = 0.9020 (229.7 examples/sec; 0.279 sec/batch)
2017-05-09 01:21:59.876692: step 460, loss = 0.3554, acc = 0.8860 (237.4 examples/sec; 0.270 sec/batch)
2017-05-09 01:22:05.404682: step 480, loss = 0.3497, acc = 0.8800 (234.5 examples/sec; 0.273 sec/batch)
2017-05-09 01:22:11.188046: step 500, loss = 0.3028, acc = 0.9140 (245.6 examples/sec; 0.261 sec/batch)
2017-05-09 01:22:16.673128: step 520, loss = 0.3358, acc = 0.8980 (231.8 examples/sec; 0.276 sec/batch)
2017-05-09 01:22:22.218269: step 540, loss = 0.3365, acc = 0.8860 (236.0 examples/sec; 0.271 sec/batch)
2017-05-09 01:22:27.848649: step 560, loss = 0.3409, acc = 0.8880 (218.9 examples/sec; 0.292 sec/batch)
2017-05-09 01:22:33.571083: step 580, loss = 0.3296, acc = 0.8960 (195.4 examples/sec; 0.327 sec/batch)
2017-05-09 01:22:39.219017: step 600, loss = 0.3177, acc = 0.8920 (225.4 examples/sec; 0.284 sec/batch)
2017-05-09 01:22:44.676301: step 620, loss = 0.3163, acc = 0.9000 (229.9 examples/sec; 0.278 sec/batch)
2017-05-09 01:22:50.394492: step 640, loss = 0.3006, acc = 0.9080 (229.9 examples/sec; 0.278 sec/batch)
2017-05-09 01:22:55.913729: step 660, loss = 0.3006, acc = 0.8980 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 01:23:01.643330: step 680, loss = 0.2825, acc = 0.9180 (233.6 examples/sec; 0.274 sec/batch)
2017-05-09 01:23:07.450631: step 700, loss = 0.2823, acc = 0.9140 (227.5 examples/sec; 0.281 sec/batch)
2017-05-09 01:23:12.898014: step 720, loss = 0.2893, acc = 0.9220 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 01:23:18.710708: step 740, loss = 0.3009, acc = 0.8840 (226.8 examples/sec; 0.282 sec/batch)
2017-05-09 01:23:24.217688: step 760, loss = 0.2706, acc = 0.9180 (231.1 examples/sec; 0.277 sec/batch)
2017-05-09 01:23:29.803602: step 780, loss = 0.3120, acc = 0.8920 (229.1 examples/sec; 0.279 sec/batch)
2017-05-09 01:23:35.535773: step 800, loss = 0.2795, acc = 0.9100 (237.5 examples/sec; 0.269 sec/batch)
2017-05-09 01:23:41.211960: step 820, loss = 0.3159, acc = 0.8920 (220.8 examples/sec; 0.290 sec/batch)
2017-05-09 01:23:46.842451: step 840, loss = 0.3087, acc = 0.8820 (228.8 examples/sec; 0.280 sec/batch)
2017-05-09 01:23:52.657594: step 860, loss = 0.2528, acc = 0.9340 (212.1 examples/sec; 0.302 sec/batch)
2017-05-09 01:23:58.274327: step 880, loss = 0.2771, acc = 0.9160 (227.1 examples/sec; 0.282 sec/batch)
2017-05-09 01:24:04.348374: step 900, loss = 0.3170, acc = 0.8780 (217.5 examples/sec; 0.294 sec/batch)
2017-05-09 01:24:10.031369: step 920, loss = 0.2660, acc = 0.9140 (228.0 examples/sec; 0.281 sec/batch)
2017-05-09 01:24:15.700608: step 940, loss = 0.2700, acc = 0.9200 (222.3 examples/sec; 0.288 sec/batch)
2017-05-09 01:24:21.501832: step 960, loss = 0.2692, acc = 0.9260 (234.8 examples/sec; 0.273 sec/batch)
2017-05-09 01:24:27.168070: step 980, loss = 0.2799, acc = 0.8940 (220.7 examples/sec; 0.290 sec/batch)
2017-05-09 01:24:32.968086: step 1000, loss = 0.2992, acc = 0.9020 (237.9 examples/sec; 0.269 sec/batch)
[Eval] 2017-05-09 01:24:47.265546: step 1000, acc = 0.9261, f1 = 0.9239
[Test] 2017-05-09 01:24:57.155062: step 1000, acc = 0.9123, f1 = 0.9115
[Status] 2017-05-09 01:24:57.155169: step 1000, maxindex = 1000, maxdev = 0.9261, maxtst = 0.9123
2017-05-09 01:25:06.991153: step 1020, loss = 0.2646, acc = 0.9120 (232.5 examples/sec; 0.275 sec/batch)
2017-05-09 01:25:12.570055: step 1040, loss = 0.3013, acc = 0.8980 (233.3 examples/sec; 0.274 sec/batch)
2017-05-09 01:25:18.475687: step 1060, loss = 0.2697, acc = 0.9100 (213.6 examples/sec; 0.300 sec/batch)
2017-05-09 01:25:24.104130: step 1080, loss = 0.2647, acc = 0.9200 (223.5 examples/sec; 0.286 sec/batch)
2017-05-09 01:25:29.926882: step 1100, loss = 0.2790, acc = 0.9080 (201.9 examples/sec; 0.317 sec/batch)
2017-05-09 01:25:35.551832: step 1120, loss = 0.2583, acc = 0.9060 (216.9 examples/sec; 0.295 sec/batch)
2017-05-09 01:25:41.289516: step 1140, loss = 0.2621, acc = 0.9240 (218.4 examples/sec; 0.293 sec/batch)
2017-05-09 01:25:47.027209: step 1160, loss = 0.3089, acc = 0.8920 (229.9 examples/sec; 0.278 sec/batch)
2017-05-09 01:25:52.675650: step 1180, loss = 0.2433, acc = 0.9280 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 01:25:58.393138: step 1200, loss = 0.2858, acc = 0.9000 (228.8 examples/sec; 0.280 sec/batch)
2017-05-09 01:26:04.011299: step 1220, loss = 0.2398, acc = 0.9360 (228.5 examples/sec; 0.280 sec/batch)
2017-05-09 01:26:09.688857: step 1240, loss = 0.3081, acc = 0.8920 (228.7 examples/sec; 0.280 sec/batch)
2017-05-09 01:26:15.549988: step 1260, loss = 0.2658, acc = 0.9060 (225.2 examples/sec; 0.284 sec/batch)
2017-05-09 01:26:21.176314: step 1280, loss = 0.2692, acc = 0.9140 (226.3 examples/sec; 0.283 sec/batch)
2017-05-09 01:26:26.769206: step 1300, loss = 0.2781, acc = 0.9120 (229.7 examples/sec; 0.279 sec/batch)
2017-05-09 01:26:32.737053: step 1320, loss = 0.2585, acc = 0.9180 (224.0 examples/sec; 0.286 sec/batch)
2017-05-09 01:26:38.284914: step 1340, loss = 0.2686, acc = 0.9080 (232.7 examples/sec; 0.275 sec/batch)
2017-05-09 01:26:43.898615: step 1360, loss = 0.2548, acc = 0.9120 (226.2 examples/sec; 0.283 sec/batch)
2017-05-09 01:26:49.740350: step 1380, loss = 0.2327, acc = 0.9480 (230.9 examples/sec; 0.277 sec/batch)
2017-05-09 01:26:55.412998: step 1400, loss = 0.2641, acc = 0.9060 (215.8 examples/sec; 0.297 sec/batch)
2017-05-09 01:27:01.094086: step 1420, loss = 0.2603, acc = 0.9160 (229.3 examples/sec; 0.279 sec/batch)
2017-05-09 01:27:06.609243: step 1440, loss = 0.3143, acc = 0.8740 (231.0 examples/sec; 0.277 sec/batch)
2017-05-09 01:27:12.252960: step 1460, loss = 0.2099, acc = 0.9460 (218.7 examples/sec; 0.293 sec/batch)
2017-05-09 01:27:18.126892: step 1480, loss = 0.2846, acc = 0.9160 (228.4 examples/sec; 0.280 sec/batch)
2017-05-09 01:27:23.889165: step 1500, loss = 0.2602, acc = 0.9160 (216.4 examples/sec; 0.296 sec/batch)
2017-05-09 01:27:29.534337: step 1520, loss = 0.2235, acc = 0.9440 (218.0 examples/sec; 0.294 sec/batch)
2017-05-09 01:27:35.074877: step 1540, loss = 0.2519, acc = 0.9140 (235.7 examples/sec; 0.271 sec/batch)
2017-05-09 01:27:40.758253: step 1560, loss = 0.2464, acc = 0.9220 (223.4 examples/sec; 0.287 sec/batch)
2017-05-09 01:27:46.728336: step 1580, loss = 0.2472, acc = 0.9300 (217.1 examples/sec; 0.295 sec/batch)
2017-05-09 01:27:52.475888: step 1600, loss = 0.2488, acc = 0.9200 (232.8 examples/sec; 0.275 sec/batch)
2017-05-09 01:27:58.170603: step 1620, loss = 0.2489, acc = 0.9220 (224.0 examples/sec; 0.286 sec/batch)
2017-05-09 01:28:04.082556: step 1640, loss = 0.2438, acc = 0.9300 (230.2 examples/sec; 0.278 sec/batch)
2017-05-09 01:28:09.816425: step 1660, loss = 0.2498, acc = 0.9220 (228.4 examples/sec; 0.280 sec/batch)
2017-05-09 01:28:15.689145: step 1680, loss = 0.2602, acc = 0.9120 (216.3 examples/sec; 0.296 sec/batch)
2017-05-09 01:28:21.655750: step 1700, loss = 0.2375, acc = 0.9360 (218.6 examples/sec; 0.293 sec/batch)
2017-05-09 01:28:27.344415: step 1720, loss = 0.2427, acc = 0.9220 (234.0 examples/sec; 0.274 sec/batch)
2017-05-09 01:28:33.170837: step 1740, loss = 0.2546, acc = 0.9140 (230.7 examples/sec; 0.277 sec/batch)
2017-05-09 01:28:38.718263: step 1760, loss = 0.2182, acc = 0.9320 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 01:28:44.270466: step 1780, loss = 0.2240, acc = 0.9320 (222.4 examples/sec; 0.288 sec/batch)
2017-05-09 01:28:50.010475: step 1800, loss = 0.2283, acc = 0.9400 (240.0 examples/sec; 0.267 sec/batch)
2017-05-09 01:28:55.587819: step 1820, loss = 0.2151, acc = 0.9400 (239.6 examples/sec; 0.267 sec/batch)
2017-05-09 01:29:01.275279: step 1840, loss = 0.2017, acc = 0.9240 (202.6 examples/sec; 0.316 sec/batch)
2017-05-09 01:29:06.880388: step 1860, loss = 0.2523, acc = 0.9040 (223.3 examples/sec; 0.287 sec/batch)
2017-05-09 01:29:12.755437: step 1880, loss = 0.2710, acc = 0.9180 (224.1 examples/sec; 0.286 sec/batch)
2017-05-09 01:29:18.579804: step 1900, loss = 0.2390, acc = 0.9280 (225.2 examples/sec; 0.284 sec/batch)
2017-05-09 01:29:24.244118: step 1920, loss = 0.2414, acc = 0.9160 (221.6 examples/sec; 0.289 sec/batch)
2017-05-09 01:29:29.723377: step 1940, loss = 0.2601, acc = 0.9100 (238.8 examples/sec; 0.268 sec/batch)
2017-05-09 01:29:35.447943: step 1960, loss = 0.2149, acc = 0.9280 (239.9 examples/sec; 0.267 sec/batch)
2017-05-09 01:29:40.904388: step 1980, loss = 0.2404, acc = 0.9240 (232.1 examples/sec; 0.276 sec/batch)
2017-05-09 01:29:46.729513: step 2000, loss = 0.2042, acc = 0.9380 (208.8 examples/sec; 0.307 sec/batch)
[Eval] 2017-05-09 01:30:00.651584: step 2000, acc = 0.9351, f1 = 0.9330
[Test] 2017-05-09 01:30:10.400673: step 2000, acc = 0.9216, f1 = 0.9209
[Status] 2017-05-09 01:30:10.400772: step 2000, maxindex = 2000, maxdev = 0.9351, maxtst = 0.9216
2017-05-09 01:30:20.321958: step 2020, loss = 0.2479, acc = 0.9020 (224.9 examples/sec; 0.285 sec/batch)
2017-05-09 01:30:25.884790: step 2040, loss = 0.2685, acc = 0.9120 (226.4 examples/sec; 0.283 sec/batch)
2017-05-09 01:30:31.470580: step 2060, loss = 0.2446, acc = 0.9220 (215.8 examples/sec; 0.297 sec/batch)
2017-05-09 01:30:36.969269: step 2080, loss = 0.2256, acc = 0.9300 (224.1 examples/sec; 0.286 sec/batch)
2017-05-09 01:30:42.692626: step 2100, loss = 0.2078, acc = 0.9400 (230.5 examples/sec; 0.278 sec/batch)
2017-05-09 01:30:48.370525: step 2120, loss = 0.2364, acc = 0.9160 (230.3 examples/sec; 0.278 sec/batch)
2017-05-09 01:30:53.833748: step 2140, loss = 0.2151, acc = 0.9320 (216.9 examples/sec; 0.295 sec/batch)
2017-05-09 01:30:59.467424: step 2160, loss = 0.2473, acc = 0.9200 (205.8 examples/sec; 0.311 sec/batch)
2017-05-09 01:31:05.080196: step 2180, loss = 0.2292, acc = 0.9340 (239.6 examples/sec; 0.267 sec/batch)
2017-05-09 01:31:10.512678: step 2200, loss = 0.2543, acc = 0.9180 (232.6 examples/sec; 0.275 sec/batch)
2017-05-09 01:31:16.221070: step 2220, loss = 0.2419, acc = 0.9200 (219.0 examples/sec; 0.292 sec/batch)
2017-05-09 01:31:21.894212: step 2240, loss = 0.2401, acc = 0.9180 (228.7 examples/sec; 0.280 sec/batch)
2017-05-09 01:31:27.327628: step 2260, loss = 0.2209, acc = 0.9380 (235.0 examples/sec; 0.272 sec/batch)
2017-05-09 01:31:33.047640: step 2280, loss = 0.2277, acc = 0.9220 (205.1 examples/sec; 0.312 sec/batch)
2017-05-09 01:31:38.492514: step 2300, loss = 0.2353, acc = 0.9280 (231.1 examples/sec; 0.277 sec/batch)
2017-05-09 01:31:44.053978: step 2320, loss = 0.2417, acc = 0.9200 (230.3 examples/sec; 0.278 sec/batch)
2017-05-09 01:31:49.634942: step 2340, loss = 0.2528, acc = 0.9160 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 01:31:55.097627: step 2360, loss = 0.2279, acc = 0.9240 (235.8 examples/sec; 0.271 sec/batch)
2017-05-09 01:32:00.697164: step 2380, loss = 0.2577, acc = 0.9140 (243.5 examples/sec; 0.263 sec/batch)
2017-05-09 01:32:06.445703: step 2400, loss = 0.2100, acc = 0.9340 (233.3 examples/sec; 0.274 sec/batch)
2017-05-09 01:32:11.930792: step 2420, loss = 0.2660, acc = 0.9120 (240.4 examples/sec; 0.266 sec/batch)
2017-05-09 01:32:17.541110: step 2440, loss = 0.2024, acc = 0.9400 (211.3 examples/sec; 0.303 sec/batch)
2017-05-09 01:32:23.090883: step 2460, loss = 0.2418, acc = 0.9240 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 01:32:28.585194: step 2480, loss = 0.1939, acc = 0.9420 (228.1 examples/sec; 0.281 sec/batch)
2017-05-09 01:32:34.092462: step 2500, loss = 0.2425, acc = 0.9180 (246.0 examples/sec; 0.260 sec/batch)
2017-05-09 01:32:39.663240: step 2520, loss = 0.2127, acc = 0.9320 (235.3 examples/sec; 0.272 sec/batch)
2017-05-09 01:32:45.172409: step 2540, loss = 0.2316, acc = 0.9160 (224.3 examples/sec; 0.285 sec/batch)
2017-05-09 01:32:50.880639: step 2560, loss = 0.2359, acc = 0.9300 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 01:32:56.449233: step 2580, loss = 0.1995, acc = 0.9400 (234.7 examples/sec; 0.273 sec/batch)
2017-05-09 01:33:01.890184: step 2600, loss = 0.2213, acc = 0.9260 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 01:33:07.438058: step 2620, loss = 0.2729, acc = 0.9040 (221.7 examples/sec; 0.289 sec/batch)
2017-05-09 01:33:12.745337: step 2640, loss = 0.2303, acc = 0.9180 (246.0 examples/sec; 0.260 sec/batch)
2017-05-09 01:33:18.165848: step 2660, loss = 0.2514, acc = 0.9220 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 01:33:23.900485: step 2680, loss = 0.2550, acc = 0.9320 (226.9 examples/sec; 0.282 sec/batch)
2017-05-09 01:33:29.250625: step 2700, loss = 0.2210, acc = 0.9420 (238.5 examples/sec; 0.268 sec/batch)
2017-05-09 01:33:34.722968: step 2720, loss = 0.2454, acc = 0.9140 (218.6 examples/sec; 0.293 sec/batch)
2017-05-09 01:33:40.152694: step 2740, loss = 0.2708, acc = 0.9060 (245.6 examples/sec; 0.261 sec/batch)
2017-05-09 01:33:45.515364: step 2760, loss = 0.2351, acc = 0.9280 (255.4 examples/sec; 0.251 sec/batch)
2017-05-09 01:33:51.043795: step 2780, loss = 0.2136, acc = 0.9340 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 01:33:56.342700: step 2800, loss = 0.2513, acc = 0.9120 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 01:34:01.616927: step 2820, loss = 0.2230, acc = 0.9220 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 01:34:07.041882: step 2840, loss = 0.2043, acc = 0.9460 (230.9 examples/sec; 0.277 sec/batch)
2017-05-09 01:34:12.434964: step 2860, loss = 0.2462, acc = 0.9140 (233.6 examples/sec; 0.274 sec/batch)
2017-05-09 01:34:17.778823: step 2880, loss = 0.2242, acc = 0.9300 (242.7 examples/sec; 0.264 sec/batch)
2017-05-09 01:34:23.218550: step 2900, loss = 0.2220, acc = 0.9200 (231.0 examples/sec; 0.277 sec/batch)
2017-05-09 01:34:28.499100: step 2920, loss = 0.2159, acc = 0.9300 (237.2 examples/sec; 0.270 sec/batch)
2017-05-09 01:34:33.650054: step 2940, loss = 0.2345, acc = 0.9120 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 01:34:38.869971: step 2960, loss = 0.2414, acc = 0.9140 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 01:34:43.972861: step 2980, loss = 0.2562, acc = 0.9040 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 01:34:49.195111: step 3000, loss = 0.2053, acc = 0.9420 (221.4 examples/sec; 0.289 sec/batch)
[Eval] 2017-05-09 01:35:03.441319: step 3000, acc = 0.9404, f1 = 0.9385
[Test] 2017-05-09 01:35:12.949379: step 3000, acc = 0.9280, f1 = 0.9273
[Status] 2017-05-09 01:35:12.949472: step 3000, maxindex = 3000, maxdev = 0.9404, maxtst = 0.9280
2017-05-09 01:35:22.108287: step 3020, loss = 0.2219, acc = 0.9260 (129.2 examples/sec; 0.495 sec/batch)
2017-05-09 01:35:27.268426: step 3040, loss = 0.2291, acc = 0.9180 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 01:35:32.310880: step 3060, loss = 0.2294, acc = 0.9080 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 01:35:37.394116: step 3080, loss = 0.2068, acc = 0.9420 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 01:35:42.385069: step 3100, loss = 0.2283, acc = 0.9220 (252.9 examples/sec; 0.253 sec/batch)
2017-05-09 01:35:47.501414: step 3120, loss = 0.1904, acc = 0.9460 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 01:35:52.602990: step 3140, loss = 0.2294, acc = 0.9120 (254.6 examples/sec; 0.251 sec/batch)
2017-05-09 01:35:57.651866: step 3160, loss = 0.2168, acc = 0.9340 (252.8 examples/sec; 0.253 sec/batch)
2017-05-09 01:36:02.720080: step 3180, loss = 0.2129, acc = 0.9340 (228.7 examples/sec; 0.280 sec/batch)
2017-05-09 01:36:07.637119: step 3200, loss = 0.2288, acc = 0.9180 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 01:36:12.623120: step 3220, loss = 0.2142, acc = 0.9280 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 01:36:17.767700: step 3240, loss = 0.2321, acc = 0.9260 (224.9 examples/sec; 0.285 sec/batch)
2017-05-09 01:36:22.850244: step 3260, loss = 0.2303, acc = 0.9320 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 01:36:27.983427: step 3280, loss = 0.2262, acc = 0.9280 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 01:36:33.054291: step 3300, loss = 0.2145, acc = 0.9400 (235.9 examples/sec; 0.271 sec/batch)
2017-05-09 01:36:38.428538: step 3320, loss = 0.2259, acc = 0.9220 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 01:36:43.354872: step 3340, loss = 0.2298, acc = 0.9240 (247.3 examples/sec; 0.259 sec/batch)
2017-05-09 01:36:48.495701: step 3360, loss = 0.2040, acc = 0.9440 (227.4 examples/sec; 0.281 sec/batch)
2017-05-09 01:36:53.809408: step 3380, loss = 0.2030, acc = 0.9440 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 01:36:58.792397: step 3400, loss = 0.2156, acc = 0.9360 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 01:37:03.917394: step 3420, loss = 0.1911, acc = 0.9400 (221.7 examples/sec; 0.289 sec/batch)
2017-05-09 01:37:08.761877: step 3440, loss = 0.2252, acc = 0.9300 (251.1 examples/sec; 0.255 sec/batch)
2017-05-09 01:37:13.713595: step 3460, loss = 0.2404, acc = 0.9360 (251.3 examples/sec; 0.255 sec/batch)
2017-05-09 01:37:18.663412: step 3480, loss = 0.1972, acc = 0.9300 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 01:37:23.810237: step 3500, loss = 0.2298, acc = 0.9320 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 01:37:28.800629: step 3520, loss = 0.2400, acc = 0.9100 (246.0 examples/sec; 0.260 sec/batch)
2017-05-09 01:37:33.782350: step 3540, loss = 0.2373, acc = 0.9180 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 01:37:38.886409: step 3560, loss = 0.2027, acc = 0.9260 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 01:37:43.808487: step 3580, loss = 0.2082, acc = 0.9340 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 01:37:48.846137: step 3600, loss = 0.2251, acc = 0.9260 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 01:37:54.286186: step 3620, loss = 0.2320, acc = 0.9200 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 01:37:59.146795: step 3640, loss = 0.2223, acc = 0.9280 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 01:38:04.157658: step 3660, loss = 0.2083, acc = 0.9380 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 01:38:09.298352: step 3680, loss = 0.2320, acc = 0.9280 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 01:38:14.283575: step 3700, loss = 0.1983, acc = 0.9440 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 01:38:19.477512: step 3720, loss = 0.2024, acc = 0.9200 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 01:38:24.937362: step 3740, loss = 0.2230, acc = 0.9280 (228.1 examples/sec; 0.281 sec/batch)
2017-05-09 01:38:30.012516: step 3760, loss = 0.2341, acc = 0.9180 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 01:38:35.189479: step 3780, loss = 0.2356, acc = 0.9280 (233.7 examples/sec; 0.274 sec/batch)
2017-05-09 01:38:40.142255: step 3800, loss = 0.2050, acc = 0.9400 (248.1 examples/sec; 0.258 sec/batch)
2017-05-09 01:38:45.066155: step 3820, loss = 0.2020, acc = 0.9260 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 01:38:49.898022: step 3840, loss = 0.2221, acc = 0.9300 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 01:38:55.019014: step 3860, loss = 0.2261, acc = 0.9220 (256.1 examples/sec; 0.250 sec/batch)
2017-05-09 01:39:00.254432: step 3880, loss = 0.2122, acc = 0.9300 (248.3 examples/sec; 0.258 sec/batch)
2017-05-09 01:39:05.287810: step 3900, loss = 0.2239, acc = 0.9260 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 01:39:10.410896: step 3920, loss = 0.1903, acc = 0.9380 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 01:39:15.458973: step 3940, loss = 0.2339, acc = 0.9300 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 01:39:20.495456: step 3960, loss = 0.2141, acc = 0.9360 (244.7 examples/sec; 0.262 sec/batch)
2017-05-09 01:39:25.661452: step 3980, loss = 0.2178, acc = 0.9280 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 01:39:30.712758: step 4000, loss = 0.2344, acc = 0.9260 (272.0 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 01:39:45.136254: step 4000, acc = 0.9435, f1 = 0.9418
[Test] 2017-05-09 01:39:55.379985: step 4000, acc = 0.9312, f1 = 0.9306
[Status] 2017-05-09 01:39:55.380076: step 4000, maxindex = 4000, maxdev = 0.9435, maxtst = 0.9312
2017-05-09 01:40:03.659963: step 4020, loss = 0.2188, acc = 0.9360 (237.2 examples/sec; 0.270 sec/batch)
2017-05-09 01:40:09.535235: step 4040, loss = 0.1795, acc = 0.9600 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 01:40:14.561388: step 4060, loss = 0.1993, acc = 0.9400 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 01:40:19.490756: step 4080, loss = 0.1916, acc = 0.9380 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 01:40:24.494442: step 4100, loss = 0.1874, acc = 0.9340 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 01:40:29.665882: step 4120, loss = 0.2103, acc = 0.9240 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 01:40:34.627632: step 4140, loss = 0.2189, acc = 0.9240 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 01:40:39.656259: step 4160, loss = 0.2116, acc = 0.9280 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 01:40:44.575185: step 4180, loss = 0.2282, acc = 0.9180 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 01:40:49.585412: step 4200, loss = 0.2088, acc = 0.9360 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 01:40:54.759707: step 4220, loss = 0.1625, acc = 0.9540 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 01:40:59.594412: step 4240, loss = 0.2233, acc = 0.9300 (258.3 examples/sec; 0.248 sec/batch)
2017-05-09 01:41:04.597681: step 4260, loss = 0.2243, acc = 0.9160 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 01:41:09.686011: step 4280, loss = 0.2280, acc = 0.9260 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 01:41:14.732398: step 4300, loss = 0.2505, acc = 0.9060 (247.8 examples/sec; 0.258 sec/batch)
2017-05-09 01:41:19.642987: step 4320, loss = 0.2111, acc = 0.9360 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 01:41:24.616139: step 4340, loss = 0.1988, acc = 0.9360 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 01:41:29.537254: step 4360, loss = 0.2234, acc = 0.9260 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 01:41:34.375326: step 4380, loss = 0.2319, acc = 0.9240 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 01:41:39.619994: step 4400, loss = 0.2171, acc = 0.9280 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 01:41:44.642311: step 4420, loss = 0.2048, acc = 0.9380 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 01:41:49.656238: step 4440, loss = 0.1970, acc = 0.9380 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 01:41:54.840497: step 4460, loss = 0.2120, acc = 0.9280 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 01:41:59.767487: step 4480, loss = 0.2298, acc = 0.9220 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 01:42:04.625325: step 4500, loss = 0.2249, acc = 0.9200 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 01:42:09.656946: step 4520, loss = 0.2035, acc = 0.9300 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 01:42:14.441291: step 4540, loss = 0.2422, acc = 0.9020 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 01:42:19.261952: step 4560, loss = 0.1856, acc = 0.9440 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 01:42:24.311427: step 4580, loss = 0.2008, acc = 0.9260 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 01:42:29.174889: step 4600, loss = 0.2075, acc = 0.9440 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 01:42:34.031206: step 4620, loss = 0.2084, acc = 0.9340 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 01:42:39.132749: step 4640, loss = 0.1947, acc = 0.9260 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 01:42:43.939317: step 4660, loss = 0.2135, acc = 0.9320 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 01:42:48.844346: step 4680, loss = 0.2035, acc = 0.9360 (242.8 examples/sec; 0.264 sec/batch)
2017-05-09 01:42:53.952365: step 4700, loss = 0.2133, acc = 0.9200 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 01:42:58.947673: step 4720, loss = 0.2362, acc = 0.9260 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 01:43:03.791644: step 4740, loss = 0.2253, acc = 0.9200 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 01:43:08.710605: step 4760, loss = 0.1843, acc = 0.9400 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 01:43:13.591487: step 4780, loss = 0.2360, acc = 0.9260 (257.9 examples/sec; 0.248 sec/batch)
2017-05-09 01:43:18.432856: step 4800, loss = 0.1954, acc = 0.9320 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 01:43:23.462867: step 4820, loss = 0.2503, acc = 0.9120 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 01:43:28.454111: step 4840, loss = 0.2120, acc = 0.9360 (241.7 examples/sec; 0.265 sec/batch)
2017-05-09 01:43:33.279277: step 4860, loss = 0.2000, acc = 0.9320 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 01:43:38.391627: step 4880, loss = 0.2100, acc = 0.9300 (257.3 examples/sec; 0.249 sec/batch)
2017-05-09 01:43:43.705885: step 4900, loss = 0.2042, acc = 0.9300 (201.7 examples/sec; 0.317 sec/batch)
2017-05-09 01:43:48.611187: step 4920, loss = 0.2060, acc = 0.9380 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 01:43:53.613520: step 4940, loss = 0.2019, acc = 0.9260 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 01:43:58.562607: step 4960, loss = 0.2206, acc = 0.9240 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 01:44:03.370490: step 4980, loss = 0.2187, acc = 0.9220 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 01:44:08.440613: step 5000, loss = 0.1879, acc = 0.9400 (267.2 examples/sec; 0.240 sec/batch)
[Eval] 2017-05-09 01:44:22.665088: step 5000, acc = 0.9470, f1 = 0.9455
[Test] 2017-05-09 01:44:32.267471: step 5000, acc = 0.9348, f1 = 0.9342
[Status] 2017-05-09 01:44:32.267560: step 5000, maxindex = 5000, maxdev = 0.9470, maxtst = 0.9348
2017-05-09 01:44:40.186914: step 5020, loss = 0.1992, acc = 0.9400 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 01:44:45.952891: step 5040, loss = 0.1861, acc = 0.9380 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 01:44:50.847231: step 5060, loss = 0.1712, acc = 0.9540 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 01:44:55.895706: step 5080, loss = 0.2243, acc = 0.9220 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 01:45:00.650200: step 5100, loss = 0.2100, acc = 0.9260 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 01:45:05.544803: step 5120, loss = 0.2058, acc = 0.9320 (252.9 examples/sec; 0.253 sec/batch)
2017-05-09 01:45:10.595123: step 5140, loss = 0.2154, acc = 0.9240 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 01:45:15.524251: step 5160, loss = 0.1924, acc = 0.9420 (243.8 examples/sec; 0.262 sec/batch)
2017-05-09 01:45:20.438253: step 5180, loss = 0.1886, acc = 0.9400 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 01:45:25.515307: step 5200, loss = 0.1665, acc = 0.9500 (247.4 examples/sec; 0.259 sec/batch)
2017-05-09 01:45:30.456489: step 5220, loss = 0.2086, acc = 0.9280 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 01:45:35.387036: step 5240, loss = 0.1985, acc = 0.9400 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 01:45:40.255719: step 5260, loss = 0.1968, acc = 0.9340 (250.4 examples/sec; 0.256 sec/batch)
2017-05-09 01:45:45.197681: step 5280, loss = 0.2023, acc = 0.9260 (256.0 examples/sec; 0.250 sec/batch)
2017-05-09 01:45:50.032645: step 5300, loss = 0.2087, acc = 0.9300 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 01:45:55.100963: step 5320, loss = 0.1808, acc = 0.9460 (251.5 examples/sec; 0.254 sec/batch)
2017-05-09 01:45:59.924888: step 5340, loss = 0.2176, acc = 0.9280 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 01:46:04.925467: step 5360, loss = 0.1782, acc = 0.9480 (255.1 examples/sec; 0.251 sec/batch)
2017-05-09 01:46:09.821178: step 5380, loss = 0.1751, acc = 0.9520 (254.8 examples/sec; 0.251 sec/batch)
2017-05-09 01:46:14.697340: step 5400, loss = 0.2051, acc = 0.9240 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 01:46:19.570978: step 5420, loss = 0.2288, acc = 0.9200 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 01:46:24.530219: step 5440, loss = 0.2079, acc = 0.9260 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 01:46:29.360345: step 5460, loss = 0.1920, acc = 0.9520 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 01:46:34.259286: step 5480, loss = 0.2052, acc = 0.9320 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 01:46:39.116461: step 5500, loss = 0.1719, acc = 0.9520 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 01:46:43.908228: step 5520, loss = 0.1785, acc = 0.9520 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 01:46:48.613642: step 5540, loss = 0.1833, acc = 0.9340 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 01:46:53.568061: step 5560, loss = 0.2081, acc = 0.9260 (208.2 examples/sec; 0.307 sec/batch)
2017-05-09 01:46:58.342807: step 5580, loss = 0.1853, acc = 0.9400 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 01:47:03.001029: step 5600, loss = 0.1756, acc = 0.9420 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 01:47:07.761114: step 5620, loss = 0.1918, acc = 0.9460 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 01:47:12.629337: step 5640, loss = 0.1810, acc = 0.9340 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 01:47:17.459656: step 5660, loss = 0.1887, acc = 0.9400 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 01:47:22.217290: step 5680, loss = 0.1651, acc = 0.9540 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 01:47:27.144691: step 5700, loss = 0.2192, acc = 0.9280 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 01:47:32.040774: step 5720, loss = 0.1796, acc = 0.9320 (246.5 examples/sec; 0.260 sec/batch)
2017-05-09 01:47:36.869014: step 5740, loss = 0.1801, acc = 0.9420 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 01:47:41.778280: step 5760, loss = 0.2200, acc = 0.9240 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 01:47:46.538632: step 5780, loss = 0.1745, acc = 0.9440 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 01:47:51.272716: step 5800, loss = 0.1616, acc = 0.9500 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 01:47:56.197882: step 5820, loss = 0.1954, acc = 0.9400 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 01:48:00.891250: step 5840, loss = 0.2083, acc = 0.9340 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 01:48:05.740014: step 5860, loss = 0.1705, acc = 0.9380 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 01:48:10.641688: step 5880, loss = 0.2093, acc = 0.9340 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 01:48:15.339327: step 5900, loss = 0.2027, acc = 0.9220 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 01:48:20.294854: step 5920, loss = 0.2063, acc = 0.9280 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 01:48:25.163661: step 5940, loss = 0.1986, acc = 0.9240 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 01:48:30.006694: step 5960, loss = 0.2174, acc = 0.9180 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 01:48:34.753785: step 5980, loss = 0.2094, acc = 0.9360 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 01:48:39.731260: step 6000, loss = 0.2266, acc = 0.9120 (228.6 examples/sec; 0.280 sec/batch)
[Eval] 2017-05-09 01:48:53.896536: step 6000, acc = 0.9458, f1 = 0.9444
[Test] 2017-05-09 01:49:03.536568: step 6000, acc = 0.9336, f1 = 0.9330
[Status] 2017-05-09 01:49:03.536638: step 6000, maxindex = 5000, maxdev = 0.9470, maxtst = 0.9348
2017-05-09 01:49:08.303592: step 6020, loss = 0.2027, acc = 0.9320 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 01:49:13.231269: step 6040, loss = 0.1954, acc = 0.9400 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 01:49:18.747754: step 6060, loss = 0.1821, acc = 0.9500 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 01:49:23.857773: step 6080, loss = 0.1961, acc = 0.9440 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 01:49:28.840496: step 6100, loss = 0.2150, acc = 0.9220 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 01:49:33.678116: step 6120, loss = 0.2113, acc = 0.9340 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 01:49:38.402239: step 6140, loss = 0.2142, acc = 0.9140 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 01:49:43.271377: step 6160, loss = 0.1961, acc = 0.9300 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 01:49:47.998322: step 6180, loss = 0.1928, acc = 0.9280 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 01:49:52.697227: step 6200, loss = 0.1956, acc = 0.9360 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 01:49:57.907230: step 6220, loss = 0.1978, acc = 0.9440 (259.6 examples/sec; 0.247 sec/batch)
2017-05-09 01:50:02.586879: step 6240, loss = 0.2247, acc = 0.9300 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 01:50:07.351909: step 6260, loss = 0.1846, acc = 0.9380 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 01:50:12.256750: step 6280, loss = 0.1737, acc = 0.9480 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 01:50:16.931964: step 6300, loss = 0.1857, acc = 0.9400 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 01:50:21.746774: step 6320, loss = 0.2062, acc = 0.9300 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 01:50:26.833646: step 6340, loss = 0.1803, acc = 0.9300 (251.6 examples/sec; 0.254 sec/batch)
2017-05-09 01:50:31.462219: step 6360, loss = 0.1656, acc = 0.9560 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 01:50:36.107563: step 6380, loss = 0.1999, acc = 0.9400 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 01:50:41.048025: step 6400, loss = 0.2069, acc = 0.9380 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 01:50:45.803677: step 6420, loss = 0.2108, acc = 0.9140 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 01:50:50.548354: step 6440, loss = 0.2014, acc = 0.9320 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 01:50:55.580585: step 6460, loss = 0.1972, acc = 0.9400 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 01:51:00.299161: step 6480, loss = 0.1873, acc = 0.9480 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 01:51:05.061864: step 6500, loss = 0.2134, acc = 0.9220 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 01:51:09.912856: step 6520, loss = 0.1596, acc = 0.9500 (277.7 examples/sec; 0.231 sec/batch)
2017-05-09 01:51:14.651095: step 6540, loss = 0.1929, acc = 0.9360 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 01:51:19.366677: step 6560, loss = 0.2005, acc = 0.9440 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 01:51:24.352287: step 6580, loss = 0.1848, acc = 0.9460 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 01:51:29.130294: step 6600, loss = 0.1963, acc = 0.9420 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 01:51:33.985783: step 6620, loss = 0.1942, acc = 0.9400 (256.5 examples/sec; 0.250 sec/batch)
2017-05-09 01:51:38.814851: step 6640, loss = 0.2040, acc = 0.9280 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 01:51:43.736014: step 6660, loss = 0.1611, acc = 0.9540 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 01:51:48.428546: step 6680, loss = 0.1942, acc = 0.9260 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 01:51:53.363778: step 6700, loss = 0.1873, acc = 0.9380 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 01:51:58.310470: step 6720, loss = 0.1889, acc = 0.9320 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 01:52:03.149971: step 6740, loss = 0.1968, acc = 0.9400 (253.5 examples/sec; 0.253 sec/batch)
2017-05-09 01:52:07.928172: step 6760, loss = 0.1850, acc = 0.9360 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 01:52:12.993262: step 6780, loss = 0.1743, acc = 0.9480 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 01:52:17.817587: step 6800, loss = 0.2051, acc = 0.9300 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 01:52:22.624765: step 6820, loss = 0.1471, acc = 0.9620 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 01:52:27.691585: step 6840, loss = 0.1906, acc = 0.9460 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 01:52:32.536728: step 6860, loss = 0.1999, acc = 0.9240 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 01:52:37.340139: step 6880, loss = 0.1704, acc = 0.9480 (257.7 examples/sec; 0.248 sec/batch)
2017-05-09 01:52:42.412828: step 6900, loss = 0.2164, acc = 0.9260 (255.9 examples/sec; 0.250 sec/batch)
2017-05-09 01:52:47.259528: step 6920, loss = 0.1967, acc = 0.9300 (249.3 examples/sec; 0.257 sec/batch)
2017-05-09 01:52:51.981619: step 6940, loss = 0.1934, acc = 0.9380 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 01:52:56.935155: step 6960, loss = 0.2171, acc = 0.9200 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 01:53:01.656106: step 6980, loss = 0.1743, acc = 0.9440 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 01:53:06.433216: step 7000, loss = 0.1775, acc = 0.9420 (261.4 examples/sec; 0.245 sec/batch)
[Eval] 2017-05-09 01:53:20.439331: step 7000, acc = 0.9488, f1 = 0.9474
[Test] 2017-05-09 01:53:30.175648: step 7000, acc = 0.9361, f1 = 0.9355
[Status] 2017-05-09 01:53:30.175747: step 7000, maxindex = 7000, maxdev = 0.9488, maxtst = 0.9361
2017-05-09 01:53:38.097224: step 7020, loss = 0.1861, acc = 0.9440 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 01:53:43.028426: step 7040, loss = 0.2069, acc = 0.9320 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 01:53:48.767789: step 7060, loss = 0.1760, acc = 0.9560 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 01:53:53.549738: step 7080, loss = 0.2005, acc = 0.9380 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 01:53:58.384754: step 7100, loss = 0.1696, acc = 0.9580 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 01:54:03.151435: step 7120, loss = 0.1682, acc = 0.9540 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 01:54:07.933462: step 7140, loss = 0.2111, acc = 0.9240 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 01:54:13.100516: step 7160, loss = 0.1973, acc = 0.9360 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 01:54:17.899438: step 7180, loss = 0.2282, acc = 0.9220 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 01:54:22.583858: step 7200, loss = 0.1650, acc = 0.9560 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 01:54:27.593315: step 7220, loss = 0.1676, acc = 0.9420 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 01:54:32.361746: step 7240, loss = 0.1996, acc = 0.9320 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 01:54:36.983348: step 7260, loss = 0.1600, acc = 0.9560 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 01:54:41.861177: step 7280, loss = 0.1401, acc = 0.9640 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 01:54:46.628199: step 7300, loss = 0.2036, acc = 0.9200 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 01:54:51.375500: step 7320, loss = 0.1831, acc = 0.9420 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 01:54:56.208691: step 7340, loss = 0.2320, acc = 0.9100 (242.7 examples/sec; 0.264 sec/batch)
2017-05-09 01:55:00.956617: step 7360, loss = 0.1755, acc = 0.9440 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 01:55:05.624118: step 7380, loss = 0.2104, acc = 0.9340 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 01:55:10.310449: step 7400, loss = 0.2032, acc = 0.9340 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 01:55:15.193784: step 7420, loss = 0.1942, acc = 0.9380 (244.1 examples/sec; 0.262 sec/batch)
2017-05-09 01:55:19.996186: step 7440, loss = 0.1919, acc = 0.9340 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 01:55:24.683019: step 7460, loss = 0.1923, acc = 0.9460 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 01:55:29.561146: step 7480, loss = 0.1972, acc = 0.9300 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 01:55:34.286515: step 7500, loss = 0.2023, acc = 0.9260 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 01:55:39.086947: step 7520, loss = 0.1792, acc = 0.9440 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 01:55:43.892837: step 7540, loss = 0.2042, acc = 0.9340 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 01:55:48.525415: step 7560, loss = 0.2301, acc = 0.9160 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 01:55:53.334368: step 7580, loss = 0.1942, acc = 0.9480 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 01:55:58.252881: step 7600, loss = 0.1633, acc = 0.9540 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 01:56:03.056115: step 7620, loss = 0.1724, acc = 0.9500 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 01:56:07.724848: step 7640, loss = 0.1953, acc = 0.9260 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 01:56:12.582134: step 7660, loss = 0.2172, acc = 0.9100 (243.5 examples/sec; 0.263 sec/batch)
2017-05-09 01:56:17.269846: step 7680, loss = 0.1749, acc = 0.9560 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 01:56:21.974806: step 7700, loss = 0.1977, acc = 0.9280 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 01:56:26.571413: step 7720, loss = 0.1903, acc = 0.9380 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 01:56:31.787013: step 7740, loss = 0.1889, acc = 0.9360 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 01:56:36.474255: step 7760, loss = 0.1894, acc = 0.9380 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 01:56:41.246830: step 7780, loss = 0.1715, acc = 0.9580 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 01:56:46.015928: step 7800, loss = 0.1648, acc = 0.9520 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 01:56:50.758495: step 7820, loss = 0.1916, acc = 0.9340 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 01:56:55.506114: step 7840, loss = 0.1771, acc = 0.9460 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 01:57:00.357350: step 7860, loss = 0.2096, acc = 0.9240 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 01:57:05.048510: step 7880, loss = 0.1557, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 01:57:09.817932: step 7900, loss = 0.2019, acc = 0.9300 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 01:57:14.731363: step 7920, loss = 0.1587, acc = 0.9560 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 01:57:19.422841: step 7940, loss = 0.1774, acc = 0.9300 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 01:57:24.080011: step 7960, loss = 0.2154, acc = 0.9220 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 01:57:28.942758: step 7980, loss = 0.1991, acc = 0.9280 (234.0 examples/sec; 0.274 sec/batch)
2017-05-09 01:57:33.639583: step 8000, loss = 0.1654, acc = 0.9600 (270.5 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 01:57:48.061395: step 8000, acc = 0.9509, f1 = 0.9494
[Test] 2017-05-09 01:57:58.126495: step 8000, acc = 0.9387, f1 = 0.9382
[Status] 2017-05-09 01:57:58.126596: step 8000, maxindex = 8000, maxdev = 0.9509, maxtst = 0.9387
2017-05-09 01:58:06.340977: step 8020, loss = 0.1743, acc = 0.9420 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 01:58:11.154994: step 8040, loss = 0.1859, acc = 0.9400 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 01:58:16.734641: step 8060, loss = 0.1591, acc = 0.9500 (158.6 examples/sec; 0.404 sec/batch)
2017-05-09 01:58:21.552501: step 8080, loss = 0.1967, acc = 0.9460 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 01:58:26.201372: step 8100, loss = 0.2124, acc = 0.9300 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 01:58:31.019161: step 8120, loss = 0.1709, acc = 0.9500 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 01:58:35.830878: step 8140, loss = 0.1776, acc = 0.9340 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 01:58:40.502282: step 8160, loss = 0.1956, acc = 0.9180 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 01:58:45.292559: step 8180, loss = 0.1812, acc = 0.9400 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 01:58:50.087686: step 8200, loss = 0.2067, acc = 0.9360 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 01:58:54.830248: step 8220, loss = 0.2041, acc = 0.9320 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 01:58:59.662635: step 8240, loss = 0.1950, acc = 0.9300 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 01:59:04.318199: step 8260, loss = 0.1652, acc = 0.9480 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 01:59:09.026920: step 8280, loss = 0.1952, acc = 0.9340 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 01:59:13.789013: step 8300, loss = 0.1927, acc = 0.9420 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 01:59:18.763539: step 8320, loss = 0.1806, acc = 0.9420 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 01:59:23.380185: step 8340, loss = 0.1741, acc = 0.9440 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 01:59:28.093027: step 8360, loss = 0.1777, acc = 0.9360 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 01:59:33.300174: step 8380, loss = 0.1752, acc = 0.9460 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 01:59:38.038617: step 8400, loss = 0.2124, acc = 0.9280 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 01:59:42.776526: step 8420, loss = 0.2114, acc = 0.9300 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 01:59:47.604313: step 8440, loss = 0.2046, acc = 0.9300 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 01:59:52.258713: step 8460, loss = 0.1986, acc = 0.9280 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 01:59:57.007338: step 8480, loss = 0.1785, acc = 0.9460 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 02:00:01.984213: step 8500, loss = 0.1593, acc = 0.9520 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 02:00:06.731799: step 8520, loss = 0.1973, acc = 0.9420 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 02:00:11.418804: step 8540, loss = 0.1594, acc = 0.9500 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 02:00:16.250771: step 8560, loss = 0.1727, acc = 0.9480 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 02:00:21.139852: step 8580, loss = 0.2019, acc = 0.9440 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 02:00:25.919243: step 8600, loss = 0.2089, acc = 0.9300 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 02:00:30.742221: step 8620, loss = 0.2070, acc = 0.9320 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 02:00:35.417847: step 8640, loss = 0.1909, acc = 0.9360 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:00:40.133665: step 8660, loss = 0.1623, acc = 0.9380 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 02:00:44.975308: step 8680, loss = 0.1772, acc = 0.9480 (222.8 examples/sec; 0.287 sec/batch)
2017-05-09 02:00:49.591448: step 8700, loss = 0.1856, acc = 0.9260 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 02:00:54.370719: step 8720, loss = 0.2061, acc = 0.9340 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 02:00:59.037849: step 8740, loss = 0.1746, acc = 0.9420 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 02:01:03.895010: step 8760, loss = 0.1935, acc = 0.9320 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 02:01:08.609038: step 8780, loss = 0.1842, acc = 0.9380 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:01:13.347556: step 8800, loss = 0.1789, acc = 0.9420 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 02:01:18.353687: step 8820, loss = 0.1988, acc = 0.9320 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 02:01:23.033909: step 8840, loss = 0.1571, acc = 0.9420 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:01:27.857231: step 8860, loss = 0.1339, acc = 0.9600 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 02:01:32.832438: step 8880, loss = 0.1964, acc = 0.9320 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 02:01:37.591249: step 8900, loss = 0.1910, acc = 0.9300 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 02:01:42.451517: step 8920, loss = 0.1606, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 02:01:47.541305: step 8940, loss = 0.1880, acc = 0.9320 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 02:01:52.382227: step 8960, loss = 0.1954, acc = 0.9220 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 02:01:57.083539: step 8980, loss = 0.1916, acc = 0.9360 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:02:01.940896: step 9000, loss = 0.1787, acc = 0.9360 (265.4 examples/sec; 0.241 sec/batch)
[Eval] 2017-05-09 02:02:15.797439: step 9000, acc = 0.9480, f1 = 0.9463
[Test] 2017-05-09 02:02:25.182286: step 9000, acc = 0.9362, f1 = 0.9356
[Status] 2017-05-09 02:02:25.182363: step 9000, maxindex = 8000, maxdev = 0.9509, maxtst = 0.9387
2017-05-09 02:02:29.990298: step 9020, loss = 0.1771, acc = 0.9440 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 02:02:34.639509: step 9040, loss = 0.1657, acc = 0.9480 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 02:02:39.244924: step 9060, loss = 0.1818, acc = 0.9420 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 02:02:45.148167: step 9080, loss = 0.1943, acc = 0.9320 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 02:02:49.905839: step 9100, loss = 0.1946, acc = 0.9360 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 02:02:54.663185: step 9120, loss = 0.1750, acc = 0.9480 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 02:02:59.466770: step 9140, loss = 0.2056, acc = 0.9280 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 02:03:04.455487: step 9160, loss = 0.1796, acc = 0.9340 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 02:03:09.178787: step 9180, loss = 0.2078, acc = 0.9260 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 02:03:13.951879: step 9200, loss = 0.2052, acc = 0.9240 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 02:03:19.053684: step 9220, loss = 0.1882, acc = 0.9300 (248.7 examples/sec; 0.257 sec/batch)
2017-05-09 02:03:23.726481: step 9240, loss = 0.1696, acc = 0.9520 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 02:03:28.610286: step 9260, loss = 0.2003, acc = 0.9360 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 02:03:33.600680: step 9280, loss = 0.1758, acc = 0.9480 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 02:03:38.314667: step 9300, loss = 0.1571, acc = 0.9560 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 02:03:42.963678: step 9320, loss = 0.1865, acc = 0.9460 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 02:03:47.801031: step 9340, loss = 0.2009, acc = 0.9320 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 02:03:52.510516: step 9360, loss = 0.1428, acc = 0.9600 (247.5 examples/sec; 0.259 sec/batch)
2017-05-09 02:03:57.326805: step 9380, loss = 0.1508, acc = 0.9540 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 02:04:02.082123: step 9400, loss = 0.1564, acc = 0.9440 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 02:04:06.804480: step 9420, loss = 0.1601, acc = 0.9500 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 02:04:11.590835: step 9440, loss = 0.1749, acc = 0.9480 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 02:04:16.530949: step 9460, loss = 0.1504, acc = 0.9480 (223.0 examples/sec; 0.287 sec/batch)
2017-05-09 02:04:21.257034: step 9480, loss = 0.1672, acc = 0.9400 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 02:04:26.269670: step 9500, loss = 0.1608, acc = 0.9420 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 02:04:30.844574: step 9520, loss = 0.1586, acc = 0.9540 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 02:04:35.719670: step 9540, loss = 0.1547, acc = 0.9560 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 02:04:40.541488: step 9560, loss = 0.1878, acc = 0.9340 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 02:04:45.268062: step 9580, loss = 0.1836, acc = 0.9400 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 02:04:50.181248: step 9600, loss = 0.1878, acc = 0.9280 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 02:04:54.971867: step 9620, loss = 0.1754, acc = 0.9320 (255.1 examples/sec; 0.251 sec/batch)
2017-05-09 02:04:59.665006: step 9640, loss = 0.1748, acc = 0.9440 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:05:04.610755: step 9660, loss = 0.2027, acc = 0.9360 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 02:05:09.186967: step 9680, loss = 0.1705, acc = 0.9520 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 02:05:14.005027: step 9700, loss = 0.1986, acc = 0.9520 (251.5 examples/sec; 0.255 sec/batch)
2017-05-09 02:05:18.864206: step 9720, loss = 0.1962, acc = 0.9420 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 02:05:23.680949: step 9740, loss = 0.1624, acc = 0.9520 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 02:05:28.520227: step 9760, loss = 0.1958, acc = 0.9360 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 02:05:33.333633: step 9780, loss = 0.1635, acc = 0.9440 (236.6 examples/sec; 0.271 sec/batch)
2017-05-09 02:05:38.123354: step 9800, loss = 0.1678, acc = 0.9480 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 02:05:42.872777: step 9820, loss = 0.2131, acc = 0.9260 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 02:05:47.625753: step 9840, loss = 0.1530, acc = 0.9620 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 02:05:52.593200: step 9860, loss = 0.1555, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 02:05:57.495570: step 9880, loss = 0.1687, acc = 0.9500 (254.1 examples/sec; 0.252 sec/batch)
2017-05-09 02:06:02.434233: step 9900, loss = 0.2000, acc = 0.9360 (256.7 examples/sec; 0.249 sec/batch)
2017-05-09 02:06:07.341666: step 9920, loss = 0.2060, acc = 0.9260 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 02:06:12.056568: step 9940, loss = 0.2000, acc = 0.9360 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 02:06:16.745678: step 9960, loss = 0.1667, acc = 0.9480 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 02:06:21.746918: step 9980, loss = 0.2100, acc = 0.9300 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 02:06:27.082403: step 10000, loss = 0.1644, acc = 0.9500 (273.1 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 02:06:41.083898: step 10000, acc = 0.9523, f1 = 0.9509
[Test] 2017-05-09 02:06:50.879031: step 10000, acc = 0.9409, f1 = 0.9404
[Status] 2017-05-09 02:06:50.879138: step 10000, maxindex = 10000, maxdev = 0.9523, maxtst = 0.9409
2017-05-09 02:06:59.116840: step 10020, loss = 0.1873, acc = 0.9260 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 02:07:04.140575: step 10040, loss = 0.1610, acc = 0.9420 (233.5 examples/sec; 0.274 sec/batch)
2017-05-09 02:07:08.805816: step 10060, loss = 0.1757, acc = 0.9360 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 02:07:14.297009: step 10080, loss = 0.1501, acc = 0.9460 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 02:07:19.162619: step 10100, loss = 0.1410, acc = 0.9680 (237.7 examples/sec; 0.269 sec/batch)
2017-05-09 02:07:23.821337: step 10120, loss = 0.1976, acc = 0.9380 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:07:28.609591: step 10140, loss = 0.1786, acc = 0.9500 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 02:07:33.411053: step 10160, loss = 0.1743, acc = 0.9440 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 02:07:38.163671: step 10180, loss = 0.2049, acc = 0.9260 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 02:07:42.980594: step 10200, loss = 0.1671, acc = 0.9500 (240.8 examples/sec; 0.266 sec/batch)
2017-05-09 02:07:47.614921: step 10220, loss = 0.1694, acc = 0.9380 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 02:07:52.335328: step 10240, loss = 0.2084, acc = 0.9280 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 02:07:57.033961: step 10260, loss = 0.1703, acc = 0.9440 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 02:08:01.739298: step 10280, loss = 0.1828, acc = 0.9420 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 02:08:06.514549: step 10300, loss = 0.1756, acc = 0.9480 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 02:08:11.258168: step 10320, loss = 0.1726, acc = 0.9500 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 02:08:15.965662: step 10340, loss = 0.1750, acc = 0.9480 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 02:08:20.773116: step 10360, loss = 0.1719, acc = 0.9580 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:08:25.397097: step 10380, loss = 0.1698, acc = 0.9460 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 02:08:29.976283: step 10400, loss = 0.1736, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 02:08:34.828202: step 10420, loss = 0.2256, acc = 0.9160 (225.5 examples/sec; 0.284 sec/batch)
2017-05-09 02:08:39.509471: step 10440, loss = 0.1692, acc = 0.9460 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 02:08:44.148253: step 10460, loss = 0.1974, acc = 0.9200 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:08:48.978352: step 10480, loss = 0.1764, acc = 0.9400 (255.1 examples/sec; 0.251 sec/batch)
2017-05-09 02:08:53.752493: step 10500, loss = 0.1953, acc = 0.9320 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 02:08:58.437028: step 10520, loss = 0.1460, acc = 0.9540 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 02:09:03.140714: step 10540, loss = 0.1586, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 02:09:08.012505: step 10560, loss = 0.2010, acc = 0.9320 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 02:09:12.771943: step 10580, loss = 0.1711, acc = 0.9440 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 02:09:17.451606: step 10600, loss = 0.1962, acc = 0.9420 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 02:09:22.240254: step 10620, loss = 0.1663, acc = 0.9520 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 02:09:26.993224: step 10640, loss = 0.1921, acc = 0.9340 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:09:31.723080: step 10660, loss = 0.2005, acc = 0.9360 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 02:09:36.513554: step 10680, loss = 0.1681, acc = 0.9360 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:09:41.292169: step 10700, loss = 0.1666, acc = 0.9440 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 02:09:45.934131: step 10720, loss = 0.1605, acc = 0.9520 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 02:09:50.723240: step 10740, loss = 0.1772, acc = 0.9500 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 02:09:55.479888: step 10760, loss = 0.1518, acc = 0.9480 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 02:10:00.252156: step 10780, loss = 0.2083, acc = 0.9400 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 02:10:05.195988: step 10800, loss = 0.1607, acc = 0.9380 (213.1 examples/sec; 0.300 sec/batch)
2017-05-09 02:10:10.050979: step 10820, loss = 0.2007, acc = 0.9340 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 02:10:14.685293: step 10840, loss = 0.1679, acc = 0.9480 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 02:10:19.592587: step 10860, loss = 0.2496, acc = 0.8980 (240.5 examples/sec; 0.266 sec/batch)
2017-05-09 02:10:24.660941: step 10880, loss = 0.1799, acc = 0.9440 (258.6 examples/sec; 0.248 sec/batch)
2017-05-09 02:10:29.418124: step 10900, loss = 0.1685, acc = 0.9340 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 02:10:34.051922: step 10920, loss = 0.1933, acc = 0.9260 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 02:10:38.937690: step 10940, loss = 0.1961, acc = 0.9220 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 02:10:43.653127: step 10960, loss = 0.1778, acc = 0.9480 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 02:10:48.322399: step 10980, loss = 0.1719, acc = 0.9440 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 02:10:53.196524: step 11000, loss = 0.1774, acc = 0.9380 (247.1 examples/sec; 0.259 sec/batch)
[Eval] 2017-05-09 02:11:07.416885: step 11000, acc = 0.9525, f1 = 0.9512
[Test] 2017-05-09 02:11:17.017498: step 11000, acc = 0.9417, f1 = 0.9412
[Status] 2017-05-09 02:11:17.017626: step 11000, maxindex = 11000, maxdev = 0.9525, maxtst = 0.9417
2017-05-09 02:11:24.996483: step 11020, loss = 0.1830, acc = 0.9460 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 02:11:29.637726: step 11040, loss = 0.1544, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 02:11:34.360786: step 11060, loss = 0.1891, acc = 0.9240 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 02:11:39.148195: step 11080, loss = 0.1822, acc = 0.9240 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 02:11:45.193291: step 11100, loss = 0.1963, acc = 0.9380 (265.0 examples/sec; 0.241 sec/batch)
2017-05-09 02:11:50.151499: step 11120, loss = 0.1918, acc = 0.9360 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 02:11:54.986883: step 11140, loss = 0.1593, acc = 0.9460 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 02:11:59.648812: step 11160, loss = 0.1942, acc = 0.9360 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 02:12:04.447306: step 11180, loss = 0.1658, acc = 0.9480 (238.0 examples/sec; 0.269 sec/batch)
2017-05-09 02:12:09.355312: step 11200, loss = 0.1921, acc = 0.9280 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 02:12:14.070767: step 11220, loss = 0.1621, acc = 0.9400 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 02:12:18.817508: step 11240, loss = 0.1664, acc = 0.9420 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:12:23.635742: step 11260, loss = 0.1879, acc = 0.9320 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 02:12:28.327054: step 11280, loss = 0.1855, acc = 0.9400 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 02:12:33.285365: step 11300, loss = 0.1881, acc = 0.9400 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:12:38.031045: step 11320, loss = 0.1696, acc = 0.9360 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 02:12:42.832585: step 11340, loss = 0.1880, acc = 0.9380 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 02:12:47.429504: step 11360, loss = 0.2032, acc = 0.9400 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 02:12:52.035009: step 11380, loss = 0.1872, acc = 0.9240 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 02:12:56.762101: step 11400, loss = 0.1857, acc = 0.9340 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 02:13:01.525451: step 11420, loss = 0.1912, acc = 0.9340 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 02:13:06.324276: step 11440, loss = 0.1841, acc = 0.9420 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 02:13:11.054379: step 11460, loss = 0.1581, acc = 0.9560 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 02:13:15.705778: step 11480, loss = 0.1923, acc = 0.9420 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 02:13:20.417109: step 11500, loss = 0.1596, acc = 0.9520 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 02:13:25.242838: step 11520, loss = 0.1788, acc = 0.9400 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 02:13:30.015599: step 11540, loss = 0.1611, acc = 0.9500 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 02:13:34.750874: step 11560, loss = 0.1984, acc = 0.9360 (234.2 examples/sec; 0.273 sec/batch)
2017-05-09 02:13:39.400735: step 11580, loss = 0.1764, acc = 0.9320 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 02:13:44.032737: step 11600, loss = 0.1627, acc = 0.9460 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 02:13:48.737489: step 11620, loss = 0.1834, acc = 0.9360 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 02:13:53.519195: step 11640, loss = 0.1646, acc = 0.9520 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 02:13:58.150611: step 11660, loss = 0.1734, acc = 0.9380 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 02:14:02.693084: step 11680, loss = 0.1598, acc = 0.9460 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 02:14:07.592046: step 11700, loss = 0.1585, acc = 0.9440 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:14:12.168996: step 11720, loss = 0.1850, acc = 0.9200 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 02:14:16.829186: step 11740, loss = 0.1772, acc = 0.9520 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 02:14:21.675954: step 11760, loss = 0.1599, acc = 0.9440 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 02:14:26.189669: step 11780, loss = 0.1888, acc = 0.9300 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 02:14:30.931196: step 11800, loss = 0.1699, acc = 0.9480 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 02:14:35.746506: step 11820, loss = 0.1728, acc = 0.9440 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:14:40.572982: step 11840, loss = 0.1911, acc = 0.9420 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 02:14:45.269616: step 11860, loss = 0.1836, acc = 0.9380 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 02:14:50.026458: step 11880, loss = 0.1984, acc = 0.9260 (244.5 examples/sec; 0.262 sec/batch)
2017-05-09 02:14:55.059688: step 11900, loss = 0.1627, acc = 0.9540 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 02:14:59.685535: step 11920, loss = 0.2208, acc = 0.9240 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:15:04.373810: step 11940, loss = 0.1628, acc = 0.9400 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 02:15:09.082469: step 11960, loss = 0.1724, acc = 0.9360 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 02:15:13.777290: step 11980, loss = 0.1475, acc = 0.9600 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 02:15:18.550425: step 12000, loss = 0.1747, acc = 0.9420 (272.2 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 02:15:32.839475: step 12000, acc = 0.9534, f1 = 0.9520
[Test] 2017-05-09 02:15:42.526201: step 12000, acc = 0.9422, f1 = 0.9417
[Status] 2017-05-09 02:15:42.526302: step 12000, maxindex = 12000, maxdev = 0.9534, maxtst = 0.9422
2017-05-09 02:15:50.606595: step 12020, loss = 0.1783, acc = 0.9320 (235.6 examples/sec; 0.272 sec/batch)
2017-05-09 02:15:55.333238: step 12040, loss = 0.1733, acc = 0.9320 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 02:16:00.004360: step 12060, loss = 0.1793, acc = 0.9340 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 02:16:04.673363: step 12080, loss = 0.1847, acc = 0.9320 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 02:16:10.117589: step 12100, loss = 0.1862, acc = 0.9240 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 02:16:14.668091: step 12120, loss = 0.1601, acc = 0.9460 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 02:16:19.469574: step 12140, loss = 0.1635, acc = 0.9520 (255.4 examples/sec; 0.251 sec/batch)
2017-05-09 02:16:24.273076: step 12160, loss = 0.1992, acc = 0.9280 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 02:16:28.953192: step 12180, loss = 0.1666, acc = 0.9480 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 02:16:33.992425: step 12200, loss = 0.1885, acc = 0.9420 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 02:16:38.782584: step 12220, loss = 0.1665, acc = 0.9540 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 02:16:43.479741: step 12240, loss = 0.1640, acc = 0.9440 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:16:48.113582: step 12260, loss = 0.1496, acc = 0.9560 (268.3 examples/sec; 0.238 sec/batch)
2017-05-09 02:16:52.885617: step 12280, loss = 0.1593, acc = 0.9400 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 02:16:57.562789: step 12300, loss = 0.1837, acc = 0.9460 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 02:17:02.289389: step 12320, loss = 0.1745, acc = 0.9360 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 02:17:07.084401: step 12340, loss = 0.1754, acc = 0.9440 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 02:17:11.827365: step 12360, loss = 0.1644, acc = 0.9400 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 02:17:16.573072: step 12380, loss = 0.1741, acc = 0.9440 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 02:17:21.563139: step 12400, loss = 0.1663, acc = 0.9620 (218.8 examples/sec; 0.293 sec/batch)
2017-05-09 02:17:26.165219: step 12420, loss = 0.1617, acc = 0.9460 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 02:17:30.821341: step 12440, loss = 0.1856, acc = 0.9440 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 02:17:35.744514: step 12460, loss = 0.1933, acc = 0.9240 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 02:17:40.628844: step 12480, loss = 0.1637, acc = 0.9460 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 02:17:45.390046: step 12500, loss = 0.1848, acc = 0.9400 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:17:50.068510: step 12520, loss = 0.1493, acc = 0.9540 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 02:17:54.850919: step 12540, loss = 0.1794, acc = 0.9420 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 02:17:59.729964: step 12560, loss = 0.1883, acc = 0.9380 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 02:18:04.457070: step 12580, loss = 0.1845, acc = 0.9340 (259.0 examples/sec; 0.247 sec/batch)
2017-05-09 02:18:09.146828: step 12600, loss = 0.1396, acc = 0.9600 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:18:13.907789: step 12620, loss = 0.1687, acc = 0.9580 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 02:18:18.514389: step 12640, loss = 0.1756, acc = 0.9360 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 02:18:23.303843: step 12660, loss = 0.1891, acc = 0.9360 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 02:18:28.134215: step 12680, loss = 0.1984, acc = 0.9340 (259.6 examples/sec; 0.247 sec/batch)
2017-05-09 02:18:32.778623: step 12700, loss = 0.1460, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 02:18:37.653254: step 12720, loss = 0.1557, acc = 0.9500 (236.3 examples/sec; 0.271 sec/batch)
2017-05-09 02:18:42.409981: step 12740, loss = 0.1543, acc = 0.9600 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 02:18:47.052339: step 12760, loss = 0.1619, acc = 0.9480 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 02:18:51.827428: step 12780, loss = 0.1515, acc = 0.9460 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 02:18:56.656401: step 12800, loss = 0.1751, acc = 0.9440 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 02:19:01.304236: step 12820, loss = 0.2248, acc = 0.9200 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 02:19:06.037545: step 12840, loss = 0.1564, acc = 0.9480 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 02:19:11.222466: step 12860, loss = 0.1672, acc = 0.9300 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 02:19:16.272871: step 12880, loss = 0.2010, acc = 0.9240 (193.1 examples/sec; 0.331 sec/batch)
2017-05-09 02:19:20.987197: step 12900, loss = 0.1841, acc = 0.9380 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 02:19:25.793798: step 12920, loss = 0.2208, acc = 0.9240 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 02:19:30.452669: step 12940, loss = 0.1686, acc = 0.9440 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 02:19:35.206730: step 12960, loss = 0.1769, acc = 0.9360 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 02:19:39.809098: step 12980, loss = 0.1433, acc = 0.9580 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 02:19:44.504624: step 13000, loss = 0.1461, acc = 0.9500 (276.4 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 02:19:58.483808: step 13000, acc = 0.9535, f1 = 0.9521
[Test] 2017-05-09 02:20:07.814600: step 13000, acc = 0.9424, f1 = 0.9419
[Status] 2017-05-09 02:20:07.814688: step 13000, maxindex = 13000, maxdev = 0.9535, maxtst = 0.9424
2017-05-09 02:20:16.134100: step 13020, loss = 0.1999, acc = 0.9180 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 02:20:20.735263: step 13040, loss = 0.1720, acc = 0.9360 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 02:20:25.504016: step 13060, loss = 0.1706, acc = 0.9500 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 02:20:30.177240: step 13080, loss = 0.1927, acc = 0.9340 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 02:20:36.045412: step 13100, loss = 0.1828, acc = 0.9280 (133.7 examples/sec; 0.479 sec/batch)
2017-05-09 02:20:40.762849: step 13120, loss = 0.1705, acc = 0.9420 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 02:20:45.560503: step 13140, loss = 0.1498, acc = 0.9540 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 02:20:50.246341: step 13160, loss = 0.2103, acc = 0.9200 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 02:20:55.476002: step 13180, loss = 0.1740, acc = 0.9380 (222.8 examples/sec; 0.287 sec/batch)
2017-05-09 02:21:00.222153: step 13200, loss = 0.1622, acc = 0.9540 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 02:21:04.988379: step 13220, loss = 0.1614, acc = 0.9520 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 02:21:09.831070: step 13240, loss = 0.1658, acc = 0.9380 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 02:21:14.639168: step 13260, loss = 0.1711, acc = 0.9420 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 02:21:19.337466: step 13280, loss = 0.1766, acc = 0.9360 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 02:21:24.185919: step 13300, loss = 0.1704, acc = 0.9500 (233.7 examples/sec; 0.274 sec/batch)
2017-05-09 02:21:28.890185: step 13320, loss = 0.1484, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 02:21:33.583964: step 13340, loss = 0.1632, acc = 0.9420 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 02:21:38.327755: step 13360, loss = 0.1654, acc = 0.9420 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 02:21:42.997421: step 13380, loss = 0.1700, acc = 0.9400 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 02:21:47.690926: step 13400, loss = 0.1810, acc = 0.9340 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 02:21:52.341305: step 13420, loss = 0.1649, acc = 0.9480 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 02:21:57.099570: step 13440, loss = 0.1825, acc = 0.9500 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 02:22:01.727749: step 13460, loss = 0.1414, acc = 0.9580 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 02:22:06.277065: step 13480, loss = 0.1528, acc = 0.9600 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 02:22:11.091766: step 13500, loss = 0.2102, acc = 0.9320 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 02:22:16.269777: step 13520, loss = 0.1983, acc = 0.9340 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 02:22:20.862363: step 13540, loss = 0.2204, acc = 0.9060 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 02:22:25.692112: step 13560, loss = 0.1907, acc = 0.9280 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 02:22:30.385942: step 13580, loss = 0.1806, acc = 0.9380 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 02:22:34.994485: step 13600, loss = 0.1853, acc = 0.9280 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 02:22:39.628927: step 13620, loss = 0.1759, acc = 0.9460 (251.5 examples/sec; 0.255 sec/batch)
2017-05-09 02:22:44.286581: step 13640, loss = 0.1725, acc = 0.9500 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 02:22:48.860968: step 13660, loss = 0.2169, acc = 0.9160 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 02:22:53.537293: step 13680, loss = 0.1731, acc = 0.9420 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 02:22:58.151568: step 13700, loss = 0.2076, acc = 0.9280 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 02:23:02.964541: step 13720, loss = 0.1606, acc = 0.9480 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 02:23:07.671248: step 13740, loss = 0.1736, acc = 0.9540 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:23:12.283831: step 13760, loss = 0.1844, acc = 0.9300 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 02:23:17.018697: step 13780, loss = 0.1711, acc = 0.9360 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 02:23:21.658231: step 13800, loss = 0.1712, acc = 0.9480 (260.7 examples/sec; 0.245 sec/batch)
2017-05-09 02:23:26.274630: step 13820, loss = 0.1681, acc = 0.9500 (292.9 examples/sec; 0.219 sec/batch)
2017-05-09 02:23:30.759824: step 13840, loss = 0.1719, acc = 0.9480 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 02:23:35.797036: step 13860, loss = 0.2100, acc = 0.9280 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 02:23:40.439220: step 13880, loss = 0.1983, acc = 0.9340 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 02:23:44.999071: step 13900, loss = 0.1263, acc = 0.9600 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 02:23:49.927733: step 13920, loss = 0.1606, acc = 0.9480 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 02:23:54.513583: step 13940, loss = 0.1416, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 02:23:59.133260: step 13960, loss = 0.1587, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:24:03.959314: step 13980, loss = 0.1806, acc = 0.9440 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 02:24:08.597728: step 14000, loss = 0.1903, acc = 0.9300 (268.4 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 02:24:22.837930: step 14000, acc = 0.9547, f1 = 0.9533
[Test] 2017-05-09 02:24:32.879688: step 14000, acc = 0.9445, f1 = 0.9440
[Status] 2017-05-09 02:24:32.879760: step 14000, maxindex = 14000, maxdev = 0.9547, maxtst = 0.9445
2017-05-09 02:24:40.731183: step 14020, loss = 0.2089, acc = 0.9220 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 02:24:45.435083: step 14040, loss = 0.2054, acc = 0.9440 (241.5 examples/sec; 0.265 sec/batch)
2017-05-09 02:24:50.072119: step 14060, loss = 0.1715, acc = 0.9360 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 02:24:54.683806: step 14080, loss = 0.1937, acc = 0.9300 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 02:24:59.383150: step 14100, loss = 0.1774, acc = 0.9380 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 02:25:04.949548: step 14120, loss = 0.1526, acc = 0.9440 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 02:25:09.626892: step 14140, loss = 0.1268, acc = 0.9660 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:25:14.270076: step 14160, loss = 0.1725, acc = 0.9460 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 02:25:19.107948: step 14180, loss = 0.1973, acc = 0.9180 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 02:25:23.791374: step 14200, loss = 0.1621, acc = 0.9460 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 02:25:28.492372: step 14220, loss = 0.2124, acc = 0.9140 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 02:25:33.128995: step 14240, loss = 0.1711, acc = 0.9420 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 02:25:37.752629: step 14260, loss = 0.1617, acc = 0.9540 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 02:25:42.605576: step 14280, loss = 0.1592, acc = 0.9460 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 02:25:47.319250: step 14300, loss = 0.1543, acc = 0.9420 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 02:25:52.007561: step 14320, loss = 0.1998, acc = 0.9280 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 02:25:56.617221: step 14340, loss = 0.1717, acc = 0.9380 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 02:26:01.399821: step 14360, loss = 0.1837, acc = 0.9320 (246.1 examples/sec; 0.260 sec/batch)
2017-05-09 02:26:06.268824: step 14380, loss = 0.1937, acc = 0.9360 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 02:26:10.928762: step 14400, loss = 0.1654, acc = 0.9420 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 02:26:15.674775: step 14420, loss = 0.1492, acc = 0.9560 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 02:26:20.590619: step 14440, loss = 0.1809, acc = 0.9540 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 02:26:25.154529: step 14460, loss = 0.1490, acc = 0.9420 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 02:26:29.675480: step 14480, loss = 0.1567, acc = 0.9560 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 02:26:34.599273: step 14500, loss = 0.1461, acc = 0.9580 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 02:26:39.265391: step 14520, loss = 0.1580, acc = 0.9440 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:26:43.901169: step 14540, loss = 0.1684, acc = 0.9420 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 02:26:48.801891: step 14560, loss = 0.1519, acc = 0.9320 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 02:26:53.310389: step 14580, loss = 0.1564, acc = 0.9380 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 02:26:58.033715: step 14600, loss = 0.1528, acc = 0.9580 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 02:27:03.049886: step 14620, loss = 0.1678, acc = 0.9420 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 02:27:07.676690: step 14640, loss = 0.1249, acc = 0.9700 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 02:27:12.312542: step 14660, loss = 0.1705, acc = 0.9480 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 02:27:17.157863: step 14680, loss = 0.1710, acc = 0.9380 (227.5 examples/sec; 0.281 sec/batch)
2017-05-09 02:27:21.689472: step 14700, loss = 0.1864, acc = 0.9320 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 02:27:26.411914: step 14720, loss = 0.1405, acc = 0.9580 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 02:27:30.980644: step 14740, loss = 0.1604, acc = 0.9540 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 02:27:35.893426: step 14760, loss = 0.1671, acc = 0.9360 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 02:27:40.552341: step 14780, loss = 0.1390, acc = 0.9680 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 02:27:45.248495: step 14800, loss = 0.1496, acc = 0.9420 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 02:27:50.129320: step 14820, loss = 0.1508, acc = 0.9480 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 02:27:54.813047: step 14840, loss = 0.1854, acc = 0.9480 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 02:27:59.463436: step 14860, loss = 0.1864, acc = 0.9260 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:28:04.315933: step 14880, loss = 0.1639, acc = 0.9480 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 02:28:08.933608: step 14900, loss = 0.1587, acc = 0.9480 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 02:28:14.135371: step 14920, loss = 0.1551, acc = 0.9440 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 02:28:18.898311: step 14940, loss = 0.1506, acc = 0.9480 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 02:28:23.683085: step 14960, loss = 0.1723, acc = 0.9340 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 02:28:28.346970: step 14980, loss = 0.1897, acc = 0.9340 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 02:28:33.161292: step 15000, loss = 0.1684, acc = 0.9360 (283.2 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 02:28:47.540124: step 15000, acc = 0.9548, f1 = 0.9534
[Test] 2017-05-09 02:28:56.785962: step 15000, acc = 0.9446, f1 = 0.9441
[Status] 2017-05-09 02:28:56.786055: step 15000, maxindex = 15000, maxdev = 0.9548, maxtst = 0.9446
2017-05-09 02:29:04.983474: step 15020, loss = 0.1430, acc = 0.9520 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 02:29:09.687146: step 15040, loss = 0.1355, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 02:29:14.493619: step 15060, loss = 0.1512, acc = 0.9420 (246.0 examples/sec; 0.260 sec/batch)
2017-05-09 02:29:19.080417: step 15080, loss = 0.1675, acc = 0.9540 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 02:29:23.775844: step 15100, loss = 0.1490, acc = 0.9600 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 02:29:29.371253: step 15120, loss = 0.1657, acc = 0.9540 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 02:29:34.252556: step 15140, loss = 0.1593, acc = 0.9520 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 02:29:38.957641: step 15160, loss = 0.1498, acc = 0.9480 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 02:29:43.589301: step 15180, loss = 0.1577, acc = 0.9480 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 02:29:48.512679: step 15200, loss = 0.1993, acc = 0.9320 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 02:29:53.310002: step 15220, loss = 0.1523, acc = 0.9520 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 02:29:58.108251: step 15240, loss = 0.1529, acc = 0.9520 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 02:30:02.888670: step 15260, loss = 0.1634, acc = 0.9320 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 02:30:07.453703: step 15280, loss = 0.1767, acc = 0.9340 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 02:30:12.005702: step 15300, loss = 0.1565, acc = 0.9560 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:30:16.832476: step 15320, loss = 0.1770, acc = 0.9420 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 02:30:21.416061: step 15340, loss = 0.1881, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 02:30:26.071484: step 15360, loss = 0.1505, acc = 0.9480 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 02:30:30.852648: step 15380, loss = 0.1605, acc = 0.9440 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 02:30:35.507663: step 15400, loss = 0.1393, acc = 0.9600 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 02:30:40.184051: step 15420, loss = 0.1512, acc = 0.9400 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 02:30:45.377409: step 15440, loss = 0.1673, acc = 0.9280 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 02:30:49.913056: step 15460, loss = 0.1715, acc = 0.9380 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 02:30:54.506659: step 15480, loss = 0.1697, acc = 0.9460 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 02:30:59.317106: step 15500, loss = 0.1634, acc = 0.9440 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 02:31:03.851283: step 15520, loss = 0.1525, acc = 0.9520 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:31:08.567294: step 15540, loss = 0.1828, acc = 0.9460 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 02:31:13.200443: step 15560, loss = 0.1551, acc = 0.9500 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 02:31:18.017304: step 15580, loss = 0.1489, acc = 0.9520 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 02:31:22.751123: step 15600, loss = 0.1575, acc = 0.9500 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 02:31:27.410184: step 15620, loss = 0.1389, acc = 0.9580 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 02:31:32.130023: step 15640, loss = 0.1980, acc = 0.9280 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 02:31:36.710216: step 15660, loss = 0.1743, acc = 0.9360 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 02:31:41.382969: step 15680, loss = 0.1648, acc = 0.9380 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 02:31:46.184580: step 15700, loss = 0.1670, acc = 0.9440 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 02:31:50.820349: step 15720, loss = 0.1785, acc = 0.9420 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 02:31:55.469359: step 15740, loss = 0.1611, acc = 0.9480 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 02:32:00.225125: step 15760, loss = 0.1451, acc = 0.9600 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 02:32:04.811578: step 15780, loss = 0.1833, acc = 0.9300 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 02:32:09.451435: step 15800, loss = 0.1575, acc = 0.9540 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 02:32:14.327851: step 15820, loss = 0.1654, acc = 0.9500 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 02:32:19.114739: step 15840, loss = 0.1420, acc = 0.9540 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 02:32:23.790485: step 15860, loss = 0.1505, acc = 0.9540 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 02:32:28.612080: step 15880, loss = 0.1457, acc = 0.9620 (250.6 examples/sec; 0.255 sec/batch)
2017-05-09 02:32:33.352227: step 15900, loss = 0.1504, acc = 0.9520 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 02:32:38.093322: step 15920, loss = 0.1563, acc = 0.9380 (254.8 examples/sec; 0.251 sec/batch)
2017-05-09 02:32:42.966624: step 15940, loss = 0.1764, acc = 0.9340 (242.4 examples/sec; 0.264 sec/batch)
2017-05-09 02:32:47.514840: step 15960, loss = 0.1621, acc = 0.9500 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 02:32:52.066716: step 15980, loss = 0.1860, acc = 0.9340 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 02:32:56.699833: step 16000, loss = 0.1578, acc = 0.9480 (267.2 examples/sec; 0.240 sec/batch)
[Eval] 2017-05-09 02:33:11.434397: step 16000, acc = 0.9554, f1 = 0.9542
[Test] 2017-05-09 02:33:20.664691: step 16000, acc = 0.9448, f1 = 0.9443
[Status] 2017-05-09 02:33:20.664763: step 16000, maxindex = 16000, maxdev = 0.9554, maxtst = 0.9448
2017-05-09 02:33:28.854498: step 16020, loss = 0.1947, acc = 0.9220 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 02:33:33.466365: step 16040, loss = 0.1619, acc = 0.9440 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 02:33:38.097541: step 16060, loss = 0.1504, acc = 0.9560 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 02:33:43.152580: step 16080, loss = 0.1574, acc = 0.9440 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 02:33:47.836816: step 16100, loss = 0.1468, acc = 0.9540 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 02:33:52.429779: step 16120, loss = 0.1785, acc = 0.9400 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 02:33:57.943874: step 16140, loss = 0.1601, acc = 0.9380 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 02:34:02.471814: step 16160, loss = 0.1315, acc = 0.9560 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 02:34:07.178036: step 16180, loss = 0.1608, acc = 0.9440 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 02:34:12.106560: step 16200, loss = 0.1620, acc = 0.9360 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 02:34:16.763327: step 16220, loss = 0.1741, acc = 0.9480 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 02:34:21.499929: step 16240, loss = 0.1411, acc = 0.9600 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 02:34:26.459613: step 16260, loss = 0.1634, acc = 0.9520 (221.3 examples/sec; 0.289 sec/batch)
2017-05-09 02:34:31.099800: step 16280, loss = 0.1761, acc = 0.9360 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 02:34:35.693669: step 16300, loss = 0.1664, acc = 0.9440 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 02:34:40.558713: step 16320, loss = 0.1642, acc = 0.9420 (241.1 examples/sec; 0.265 sec/batch)
2017-05-09 02:34:45.288023: step 16340, loss = 0.1434, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 02:34:50.057372: step 16360, loss = 0.1650, acc = 0.9460 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 02:34:54.824078: step 16380, loss = 0.1589, acc = 0.9460 (303.2 examples/sec; 0.211 sec/batch)
2017-05-09 02:34:59.563145: step 16400, loss = 0.1661, acc = 0.9400 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 02:35:04.120719: step 16420, loss = 0.1817, acc = 0.9320 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 02:35:08.726407: step 16440, loss = 0.1614, acc = 0.9460 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 02:35:13.442648: step 16460, loss = 0.1700, acc = 0.9460 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 02:35:18.058423: step 16480, loss = 0.1445, acc = 0.9540 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 02:35:22.654841: step 16500, loss = 0.2004, acc = 0.9220 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 02:35:27.448089: step 16520, loss = 0.1390, acc = 0.9540 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 02:35:32.262627: step 16540, loss = 0.1970, acc = 0.9340 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 02:35:36.969469: step 16560, loss = 0.1690, acc = 0.9420 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 02:35:41.801445: step 16580, loss = 0.1515, acc = 0.9580 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 02:35:46.479898: step 16600, loss = 0.1825, acc = 0.9360 (254.8 examples/sec; 0.251 sec/batch)
2017-05-09 02:35:51.101912: step 16620, loss = 0.1806, acc = 0.9400 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 02:35:55.930728: step 16640, loss = 0.1914, acc = 0.9320 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 02:36:00.568111: step 16660, loss = 0.1561, acc = 0.9560 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 02:36:05.207635: step 16680, loss = 0.1397, acc = 0.9520 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 02:36:09.990974: step 16700, loss = 0.1308, acc = 0.9620 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 02:36:14.863650: step 16720, loss = 0.1677, acc = 0.9560 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 02:36:19.577582: step 16740, loss = 0.1578, acc = 0.9580 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 02:36:24.384898: step 16760, loss = 0.1511, acc = 0.9520 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:36:29.334099: step 16780, loss = 0.1983, acc = 0.9260 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 02:36:33.939243: step 16800, loss = 0.1391, acc = 0.9580 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 02:36:38.536799: step 16820, loss = 0.2049, acc = 0.9380 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 02:36:43.215192: step 16840, loss = 0.1494, acc = 0.9600 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 02:36:47.801641: step 16860, loss = 0.1622, acc = 0.9500 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 02:36:52.449459: step 16880, loss = 0.1669, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:36:57.225633: step 16900, loss = 0.1523, acc = 0.9500 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 02:37:01.868320: step 16920, loss = 0.1579, acc = 0.9560 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 02:37:06.503480: step 16940, loss = 0.1686, acc = 0.9360 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 02:37:11.296695: step 16960, loss = 0.1571, acc = 0.9480 (228.9 examples/sec; 0.280 sec/batch)
2017-05-09 02:37:15.998396: step 16980, loss = 0.1764, acc = 0.9420 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 02:37:21.028628: step 17000, loss = 0.1492, acc = 0.9440 (277.1 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 02:37:35.365385: step 17000, acc = 0.9539, f1 = 0.9524
[Test] 2017-05-09 02:37:45.214184: step 17000, acc = 0.9438, f1 = 0.9433
[Status] 2017-05-09 02:37:45.214306: step 17000, maxindex = 16000, maxdev = 0.9554, maxtst = 0.9448
2017-05-09 02:37:49.809438: step 17020, loss = 0.1598, acc = 0.9520 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 02:37:54.636977: step 17040, loss = 0.1920, acc = 0.9380 (232.2 examples/sec; 0.276 sec/batch)
2017-05-09 02:37:59.330367: step 17060, loss = 0.1513, acc = 0.9560 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 02:38:03.925149: step 17080, loss = 0.1312, acc = 0.9620 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 02:38:08.519131: step 17100, loss = 0.1486, acc = 0.9520 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 02:38:13.312227: step 17120, loss = 0.1881, acc = 0.9320 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 02:38:18.983783: step 17140, loss = 0.1634, acc = 0.9560 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 02:38:23.625046: step 17160, loss = 0.1620, acc = 0.9480 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 02:38:28.525191: step 17180, loss = 0.1503, acc = 0.9520 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 02:38:33.645614: step 17200, loss = 0.1962, acc = 0.9260 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 02:38:38.290998: step 17220, loss = 0.1516, acc = 0.9520 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 02:38:43.018701: step 17240, loss = 0.1423, acc = 0.9580 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 02:38:47.692949: step 17260, loss = 0.1505, acc = 0.9540 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 02:38:52.341250: step 17280, loss = 0.2085, acc = 0.9220 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:38:57.156020: step 17300, loss = 0.1804, acc = 0.9260 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 02:39:01.998326: step 17320, loss = 0.1498, acc = 0.9520 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 02:39:06.707076: step 17340, loss = 0.1642, acc = 0.9420 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 02:39:11.740640: step 17360, loss = 0.1718, acc = 0.9340 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 02:39:16.376666: step 17380, loss = 0.1791, acc = 0.9400 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 02:39:21.086785: step 17400, loss = 0.1395, acc = 0.9620 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 02:39:26.066930: step 17420, loss = 0.1444, acc = 0.9400 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:39:30.757310: step 17440, loss = 0.1260, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 02:39:35.393995: step 17460, loss = 0.1836, acc = 0.9420 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 02:39:40.445063: step 17480, loss = 0.1544, acc = 0.9540 (231.8 examples/sec; 0.276 sec/batch)
2017-05-09 02:39:45.053614: step 17500, loss = 0.1377, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:39:49.659176: step 17520, loss = 0.1565, acc = 0.9260 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 02:39:54.486532: step 17540, loss = 0.1486, acc = 0.9480 (247.5 examples/sec; 0.259 sec/batch)
2017-05-09 02:39:59.059371: step 17560, loss = 0.1600, acc = 0.9540 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 02:40:03.735699: step 17580, loss = 0.1408, acc = 0.9680 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 02:40:08.764791: step 17600, loss = 0.1579, acc = 0.9500 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 02:40:13.486563: step 17620, loss = 0.1583, acc = 0.9460 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 02:40:18.046115: step 17640, loss = 0.1643, acc = 0.9440 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 02:40:22.674849: step 17660, loss = 0.1738, acc = 0.9460 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 02:40:27.473884: step 17680, loss = 0.1501, acc = 0.9480 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 02:40:32.227218: step 17700, loss = 0.1661, acc = 0.9480 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 02:40:36.849835: step 17720, loss = 0.1647, acc = 0.9340 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 02:40:41.628769: step 17740, loss = 0.1603, acc = 0.9580 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 02:40:46.351561: step 17760, loss = 0.1682, acc = 0.9400 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:40:51.067639: step 17780, loss = 0.1403, acc = 0.9620 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 02:40:55.772031: step 17800, loss = 0.1598, acc = 0.9540 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 02:41:00.390742: step 17820, loss = 0.1484, acc = 0.9600 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 02:41:04.989274: step 17840, loss = 0.1438, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 02:41:09.844013: step 17860, loss = 0.1435, acc = 0.9500 (233.1 examples/sec; 0.275 sec/batch)
2017-05-09 02:41:14.463146: step 17880, loss = 0.1797, acc = 0.9400 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 02:41:19.274429: step 17900, loss = 0.1488, acc = 0.9560 (257.0 examples/sec; 0.249 sec/batch)
2017-05-09 02:41:24.114848: step 17920, loss = 0.1739, acc = 0.9340 (234.7 examples/sec; 0.273 sec/batch)
2017-05-09 02:41:28.812533: step 17940, loss = 0.1578, acc = 0.9540 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 02:41:33.522071: step 17960, loss = 0.1990, acc = 0.9300 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 02:41:38.287151: step 17980, loss = 0.1354, acc = 0.9640 (245.8 examples/sec; 0.260 sec/batch)
2017-05-09 02:41:42.927481: step 18000, loss = 0.1596, acc = 0.9460 (267.2 examples/sec; 0.240 sec/batch)
[Eval] 2017-05-09 02:41:56.885405: step 18000, acc = 0.9554, f1 = 0.9541
[Test] 2017-05-09 02:42:06.443164: step 18000, acc = 0.9450, f1 = 0.9445
[Status] 2017-05-09 02:42:06.443226: step 18000, maxindex = 16000, maxdev = 0.9554, maxtst = 0.9448
2017-05-09 02:42:11.102878: step 18020, loss = 0.1443, acc = 0.9600 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:42:15.671357: step 18040, loss = 0.1746, acc = 0.9460 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 02:42:20.307923: step 18060, loss = 0.1620, acc = 0.9420 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 02:42:25.173070: step 18080, loss = 0.1356, acc = 0.9500 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 02:42:29.836592: step 18100, loss = 0.1366, acc = 0.9500 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 02:42:34.502124: step 18120, loss = 0.1837, acc = 0.9520 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 02:42:39.914640: step 18140, loss = 0.1437, acc = 0.9600 (157.5 examples/sec; 0.406 sec/batch)
2017-05-09 02:42:44.458853: step 18160, loss = 0.1980, acc = 0.9280 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:42:49.052980: step 18180, loss = 0.1845, acc = 0.9320 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 02:42:53.799919: step 18200, loss = 0.1632, acc = 0.9460 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 02:42:58.379009: step 18220, loss = 0.1714, acc = 0.9440 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 02:43:03.009444: step 18240, loss = 0.1455, acc = 0.9600 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 02:43:07.567462: step 18260, loss = 0.1485, acc = 0.9560 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 02:43:12.595800: step 18280, loss = 0.1582, acc = 0.9440 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 02:43:17.294826: step 18300, loss = 0.1503, acc = 0.9580 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 02:43:21.988021: step 18320, loss = 0.1463, acc = 0.9420 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 02:43:26.844643: step 18340, loss = 0.1610, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 02:43:31.421998: step 18360, loss = 0.1237, acc = 0.9740 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 02:43:36.177520: step 18380, loss = 0.1498, acc = 0.9620 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 02:43:41.104414: step 18400, loss = 0.1307, acc = 0.9640 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 02:43:45.809744: step 18420, loss = 0.1565, acc = 0.9500 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 02:43:50.477065: step 18440, loss = 0.1881, acc = 0.9420 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 02:43:55.363134: step 18460, loss = 0.1718, acc = 0.9360 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 02:44:00.173565: step 18480, loss = 0.1391, acc = 0.9520 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:44:04.729200: step 18500, loss = 0.1479, acc = 0.9560 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:44:09.591481: step 18520, loss = 0.1167, acc = 0.9680 (237.4 examples/sec; 0.270 sec/batch)
2017-05-09 02:44:14.228314: step 18540, loss = 0.1488, acc = 0.9440 (256.5 examples/sec; 0.250 sec/batch)
2017-05-09 02:44:18.916148: step 18560, loss = 0.1661, acc = 0.9460 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 02:44:23.642247: step 18580, loss = 0.1959, acc = 0.9300 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 02:44:28.608654: step 18600, loss = 0.1465, acc = 0.9620 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 02:44:33.238735: step 18620, loss = 0.1534, acc = 0.9420 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:44:37.915029: step 18640, loss = 0.1331, acc = 0.9540 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 02:44:42.688898: step 18660, loss = 0.1418, acc = 0.9480 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 02:44:47.267708: step 18680, loss = 0.1491, acc = 0.9520 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 02:44:51.811694: step 18700, loss = 0.1774, acc = 0.9420 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 02:44:56.538819: step 18720, loss = 0.1588, acc = 0.9480 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:45:01.297734: step 18740, loss = 0.1661, acc = 0.9440 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 02:45:05.812277: step 18760, loss = 0.1903, acc = 0.9340 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 02:45:10.754909: step 18780, loss = 0.1570, acc = 0.9480 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 02:45:15.494479: step 18800, loss = 0.1514, acc = 0.9500 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 02:45:20.121091: step 18820, loss = 0.1747, acc = 0.9400 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 02:45:24.642374: step 18840, loss = 0.1323, acc = 0.9620 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 02:45:29.520326: step 18860, loss = 0.1696, acc = 0.9520 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 02:45:34.146987: step 18880, loss = 0.1745, acc = 0.9480 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 02:45:38.823783: step 18900, loss = 0.1526, acc = 0.9460 (254.7 examples/sec; 0.251 sec/batch)
2017-05-09 02:45:43.574943: step 18920, loss = 0.1834, acc = 0.9200 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 02:45:48.657376: step 18940, loss = 0.1466, acc = 0.9640 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 02:45:53.357004: step 18960, loss = 0.1694, acc = 0.9500 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 02:45:58.109156: step 18980, loss = 0.1710, acc = 0.9440 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:46:02.691474: step 19000, loss = 0.1545, acc = 0.9480 (285.6 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 02:46:16.742770: step 19000, acc = 0.9541, f1 = 0.9528
[Test] 2017-05-09 02:46:26.398423: step 19000, acc = 0.9439, f1 = 0.9435
[Status] 2017-05-09 02:46:26.398504: step 19000, maxindex = 16000, maxdev = 0.9554, maxtst = 0.9448
2017-05-09 02:46:31.010941: step 19020, loss = 0.1865, acc = 0.9400 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 02:46:35.539082: step 19040, loss = 0.1530, acc = 0.9480 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 02:46:40.345959: step 19060, loss = 0.1786, acc = 0.9440 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:46:45.027766: step 19080, loss = 0.1470, acc = 0.9480 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 02:46:49.579660: step 19100, loss = 0.1617, acc = 0.9440 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:46:54.371956: step 19120, loss = 0.1602, acc = 0.9360 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 02:46:58.799037: step 19140, loss = 0.1699, acc = 0.9500 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 02:47:04.659715: step 19160, loss = 0.1423, acc = 0.9540 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 02:47:09.335154: step 19180, loss = 0.1456, acc = 0.9540 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 02:47:13.867608: step 19200, loss = 0.1565, acc = 0.9460 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 02:47:18.448407: step 19220, loss = 0.1578, acc = 0.9440 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 02:47:22.979419: step 19240, loss = 0.1715, acc = 0.9320 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 02:47:28.054983: step 19260, loss = 0.1749, acc = 0.9520 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 02:47:32.613840: step 19280, loss = 0.1550, acc = 0.9460 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 02:47:37.160897: step 19300, loss = 0.1375, acc = 0.9640 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 02:47:42.070621: step 19320, loss = 0.1447, acc = 0.9440 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 02:47:46.697917: step 19340, loss = 0.1489, acc = 0.9480 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 02:47:51.323794: step 19360, loss = 0.1567, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 02:47:56.169880: step 19380, loss = 0.1575, acc = 0.9440 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 02:48:00.854657: step 19400, loss = 0.1396, acc = 0.9560 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 02:48:05.570043: step 19420, loss = 0.1338, acc = 0.9500 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 02:48:10.493018: step 19440, loss = 0.1637, acc = 0.9460 (219.3 examples/sec; 0.292 sec/batch)
2017-05-09 02:48:15.049218: step 19460, loss = 0.1179, acc = 0.9700 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 02:48:19.635858: step 19480, loss = 0.1748, acc = 0.9460 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 02:48:24.237968: step 19500, loss = 0.1452, acc = 0.9540 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 02:48:29.097584: step 19520, loss = 0.1361, acc = 0.9540 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 02:48:33.901431: step 19540, loss = 0.1560, acc = 0.9440 (267.2 examples/sec; 0.239 sec/batch)
2017-05-09 02:48:38.501428: step 19560, loss = 0.1489, acc = 0.9440 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 02:48:43.434403: step 19580, loss = 0.1437, acc = 0.9440 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 02:48:48.132025: step 19600, loss = 0.1367, acc = 0.9640 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 02:48:52.791191: step 19620, loss = 0.1547, acc = 0.9560 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:48:57.700892: step 19640, loss = 0.1568, acc = 0.9500 (243.0 examples/sec; 0.263 sec/batch)
2017-05-09 02:49:02.475014: step 19660, loss = 0.1742, acc = 0.9360 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 02:49:07.088598: step 19680, loss = 0.1481, acc = 0.9560 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:49:11.826320: step 19700, loss = 0.1593, acc = 0.9540 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 02:49:16.473316: step 19720, loss = 0.1816, acc = 0.9380 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 02:49:21.107282: step 19740, loss = 0.1407, acc = 0.9540 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 02:49:25.955204: step 19760, loss = 0.1809, acc = 0.9360 (214.3 examples/sec; 0.299 sec/batch)
2017-05-09 02:49:30.527839: step 19780, loss = 0.1314, acc = 0.9600 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 02:49:35.072314: step 19800, loss = 0.1350, acc = 0.9580 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 02:49:39.827560: step 19820, loss = 0.1510, acc = 0.9420 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 02:49:44.489529: step 19840, loss = 0.1504, acc = 0.9420 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 02:49:49.094669: step 19860, loss = 0.1555, acc = 0.9560 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 02:49:53.833044: step 19880, loss = 0.1388, acc = 0.9460 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 02:49:58.603091: step 19900, loss = 0.1536, acc = 0.9460 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 02:50:03.363549: step 19920, loss = 0.1525, acc = 0.9600 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 02:50:08.027389: step 19940, loss = 0.1290, acc = 0.9560 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 02:50:12.714179: step 19960, loss = 0.1521, acc = 0.9480 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 02:50:17.238730: step 19980, loss = 0.1731, acc = 0.9480 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 02:50:21.860345: step 20000, loss = 0.1415, acc = 0.9580 (272.3 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 02:50:35.869126: step 20000, acc = 0.9540, f1 = 0.9527
[Test] 2017-05-09 02:50:45.575719: step 20000, acc = 0.9439, f1 = 0.9434
[Status] 2017-05-09 02:50:45.575778: step 20000, maxindex = 16000, maxdev = 0.9554, maxtst = 0.9448
2017-05-09 02:50:50.215052: step 20020, loss = 0.1417, acc = 0.9660 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 02:50:55.114538: step 20040, loss = 0.1677, acc = 0.9440 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:50:59.974893: step 20060, loss = 0.1501, acc = 0.9480 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 02:51:04.544767: step 20080, loss = 0.1497, acc = 0.9560 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:51:09.171425: step 20100, loss = 0.1334, acc = 0.9720 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 02:51:14.036085: step 20120, loss = 0.1508, acc = 0.9520 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 02:51:18.671549: step 20140, loss = 0.1700, acc = 0.9400 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 02:51:24.076529: step 20160, loss = 0.1723, acc = 0.9400 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 02:51:28.788408: step 20180, loss = 0.1558, acc = 0.9420 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 02:51:33.380687: step 20200, loss = 0.1638, acc = 0.9440 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 02:51:38.111032: step 20220, loss = 0.1597, acc = 0.9580 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:51:43.020819: step 20240, loss = 0.1349, acc = 0.9580 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:51:47.633054: step 20260, loss = 0.1648, acc = 0.9540 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:51:52.285447: step 20280, loss = 0.1549, acc = 0.9520 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 02:51:57.160197: step 20300, loss = 0.1341, acc = 0.9620 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 02:52:01.993686: step 20320, loss = 0.1577, acc = 0.9420 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 02:52:06.575280: step 20340, loss = 0.1329, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 02:52:11.386497: step 20360, loss = 0.1364, acc = 0.9560 (235.6 examples/sec; 0.272 sec/batch)
2017-05-09 02:52:15.975824: step 20380, loss = 0.1620, acc = 0.9400 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 02:52:20.452812: step 20400, loss = 0.1393, acc = 0.9540 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 02:52:25.087140: step 20420, loss = 0.1634, acc = 0.9440 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 02:52:29.633363: step 20440, loss = 0.1586, acc = 0.9520 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 02:52:34.276969: step 20460, loss = 0.1251, acc = 0.9620 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 02:52:38.869183: step 20480, loss = 0.1328, acc = 0.9660 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 02:52:43.823866: step 20500, loss = 0.1450, acc = 0.9460 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 02:52:48.490073: step 20520, loss = 0.1933, acc = 0.9300 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 02:52:53.146719: step 20540, loss = 0.1564, acc = 0.9520 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 02:52:57.943145: step 20560, loss = 0.1445, acc = 0.9460 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:53:02.589462: step 20580, loss = 0.1680, acc = 0.9440 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 02:53:07.295985: step 20600, loss = 0.1107, acc = 0.9760 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 02:53:12.207490: step 20620, loss = 0.1672, acc = 0.9500 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 02:53:16.836250: step 20640, loss = 0.1479, acc = 0.9500 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 02:53:21.429438: step 20660, loss = 0.1996, acc = 0.9320 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 02:53:26.290525: step 20680, loss = 0.1483, acc = 0.9540 (297.9 examples/sec; 0.215 sec/batch)
2017-05-09 02:53:30.816406: step 20700, loss = 0.1503, acc = 0.9440 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 02:53:35.364283: step 20720, loss = 0.1427, acc = 0.9540 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 02:53:40.206766: step 20740, loss = 0.1414, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 02:53:44.770777: step 20760, loss = 0.1556, acc = 0.9480 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 02:53:49.362871: step 20780, loss = 0.1713, acc = 0.9420 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 02:53:54.204051: step 20800, loss = 0.1662, acc = 0.9420 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 02:53:58.750793: step 20820, loss = 0.1393, acc = 0.9520 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 02:54:03.416216: step 20840, loss = 0.1903, acc = 0.9360 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 02:54:08.084037: step 20860, loss = 0.1517, acc = 0.9420 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 02:54:12.980800: step 20880, loss = 0.1577, acc = 0.9460 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 02:54:17.576586: step 20900, loss = 0.1395, acc = 0.9560 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 02:54:22.278724: step 20920, loss = 0.1770, acc = 0.9400 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 02:54:27.132865: step 20940, loss = 0.1957, acc = 0.9280 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 02:54:31.809993: step 20960, loss = 0.1388, acc = 0.9620 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 02:54:36.335292: step 20980, loss = 0.1685, acc = 0.9400 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 02:54:41.193721: step 21000, loss = 0.1419, acc = 0.9600 (268.7 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 02:54:55.178054: step 21000, acc = 0.9562, f1 = 0.9548
[Test] 2017-05-09 02:55:04.567833: step 21000, acc = 0.9468, f1 = 0.9463
[Status] 2017-05-09 02:55:04.567931: step 21000, maxindex = 21000, maxdev = 0.9562, maxtst = 0.9468
2017-05-09 02:55:12.345197: step 21020, loss = 0.1371, acc = 0.9640 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 02:55:17.055807: step 21040, loss = 0.1677, acc = 0.9380 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 02:55:21.740898: step 21060, loss = 0.1813, acc = 0.9440 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 02:55:26.621468: step 21080, loss = 0.1629, acc = 0.9420 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 02:55:31.172082: step 21100, loss = 0.1603, acc = 0.9520 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 02:55:35.788096: step 21120, loss = 0.1414, acc = 0.9480 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 02:55:40.440737: step 21140, loss = 0.1582, acc = 0.9520 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 02:55:45.110034: step 21160, loss = 0.1432, acc = 0.9600 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 02:55:50.772965: step 21180, loss = 0.1735, acc = 0.9440 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 02:55:55.609357: step 21200, loss = 0.1868, acc = 0.9400 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 02:56:00.230390: step 21220, loss = 0.1524, acc = 0.9600 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 02:56:04.895341: step 21240, loss = 0.1545, acc = 0.9560 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 02:56:09.677027: step 21260, loss = 0.1072, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 02:56:14.292458: step 21280, loss = 0.1443, acc = 0.9520 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 02:56:19.014358: step 21300, loss = 0.1473, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 02:56:23.815137: step 21320, loss = 0.1315, acc = 0.9580 (249.4 examples/sec; 0.257 sec/batch)
2017-05-09 02:56:28.382222: step 21340, loss = 0.1403, acc = 0.9520 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 02:56:32.962137: step 21360, loss = 0.1454, acc = 0.9560 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 02:56:37.595713: step 21380, loss = 0.1391, acc = 0.9500 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 02:56:42.250553: step 21400, loss = 0.1431, acc = 0.9520 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 02:56:46.910649: step 21420, loss = 0.1431, acc = 0.9560 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 02:56:51.476455: step 21440, loss = 0.1611, acc = 0.9440 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 02:56:56.235928: step 21460, loss = 0.1449, acc = 0.9380 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 02:57:00.754910: step 21480, loss = 0.2116, acc = 0.9340 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 02:57:05.300987: step 21500, loss = 0.1348, acc = 0.9480 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:57:10.044794: step 21520, loss = 0.1443, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 02:57:14.547591: step 21540, loss = 0.1671, acc = 0.9380 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 02:57:19.121594: step 21560, loss = 0.1652, acc = 0.9540 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 02:57:23.858545: step 21580, loss = 0.1440, acc = 0.9620 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 02:57:28.376787: step 21600, loss = 0.1415, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:57:33.198887: step 21620, loss = 0.1381, acc = 0.9580 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 02:57:37.811109: step 21640, loss = 0.1703, acc = 0.9440 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 02:57:42.440186: step 21660, loss = 0.1775, acc = 0.9440 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 02:57:47.366643: step 21680, loss = 0.1455, acc = 0.9520 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 02:57:52.019309: step 21700, loss = 0.1440, acc = 0.9640 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 02:57:56.773377: step 21720, loss = 0.1310, acc = 0.9640 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 02:58:01.389211: step 21740, loss = 0.1499, acc = 0.9540 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 02:58:06.086331: step 21760, loss = 0.1187, acc = 0.9600 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 02:58:10.934372: step 21780, loss = 0.1463, acc = 0.9500 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 02:58:15.505112: step 21800, loss = 0.1609, acc = 0.9460 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 02:58:20.228390: step 21820, loss = 0.1789, acc = 0.9400 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 02:58:25.150262: step 21840, loss = 0.1796, acc = 0.9500 (252.9 examples/sec; 0.253 sec/batch)
2017-05-09 02:58:29.852957: step 21860, loss = 0.2001, acc = 0.9320 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 02:58:34.504781: step 21880, loss = 0.1396, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 02:58:39.288287: step 21900, loss = 0.1590, acc = 0.9440 (238.1 examples/sec; 0.269 sec/batch)
2017-05-09 02:58:43.851114: step 21920, loss = 0.2063, acc = 0.9340 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 02:58:48.399847: step 21940, loss = 0.1552, acc = 0.9420 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 02:58:53.194645: step 21960, loss = 0.1778, acc = 0.9460 (245.2 examples/sec; 0.261 sec/batch)
2017-05-09 02:58:57.981050: step 21980, loss = 0.1523, acc = 0.9540 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 02:59:02.683148: step 22000, loss = 0.1644, acc = 0.9420 (276.5 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 02:59:16.774886: step 22000, acc = 0.9566, f1 = 0.9552
[Test] 2017-05-09 02:59:26.499927: step 22000, acc = 0.9476, f1 = 0.9471
[Status] 2017-05-09 02:59:26.500014: step 22000, maxindex = 22000, maxdev = 0.9566, maxtst = 0.9476
2017-05-09 02:59:34.411712: step 22020, loss = 0.1374, acc = 0.9560 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 02:59:39.246440: step 22040, loss = 0.1476, acc = 0.9480 (229.0 examples/sec; 0.280 sec/batch)
2017-05-09 02:59:44.113401: step 22060, loss = 0.1630, acc = 0.9400 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 02:59:48.657673: step 22080, loss = 0.1508, acc = 0.9460 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 02:59:53.256740: step 22100, loss = 0.1346, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 02:59:58.150596: step 22120, loss = 0.1363, acc = 0.9600 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 03:00:02.927788: step 22140, loss = 0.1383, acc = 0.9580 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 03:00:07.644758: step 22160, loss = 0.1738, acc = 0.9340 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 03:00:13.076255: step 22180, loss = 0.1632, acc = 0.9540 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 03:00:17.692642: step 22200, loss = 0.1422, acc = 0.9620 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 03:00:22.269815: step 22220, loss = 0.1766, acc = 0.9420 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 03:00:27.070250: step 22240, loss = 0.1347, acc = 0.9560 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 03:00:31.701855: step 22260, loss = 0.1418, acc = 0.9580 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 03:00:36.298529: step 22280, loss = 0.1648, acc = 0.9420 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 03:00:41.200052: step 22300, loss = 0.1462, acc = 0.9520 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 03:00:46.047905: step 22320, loss = 0.1406, acc = 0.9580 (254.0 examples/sec; 0.252 sec/batch)
2017-05-09 03:00:50.518209: step 22340, loss = 0.1394, acc = 0.9480 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 03:00:55.302310: step 22360, loss = 0.1325, acc = 0.9660 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 03:00:59.944062: step 22380, loss = 0.1471, acc = 0.9640 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:01:04.718168: step 22400, loss = 0.1544, acc = 0.9400 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 03:01:09.364938: step 22420, loss = 0.1578, acc = 0.9580 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 03:01:13.984759: step 22440, loss = 0.1352, acc = 0.9540 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:01:18.673733: step 22460, loss = 0.1362, acc = 0.9560 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 03:01:23.431294: step 22480, loss = 0.1361, acc = 0.9580 (255.2 examples/sec; 0.251 sec/batch)
2017-05-09 03:01:28.306168: step 22500, loss = 0.1422, acc = 0.9520 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 03:01:32.966095: step 22520, loss = 0.1511, acc = 0.9520 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:01:37.656372: step 22540, loss = 0.1188, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:01:42.865476: step 22560, loss = 0.1492, acc = 0.9400 (203.3 examples/sec; 0.315 sec/batch)
2017-05-09 03:01:47.547315: step 22580, loss = 0.1469, acc = 0.9520 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 03:01:52.273091: step 22600, loss = 0.1660, acc = 0.9400 (262.0 examples/sec; 0.244 sec/batch)
2017-05-09 03:01:57.196466: step 22620, loss = 0.1579, acc = 0.9380 (254.8 examples/sec; 0.251 sec/batch)
2017-05-09 03:02:01.803446: step 22640, loss = 0.1234, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 03:02:06.455306: step 22660, loss = 0.1497, acc = 0.9440 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 03:02:11.409583: step 22680, loss = 0.1500, acc = 0.9580 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 03:02:16.156551: step 22700, loss = 0.1246, acc = 0.9680 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:02:20.845922: step 22720, loss = 0.1460, acc = 0.9600 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 03:02:25.565625: step 22740, loss = 0.1497, acc = 0.9480 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 03:02:30.338684: step 22760, loss = 0.1559, acc = 0.9400 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 03:02:35.351979: step 22780, loss = 0.1505, acc = 0.9460 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 03:02:40.103994: step 22800, loss = 0.1277, acc = 0.9700 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 03:02:44.765292: step 22820, loss = 0.1511, acc = 0.9520 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 03:02:49.493282: step 22840, loss = 0.1582, acc = 0.9480 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 03:02:54.273683: step 22860, loss = 0.1587, acc = 0.9460 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 03:02:59.055018: step 22880, loss = 0.1466, acc = 0.9460 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 03:03:03.737354: step 22900, loss = 0.1549, acc = 0.9520 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:03:08.474054: step 22920, loss = 0.1512, acc = 0.9400 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 03:03:13.240459: step 22940, loss = 0.1609, acc = 0.9400 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 03:03:17.849426: step 22960, loss = 0.1632, acc = 0.9420 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 03:03:22.602202: step 22980, loss = 0.1253, acc = 0.9620 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 03:03:27.375112: step 23000, loss = 0.1459, acc = 0.9600 (293.2 examples/sec; 0.218 sec/batch)
[Eval] 2017-05-09 03:03:41.418926: step 23000, acc = 0.9578, f1 = 0.9565
[Test] 2017-05-09 03:03:50.810647: step 23000, acc = 0.9483, f1 = 0.9478
[Status] 2017-05-09 03:03:50.810754: step 23000, maxindex = 23000, maxdev = 0.9578, maxtst = 0.9483
2017-05-09 03:03:59.013584: step 23020, loss = 0.1607, acc = 0.9420 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 03:04:03.569889: step 23040, loss = 0.1506, acc = 0.9460 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 03:04:08.494133: step 23060, loss = 0.1884, acc = 0.9240 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 03:04:13.098585: step 23080, loss = 0.1359, acc = 0.9540 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 03:04:17.794340: step 23100, loss = 0.1524, acc = 0.9500 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:04:22.743581: step 23120, loss = 0.1717, acc = 0.9380 (213.6 examples/sec; 0.300 sec/batch)
2017-05-09 03:04:27.505159: step 23140, loss = 0.1764, acc = 0.9280 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 03:04:32.381488: step 23160, loss = 0.1371, acc = 0.9520 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 03:04:38.400618: step 23180, loss = 0.1381, acc = 0.9540 (110.9 examples/sec; 0.577 sec/batch)
2017-05-09 03:04:43.048819: step 23200, loss = 0.1381, acc = 0.9540 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:04:47.589619: step 23220, loss = 0.1499, acc = 0.9480 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 03:04:52.261198: step 23240, loss = 0.1388, acc = 0.9520 (258.2 examples/sec; 0.248 sec/batch)
2017-05-09 03:04:56.848872: step 23260, loss = 0.1471, acc = 0.9460 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 03:05:01.596402: step 23280, loss = 0.1423, acc = 0.9540 (248.3 examples/sec; 0.258 sec/batch)
2017-05-09 03:05:06.194801: step 23300, loss = 0.2001, acc = 0.9260 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 03:05:11.135337: step 23320, loss = 0.1268, acc = 0.9600 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 03:05:15.897414: step 23340, loss = 0.1655, acc = 0.9440 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 03:05:20.577514: step 23360, loss = 0.1567, acc = 0.9540 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 03:05:25.525483: step 23380, loss = 0.1324, acc = 0.9580 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 03:05:30.189346: step 23400, loss = 0.1821, acc = 0.9340 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 03:05:34.914205: step 23420, loss = 0.1728, acc = 0.9440 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 03:05:39.584285: step 23440, loss = 0.1436, acc = 0.9540 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 03:05:44.299115: step 23460, loss = 0.1383, acc = 0.9520 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:05:48.965516: step 23480, loss = 0.1633, acc = 0.9520 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:05:53.815348: step 23500, loss = 0.1413, acc = 0.9500 (226.5 examples/sec; 0.283 sec/batch)
2017-05-09 03:05:58.565511: step 23520, loss = 0.1568, acc = 0.9460 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 03:06:03.168055: step 23540, loss = 0.1784, acc = 0.9320 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 03:06:07.674823: step 23560, loss = 0.1721, acc = 0.9360 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 03:06:12.420019: step 23580, loss = 0.1555, acc = 0.9500 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 03:06:17.097688: step 23600, loss = 0.1384, acc = 0.9500 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 03:06:21.685568: step 23620, loss = 0.1614, acc = 0.9560 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 03:06:26.472881: step 23640, loss = 0.1299, acc = 0.9660 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 03:06:31.088236: step 23660, loss = 0.1911, acc = 0.9380 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 03:06:35.695474: step 23680, loss = 0.1369, acc = 0.9500 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 03:06:40.379013: step 23700, loss = 0.1793, acc = 0.9380 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 03:06:45.031177: step 23720, loss = 0.1251, acc = 0.9620 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 03:06:49.668514: step 23740, loss = 0.1929, acc = 0.9320 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:06:54.471408: step 23760, loss = 0.1501, acc = 0.9660 (269.5 examples/sec; 0.238 sec/batch)
2017-05-09 03:06:59.022284: step 23780, loss = 0.1602, acc = 0.9600 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 03:07:03.767357: step 23800, loss = 0.1667, acc = 0.9440 (247.1 examples/sec; 0.259 sec/batch)
2017-05-09 03:07:08.387081: step 23820, loss = 0.1443, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 03:07:12.991738: step 23840, loss = 0.1463, acc = 0.9460 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 03:07:17.707826: step 23860, loss = 0.1575, acc = 0.9440 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 03:07:22.557820: step 23880, loss = 0.1848, acc = 0.9340 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 03:07:27.317922: step 23900, loss = 0.1595, acc = 0.9380 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 03:07:31.926782: step 23920, loss = 0.1392, acc = 0.9540 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 03:07:37.172820: step 23940, loss = 0.1609, acc = 0.9440 (207.7 examples/sec; 0.308 sec/batch)
2017-05-09 03:07:41.907669: step 23960, loss = 0.1609, acc = 0.9460 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 03:07:46.472286: step 23980, loss = 0.1443, acc = 0.9580 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 03:07:51.186902: step 24000, loss = 0.1349, acc = 0.9660 (269.6 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 03:08:04.867378: step 24000, acc = 0.9583, f1 = 0.9571
[Test] 2017-05-09 03:08:14.569700: step 24000, acc = 0.9486, f1 = 0.9482
[Status] 2017-05-09 03:08:14.569805: step 24000, maxindex = 24000, maxdev = 0.9583, maxtst = 0.9486
2017-05-09 03:08:22.526017: step 24020, loss = 0.1637, acc = 0.9380 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 03:08:27.097128: step 24040, loss = 0.1126, acc = 0.9660 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 03:08:31.853232: step 24060, loss = 0.1370, acc = 0.9600 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:08:36.512608: step 24080, loss = 0.1559, acc = 0.9560 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 03:08:41.292936: step 24100, loss = 0.1506, acc = 0.9480 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 03:08:45.913997: step 24120, loss = 0.1042, acc = 0.9740 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 03:08:50.587895: step 24140, loss = 0.1413, acc = 0.9480 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 03:08:55.559505: step 24160, loss = 0.1585, acc = 0.9500 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 03:09:00.087414: step 24180, loss = 0.1714, acc = 0.9480 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 03:09:05.450549: step 24200, loss = 0.1561, acc = 0.9460 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 03:09:10.012692: step 24220, loss = 0.1623, acc = 0.9540 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 03:09:14.664413: step 24240, loss = 0.1528, acc = 0.9440 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 03:09:19.355628: step 24260, loss = 0.1294, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 03:09:24.178536: step 24280, loss = 0.1535, acc = 0.9480 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:09:28.657697: step 24300, loss = 0.1612, acc = 0.9440 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 03:09:33.141517: step 24320, loss = 0.1730, acc = 0.9380 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:09:37.730876: step 24340, loss = 0.1341, acc = 0.9560 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 03:09:42.302194: step 24360, loss = 0.1361, acc = 0.9560 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 03:09:46.966065: step 24380, loss = 0.1292, acc = 0.9660 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 03:09:51.722583: step 24400, loss = 0.1463, acc = 0.9560 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 03:09:56.323874: step 24420, loss = 0.1488, acc = 0.9480 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:10:00.966607: step 24440, loss = 0.1503, acc = 0.9540 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 03:10:05.738483: step 24460, loss = 0.1386, acc = 0.9580 (226.8 examples/sec; 0.282 sec/batch)
2017-05-09 03:10:10.405005: step 24480, loss = 0.1197, acc = 0.9600 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 03:10:14.935519: step 24500, loss = 0.1554, acc = 0.9440 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 03:10:19.509058: step 24520, loss = 0.1558, acc = 0.9460 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 03:10:24.428327: step 24540, loss = 0.1487, acc = 0.9500 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 03:10:29.065329: step 24560, loss = 0.1605, acc = 0.9480 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 03:10:33.703204: step 24580, loss = 0.1333, acc = 0.9660 (255.1 examples/sec; 0.251 sec/batch)
2017-05-09 03:10:38.724598: step 24600, loss = 0.1310, acc = 0.9540 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 03:10:43.412549: step 24620, loss = 0.1517, acc = 0.9420 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:10:47.918902: step 24640, loss = 0.1724, acc = 0.9440 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 03:10:52.928212: step 24660, loss = 0.1513, acc = 0.9480 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 03:10:57.474285: step 24680, loss = 0.1546, acc = 0.9500 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 03:11:02.076157: step 24700, loss = 0.1579, acc = 0.9460 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 03:11:06.713992: step 24720, loss = 0.1797, acc = 0.9260 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 03:11:11.366207: step 24740, loss = 0.1465, acc = 0.9520 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 03:11:15.925226: step 24760, loss = 0.1466, acc = 0.9580 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 03:11:20.827826: step 24780, loss = 0.1589, acc = 0.9520 (249.3 examples/sec; 0.257 sec/batch)
2017-05-09 03:11:25.400597: step 24800, loss = 0.1543, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 03:11:30.047306: step 24820, loss = 0.1236, acc = 0.9620 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 03:11:34.773150: step 24840, loss = 0.1593, acc = 0.9340 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 03:11:39.659764: step 24860, loss = 0.1487, acc = 0.9580 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 03:11:44.371537: step 24880, loss = 0.1548, acc = 0.9440 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 03:11:48.966405: step 24900, loss = 0.1583, acc = 0.9520 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 03:11:53.855877: step 24920, loss = 0.1350, acc = 0.9620 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 03:11:58.511400: step 24940, loss = 0.1966, acc = 0.9400 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 03:12:03.149029: step 24960, loss = 0.1403, acc = 0.9580 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 03:12:07.917651: step 24980, loss = 0.1391, acc = 0.9500 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 03:12:12.540409: step 25000, loss = 0.1838, acc = 0.9240 (272.2 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 03:12:26.493813: step 25000, acc = 0.9585, f1 = 0.9573
[Test] 2017-05-09 03:12:36.479411: step 25000, acc = 0.9497, f1 = 0.9492
[Status] 2017-05-09 03:12:36.479517: step 25000, maxindex = 25000, maxdev = 0.9585, maxtst = 0.9497
2017-05-09 03:12:44.448223: step 25020, loss = 0.1496, acc = 0.9540 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 03:12:49.123410: step 25040, loss = 0.1707, acc = 0.9460 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 03:12:53.955614: step 25060, loss = 0.1672, acc = 0.9440 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 03:12:58.510351: step 25080, loss = 0.1476, acc = 0.9600 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:13:03.093621: step 25100, loss = 0.1530, acc = 0.9360 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 03:13:07.886305: step 25120, loss = 0.1283, acc = 0.9640 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 03:13:12.485250: step 25140, loss = 0.1677, acc = 0.9460 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 03:13:17.087495: step 25160, loss = 0.1353, acc = 0.9600 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 03:13:21.754703: step 25180, loss = 0.1293, acc = 0.9680 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 03:13:27.228310: step 25200, loss = 0.1346, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:13:31.895676: step 25220, loss = 0.1434, acc = 0.9500 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 03:13:36.745914: step 25240, loss = 0.1614, acc = 0.9440 (249.5 examples/sec; 0.256 sec/batch)
2017-05-09 03:13:41.627254: step 25260, loss = 0.1599, acc = 0.9540 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 03:13:46.378101: step 25280, loss = 0.1166, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 03:13:51.188445: step 25300, loss = 0.1756, acc = 0.9440 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 03:13:55.835835: step 25320, loss = 0.1286, acc = 0.9580 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 03:14:00.310553: step 25340, loss = 0.1452, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 03:14:05.186765: step 25360, loss = 0.1283, acc = 0.9520 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 03:14:09.857315: step 25380, loss = 0.1324, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 03:14:14.467106: step 25400, loss = 0.1367, acc = 0.9620 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 03:14:19.292507: step 25420, loss = 0.1691, acc = 0.9380 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:14:23.720205: step 25440, loss = 0.1123, acc = 0.9720 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 03:14:28.339755: step 25460, loss = 0.1657, acc = 0.9400 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 03:14:32.883877: step 25480, loss = 0.1410, acc = 0.9520 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 03:14:37.630997: step 25500, loss = 0.1447, acc = 0.9500 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 03:14:42.207023: step 25520, loss = 0.1281, acc = 0.9540 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 03:14:46.680431: step 25540, loss = 0.1809, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 03:14:51.494349: step 25560, loss = 0.1540, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 03:14:56.015922: step 25580, loss = 0.1313, acc = 0.9640 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 03:15:00.574021: step 25600, loss = 0.1575, acc = 0.9500 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 03:15:05.382631: step 25620, loss = 0.1369, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 03:15:10.139439: step 25640, loss = 0.1245, acc = 0.9640 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 03:15:14.764942: step 25660, loss = 0.1861, acc = 0.9380 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:15:19.504704: step 25680, loss = 0.1337, acc = 0.9620 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 03:15:24.068044: step 25700, loss = 0.1365, acc = 0.9520 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 03:15:28.647022: step 25720, loss = 0.1360, acc = 0.9460 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 03:15:33.470175: step 25740, loss = 0.1433, acc = 0.9620 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 03:15:37.982863: step 25760, loss = 0.1512, acc = 0.9440 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 03:15:42.728077: step 25780, loss = 0.1586, acc = 0.9440 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 03:15:47.399649: step 25800, loss = 0.1376, acc = 0.9620 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 03:15:52.160380: step 25820, loss = 0.1502, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 03:15:56.838153: step 25840, loss = 0.1609, acc = 0.9400 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 03:16:01.628366: step 25860, loss = 0.1645, acc = 0.9380 (244.7 examples/sec; 0.262 sec/batch)
2017-05-09 03:16:06.278780: step 25880, loss = 0.1391, acc = 0.9500 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 03:16:10.889419: step 25900, loss = 0.1763, acc = 0.9340 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 03:16:15.503924: step 25920, loss = 0.1398, acc = 0.9540 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 03:16:20.249378: step 25940, loss = 0.1329, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 03:16:24.842701: step 25960, loss = 0.1726, acc = 0.9420 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 03:16:29.505488: step 25980, loss = 0.1293, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 03:16:34.561966: step 26000, loss = 0.1499, acc = 0.9500 (290.2 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 03:16:48.697926: step 26000, acc = 0.9591, f1 = 0.9578
[Test] 2017-05-09 03:16:58.015188: step 26000, acc = 0.9494, f1 = 0.9490
[Status] 2017-05-09 03:16:58.015281: step 26000, maxindex = 26000, maxdev = 0.9591, maxtst = 0.9494
2017-05-09 03:17:05.902290: step 26020, loss = 0.1057, acc = 0.9820 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 03:17:10.441746: step 26040, loss = 0.1543, acc = 0.9400 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 03:17:14.970802: step 26060, loss = 0.1548, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 03:17:19.781094: step 26080, loss = 0.1793, acc = 0.9500 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 03:17:24.321706: step 26100, loss = 0.1444, acc = 0.9480 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 03:17:28.963523: step 26120, loss = 0.1449, acc = 0.9600 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 03:17:33.820555: step 26140, loss = 0.1437, acc = 0.9400 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 03:17:38.306744: step 26160, loss = 0.1584, acc = 0.9540 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 03:17:42.915973: step 26180, loss = 0.1777, acc = 0.9500 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 03:17:47.548813: step 26200, loss = 0.1202, acc = 0.9620 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 03:17:52.827561: step 26220, loss = 0.1751, acc = 0.9280 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 03:17:57.385379: step 26240, loss = 0.1650, acc = 0.9420 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 03:18:02.274245: step 26260, loss = 0.1214, acc = 0.9620 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 03:18:06.994699: step 26280, loss = 0.1464, acc = 0.9560 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:18:11.642760: step 26300, loss = 0.1424, acc = 0.9660 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 03:18:16.400531: step 26320, loss = 0.1514, acc = 0.9460 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 03:18:20.964950: step 26340, loss = 0.1412, acc = 0.9480 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 03:18:25.596900: step 26360, loss = 0.1662, acc = 0.9520 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 03:18:30.305173: step 26380, loss = 0.1382, acc = 0.9660 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 03:18:34.881965: step 26400, loss = 0.1647, acc = 0.9480 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 03:18:39.548560: step 26420, loss = 0.1195, acc = 0.9700 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 03:18:44.256641: step 26440, loss = 0.1466, acc = 0.9580 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 03:18:49.168299: step 26460, loss = 0.1360, acc = 0.9580 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 03:18:53.795509: step 26480, loss = 0.1552, acc = 0.9500 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 03:18:58.508705: step 26500, loss = 0.1209, acc = 0.9580 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 03:19:03.228347: step 26520, loss = 0.1727, acc = 0.9420 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 03:19:07.800697: step 26540, loss = 0.1276, acc = 0.9560 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 03:19:12.443623: step 26560, loss = 0.1270, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 03:19:17.295307: step 26580, loss = 0.1327, acc = 0.9540 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 03:19:22.887899: step 26600, loss = 0.1829, acc = 0.9380 (210.9 examples/sec; 0.304 sec/batch)
2017-05-09 03:19:27.617922: step 26620, loss = 0.1299, acc = 0.9640 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 03:19:32.312645: step 26640, loss = 0.1654, acc = 0.9400 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 03:19:36.852074: step 26660, loss = 0.1724, acc = 0.9380 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 03:19:41.467808: step 26680, loss = 0.1385, acc = 0.9660 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 03:19:46.177650: step 26700, loss = 0.1804, acc = 0.9440 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 03:19:50.775207: step 26720, loss = 0.1648, acc = 0.9500 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 03:19:55.439589: step 26740, loss = 0.1641, acc = 0.9380 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 03:20:00.256983: step 26760, loss = 0.1346, acc = 0.9500 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 03:20:04.854250: step 26780, loss = 0.1450, acc = 0.9460 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 03:20:09.493947: step 26800, loss = 0.1562, acc = 0.9440 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 03:20:14.248295: step 26820, loss = 0.1448, acc = 0.9580 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 03:20:19.097360: step 26840, loss = 0.1266, acc = 0.9580 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 03:20:23.725660: step 26860, loss = 0.1484, acc = 0.9540 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 03:20:28.446536: step 26880, loss = 0.1640, acc = 0.9440 (232.0 examples/sec; 0.276 sec/batch)
2017-05-09 03:20:33.016103: step 26900, loss = 0.1941, acc = 0.9260 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 03:20:37.753166: step 26920, loss = 0.1497, acc = 0.9400 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 03:20:42.401411: step 26940, loss = 0.1297, acc = 0.9620 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 03:20:47.195901: step 26960, loss = 0.1324, acc = 0.9520 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 03:20:51.739500: step 26980, loss = 0.1555, acc = 0.9480 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 03:20:56.444383: step 27000, loss = 0.1389, acc = 0.9500 (280.1 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 03:21:10.500082: step 27000, acc = 0.9587, f1 = 0.9574
[Test] 2017-05-09 03:21:20.075336: step 27000, acc = 0.9498, f1 = 0.9494
[Status] 2017-05-09 03:21:20.075424: step 27000, maxindex = 26000, maxdev = 0.9591, maxtst = 0.9494
2017-05-09 03:21:24.643649: step 27020, loss = 0.1650, acc = 0.9480 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 03:21:29.318544: step 27040, loss = 0.1405, acc = 0.9580 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 03:21:33.834879: step 27060, loss = 0.1286, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:21:38.759650: step 27080, loss = 0.1378, acc = 0.9620 (191.9 examples/sec; 0.333 sec/batch)
2017-05-09 03:21:43.696682: step 27100, loss = 0.1624, acc = 0.9480 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 03:21:48.200963: step 27120, loss = 0.1455, acc = 0.9360 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:21:52.889384: step 27140, loss = 0.1633, acc = 0.9420 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 03:21:57.924924: step 27160, loss = 0.1035, acc = 0.9760 (215.3 examples/sec; 0.297 sec/batch)
2017-05-09 03:22:02.786266: step 27180, loss = 0.1589, acc = 0.9440 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 03:22:07.499906: step 27200, loss = 0.1700, acc = 0.9420 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 03:22:13.271633: step 27220, loss = 0.1476, acc = 0.9400 (301.7 examples/sec; 0.212 sec/batch)
2017-05-09 03:22:17.929078: step 27240, loss = 0.1313, acc = 0.9500 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 03:22:22.460451: step 27260, loss = 0.1416, acc = 0.9500 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 03:22:27.240168: step 27280, loss = 0.1118, acc = 0.9700 (222.0 examples/sec; 0.288 sec/batch)
2017-05-09 03:22:31.914754: step 27300, loss = 0.1148, acc = 0.9580 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 03:22:36.507775: step 27320, loss = 0.1444, acc = 0.9520 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 03:22:41.149431: step 27340, loss = 0.1399, acc = 0.9540 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 03:22:45.814799: step 27360, loss = 0.1574, acc = 0.9540 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 03:22:50.427564: step 27380, loss = 0.1422, acc = 0.9600 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 03:22:55.114589: step 27400, loss = 0.1621, acc = 0.9440 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 03:22:59.864667: step 27420, loss = 0.1673, acc = 0.9400 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 03:23:04.473341: step 27440, loss = 0.1546, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 03:23:09.086206: step 27460, loss = 0.1617, acc = 0.9420 (255.8 examples/sec; 0.250 sec/batch)
2017-05-09 03:23:13.995041: step 27480, loss = 0.1400, acc = 0.9460 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 03:23:18.682876: step 27500, loss = 0.1368, acc = 0.9540 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 03:23:23.422629: step 27520, loss = 0.1276, acc = 0.9600 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 03:23:28.248372: step 27540, loss = 0.1524, acc = 0.9600 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 03:23:32.805854: step 27560, loss = 0.1346, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 03:23:37.519806: step 27580, loss = 0.1511, acc = 0.9440 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 03:23:42.322839: step 27600, loss = 0.1584, acc = 0.9520 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 03:23:46.993647: step 27620, loss = 0.1613, acc = 0.9440 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 03:23:51.739457: step 27640, loss = 0.1434, acc = 0.9580 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 03:23:56.392875: step 27660, loss = 0.1415, acc = 0.9440 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 03:24:01.293619: step 27680, loss = 0.1554, acc = 0.9540 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 03:24:05.858416: step 27700, loss = 0.1691, acc = 0.9480 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 03:24:10.582468: step 27720, loss = 0.1310, acc = 0.9540 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 03:24:15.317482: step 27740, loss = 0.1471, acc = 0.9420 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 03:24:19.977284: step 27760, loss = 0.1343, acc = 0.9620 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 03:24:24.567551: step 27780, loss = 0.1402, acc = 0.9480 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 03:24:29.390587: step 27800, loss = 0.1498, acc = 0.9420 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 03:24:34.094388: step 27820, loss = 0.1117, acc = 0.9640 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 03:24:38.587622: step 27840, loss = 0.1541, acc = 0.9600 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:24:43.382626: step 27860, loss = 0.1630, acc = 0.9460 (242.4 examples/sec; 0.264 sec/batch)
2017-05-09 03:24:47.955496: step 27880, loss = 0.1678, acc = 0.9460 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 03:24:52.764735: step 27900, loss = 0.1319, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 03:24:57.436930: step 27920, loss = 0.1520, acc = 0.9500 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 03:25:02.119097: step 27940, loss = 0.1473, acc = 0.9500 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 03:25:06.769301: step 27960, loss = 0.1604, acc = 0.9440 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:25:11.338729: step 27980, loss = 0.1499, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 03:25:16.363785: step 28000, loss = 0.1544, acc = 0.9440 (280.3 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 03:25:30.471481: step 28000, acc = 0.9586, f1 = 0.9574
[Test] 2017-05-09 03:25:39.726756: step 28000, acc = 0.9496, f1 = 0.9492
[Status] 2017-05-09 03:25:39.726847: step 28000, maxindex = 26000, maxdev = 0.9591, maxtst = 0.9494
2017-05-09 03:25:44.392758: step 28020, loss = 0.1243, acc = 0.9640 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 03:25:49.019836: step 28040, loss = 0.1444, acc = 0.9480 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 03:25:53.559380: step 28060, loss = 0.1266, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 03:25:58.183899: step 28080, loss = 0.1800, acc = 0.9240 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 03:26:03.072267: step 28100, loss = 0.1328, acc = 0.9700 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 03:26:07.850568: step 28120, loss = 0.1696, acc = 0.9400 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 03:26:12.590292: step 28140, loss = 0.1304, acc = 0.9560 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 03:26:17.352584: step 28160, loss = 0.1540, acc = 0.9480 (228.9 examples/sec; 0.280 sec/batch)
2017-05-09 03:26:22.054925: step 28180, loss = 0.1557, acc = 0.9480 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 03:26:26.703064: step 28200, loss = 0.1301, acc = 0.9580 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 03:26:32.253080: step 28220, loss = 0.1662, acc = 0.9600 (139.9 examples/sec; 0.458 sec/batch)
2017-05-09 03:26:36.998549: step 28240, loss = 0.1316, acc = 0.9620 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 03:26:41.659220: step 28260, loss = 0.1424, acc = 0.9460 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 03:26:46.365469: step 28280, loss = 0.1536, acc = 0.9480 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 03:26:51.090681: step 28300, loss = 0.1474, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 03:26:55.771507: step 28320, loss = 0.1359, acc = 0.9540 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 03:27:00.390140: step 28340, loss = 0.1351, acc = 0.9480 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 03:27:05.058921: step 28360, loss = 0.1148, acc = 0.9700 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 03:27:09.620828: step 28380, loss = 0.1281, acc = 0.9620 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 03:27:14.200645: step 28400, loss = 0.1299, acc = 0.9660 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 03:27:19.118319: step 28420, loss = 0.1400, acc = 0.9480 (219.3 examples/sec; 0.292 sec/batch)
2017-05-09 03:27:23.688244: step 28440, loss = 0.1403, acc = 0.9460 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:27:28.211125: step 28460, loss = 0.1303, acc = 0.9660 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:27:33.073046: step 28480, loss = 0.1553, acc = 0.9440 (231.6 examples/sec; 0.276 sec/batch)
2017-05-09 03:27:37.567148: step 28500, loss = 0.1305, acc = 0.9700 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 03:27:42.164579: step 28520, loss = 0.1321, acc = 0.9580 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 03:27:46.781134: step 28540, loss = 0.1627, acc = 0.9480 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 03:27:51.591529: step 28560, loss = 0.1433, acc = 0.9480 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 03:27:56.177319: step 28580, loss = 0.1314, acc = 0.9560 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 03:28:00.738898: step 28600, loss = 0.1491, acc = 0.9500 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 03:28:05.354906: step 28620, loss = 0.1282, acc = 0.9720 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 03:28:09.862432: step 28640, loss = 0.1398, acc = 0.9520 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 03:28:14.525139: step 28660, loss = 0.1617, acc = 0.9400 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 03:28:19.372370: step 28680, loss = 0.1420, acc = 0.9520 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 03:28:23.983549: step 28700, loss = 0.1521, acc = 0.9500 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 03:28:28.458222: step 28720, loss = 0.1585, acc = 0.9480 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 03:28:33.371885: step 28740, loss = 0.1379, acc = 0.9480 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 03:28:37.859597: step 28760, loss = 0.1672, acc = 0.9440 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:28:42.495278: step 28780, loss = 0.1084, acc = 0.9740 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 03:28:47.254444: step 28800, loss = 0.1232, acc = 0.9660 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 03:28:51.886568: step 28820, loss = 0.1319, acc = 0.9500 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 03:28:56.579933: step 28840, loss = 0.1679, acc = 0.9360 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 03:29:01.197226: step 28860, loss = 0.1519, acc = 0.9540 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:29:05.928612: step 28880, loss = 0.1549, acc = 0.9420 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 03:29:10.779924: step 28900, loss = 0.1396, acc = 0.9580 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 03:29:15.397013: step 28920, loss = 0.1515, acc = 0.9640 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 03:29:20.286528: step 28940, loss = 0.1252, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 03:29:24.913908: step 28960, loss = 0.1304, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 03:29:29.645422: step 28980, loss = 0.1405, acc = 0.9540 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:29:34.452163: step 29000, loss = 0.1346, acc = 0.9640 (272.2 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 03:29:48.810222: step 29000, acc = 0.9576, f1 = 0.9562
[Test] 2017-05-09 03:29:58.095588: step 29000, acc = 0.9468, f1 = 0.9463
[Status] 2017-05-09 03:29:58.095682: step 29000, maxindex = 26000, maxdev = 0.9591, maxtst = 0.9494
2017-05-09 03:30:02.900138: step 29020, loss = 0.1212, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 03:30:07.394261: step 29040, loss = 0.1578, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 03:30:12.013635: step 29060, loss = 0.1468, acc = 0.9520 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 03:30:16.722325: step 29080, loss = 0.1370, acc = 0.9540 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 03:30:21.348229: step 29100, loss = 0.1220, acc = 0.9660 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 03:30:26.057860: step 29120, loss = 0.1423, acc = 0.9540 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 03:30:30.967397: step 29140, loss = 0.1497, acc = 0.9540 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:30:35.597303: step 29160, loss = 0.1674, acc = 0.9420 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 03:30:40.288671: step 29180, loss = 0.1557, acc = 0.9580 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 03:30:44.883237: step 29200, loss = 0.1567, acc = 0.9360 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 03:30:49.880295: step 29220, loss = 0.1392, acc = 0.9560 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 03:30:55.643752: step 29240, loss = 0.1499, acc = 0.9400 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 03:31:00.542035: step 29260, loss = 0.1040, acc = 0.9820 (221.3 examples/sec; 0.289 sec/batch)
2017-05-09 03:31:05.264487: step 29280, loss = 0.1375, acc = 0.9520 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 03:31:09.916912: step 29300, loss = 0.1389, acc = 0.9640 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 03:31:14.587657: step 29320, loss = 0.1452, acc = 0.9520 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 03:31:19.398352: step 29340, loss = 0.1407, acc = 0.9540 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 03:31:24.001627: step 29360, loss = 0.1385, acc = 0.9480 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 03:31:28.758988: step 29380, loss = 0.1408, acc = 0.9480 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 03:31:33.443408: step 29400, loss = 0.1405, acc = 0.9480 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 03:31:38.034733: step 29420, loss = 0.1339, acc = 0.9520 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 03:31:42.692171: step 29440, loss = 0.1588, acc = 0.9480 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 03:31:47.484104: step 29460, loss = 0.1581, acc = 0.9520 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:31:52.049740: step 29480, loss = 0.1395, acc = 0.9540 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 03:31:56.700654: step 29500, loss = 0.1424, acc = 0.9520 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 03:32:01.654333: step 29520, loss = 0.0971, acc = 0.9740 (229.2 examples/sec; 0.279 sec/batch)
2017-05-09 03:32:06.294726: step 29540, loss = 0.1287, acc = 0.9520 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 03:32:10.891944: step 29560, loss = 0.1329, acc = 0.9600 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 03:32:15.684788: step 29580, loss = 0.1537, acc = 0.9560 (238.4 examples/sec; 0.268 sec/batch)
2017-05-09 03:32:20.414649: step 29600, loss = 0.1602, acc = 0.9400 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 03:32:24.951140: step 29620, loss = 0.1387, acc = 0.9660 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 03:32:29.734776: step 29640, loss = 0.1744, acc = 0.9520 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 03:32:34.490378: step 29660, loss = 0.1538, acc = 0.9440 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 03:32:39.033400: step 29680, loss = 0.1266, acc = 0.9640 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 03:32:43.620512: step 29700, loss = 0.1554, acc = 0.9560 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:32:48.239414: step 29720, loss = 0.1309, acc = 0.9580 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 03:32:52.917237: step 29740, loss = 0.1342, acc = 0.9540 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 03:32:57.582814: step 29760, loss = 0.1279, acc = 0.9540 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 03:33:02.304497: step 29780, loss = 0.1509, acc = 0.9480 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 03:33:06.899698: step 29800, loss = 0.1198, acc = 0.9520 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 03:33:11.612914: step 29820, loss = 0.1254, acc = 0.9660 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 03:33:16.291478: step 29840, loss = 0.1553, acc = 0.9420 (242.6 examples/sec; 0.264 sec/batch)
2017-05-09 03:33:20.838330: step 29860, loss = 0.1629, acc = 0.9560 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 03:33:25.507789: step 29880, loss = 0.1321, acc = 0.9560 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 03:33:30.138025: step 29900, loss = 0.1367, acc = 0.9500 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 03:33:34.913224: step 29920, loss = 0.1509, acc = 0.9460 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 03:33:39.533497: step 29940, loss = 0.1637, acc = 0.9440 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 03:33:44.242867: step 29960, loss = 0.1424, acc = 0.9520 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 03:33:49.208176: step 29980, loss = 0.1480, acc = 0.9420 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 03:33:53.767268: step 30000, loss = 0.1639, acc = 0.9440 (276.4 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 03:34:08.126698: step 30000, acc = 0.9592, f1 = 0.9580
[Test] 2017-05-09 03:34:17.730431: step 30000, acc = 0.9499, f1 = 0.9495
[Status] 2017-05-09 03:34:17.730545: step 30000, maxindex = 30000, maxdev = 0.9592, maxtst = 0.9499
2017-05-09 03:34:25.708544: step 30020, loss = 0.1660, acc = 0.9380 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 03:34:30.540947: step 30040, loss = 0.1671, acc = 0.9400 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 03:34:35.195843: step 30060, loss = 0.1504, acc = 0.9480 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 03:34:39.747039: step 30080, loss = 0.1387, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:34:44.581378: step 30100, loss = 0.1355, acc = 0.9620 (215.2 examples/sec; 0.297 sec/batch)
2017-05-09 03:34:49.200458: step 30120, loss = 0.1381, acc = 0.9560 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 03:34:53.733961: step 30140, loss = 0.1329, acc = 0.9620 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:34:58.364413: step 30160, loss = 0.1481, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 03:35:03.031395: step 30180, loss = 0.1169, acc = 0.9600 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 03:35:07.541767: step 30200, loss = 0.1535, acc = 0.9520 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 03:35:12.065054: step 30220, loss = 0.1354, acc = 0.9520 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:35:17.646984: step 30240, loss = 0.1391, acc = 0.9480 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 03:35:22.328497: step 30260, loss = 0.1409, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 03:35:26.942264: step 30280, loss = 0.1215, acc = 0.9720 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 03:35:31.925121: step 30300, loss = 0.1504, acc = 0.9560 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 03:35:36.616331: step 30320, loss = 0.1712, acc = 0.9560 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 03:35:41.155654: step 30340, loss = 0.1283, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 03:35:46.027071: step 30360, loss = 0.1386, acc = 0.9560 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:35:50.588025: step 30380, loss = 0.1222, acc = 0.9660 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 03:35:55.260151: step 30400, loss = 0.1632, acc = 0.9400 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 03:35:59.928665: step 30420, loss = 0.1188, acc = 0.9700 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 03:36:04.431004: step 30440, loss = 0.1258, acc = 0.9600 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 03:36:08.991493: step 30460, loss = 0.1397, acc = 0.9600 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 03:36:13.740230: step 30480, loss = 0.1244, acc = 0.9600 (226.7 examples/sec; 0.282 sec/batch)
2017-05-09 03:36:18.586765: step 30500, loss = 0.1504, acc = 0.9520 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 03:36:23.266544: step 30520, loss = 0.1310, acc = 0.9600 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:36:27.931035: step 30540, loss = 0.1480, acc = 0.9500 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 03:36:32.793759: step 30560, loss = 0.1411, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 03:36:37.450096: step 30580, loss = 0.1585, acc = 0.9440 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:36:42.055184: step 30600, loss = 0.1177, acc = 0.9720 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 03:36:46.830544: step 30620, loss = 0.1203, acc = 0.9600 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:36:51.425422: step 30640, loss = 0.1514, acc = 0.9480 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 03:36:56.098138: step 30660, loss = 0.1339, acc = 0.9480 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 03:37:00.871400: step 30680, loss = 0.1200, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 03:37:05.500770: step 30700, loss = 0.1680, acc = 0.9460 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 03:37:10.154726: step 30720, loss = 0.1341, acc = 0.9700 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 03:37:15.099741: step 30740, loss = 0.1304, acc = 0.9580 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 03:37:19.750539: step 30760, loss = 0.1216, acc = 0.9600 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 03:37:24.361652: step 30780, loss = 0.1085, acc = 0.9700 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 03:37:28.970995: step 30800, loss = 0.1513, acc = 0.9440 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 03:37:33.435739: step 30820, loss = 0.1128, acc = 0.9640 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 03:37:38.098396: step 30840, loss = 0.1369, acc = 0.9540 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 03:37:42.870077: step 30860, loss = 0.1466, acc = 0.9540 (246.6 examples/sec; 0.260 sec/batch)
2017-05-09 03:37:47.608709: step 30880, loss = 0.1426, acc = 0.9640 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 03:37:52.154189: step 30900, loss = 0.1369, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 03:37:56.959075: step 30920, loss = 0.1421, acc = 0.9500 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 03:38:01.643462: step 30940, loss = 0.1545, acc = 0.9500 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 03:38:06.220563: step 30960, loss = 0.1567, acc = 0.9480 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 03:38:10.805835: step 30980, loss = 0.1290, acc = 0.9500 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 03:38:15.444382: step 31000, loss = 0.1040, acc = 0.9720 (273.7 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 03:38:29.899939: step 31000, acc = 0.9573, f1 = 0.9559
[Test] 2017-05-09 03:38:39.198293: step 31000, acc = 0.9461, f1 = 0.9456
[Status] 2017-05-09 03:38:39.198413: step 31000, maxindex = 30000, maxdev = 0.9592, maxtst = 0.9499
2017-05-09 03:38:43.877517: step 31020, loss = 0.1250, acc = 0.9580 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 03:38:48.440463: step 31040, loss = 0.1681, acc = 0.9360 (263.9 examples/sec; 0.242 sec/batch)
2017-05-09 03:38:52.973427: step 31060, loss = 0.1325, acc = 0.9460 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 03:38:57.768153: step 31080, loss = 0.1534, acc = 0.9540 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 03:39:02.330980: step 31100, loss = 0.1626, acc = 0.9480 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 03:39:06.858607: step 31120, loss = 0.1408, acc = 0.9560 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 03:39:11.640714: step 31140, loss = 0.1373, acc = 0.9440 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 03:39:16.254879: step 31160, loss = 0.1274, acc = 0.9600 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 03:39:20.835124: step 31180, loss = 0.1435, acc = 0.9540 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:39:25.680946: step 31200, loss = 0.1543, acc = 0.9540 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 03:39:30.346144: step 31220, loss = 0.1424, acc = 0.9620 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 03:39:34.931903: step 31240, loss = 0.1208, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 03:39:40.886820: step 31260, loss = 0.1419, acc = 0.9480 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 03:39:45.428536: step 31280, loss = 0.1621, acc = 0.9540 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 03:39:49.921784: step 31300, loss = 0.1694, acc = 0.9320 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:39:54.722852: step 31320, loss = 0.1486, acc = 0.9480 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:39:59.242509: step 31340, loss = 0.1479, acc = 0.9560 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 03:40:03.757666: step 31360, loss = 0.1455, acc = 0.9520 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 03:40:08.743759: step 31380, loss = 0.1470, acc = 0.9580 (219.4 examples/sec; 0.292 sec/batch)
2017-05-09 03:40:13.391977: step 31400, loss = 0.1565, acc = 0.9520 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 03:40:18.045013: step 31420, loss = 0.1401, acc = 0.9660 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 03:40:22.957163: step 31440, loss = 0.1393, acc = 0.9500 (226.7 examples/sec; 0.282 sec/batch)
2017-05-09 03:40:27.547147: step 31460, loss = 0.1288, acc = 0.9480 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 03:40:32.173624: step 31480, loss = 0.1447, acc = 0.9600 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 03:40:36.697692: step 31500, loss = 0.1302, acc = 0.9520 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 03:40:41.676078: step 31520, loss = 0.1313, acc = 0.9500 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 03:40:46.270690: step 31540, loss = 0.1477, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 03:40:50.924649: step 31560, loss = 0.1579, acc = 0.9420 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 03:40:55.685791: step 31580, loss = 0.1389, acc = 0.9520 (298.4 examples/sec; 0.214 sec/batch)
2017-05-09 03:41:00.249151: step 31600, loss = 0.1755, acc = 0.9420 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 03:41:04.924255: step 31620, loss = 0.1303, acc = 0.9620 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 03:41:10.031604: step 31640, loss = 0.1688, acc = 0.9440 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 03:41:14.665533: step 31660, loss = 0.1161, acc = 0.9700 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 03:41:19.232332: step 31680, loss = 0.1346, acc = 0.9620 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 03:41:24.096712: step 31700, loss = 0.1347, acc = 0.9480 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 03:41:28.682645: step 31720, loss = 0.1222, acc = 0.9620 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 03:41:33.308005: step 31740, loss = 0.1514, acc = 0.9480 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 03:41:38.161660: step 31760, loss = 0.1242, acc = 0.9620 (205.1 examples/sec; 0.312 sec/batch)
2017-05-09 03:41:42.868214: step 31780, loss = 0.1609, acc = 0.9420 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 03:41:47.623234: step 31800, loss = 0.1297, acc = 0.9600 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 03:41:52.363349: step 31820, loss = 0.1764, acc = 0.9320 (235.6 examples/sec; 0.272 sec/batch)
2017-05-09 03:41:56.994561: step 31840, loss = 0.1597, acc = 0.9460 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:42:01.528052: step 31860, loss = 0.1428, acc = 0.9560 (292.9 examples/sec; 0.218 sec/batch)
2017-05-09 03:42:06.407986: step 31880, loss = 0.1430, acc = 0.9580 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 03:42:11.128505: step 31900, loss = 0.1540, acc = 0.9500 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 03:42:15.731949: step 31920, loss = 0.1625, acc = 0.9460 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 03:42:20.318493: step 31940, loss = 0.1338, acc = 0.9560 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 03:42:25.082848: step 31960, loss = 0.1653, acc = 0.9380 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:42:29.764679: step 31980, loss = 0.1428, acc = 0.9600 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:42:34.419537: step 32000, loss = 0.1461, acc = 0.9560 (269.9 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 03:42:48.799860: step 32000, acc = 0.9592, f1 = 0.9579
[Test] 2017-05-09 03:42:58.503557: step 32000, acc = 0.9507, f1 = 0.9503
[Status] 2017-05-09 03:42:58.503626: step 32000, maxindex = 30000, maxdev = 0.9592, maxtst = 0.9499
2017-05-09 03:43:03.051766: step 32020, loss = 0.1322, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 03:43:07.953330: step 32040, loss = 0.1002, acc = 0.9740 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:43:12.557370: step 32060, loss = 0.1440, acc = 0.9440 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:43:17.100529: step 32080, loss = 0.1410, acc = 0.9520 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 03:43:21.912752: step 32100, loss = 0.1296, acc = 0.9620 (225.7 examples/sec; 0.284 sec/batch)
2017-05-09 03:43:26.504118: step 32120, loss = 0.1539, acc = 0.9500 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 03:43:31.111913: step 32140, loss = 0.1470, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 03:43:35.894529: step 32160, loss = 0.1536, acc = 0.9500 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 03:43:40.449084: step 32180, loss = 0.1340, acc = 0.9520 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 03:43:45.188775: step 32200, loss = 0.1409, acc = 0.9440 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 03:43:49.929099: step 32220, loss = 0.1280, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 03:43:54.548883: step 32240, loss = 0.1676, acc = 0.9520 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 03:43:59.839861: step 32260, loss = 0.1475, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 03:44:04.593058: step 32280, loss = 0.1150, acc = 0.9660 (297.2 examples/sec; 0.215 sec/batch)
2017-05-09 03:44:09.292629: step 32300, loss = 0.1457, acc = 0.9460 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 03:44:13.943758: step 32320, loss = 0.1228, acc = 0.9640 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 03:44:18.541076: step 32340, loss = 0.1213, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:44:23.466673: step 32360, loss = 0.1247, acc = 0.9560 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:44:27.911505: step 32380, loss = 0.1224, acc = 0.9600 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:44:32.425725: step 32400, loss = 0.1189, acc = 0.9700 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 03:44:37.098097: step 32420, loss = 0.1566, acc = 0.9340 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 03:44:41.630984: step 32440, loss = 0.1211, acc = 0.9560 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 03:44:46.156516: step 32460, loss = 0.1530, acc = 0.9500 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 03:44:51.008157: step 32480, loss = 0.1414, acc = 0.9520 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 03:44:55.677672: step 32500, loss = 0.1365, acc = 0.9440 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 03:45:00.311690: step 32520, loss = 0.1672, acc = 0.9520 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 03:45:05.247608: step 32540, loss = 0.1680, acc = 0.9500 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 03:45:09.811046: step 32560, loss = 0.1353, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 03:45:14.407465: step 32580, loss = 0.1158, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 03:45:19.241705: step 32600, loss = 0.1191, acc = 0.9620 (234.6 examples/sec; 0.273 sec/batch)
2017-05-09 03:45:23.857818: step 32620, loss = 0.1360, acc = 0.9560 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:45:28.491022: step 32640, loss = 0.1181, acc = 0.9660 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 03:45:33.132190: step 32660, loss = 0.1203, acc = 0.9740 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 03:45:37.775983: step 32680, loss = 0.1505, acc = 0.9680 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 03:45:42.373815: step 32700, loss = 0.1196, acc = 0.9680 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 03:45:47.009730: step 32720, loss = 0.1158, acc = 0.9660 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 03:45:51.892115: step 32740, loss = 0.1424, acc = 0.9520 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:45:56.492614: step 32760, loss = 0.1490, acc = 0.9520 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 03:46:01.040889: step 32780, loss = 0.1298, acc = 0.9580 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 03:46:05.794700: step 32800, loss = 0.1421, acc = 0.9380 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 03:46:10.355926: step 32820, loss = 0.1319, acc = 0.9520 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 03:46:14.982385: step 32840, loss = 0.1633, acc = 0.9440 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:46:19.729345: step 32860, loss = 0.1558, acc = 0.9500 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 03:46:24.272607: step 32880, loss = 0.1442, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 03:46:29.023295: step 32900, loss = 0.1395, acc = 0.9520 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 03:46:33.770565: step 32920, loss = 0.1413, acc = 0.9560 (242.2 examples/sec; 0.264 sec/batch)
2017-05-09 03:46:38.344752: step 32940, loss = 0.1476, acc = 0.9540 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 03:46:43.065071: step 32960, loss = 0.1296, acc = 0.9560 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:46:47.624224: step 32980, loss = 0.1491, acc = 0.9540 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:46:52.270598: step 33000, loss = 0.1402, acc = 0.9620 (287.8 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 03:47:06.696591: step 33000, acc = 0.9561, f1 = 0.9548
[Test] 2017-05-09 03:47:16.309161: step 33000, acc = 0.9471, f1 = 0.9466
[Status] 2017-05-09 03:47:16.309230: step 33000, maxindex = 30000, maxdev = 0.9592, maxtst = 0.9499
2017-05-09 03:47:20.896552: step 33020, loss = 0.1242, acc = 0.9640 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 03:47:25.420675: step 33040, loss = 0.1461, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 03:47:30.040405: step 33060, loss = 0.1510, acc = 0.9560 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 03:47:34.741255: step 33080, loss = 0.1384, acc = 0.9540 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 03:47:39.394068: step 33100, loss = 0.1660, acc = 0.9360 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 03:47:44.051061: step 33120, loss = 0.1391, acc = 0.9520 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 03:47:48.712502: step 33140, loss = 0.1227, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 03:47:53.376964: step 33160, loss = 0.1444, acc = 0.9560 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 03:47:58.020067: step 33180, loss = 0.1300, acc = 0.9560 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 03:48:02.730119: step 33200, loss = 0.1318, acc = 0.9680 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 03:48:07.377081: step 33220, loss = 0.1405, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:48:12.044478: step 33240, loss = 0.1397, acc = 0.9640 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 03:48:18.028323: step 33260, loss = 0.1482, acc = 0.9520 (125.0 examples/sec; 0.512 sec/batch)
2017-05-09 03:48:22.849011: step 33280, loss = 0.1297, acc = 0.9540 (255.6 examples/sec; 0.250 sec/batch)
2017-05-09 03:48:27.456914: step 33300, loss = 0.1460, acc = 0.9560 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 03:48:32.263344: step 33320, loss = 0.1258, acc = 0.9600 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 03:48:36.892593: step 33340, loss = 0.1528, acc = 0.9540 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 03:48:41.523650: step 33360, loss = 0.1686, acc = 0.9380 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 03:48:46.276640: step 33380, loss = 0.1422, acc = 0.9560 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 03:48:50.921443: step 33400, loss = 0.1485, acc = 0.9580 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 03:48:55.483098: step 33420, loss = 0.1363, acc = 0.9500 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 03:49:00.235401: step 33440, loss = 0.1351, acc = 0.9620 (240.6 examples/sec; 0.266 sec/batch)
2017-05-09 03:49:04.728375: step 33460, loss = 0.1535, acc = 0.9440 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 03:49:09.413950: step 33480, loss = 0.1273, acc = 0.9600 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 03:49:14.062551: step 33500, loss = 0.1584, acc = 0.9480 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 03:49:18.819769: step 33520, loss = 0.1275, acc = 0.9560 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 03:49:23.426346: step 33540, loss = 0.1410, acc = 0.9580 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 03:49:28.018991: step 33560, loss = 0.1161, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 03:49:32.853507: step 33580, loss = 0.1390, acc = 0.9580 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 03:49:37.406866: step 33600, loss = 0.1714, acc = 0.9360 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 03:49:41.911111: step 33620, loss = 0.1545, acc = 0.9520 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 03:49:46.694615: step 33640, loss = 0.1338, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 03:49:51.465025: step 33660, loss = 0.1437, acc = 0.9580 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 03:49:56.102783: step 33680, loss = 0.1320, acc = 0.9580 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 03:50:00.864944: step 33700, loss = 0.1173, acc = 0.9620 (237.5 examples/sec; 0.269 sec/batch)
2017-05-09 03:50:05.793180: step 33720, loss = 0.1554, acc = 0.9500 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 03:50:10.397323: step 33740, loss = 0.1218, acc = 0.9680 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 03:50:15.031013: step 33760, loss = 0.1703, acc = 0.9340 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 03:50:19.657464: step 33780, loss = 0.1604, acc = 0.9440 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 03:50:24.496664: step 33800, loss = 0.1568, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 03:50:29.083881: step 33820, loss = 0.1438, acc = 0.9440 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 03:50:33.853846: step 33840, loss = 0.0994, acc = 0.9760 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 03:50:38.438468: step 33860, loss = 0.1628, acc = 0.9380 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 03:50:43.104613: step 33880, loss = 0.1595, acc = 0.9460 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 03:50:47.743720: step 33900, loss = 0.1434, acc = 0.9500 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 03:50:52.322621: step 33920, loss = 0.1299, acc = 0.9580 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 03:50:56.836159: step 33940, loss = 0.1405, acc = 0.9580 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 03:51:01.631719: step 33960, loss = 0.1471, acc = 0.9520 (215.9 examples/sec; 0.296 sec/batch)
2017-05-09 03:51:06.220124: step 33980, loss = 0.1502, acc = 0.9560 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 03:51:10.798119: step 34000, loss = 0.1235, acc = 0.9720 (285.9 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 03:51:24.850863: step 34000, acc = 0.9600, f1 = 0.9588
[Test] 2017-05-09 03:51:34.518284: step 34000, acc = 0.9505, f1 = 0.9501
[Status] 2017-05-09 03:51:34.518372: step 34000, maxindex = 34000, maxdev = 0.9600, maxtst = 0.9505
2017-05-09 03:51:42.501163: step 34020, loss = 0.1575, acc = 0.9520 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 03:51:47.248767: step 34040, loss = 0.1293, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 03:51:51.738771: step 34060, loss = 0.1016, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 03:51:56.245096: step 34080, loss = 0.1310, acc = 0.9560 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 03:52:00.885951: step 34100, loss = 0.1625, acc = 0.9440 (256.5 examples/sec; 0.250 sec/batch)
2017-05-09 03:52:05.503007: step 34120, loss = 0.1491, acc = 0.9480 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 03:52:10.103602: step 34140, loss = 0.1511, acc = 0.9520 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 03:52:14.762328: step 34160, loss = 0.1404, acc = 0.9520 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 03:52:19.395808: step 34180, loss = 0.1243, acc = 0.9640 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 03:52:23.888380: step 34200, loss = 0.1695, acc = 0.9380 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:52:28.477425: step 34220, loss = 0.1306, acc = 0.9540 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 03:52:33.116908: step 34240, loss = 0.1519, acc = 0.9520 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 03:52:37.739738: step 34260, loss = 0.1740, acc = 0.9360 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 03:52:42.971182: step 34280, loss = 0.1406, acc = 0.9460 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 03:52:47.841066: step 34300, loss = 0.1226, acc = 0.9640 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 03:52:52.481667: step 34320, loss = 0.1477, acc = 0.9500 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 03:52:57.129544: step 34340, loss = 0.1284, acc = 0.9660 (256.9 examples/sec; 0.249 sec/batch)
2017-05-09 03:53:01.960015: step 34360, loss = 0.1384, acc = 0.9560 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 03:53:06.658868: step 34380, loss = 0.1456, acc = 0.9440 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 03:53:11.395036: step 34400, loss = 0.1436, acc = 0.9560 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 03:53:16.188540: step 34420, loss = 0.1076, acc = 0.9680 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 03:53:20.892952: step 34440, loss = 0.1441, acc = 0.9500 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 03:53:25.457264: step 34460, loss = 0.1169, acc = 0.9700 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 03:53:30.138712: step 34480, loss = 0.1530, acc = 0.9440 (232.2 examples/sec; 0.276 sec/batch)
2017-05-09 03:53:34.735848: step 34500, loss = 0.1447, acc = 0.9500 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 03:53:39.325671: step 34520, loss = 0.1473, acc = 0.9460 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 03:53:43.975111: step 34540, loss = 0.1359, acc = 0.9560 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 03:53:48.714459: step 34560, loss = 0.1292, acc = 0.9520 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 03:53:53.364780: step 34580, loss = 0.1572, acc = 0.9480 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 03:53:58.067669: step 34600, loss = 0.1381, acc = 0.9540 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 03:54:02.710732: step 34620, loss = 0.1638, acc = 0.9540 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 03:54:07.260649: step 34640, loss = 0.1369, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 03:54:11.763948: step 34660, loss = 0.1352, acc = 0.9540 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 03:54:16.476792: step 34680, loss = 0.1277, acc = 0.9620 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 03:54:21.205483: step 34700, loss = 0.1063, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 03:54:25.729060: step 34720, loss = 0.1508, acc = 0.9520 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:54:30.564630: step 34740, loss = 0.1223, acc = 0.9580 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 03:54:35.105759: step 34760, loss = 0.1563, acc = 0.9560 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 03:54:40.004556: step 34780, loss = 0.1311, acc = 0.9660 (246.3 examples/sec; 0.260 sec/batch)
2017-05-09 03:54:44.772258: step 34800, loss = 0.1551, acc = 0.9540 (231.8 examples/sec; 0.276 sec/batch)
2017-05-09 03:54:49.391657: step 34820, loss = 0.1355, acc = 0.9640 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 03:54:53.851168: step 34840, loss = 0.1492, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 03:54:58.372697: step 34860, loss = 0.1388, acc = 0.9540 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 03:55:03.114972: step 34880, loss = 0.1402, acc = 0.9620 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 03:55:07.682252: step 34900, loss = 0.1278, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 03:55:12.246877: step 34920, loss = 0.1309, acc = 0.9620 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 03:55:17.044207: step 34940, loss = 0.1659, acc = 0.9440 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 03:55:21.525477: step 34960, loss = 0.1329, acc = 0.9620 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 03:55:26.105669: step 34980, loss = 0.1581, acc = 0.9460 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 03:55:30.931924: step 35000, loss = 0.1208, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 03:55:45.026216: step 35000, acc = 0.9607, f1 = 0.9594
[Test] 2017-05-09 03:55:54.360883: step 35000, acc = 0.9524, f1 = 0.9520
[Status] 2017-05-09 03:55:54.360977: step 35000, maxindex = 35000, maxdev = 0.9607, maxtst = 0.9524
2017-05-09 03:56:02.602903: step 35020, loss = 0.1397, acc = 0.9540 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 03:56:07.175196: step 35040, loss = 0.1429, acc = 0.9500 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 03:56:11.930484: step 35060, loss = 0.1489, acc = 0.9440 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 03:56:16.549843: step 35080, loss = 0.1457, acc = 0.9540 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 03:56:21.158350: step 35100, loss = 0.1292, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 03:56:25.897052: step 35120, loss = 0.1287, acc = 0.9580 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 03:56:30.623648: step 35140, loss = 0.1585, acc = 0.9480 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 03:56:35.098072: step 35160, loss = 0.1340, acc = 0.9540 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 03:56:39.772538: step 35180, loss = 0.1389, acc = 0.9500 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 03:56:44.375125: step 35200, loss = 0.1203, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 03:56:48.984232: step 35220, loss = 0.1509, acc = 0.9380 (296.3 examples/sec; 0.216 sec/batch)
2017-05-09 03:56:53.830014: step 35240, loss = 0.1425, acc = 0.9480 (241.2 examples/sec; 0.265 sec/batch)
2017-05-09 03:56:58.342962: step 35260, loss = 0.1394, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 03:57:04.101908: step 35280, loss = 0.1310, acc = 0.9540 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 03:57:09.057482: step 35300, loss = 0.1228, acc = 0.9680 (216.9 examples/sec; 0.295 sec/batch)
2017-05-09 03:57:13.724409: step 35320, loss = 0.1287, acc = 0.9560 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 03:57:18.284907: step 35340, loss = 0.1337, acc = 0.9620 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 03:57:22.899042: step 35360, loss = 0.1229, acc = 0.9640 (247.7 examples/sec; 0.258 sec/batch)
2017-05-09 03:57:27.448139: step 35380, loss = 0.1409, acc = 0.9560 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 03:57:32.409692: step 35400, loss = 0.1022, acc = 0.9680 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 03:57:37.015724: step 35420, loss = 0.1496, acc = 0.9540 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 03:57:41.773411: step 35440, loss = 0.1033, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 03:57:46.346467: step 35460, loss = 0.1351, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 03:57:51.025898: step 35480, loss = 0.1567, acc = 0.9420 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 03:57:55.798320: step 35500, loss = 0.1292, acc = 0.9520 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 03:58:00.360375: step 35520, loss = 0.1246, acc = 0.9600 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 03:58:05.054525: step 35540, loss = 0.1229, acc = 0.9580 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 03:58:09.756321: step 35560, loss = 0.1372, acc = 0.9540 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 03:58:14.311458: step 35580, loss = 0.1363, acc = 0.9540 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 03:58:18.873638: step 35600, loss = 0.1544, acc = 0.9480 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 03:58:23.809289: step 35620, loss = 0.1368, acc = 0.9700 (223.3 examples/sec; 0.287 sec/batch)
2017-05-09 03:58:28.485732: step 35640, loss = 0.1185, acc = 0.9600 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 03:58:33.072578: step 35660, loss = 0.1656, acc = 0.9440 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 03:58:37.783142: step 35680, loss = 0.1341, acc = 0.9540 (249.0 examples/sec; 0.257 sec/batch)
2017-05-09 03:58:42.317356: step 35700, loss = 0.1710, acc = 0.9440 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 03:58:46.917687: step 35720, loss = 0.1113, acc = 0.9800 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 03:58:51.619666: step 35740, loss = 0.1156, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 03:58:56.246207: step 35760, loss = 0.1212, acc = 0.9620 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 03:59:01.009920: step 35780, loss = 0.1293, acc = 0.9620 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 03:59:05.614974: step 35800, loss = 0.1254, acc = 0.9560 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 03:59:10.180044: step 35820, loss = 0.1794, acc = 0.9440 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 03:59:14.888058: step 35840, loss = 0.1302, acc = 0.9620 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 03:59:19.638745: step 35860, loss = 0.1322, acc = 0.9640 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 03:59:24.310414: step 35880, loss = 0.1405, acc = 0.9560 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 03:59:28.975096: step 35900, loss = 0.1349, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 03:59:33.665439: step 35920, loss = 0.1275, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 03:59:38.355825: step 35940, loss = 0.1384, acc = 0.9560 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 03:59:42.984405: step 35960, loss = 0.1467, acc = 0.9500 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 03:59:47.526205: step 35980, loss = 0.1164, acc = 0.9660 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 03:59:52.241256: step 36000, loss = 0.1320, acc = 0.9640 (242.2 examples/sec; 0.264 sec/batch)
[Eval] 2017-05-09 04:00:06.255814: step 36000, acc = 0.9596, f1 = 0.9583
[Test] 2017-05-09 04:00:15.684765: step 36000, acc = 0.9512, f1 = 0.9508
[Status] 2017-05-09 04:00:15.684872: step 36000, maxindex = 35000, maxdev = 0.9607, maxtst = 0.9524
2017-05-09 04:00:20.284878: step 36020, loss = 0.1234, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 04:00:25.076027: step 36040, loss = 0.1332, acc = 0.9620 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 04:00:29.731426: step 36060, loss = 0.1354, acc = 0.9620 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 04:00:34.307773: step 36080, loss = 0.1387, acc = 0.9540 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 04:00:38.946722: step 36100, loss = 0.1356, acc = 0.9620 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 04:00:43.578953: step 36120, loss = 0.1321, acc = 0.9560 (288.9 examples/sec; 0.221 sec/batch)
2017-05-09 04:00:48.191396: step 36140, loss = 0.1375, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:00:53.029084: step 36160, loss = 0.1531, acc = 0.9480 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 04:00:57.613776: step 36180, loss = 0.1421, acc = 0.9500 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:01:02.188585: step 36200, loss = 0.1239, acc = 0.9600 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:01:06.900265: step 36220, loss = 0.1496, acc = 0.9500 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 04:01:11.384693: step 36240, loss = 0.1531, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 04:01:16.122814: step 36260, loss = 0.1162, acc = 0.9660 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 04:01:20.865743: step 36280, loss = 0.1428, acc = 0.9520 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 04:01:26.292877: step 36300, loss = 0.1343, acc = 0.9580 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 04:01:31.008784: step 36320, loss = 0.1411, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:01:35.699983: step 36340, loss = 0.1472, acc = 0.9480 (245.9 examples/sec; 0.260 sec/batch)
2017-05-09 04:01:40.195953: step 36360, loss = 0.1569, acc = 0.9400 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 04:01:44.782979: step 36380, loss = 0.1390, acc = 0.9520 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:01:49.330739: step 36400, loss = 0.1397, acc = 0.9580 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 04:01:53.973422: step 36420, loss = 0.1418, acc = 0.9660 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 04:01:58.651649: step 36440, loss = 0.1400, acc = 0.9500 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 04:02:03.134333: step 36460, loss = 0.1387, acc = 0.9540 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 04:02:07.955990: step 36480, loss = 0.1244, acc = 0.9440 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 04:02:12.516254: step 36500, loss = 0.1452, acc = 0.9500 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 04:02:17.063297: step 36520, loss = 0.1277, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 04:02:21.948117: step 36540, loss = 0.1464, acc = 0.9580 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 04:02:26.546671: step 36560, loss = 0.1316, acc = 0.9540 (299.9 examples/sec; 0.213 sec/batch)
2017-05-09 04:02:31.791690: step 36580, loss = 0.1393, acc = 0.9520 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 04:02:36.624704: step 36600, loss = 0.1470, acc = 0.9540 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:02:41.288738: step 36620, loss = 0.1384, acc = 0.9600 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 04:02:45.952184: step 36640, loss = 0.1374, acc = 0.9540 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 04:02:50.720445: step 36660, loss = 0.1365, acc = 0.9580 (241.5 examples/sec; 0.265 sec/batch)
2017-05-09 04:02:55.241144: step 36680, loss = 0.1462, acc = 0.9540 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 04:02:59.900701: step 36700, loss = 0.1244, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 04:03:04.655696: step 36720, loss = 0.1596, acc = 0.9400 (246.6 examples/sec; 0.259 sec/batch)
2017-05-09 04:03:09.441024: step 36740, loss = 0.1403, acc = 0.9640 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 04:03:14.007796: step 36760, loss = 0.1341, acc = 0.9640 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 04:03:18.647300: step 36780, loss = 0.1253, acc = 0.9560 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 04:03:23.587366: step 36800, loss = 0.1226, acc = 0.9580 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 04:03:28.283983: step 36820, loss = 0.1638, acc = 0.9480 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 04:03:32.957402: step 36840, loss = 0.1535, acc = 0.9540 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 04:03:37.733394: step 36860, loss = 0.1631, acc = 0.9480 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 04:03:42.406453: step 36880, loss = 0.1434, acc = 0.9520 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 04:03:47.025105: step 36900, loss = 0.1255, acc = 0.9640 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 04:03:52.014865: step 36920, loss = 0.1288, acc = 0.9560 (194.6 examples/sec; 0.329 sec/batch)
2017-05-09 04:03:56.596232: step 36940, loss = 0.1526, acc = 0.9480 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 04:04:01.136772: step 36960, loss = 0.1658, acc = 0.9460 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 04:04:06.082445: step 36980, loss = 0.1560, acc = 0.9440 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 04:04:10.707477: step 37000, loss = 0.1297, acc = 0.9620 (263.4 examples/sec; 0.243 sec/batch)
[Eval] 2017-05-09 04:04:24.826019: step 37000, acc = 0.9595, f1 = 0.9583
[Test] 2017-05-09 04:04:34.749721: step 37000, acc = 0.9504, f1 = 0.9500
[Status] 2017-05-09 04:04:34.749811: step 37000, maxindex = 35000, maxdev = 0.9607, maxtst = 0.9524
2017-05-09 04:04:39.333786: step 37020, loss = 0.1222, acc = 0.9660 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 04:04:43.981323: step 37040, loss = 0.1998, acc = 0.9320 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 04:04:48.779104: step 37060, loss = 0.1212, acc = 0.9620 (247.7 examples/sec; 0.258 sec/batch)
2017-05-09 04:04:53.326811: step 37080, loss = 0.1409, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 04:04:58.032345: step 37100, loss = 0.1611, acc = 0.9300 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 04:05:02.687839: step 37120, loss = 0.1540, acc = 0.9500 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 04:05:07.293476: step 37140, loss = 0.1147, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:05:11.859447: step 37160, loss = 0.1466, acc = 0.9520 (300.5 examples/sec; 0.213 sec/batch)
2017-05-09 04:05:16.389133: step 37180, loss = 0.1375, acc = 0.9520 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 04:05:21.031446: step 37200, loss = 0.1337, acc = 0.9580 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 04:05:25.675601: step 37220, loss = 0.1524, acc = 0.9580 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 04:05:30.350986: step 37240, loss = 0.1328, acc = 0.9600 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 04:05:35.047645: step 37260, loss = 0.1387, acc = 0.9580 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 04:05:39.669548: step 37280, loss = 0.1266, acc = 0.9520 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 04:05:45.898265: step 37300, loss = 0.1317, acc = 0.9500 (245.9 examples/sec; 0.260 sec/batch)
2017-05-09 04:05:50.791266: step 37320, loss = 0.1367, acc = 0.9600 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 04:05:55.462924: step 37340, loss = 0.1701, acc = 0.9400 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 04:06:00.109494: step 37360, loss = 0.1409, acc = 0.9540 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 04:06:04.988864: step 37380, loss = 0.1419, acc = 0.9500 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 04:06:09.509714: step 37400, loss = 0.1343, acc = 0.9580 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 04:06:14.257429: step 37420, loss = 0.1591, acc = 0.9500 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 04:06:18.870808: step 37440, loss = 0.1292, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 04:06:23.675973: step 37460, loss = 0.1427, acc = 0.9620 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:06:28.229371: step 37480, loss = 0.1134, acc = 0.9640 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 04:06:32.937951: step 37500, loss = 0.1259, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 04:06:37.597812: step 37520, loss = 0.1482, acc = 0.9480 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 04:06:42.237063: step 37540, loss = 0.1406, acc = 0.9620 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 04:06:46.922021: step 37560, loss = 0.1324, acc = 0.9580 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 04:06:51.593451: step 37580, loss = 0.1309, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:06:56.230498: step 37600, loss = 0.1451, acc = 0.9480 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:07:00.792333: step 37620, loss = 0.1270, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 04:07:05.495821: step 37640, loss = 0.1045, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 04:07:10.139752: step 37660, loss = 0.1082, acc = 0.9640 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 04:07:14.683912: step 37680, loss = 0.1347, acc = 0.9560 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 04:07:19.490545: step 37700, loss = 0.1421, acc = 0.9560 (238.4 examples/sec; 0.268 sec/batch)
2017-05-09 04:07:24.103073: step 37720, loss = 0.1355, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 04:07:28.820877: step 37740, loss = 0.1558, acc = 0.9480 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 04:07:33.332519: step 37760, loss = 0.1706, acc = 0.9380 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 04:07:38.103936: step 37780, loss = 0.1281, acc = 0.9500 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 04:07:42.767335: step 37800, loss = 0.1377, acc = 0.9520 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 04:07:47.463764: step 37820, loss = 0.1301, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 04:07:52.180304: step 37840, loss = 0.1105, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 04:07:56.837947: step 37860, loss = 0.1177, acc = 0.9640 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 04:08:01.335569: step 37880, loss = 0.1504, acc = 0.9480 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 04:08:06.095991: step 37900, loss = 0.1379, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 04:08:10.736084: step 37920, loss = 0.1093, acc = 0.9720 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 04:08:15.248157: step 37940, loss = 0.1506, acc = 0.9420 (295.6 examples/sec; 0.216 sec/batch)
2017-05-09 04:08:20.017511: step 37960, loss = 0.1691, acc = 0.9400 (236.8 examples/sec; 0.270 sec/batch)
2017-05-09 04:08:24.699625: step 37980, loss = 0.1621, acc = 0.9440 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 04:08:29.248767: step 38000, loss = 0.1497, acc = 0.9600 (284.1 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 04:08:43.235463: step 38000, acc = 0.9611, f1 = 0.9599
[Test] 2017-05-09 04:08:53.145153: step 38000, acc = 0.9522, f1 = 0.9518
[Status] 2017-05-09 04:08:53.145259: step 38000, maxindex = 38000, maxdev = 0.9611, maxtst = 0.9522
2017-05-09 04:09:01.284556: step 38020, loss = 0.1255, acc = 0.9620 (228.6 examples/sec; 0.280 sec/batch)
2017-05-09 04:09:06.046326: step 38040, loss = 0.1400, acc = 0.9480 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:09:10.663806: step 38060, loss = 0.1411, acc = 0.9560 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 04:09:15.248946: step 38080, loss = 0.1255, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 04:09:20.032940: step 38100, loss = 0.1208, acc = 0.9600 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 04:09:24.616734: step 38120, loss = 0.1351, acc = 0.9520 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 04:09:29.434787: step 38140, loss = 0.1135, acc = 0.9620 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 04:09:34.153881: step 38160, loss = 0.1056, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 04:09:38.899178: step 38180, loss = 0.1528, acc = 0.9420 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 04:09:43.495661: step 38200, loss = 0.1484, acc = 0.9520 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:09:48.386061: step 38220, loss = 0.1457, acc = 0.9560 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 04:09:52.858480: step 38240, loss = 0.1277, acc = 0.9580 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 04:09:57.401505: step 38260, loss = 0.1706, acc = 0.9520 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 04:10:02.143405: step 38280, loss = 0.1320, acc = 0.9560 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 04:10:07.249692: step 38300, loss = 0.1271, acc = 0.9540 (161.3 examples/sec; 0.397 sec/batch)
2017-05-09 04:10:11.914071: step 38320, loss = 0.1391, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 04:10:16.602710: step 38340, loss = 0.1268, acc = 0.9580 (254.6 examples/sec; 0.251 sec/batch)
2017-05-09 04:10:21.223054: step 38360, loss = 0.1286, acc = 0.9460 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 04:10:25.839215: step 38380, loss = 0.1378, acc = 0.9540 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 04:10:30.549005: step 38400, loss = 0.1403, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 04:10:35.256333: step 38420, loss = 0.1170, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 04:10:39.913781: step 38440, loss = 0.1109, acc = 0.9640 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:10:44.496709: step 38460, loss = 0.1314, acc = 0.9560 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 04:10:49.204136: step 38480, loss = 0.1341, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 04:10:53.724418: step 38500, loss = 0.1303, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 04:10:58.292327: step 38520, loss = 0.1598, acc = 0.9420 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 04:11:03.245057: step 38540, loss = 0.1129, acc = 0.9580 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 04:11:07.785303: step 38560, loss = 0.1686, acc = 0.9440 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 04:11:12.343842: step 38580, loss = 0.1461, acc = 0.9480 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 04:11:17.102694: step 38600, loss = 0.1106, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 04:11:21.735406: step 38620, loss = 0.1403, acc = 0.9540 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 04:11:26.297272: step 38640, loss = 0.1150, acc = 0.9620 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 04:11:30.941820: step 38660, loss = 0.1620, acc = 0.9500 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 04:11:35.566397: step 38680, loss = 0.1134, acc = 0.9700 (254.7 examples/sec; 0.251 sec/batch)
2017-05-09 04:11:40.129902: step 38700, loss = 0.1151, acc = 0.9660 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 04:11:44.676171: step 38720, loss = 0.1533, acc = 0.9600 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 04:11:49.384144: step 38740, loss = 0.1473, acc = 0.9420 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 04:11:54.050843: step 38760, loss = 0.1499, acc = 0.9480 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 04:11:58.613573: step 38780, loss = 0.1225, acc = 0.9600 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 04:12:03.361178: step 38800, loss = 0.1198, acc = 0.9600 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 04:12:07.947420: step 38820, loss = 0.1683, acc = 0.9360 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 04:12:12.636622: step 38840, loss = 0.2018, acc = 0.9260 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:12:17.361699: step 38860, loss = 0.1440, acc = 0.9520 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 04:12:21.981665: step 38880, loss = 0.1481, acc = 0.9460 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 04:12:26.606131: step 38900, loss = 0.1350, acc = 0.9700 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 04:12:31.833529: step 38920, loss = 0.1496, acc = 0.9480 (227.2 examples/sec; 0.282 sec/batch)
2017-05-09 04:12:36.478370: step 38940, loss = 0.1218, acc = 0.9660 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 04:12:41.075037: step 38960, loss = 0.1274, acc = 0.9600 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 04:12:45.818776: step 38980, loss = 0.1228, acc = 0.9660 (239.5 examples/sec; 0.267 sec/batch)
2017-05-09 04:12:50.557246: step 39000, loss = 0.1356, acc = 0.9560 (269.4 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 04:13:04.606294: step 39000, acc = 0.9590, f1 = 0.9577
[Test] 2017-05-09 04:13:13.942348: step 39000, acc = 0.9498, f1 = 0.9493
[Status] 2017-05-09 04:13:13.942438: step 39000, maxindex = 38000, maxdev = 0.9611, maxtst = 0.9522
2017-05-09 04:13:18.680124: step 39020, loss = 0.1414, acc = 0.9520 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 04:13:23.226019: step 39040, loss = 0.1423, acc = 0.9480 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 04:13:27.791294: step 39060, loss = 0.1419, acc = 0.9540 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 04:13:32.497705: step 39080, loss = 0.1401, acc = 0.9520 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:13:37.085277: step 39100, loss = 0.1119, acc = 0.9740 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 04:13:41.678394: step 39120, loss = 0.1108, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 04:13:46.396647: step 39140, loss = 0.1279, acc = 0.9580 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:13:51.045982: step 39160, loss = 0.1373, acc = 0.9620 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 04:13:55.641175: step 39180, loss = 0.1426, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 04:14:00.357716: step 39200, loss = 0.1080, acc = 0.9660 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 04:14:04.906516: step 39220, loss = 0.1358, acc = 0.9560 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 04:14:09.503620: step 39240, loss = 0.1139, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 04:14:14.077843: step 39260, loss = 0.1145, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 04:14:18.825878: step 39280, loss = 0.1343, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 04:14:23.411681: step 39300, loss = 0.1549, acc = 0.9440 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 04:14:29.037162: step 39320, loss = 0.1086, acc = 0.9740 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 04:14:33.746995: step 39340, loss = 0.1004, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 04:14:38.347937: step 39360, loss = 0.1105, acc = 0.9720 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 04:14:42.959981: step 39380, loss = 0.1435, acc = 0.9500 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 04:14:47.628705: step 39400, loss = 0.1229, acc = 0.9580 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 04:14:52.153238: step 39420, loss = 0.1278, acc = 0.9600 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 04:14:56.819635: step 39440, loss = 0.1127, acc = 0.9680 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 04:15:01.640100: step 39460, loss = 0.0985, acc = 0.9740 (229.1 examples/sec; 0.279 sec/batch)
2017-05-09 04:15:06.234502: step 39480, loss = 0.1412, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 04:15:10.861845: step 39500, loss = 0.1400, acc = 0.9560 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 04:15:15.529984: step 39520, loss = 0.1379, acc = 0.9460 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 04:15:20.560709: step 39540, loss = 0.1417, acc = 0.9440 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 04:15:25.223711: step 39560, loss = 0.1264, acc = 0.9580 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 04:15:29.867535: step 39580, loss = 0.1483, acc = 0.9400 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 04:15:34.548586: step 39600, loss = 0.1291, acc = 0.9500 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 04:15:39.053232: step 39620, loss = 0.1479, acc = 0.9540 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 04:15:43.624026: step 39640, loss = 0.1496, acc = 0.9500 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 04:15:48.360121: step 39660, loss = 0.1130, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:15:52.867000: step 39680, loss = 0.1313, acc = 0.9540 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:15:57.335818: step 39700, loss = 0.1437, acc = 0.9460 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 04:16:01.928408: step 39720, loss = 0.1306, acc = 0.9560 (308.3 examples/sec; 0.208 sec/batch)
2017-05-09 04:16:06.440593: step 39740, loss = 0.1165, acc = 0.9660 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:16:10.935373: step 39760, loss = 0.1423, acc = 0.9580 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 04:16:15.497294: step 39780, loss = 0.1341, acc = 0.9600 (256.5 examples/sec; 0.249 sec/batch)
2017-05-09 04:16:20.114616: step 39800, loss = 0.1295, acc = 0.9520 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:16:24.601446: step 39820, loss = 0.1198, acc = 0.9600 (297.0 examples/sec; 0.216 sec/batch)
2017-05-09 04:16:29.331234: step 39840, loss = 0.1149, acc = 0.9660 (250.9 examples/sec; 0.255 sec/batch)
2017-05-09 04:16:33.997243: step 39860, loss = 0.1202, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 04:16:38.700577: step 39880, loss = 0.1363, acc = 0.9560 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 04:16:43.270336: step 39900, loss = 0.1322, acc = 0.9500 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 04:16:47.944067: step 39920, loss = 0.1344, acc = 0.9480 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 04:16:52.471333: step 39940, loss = 0.1372, acc = 0.9580 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 04:16:57.071597: step 39960, loss = 0.1350, acc = 0.9660 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 04:17:01.833497: step 39980, loss = 0.1464, acc = 0.9560 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 04:17:06.419264: step 40000, loss = 0.1075, acc = 0.9660 (273.5 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 04:17:20.362131: step 40000, acc = 0.9614, f1 = 0.9602
[Test] 2017-05-09 04:17:30.382749: step 40000, acc = 0.9523, f1 = 0.9520
[Status] 2017-05-09 04:17:30.382834: step 40000, maxindex = 40000, maxdev = 0.9614, maxtst = 0.9523
2017-05-09 04:17:38.300591: step 40020, loss = 0.1083, acc = 0.9700 (299.0 examples/sec; 0.214 sec/batch)
2017-05-09 04:17:42.748376: step 40040, loss = 0.1430, acc = 0.9600 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 04:17:47.364267: step 40060, loss = 0.1495, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 04:17:52.030953: step 40080, loss = 0.1566, acc = 0.9480 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 04:17:56.613429: step 40100, loss = 0.1394, acc = 0.9520 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 04:18:01.235233: step 40120, loss = 0.1308, acc = 0.9580 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 04:18:05.785451: step 40140, loss = 0.1213, acc = 0.9660 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 04:18:10.414572: step 40160, loss = 0.1173, acc = 0.9660 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 04:18:15.249029: step 40180, loss = 0.1533, acc = 0.9540 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 04:18:19.801063: step 40200, loss = 0.1361, acc = 0.9560 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:18:24.344565: step 40220, loss = 0.1450, acc = 0.9480 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 04:18:29.049491: step 40240, loss = 0.1291, acc = 0.9580 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 04:18:33.610431: step 40260, loss = 0.1274, acc = 0.9620 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 04:18:38.217970: step 40280, loss = 0.1224, acc = 0.9580 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 04:18:42.763214: step 40300, loss = 0.1299, acc = 0.9580 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 04:18:48.235901: step 40320, loss = 0.1087, acc = 0.9720 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 04:18:52.741629: step 40340, loss = 0.1240, acc = 0.9660 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 04:18:57.582519: step 40360, loss = 0.1572, acc = 0.9520 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 04:19:02.491320: step 40380, loss = 0.1510, acc = 0.9480 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 04:19:07.085490: step 40400, loss = 0.1116, acc = 0.9660 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 04:19:11.711553: step 40420, loss = 0.1335, acc = 0.9620 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:19:16.386518: step 40440, loss = 0.1451, acc = 0.9500 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:19:20.955545: step 40460, loss = 0.1549, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 04:19:25.571676: step 40480, loss = 0.1681, acc = 0.9520 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 04:19:30.323637: step 40500, loss = 0.1665, acc = 0.9340 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 04:19:35.019856: step 40520, loss = 0.1206, acc = 0.9580 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 04:19:39.649154: step 40540, loss = 0.1419, acc = 0.9540 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 04:19:44.539495: step 40560, loss = 0.1328, acc = 0.9640 (208.2 examples/sec; 0.307 sec/batch)
2017-05-09 04:19:49.050620: step 40580, loss = 0.1240, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 04:19:53.574926: step 40600, loss = 0.1249, acc = 0.9560 (292.9 examples/sec; 0.219 sec/batch)
2017-05-09 04:19:58.204296: step 40620, loss = 0.1131, acc = 0.9660 (244.4 examples/sec; 0.262 sec/batch)
2017-05-09 04:20:02.750616: step 40640, loss = 0.1219, acc = 0.9660 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 04:20:07.453361: step 40660, loss = 0.1492, acc = 0.9600 (236.7 examples/sec; 0.270 sec/batch)
2017-05-09 04:20:12.144930: step 40680, loss = 0.1375, acc = 0.9480 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 04:20:16.865134: step 40700, loss = 0.1247, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 04:20:21.569280: step 40720, loss = 0.1161, acc = 0.9700 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 04:20:26.229591: step 40740, loss = 0.1178, acc = 0.9680 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 04:20:31.053884: step 40760, loss = 0.1275, acc = 0.9640 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 04:20:35.622860: step 40780, loss = 0.1154, acc = 0.9660 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:20:40.076947: step 40800, loss = 0.1514, acc = 0.9480 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 04:20:44.741942: step 40820, loss = 0.1238, acc = 0.9520 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 04:20:49.316334: step 40840, loss = 0.1225, acc = 0.9700 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 04:20:53.980556: step 40860, loss = 0.1567, acc = 0.9560 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 04:20:58.521770: step 40880, loss = 0.1225, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 04:21:03.360547: step 40900, loss = 0.1437, acc = 0.9500 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:21:07.898650: step 40920, loss = 0.1293, acc = 0.9640 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 04:21:12.478727: step 40940, loss = 0.1602, acc = 0.9380 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 04:21:17.199293: step 40960, loss = 0.1311, acc = 0.9560 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 04:21:21.821306: step 40980, loss = 0.1487, acc = 0.9640 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 04:21:26.416318: step 41000, loss = 0.1221, acc = 0.9620 (269.0 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 04:21:40.364441: step 41000, acc = 0.9586, f1 = 0.9572
[Test] 2017-05-09 04:21:49.961473: step 41000, acc = 0.9491, f1 = 0.9486
[Status] 2017-05-09 04:21:49.961545: step 41000, maxindex = 40000, maxdev = 0.9614, maxtst = 0.9523
2017-05-09 04:21:54.597094: step 41020, loss = 0.1278, acc = 0.9600 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 04:21:59.354228: step 41040, loss = 0.1261, acc = 0.9720 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 04:22:04.061186: step 41060, loss = 0.1658, acc = 0.9460 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 04:22:08.671269: step 41080, loss = 0.1274, acc = 0.9620 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 04:22:13.346757: step 41100, loss = 0.1407, acc = 0.9540 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 04:22:18.096732: step 41120, loss = 0.1375, acc = 0.9600 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 04:22:22.757357: step 41140, loss = 0.1593, acc = 0.9440 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 04:22:27.495181: step 41160, loss = 0.1255, acc = 0.9700 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 04:22:32.001379: step 41180, loss = 0.1198, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:22:36.546623: step 41200, loss = 0.0851, acc = 0.9820 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 04:22:41.281362: step 41220, loss = 0.1404, acc = 0.9520 (303.1 examples/sec; 0.211 sec/batch)
2017-05-09 04:22:45.810799: step 41240, loss = 0.1136, acc = 0.9740 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 04:22:50.407516: step 41260, loss = 0.1439, acc = 0.9540 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 04:22:54.886299: step 41280, loss = 0.0902, acc = 0.9860 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 04:22:59.621292: step 41300, loss = 0.1381, acc = 0.9520 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 04:23:04.186614: step 41320, loss = 0.1213, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 04:23:09.990609: step 41340, loss = 0.1137, acc = 0.9660 (237.7 examples/sec; 0.269 sec/batch)
2017-05-09 04:23:14.440947: step 41360, loss = 0.1444, acc = 0.9540 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 04:23:19.145143: step 41380, loss = 0.1362, acc = 0.9600 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 04:23:23.966606: step 41400, loss = 0.0943, acc = 0.9800 (232.3 examples/sec; 0.275 sec/batch)
2017-05-09 04:23:28.612140: step 41420, loss = 0.1433, acc = 0.9520 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 04:23:33.161024: step 41440, loss = 0.1474, acc = 0.9520 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 04:23:37.851742: step 41460, loss = 0.1224, acc = 0.9620 (252.1 examples/sec; 0.254 sec/batch)
2017-05-09 04:23:42.689294: step 41480, loss = 0.1497, acc = 0.9440 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 04:23:47.273848: step 41500, loss = 0.1455, acc = 0.9560 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 04:23:51.773327: step 41520, loss = 0.1064, acc = 0.9620 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 04:23:56.598031: step 41540, loss = 0.1237, acc = 0.9640 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 04:24:01.233977: step 41560, loss = 0.1287, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 04:24:05.876531: step 41580, loss = 0.1467, acc = 0.9460 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 04:24:10.596621: step 41600, loss = 0.1208, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 04:24:15.214551: step 41620, loss = 0.1291, acc = 0.9540 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 04:24:19.800864: step 41640, loss = 0.1098, acc = 0.9700 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 04:24:24.701655: step 41660, loss = 0.1578, acc = 0.9500 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:24:29.346467: step 41680, loss = 0.1443, acc = 0.9560 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 04:24:33.973659: step 41700, loss = 0.1443, acc = 0.9460 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 04:24:38.765368: step 41720, loss = 0.1220, acc = 0.9660 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 04:24:43.452172: step 41740, loss = 0.1497, acc = 0.9500 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 04:24:47.915532: step 41760, loss = 0.1554, acc = 0.9460 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 04:24:52.794527: step 41780, loss = 0.1281, acc = 0.9640 (225.7 examples/sec; 0.284 sec/batch)
2017-05-09 04:24:57.331832: step 41800, loss = 0.1351, acc = 0.9680 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 04:25:01.811080: step 41820, loss = 0.1041, acc = 0.9700 (298.6 examples/sec; 0.214 sec/batch)
2017-05-09 04:25:06.319303: step 41840, loss = 0.1556, acc = 0.9560 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 04:25:11.024616: step 41860, loss = 0.1285, acc = 0.9600 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 04:25:15.602919: step 41880, loss = 0.1237, acc = 0.9600 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 04:25:20.095037: step 41900, loss = 0.1346, acc = 0.9460 (296.3 examples/sec; 0.216 sec/batch)
2017-05-09 04:25:24.887221: step 41920, loss = 0.1157, acc = 0.9720 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 04:25:29.460462: step 41940, loss = 0.1280, acc = 0.9480 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 04:25:34.027408: step 41960, loss = 0.1577, acc = 0.9540 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 04:25:38.737354: step 41980, loss = 0.1648, acc = 0.9460 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:25:43.484123: step 42000, loss = 0.1450, acc = 0.9480 (255.8 examples/sec; 0.250 sec/batch)
[Eval] 2017-05-09 04:25:57.494605: step 42000, acc = 0.9587, f1 = 0.9573
[Test] 2017-05-09 04:26:07.110703: step 42000, acc = 0.9494, f1 = 0.9490
[Status] 2017-05-09 04:26:07.110783: step 42000, maxindex = 40000, maxdev = 0.9614, maxtst = 0.9523
2017-05-09 04:26:11.802052: step 42020, loss = 0.1260, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:26:16.435705: step 42040, loss = 0.1277, acc = 0.9600 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 04:26:21.199834: step 42060, loss = 0.1144, acc = 0.9620 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 04:26:25.633318: step 42080, loss = 0.1459, acc = 0.9520 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 04:26:30.212149: step 42100, loss = 0.1271, acc = 0.9600 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:26:35.322405: step 42120, loss = 0.0945, acc = 0.9700 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 04:26:39.913682: step 42140, loss = 0.1451, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 04:26:44.511527: step 42160, loss = 0.1055, acc = 0.9680 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 04:26:49.178978: step 42180, loss = 0.1175, acc = 0.9580 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 04:26:53.745446: step 42200, loss = 0.1764, acc = 0.9300 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 04:26:58.433859: step 42220, loss = 0.1650, acc = 0.9440 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 04:27:03.131306: step 42240, loss = 0.1123, acc = 0.9720 (243.1 examples/sec; 0.263 sec/batch)
2017-05-09 04:27:07.679758: step 42260, loss = 0.1286, acc = 0.9700 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 04:27:12.197504: step 42280, loss = 0.1409, acc = 0.9580 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 04:27:16.722506: step 42300, loss = 0.1200, acc = 0.9720 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 04:27:21.553363: step 42320, loss = 0.1243, acc = 0.9660 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 04:27:26.823012: step 42340, loss = 0.1389, acc = 0.9440 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 04:27:31.736748: step 42360, loss = 0.1203, acc = 0.9620 (201.4 examples/sec; 0.318 sec/batch)
2017-05-09 04:27:36.488499: step 42380, loss = 0.1368, acc = 0.9460 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 04:27:41.106805: step 42400, loss = 0.1283, acc = 0.9700 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 04:27:45.639216: step 42420, loss = 0.1127, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 04:27:50.546881: step 42440, loss = 0.1346, acc = 0.9580 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 04:27:55.163570: step 42460, loss = 0.1351, acc = 0.9640 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 04:27:59.770007: step 42480, loss = 0.1260, acc = 0.9680 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 04:28:04.341263: step 42500, loss = 0.1204, acc = 0.9660 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 04:28:08.925496: step 42520, loss = 0.1416, acc = 0.9500 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 04:28:13.621380: step 42540, loss = 0.1198, acc = 0.9620 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 04:28:18.306388: step 42560, loss = 0.1191, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 04:28:23.049388: step 42580, loss = 0.1439, acc = 0.9540 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 04:28:27.809580: step 42600, loss = 0.1274, acc = 0.9560 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 04:28:32.753803: step 42620, loss = 0.1079, acc = 0.9660 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 04:28:37.279632: step 42640, loss = 0.1333, acc = 0.9520 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 04:28:41.825500: step 42660, loss = 0.1190, acc = 0.9560 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 04:28:46.499306: step 42680, loss = 0.1099, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 04:28:51.163206: step 42700, loss = 0.1531, acc = 0.9440 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 04:28:55.865571: step 42720, loss = 0.1303, acc = 0.9560 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 04:29:00.859261: step 42740, loss = 0.1413, acc = 0.9500 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 04:29:05.356740: step 42760, loss = 0.1133, acc = 0.9620 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 04:29:09.863147: step 42780, loss = 0.1430, acc = 0.9620 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 04:29:14.528045: step 42800, loss = 0.1469, acc = 0.9540 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 04:29:19.414104: step 42820, loss = 0.1527, acc = 0.9520 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 04:29:23.918335: step 42840, loss = 0.1481, acc = 0.9480 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:29:28.574739: step 42860, loss = 0.1297, acc = 0.9620 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 04:29:33.231724: step 42880, loss = 0.1553, acc = 0.9460 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:29:37.799384: step 42900, loss = 0.1224, acc = 0.9600 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 04:29:42.419813: step 42920, loss = 0.1055, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 04:29:47.275979: step 42940, loss = 0.1321, acc = 0.9600 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 04:29:51.712005: step 42960, loss = 0.1360, acc = 0.9480 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 04:29:56.165397: step 42980, loss = 0.1529, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 04:30:00.801290: step 43000, loss = 0.1386, acc = 0.9460 (278.1 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 04:30:14.792923: step 43000, acc = 0.9614, f1 = 0.9601
[Test] 2017-05-09 04:30:24.327503: step 43000, acc = 0.9532, f1 = 0.9529
[Status] 2017-05-09 04:30:24.327599: step 43000, maxindex = 40000, maxdev = 0.9614, maxtst = 0.9523
2017-05-09 04:30:29.156074: step 43020, loss = 0.1397, acc = 0.9560 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 04:30:34.011422: step 43040, loss = 0.1385, acc = 0.9620 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 04:30:38.598370: step 43060, loss = 0.1223, acc = 0.9560 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 04:30:43.314345: step 43080, loss = 0.1135, acc = 0.9580 (242.2 examples/sec; 0.264 sec/batch)
2017-05-09 04:30:47.776024: step 43100, loss = 0.1416, acc = 0.9560 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 04:30:52.268553: step 43120, loss = 0.1531, acc = 0.9460 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 04:30:56.855646: step 43140, loss = 0.1305, acc = 0.9560 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 04:31:01.506428: step 43160, loss = 0.1477, acc = 0.9520 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 04:31:06.117275: step 43180, loss = 0.1491, acc = 0.9560 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 04:31:10.757771: step 43200, loss = 0.1500, acc = 0.9520 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 04:31:15.417460: step 43220, loss = 0.1256, acc = 0.9580 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 04:31:19.971046: step 43240, loss = 0.1258, acc = 0.9620 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 04:31:24.601021: step 43260, loss = 0.1155, acc = 0.9560 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:31:29.398150: step 43280, loss = 0.1636, acc = 0.9300 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 04:31:33.974691: step 43300, loss = 0.1389, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 04:31:38.521183: step 43320, loss = 0.1222, acc = 0.9620 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:31:44.226141: step 43340, loss = 0.1278, acc = 0.9580 (130.8 examples/sec; 0.489 sec/batch)
2017-05-09 04:31:48.780459: step 43360, loss = 0.1137, acc = 0.9640 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 04:31:53.290584: step 43380, loss = 0.1345, acc = 0.9580 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 04:31:57.989068: step 43400, loss = 0.1029, acc = 0.9680 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 04:32:02.576723: step 43420, loss = 0.1147, acc = 0.9700 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 04:32:07.247531: step 43440, loss = 0.1635, acc = 0.9400 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 04:32:12.651015: step 43460, loss = 0.1153, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:32:17.272940: step 43480, loss = 0.1369, acc = 0.9480 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 04:32:21.921374: step 43500, loss = 0.1146, acc = 0.9580 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:32:26.711131: step 43520, loss = 0.1560, acc = 0.9600 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 04:32:31.502007: step 43540, loss = 0.1315, acc = 0.9660 (218.3 examples/sec; 0.293 sec/batch)
2017-05-09 04:32:36.592672: step 43560, loss = 0.1344, acc = 0.9580 (176.9 examples/sec; 0.362 sec/batch)
2017-05-09 04:32:41.427695: step 43580, loss = 0.1377, acc = 0.9600 (252.8 examples/sec; 0.253 sec/batch)
2017-05-09 04:32:46.128619: step 43600, loss = 0.1159, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 04:32:50.620759: step 43620, loss = 0.1004, acc = 0.9800 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 04:32:55.222365: step 43640, loss = 0.1385, acc = 0.9520 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 04:32:59.972075: step 43660, loss = 0.1237, acc = 0.9640 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 04:33:04.713339: step 43680, loss = 0.1228, acc = 0.9620 (258.3 examples/sec; 0.248 sec/batch)
2017-05-09 04:33:09.376569: step 43700, loss = 0.1752, acc = 0.9420 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:33:14.252834: step 43720, loss = 0.1572, acc = 0.9520 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 04:33:18.880557: step 43740, loss = 0.1135, acc = 0.9660 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 04:33:23.371214: step 43760, loss = 0.1280, acc = 0.9640 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 04:33:28.076589: step 43780, loss = 0.1098, acc = 0.9640 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 04:33:32.534445: step 43800, loss = 0.1368, acc = 0.9580 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 04:33:37.067536: step 43820, loss = 0.1486, acc = 0.9480 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 04:33:41.897642: step 43840, loss = 0.1367, acc = 0.9580 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 04:33:46.549605: step 43860, loss = 0.1181, acc = 0.9660 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 04:33:51.168701: step 43880, loss = 0.1192, acc = 0.9620 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 04:33:56.024309: step 43900, loss = 0.1113, acc = 0.9620 (224.3 examples/sec; 0.285 sec/batch)
2017-05-09 04:34:00.598968: step 43920, loss = 0.1453, acc = 0.9580 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 04:34:05.228259: step 43940, loss = 0.1740, acc = 0.9400 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 04:34:10.528936: step 43960, loss = 0.1294, acc = 0.9580 (237.0 examples/sec; 0.270 sec/batch)
2017-05-09 04:34:15.185854: step 43980, loss = 0.1242, acc = 0.9560 (268.3 examples/sec; 0.238 sec/batch)
2017-05-09 04:34:19.729694: step 44000, loss = 0.1164, acc = 0.9680 (279.3 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 04:34:34.149141: step 44000, acc = 0.9603, f1 = 0.9590
[Test] 2017-05-09 04:34:43.914613: step 44000, acc = 0.9516, f1 = 0.9512
[Status] 2017-05-09 04:34:43.914722: step 44000, maxindex = 40000, maxdev = 0.9614, maxtst = 0.9523
2017-05-09 04:34:48.427891: step 44020, loss = 0.1637, acc = 0.9480 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:34:52.934617: step 44040, loss = 0.1412, acc = 0.9540 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 04:34:57.573651: step 44060, loss = 0.1210, acc = 0.9600 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 04:35:02.262896: step 44080, loss = 0.1285, acc = 0.9540 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 04:35:06.871966: step 44100, loss = 0.1267, acc = 0.9540 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:35:11.639172: step 44120, loss = 0.1208, acc = 0.9640 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:35:16.523030: step 44140, loss = 0.1232, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 04:35:21.153856: step 44160, loss = 0.1268, acc = 0.9520 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 04:35:25.869011: step 44180, loss = 0.1339, acc = 0.9540 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 04:35:30.615799: step 44200, loss = 0.1426, acc = 0.9520 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 04:35:35.240214: step 44220, loss = 0.1307, acc = 0.9560 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 04:35:39.990207: step 44240, loss = 0.1361, acc = 0.9560 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 04:35:44.690661: step 44260, loss = 0.1374, acc = 0.9540 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 04:35:49.475691: step 44280, loss = 0.1271, acc = 0.9580 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 04:35:54.132298: step 44300, loss = 0.1225, acc = 0.9620 (244.8 examples/sec; 0.261 sec/batch)
2017-05-09 04:35:58.649913: step 44320, loss = 0.1356, acc = 0.9480 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 04:36:03.371516: step 44340, loss = 0.1338, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 04:36:08.840761: step 44360, loss = 0.1469, acc = 0.9400 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 04:36:13.379824: step 44380, loss = 0.1141, acc = 0.9640 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 04:36:18.077527: step 44400, loss = 0.1157, acc = 0.9660 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:36:22.798872: step 44420, loss = 0.1145, acc = 0.9600 (265.0 examples/sec; 0.241 sec/batch)
2017-05-09 04:36:27.652671: step 44440, loss = 0.1364, acc = 0.9580 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 04:36:32.244317: step 44460, loss = 0.1182, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:36:36.866797: step 44480, loss = 0.1296, acc = 0.9540 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 04:36:41.564422: step 44500, loss = 0.1254, acc = 0.9660 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 04:36:46.235061: step 44520, loss = 0.1272, acc = 0.9560 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 04:36:50.883558: step 44540, loss = 0.1372, acc = 0.9620 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 04:36:55.680453: step 44560, loss = 0.1311, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 04:37:00.289873: step 44580, loss = 0.1287, acc = 0.9620 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 04:37:04.820871: step 44600, loss = 0.1001, acc = 0.9760 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 04:37:09.673872: step 44620, loss = 0.1161, acc = 0.9720 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 04:37:14.360675: step 44640, loss = 0.1334, acc = 0.9600 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 04:37:18.973863: step 44660, loss = 0.1194, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 04:37:23.679448: step 44680, loss = 0.1281, acc = 0.9480 (229.1 examples/sec; 0.279 sec/batch)
2017-05-09 04:37:28.261176: step 44700, loss = 0.1347, acc = 0.9640 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:37:32.810007: step 44720, loss = 0.0959, acc = 0.9780 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 04:37:37.443287: step 44740, loss = 0.1443, acc = 0.9640 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 04:37:42.165909: step 44760, loss = 0.1499, acc = 0.9560 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 04:37:46.768293: step 44780, loss = 0.1475, acc = 0.9520 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 04:37:51.412448: step 44800, loss = 0.1106, acc = 0.9740 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 04:37:56.179846: step 44820, loss = 0.1308, acc = 0.9500 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 04:38:00.875684: step 44840, loss = 0.1439, acc = 0.9500 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 04:38:05.422110: step 44860, loss = 0.1056, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 04:38:10.194819: step 44880, loss = 0.1234, acc = 0.9620 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 04:38:14.662826: step 44900, loss = 0.1119, acc = 0.9760 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 04:38:19.337976: step 44920, loss = 0.1050, acc = 0.9760 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 04:38:24.259893: step 44940, loss = 0.1160, acc = 0.9620 (215.5 examples/sec; 0.297 sec/batch)
2017-05-09 04:38:28.866417: step 44960, loss = 0.1048, acc = 0.9740 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 04:38:33.554405: step 44980, loss = 0.1135, acc = 0.9580 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 04:38:38.163538: step 45000, loss = 0.1110, acc = 0.9660 (278.3 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 04:38:52.320501: step 45000, acc = 0.9609, f1 = 0.9596
[Test] 2017-05-09 04:39:02.008442: step 45000, acc = 0.9521, f1 = 0.9517
[Status] 2017-05-09 04:39:02.008510: step 45000, maxindex = 40000, maxdev = 0.9614, maxtst = 0.9523
2017-05-09 04:39:06.587685: step 45020, loss = 0.1361, acc = 0.9540 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 04:39:11.290495: step 45040, loss = 0.1054, acc = 0.9680 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 04:39:15.704607: step 45060, loss = 0.1291, acc = 0.9580 (292.9 examples/sec; 0.218 sec/batch)
2017-05-09 04:39:20.335231: step 45080, loss = 0.1261, acc = 0.9580 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 04:39:25.050655: step 45100, loss = 0.1248, acc = 0.9580 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 04:39:29.720727: step 45120, loss = 0.1707, acc = 0.9420 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 04:39:34.351400: step 45140, loss = 0.1181, acc = 0.9600 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:39:39.342874: step 45160, loss = 0.1408, acc = 0.9600 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 04:39:43.992504: step 45180, loss = 0.1322, acc = 0.9480 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 04:39:48.720427: step 45200, loss = 0.1434, acc = 0.9440 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 04:39:53.559539: step 45220, loss = 0.1133, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:39:58.011224: step 45240, loss = 0.1222, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 04:40:02.639477: step 45260, loss = 0.1478, acc = 0.9480 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 04:40:07.681352: step 45280, loss = 0.1131, acc = 0.9660 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 04:40:12.296338: step 45300, loss = 0.1500, acc = 0.9600 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 04:40:16.832265: step 45320, loss = 0.1587, acc = 0.9480 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 04:40:21.364490: step 45340, loss = 0.1049, acc = 0.9720 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 04:40:27.042990: step 45360, loss = 0.1422, acc = 0.9560 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 04:40:31.606408: step 45380, loss = 0.1190, acc = 0.9560 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 04:40:36.216802: step 45400, loss = 0.1183, acc = 0.9600 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 04:40:40.872141: step 45420, loss = 0.1060, acc = 0.9760 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 04:40:45.572020: step 45440, loss = 0.0992, acc = 0.9780 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 04:40:50.173969: step 45460, loss = 0.1470, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 04:40:54.840652: step 45480, loss = 0.1605, acc = 0.9400 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 04:40:59.396313: step 45500, loss = 0.1028, acc = 0.9680 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 04:41:03.982105: step 45520, loss = 0.1291, acc = 0.9540 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 04:41:08.781824: step 45540, loss = 0.1167, acc = 0.9620 (253.0 examples/sec; 0.253 sec/batch)
2017-05-09 04:41:13.384412: step 45560, loss = 0.1235, acc = 0.9700 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 04:41:17.875085: step 45580, loss = 0.1241, acc = 0.9580 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 04:41:22.507356: step 45600, loss = 0.1194, acc = 0.9580 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:41:27.367379: step 45620, loss = 0.1543, acc = 0.9400 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 04:41:31.874554: step 45640, loss = 0.1262, acc = 0.9600 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:41:36.490314: step 45660, loss = 0.1197, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 04:41:41.401265: step 45680, loss = 0.1210, acc = 0.9580 (255.5 examples/sec; 0.251 sec/batch)
2017-05-09 04:41:46.080284: step 45700, loss = 0.1345, acc = 0.9540 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 04:41:50.705594: step 45720, loss = 0.1200, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 04:41:55.729538: step 45740, loss = 0.1301, acc = 0.9680 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 04:42:00.339411: step 45760, loss = 0.1362, acc = 0.9520 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:42:04.773305: step 45780, loss = 0.1318, acc = 0.9580 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 04:42:09.420282: step 45800, loss = 0.1088, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 04:42:14.123106: step 45820, loss = 0.1381, acc = 0.9540 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 04:42:18.759702: step 45840, loss = 0.1383, acc = 0.9520 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 04:42:23.374048: step 45860, loss = 0.1511, acc = 0.9460 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 04:42:28.098400: step 45880, loss = 0.1358, acc = 0.9420 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 04:42:32.827472: step 45900, loss = 0.1519, acc = 0.9560 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 04:42:37.413783: step 45920, loss = 0.1157, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 04:42:42.275827: step 45940, loss = 0.1384, acc = 0.9540 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:42:47.029765: step 45960, loss = 0.1457, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 04:42:51.630459: step 45980, loss = 0.1567, acc = 0.9440 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 04:42:56.310210: step 46000, loss = 0.1273, acc = 0.9600 (266.7 examples/sec; 0.240 sec/batch)
[Eval] 2017-05-09 04:43:10.364388: step 46000, acc = 0.9618, f1 = 0.9605
[Test] 2017-05-09 04:43:19.673543: step 46000, acc = 0.9533, f1 = 0.9529
[Status] 2017-05-09 04:43:19.673651: step 46000, maxindex = 46000, maxdev = 0.9618, maxtst = 0.9533
2017-05-09 04:43:27.539681: step 46020, loss = 0.1322, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 04:43:32.105145: step 46040, loss = 0.1084, acc = 0.9640 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:43:36.714580: step 46060, loss = 0.1083, acc = 0.9700 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 04:43:41.592128: step 46080, loss = 0.1405, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:43:46.188320: step 46100, loss = 0.1318, acc = 0.9540 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 04:43:50.798824: step 46120, loss = 0.1056, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:43:55.635767: step 46140, loss = 0.1074, acc = 0.9660 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 04:44:00.173590: step 46160, loss = 0.1228, acc = 0.9680 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 04:44:04.773405: step 46180, loss = 0.1306, acc = 0.9640 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 04:44:09.543448: step 46200, loss = 0.1175, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:44:14.115617: step 46220, loss = 0.1140, acc = 0.9600 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 04:44:18.756519: step 46240, loss = 0.1210, acc = 0.9580 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:44:23.478325: step 46260, loss = 0.1272, acc = 0.9580 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 04:44:28.002556: step 46280, loss = 0.1222, acc = 0.9700 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 04:44:32.524854: step 46300, loss = 0.1223, acc = 0.9680 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:44:37.228250: step 46320, loss = 0.1200, acc = 0.9520 (245.7 examples/sec; 0.261 sec/batch)
2017-05-09 04:44:41.931425: step 46340, loss = 0.1175, acc = 0.9540 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 04:44:46.439226: step 46360, loss = 0.1276, acc = 0.9600 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 04:44:51.725828: step 46380, loss = 0.1305, acc = 0.9620 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 04:44:56.451518: step 46400, loss = 0.1196, acc = 0.9640 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:45:00.911785: step 46420, loss = 0.1247, acc = 0.9560 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 04:45:05.543676: step 46440, loss = 0.1487, acc = 0.9560 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 04:45:10.396556: step 46460, loss = 0.0956, acc = 0.9720 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 04:45:14.881425: step 46480, loss = 0.0987, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 04:45:19.487214: step 46500, loss = 0.1059, acc = 0.9700 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 04:45:24.168628: step 46520, loss = 0.1358, acc = 0.9520 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 04:45:28.827396: step 46540, loss = 0.1374, acc = 0.9560 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 04:45:33.501171: step 46560, loss = 0.1333, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 04:45:38.317245: step 46580, loss = 0.1284, acc = 0.9580 (223.6 examples/sec; 0.286 sec/batch)
2017-05-09 04:45:42.918039: step 46600, loss = 0.1380, acc = 0.9500 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 04:45:47.534380: step 46620, loss = 0.1180, acc = 0.9600 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:45:52.141982: step 46640, loss = 0.1218, acc = 0.9640 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 04:45:56.888008: step 46660, loss = 0.1367, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 04:46:01.516439: step 46680, loss = 0.1110, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 04:46:06.045101: step 46700, loss = 0.1306, acc = 0.9640 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 04:46:10.815247: step 46720, loss = 0.1283, acc = 0.9540 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 04:46:15.451001: step 46740, loss = 0.1218, acc = 0.9620 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 04:46:20.129763: step 46760, loss = 0.1247, acc = 0.9660 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 04:46:24.751087: step 46780, loss = 0.1281, acc = 0.9700 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 04:46:29.178646: step 46800, loss = 0.1381, acc = 0.9520 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 04:46:33.850638: step 46820, loss = 0.1282, acc = 0.9540 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 04:46:38.494622: step 46840, loss = 0.1327, acc = 0.9580 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 04:46:43.187757: step 46860, loss = 0.1060, acc = 0.9640 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 04:46:47.786055: step 46880, loss = 0.1322, acc = 0.9540 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 04:46:52.509609: step 46900, loss = 0.1326, acc = 0.9520 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 04:46:57.216918: step 46920, loss = 0.1308, acc = 0.9600 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:47:01.813189: step 46940, loss = 0.1544, acc = 0.9440 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 04:47:06.331244: step 46960, loss = 0.1179, acc = 0.9700 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 04:47:11.208300: step 46980, loss = 0.1310, acc = 0.9640 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:47:15.840960: step 47000, loss = 0.0969, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 04:47:29.803253: step 47000, acc = 0.9614, f1 = 0.9601
[Test] 2017-05-09 04:47:39.467359: step 47000, acc = 0.9530, f1 = 0.9526
[Status] 2017-05-09 04:47:39.467430: step 47000, maxindex = 46000, maxdev = 0.9618, maxtst = 0.9533
2017-05-09 04:47:44.181625: step 47020, loss = 0.1228, acc = 0.9620 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 04:47:48.785384: step 47040, loss = 0.1379, acc = 0.9560 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 04:47:53.470210: step 47060, loss = 0.1281, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 04:47:58.138320: step 47080, loss = 0.1316, acc = 0.9560 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:48:02.760322: step 47100, loss = 0.0980, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:48:07.463736: step 47120, loss = 0.1358, acc = 0.9520 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 04:48:11.930365: step 47140, loss = 0.1224, acc = 0.9700 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 04:48:16.471333: step 47160, loss = 0.1267, acc = 0.9560 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:48:21.119052: step 47180, loss = 0.0903, acc = 0.9800 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 04:48:25.658847: step 47200, loss = 0.1242, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 04:48:30.490649: step 47220, loss = 0.1184, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 04:48:35.055080: step 47240, loss = 0.1309, acc = 0.9580 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 04:48:39.817871: step 47260, loss = 0.1366, acc = 0.9460 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 04:48:44.527084: step 47280, loss = 0.1333, acc = 0.9600 (244.1 examples/sec; 0.262 sec/batch)
2017-05-09 04:48:49.196468: step 47300, loss = 0.1494, acc = 0.9580 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 04:48:53.818279: step 47320, loss = 0.1609, acc = 0.9460 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 04:48:58.439544: step 47340, loss = 0.1264, acc = 0.9620 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 04:49:03.054114: step 47360, loss = 0.1017, acc = 0.9780 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 04:49:09.062553: step 47380, loss = 0.1614, acc = 0.9480 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 04:49:13.528750: step 47400, loss = 0.1087, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 04:49:18.081683: step 47420, loss = 0.1517, acc = 0.9580 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 04:49:22.673924: step 47440, loss = 0.1253, acc = 0.9580 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 04:49:27.192871: step 47460, loss = 0.1207, acc = 0.9600 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 04:49:31.682654: step 47480, loss = 0.1236, acc = 0.9580 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 04:49:36.436483: step 47500, loss = 0.1325, acc = 0.9560 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 04:49:40.985199: step 47520, loss = 0.1329, acc = 0.9560 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 04:49:45.608069: step 47540, loss = 0.1096, acc = 0.9620 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 04:49:50.336685: step 47560, loss = 0.1261, acc = 0.9700 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 04:49:54.914737: step 47580, loss = 0.1244, acc = 0.9520 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 04:49:59.530817: step 47600, loss = 0.1106, acc = 0.9720 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 04:50:04.441674: step 47620, loss = 0.1312, acc = 0.9580 (244.5 examples/sec; 0.262 sec/batch)
2017-05-09 04:50:09.334227: step 47640, loss = 0.1146, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 04:50:13.961069: step 47660, loss = 0.1093, acc = 0.9720 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 04:50:18.756581: step 47680, loss = 0.1423, acc = 0.9460 (243.4 examples/sec; 0.263 sec/batch)
2017-05-09 04:50:23.207223: step 47700, loss = 0.1363, acc = 0.9580 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 04:50:27.800777: step 47720, loss = 0.1210, acc = 0.9640 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 04:50:32.410091: step 47740, loss = 0.1182, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:50:37.071768: step 47760, loss = 0.1284, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 04:50:41.677164: step 47780, loss = 0.1324, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 04:50:46.357809: step 47800, loss = 0.1192, acc = 0.9660 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 04:50:51.106393: step 47820, loss = 0.1297, acc = 0.9660 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 04:50:55.886797: step 47840, loss = 0.1233, acc = 0.9660 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 04:51:00.483838: step 47860, loss = 0.0967, acc = 0.9780 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 04:51:05.230296: step 47880, loss = 0.1187, acc = 0.9700 (300.8 examples/sec; 0.213 sec/batch)
2017-05-09 04:51:09.746161: step 47900, loss = 0.1364, acc = 0.9580 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 04:51:14.365102: step 47920, loss = 0.1305, acc = 0.9600 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 04:51:19.107952: step 47940, loss = 0.1298, acc = 0.9640 (233.6 examples/sec; 0.274 sec/batch)
2017-05-09 04:51:23.816723: step 47960, loss = 0.1283, acc = 0.9660 (250.6 examples/sec; 0.255 sec/batch)
2017-05-09 04:51:28.523208: step 47980, loss = 0.1642, acc = 0.9460 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 04:51:33.126971: step 48000, loss = 0.1277, acc = 0.9580 (262.6 examples/sec; 0.244 sec/batch)
[Eval] 2017-05-09 04:51:47.101158: step 48000, acc = 0.9612, f1 = 0.9600
[Test] 2017-05-09 04:51:57.016763: step 48000, acc = 0.9527, f1 = 0.9524
[Status] 2017-05-09 04:51:57.016843: step 48000, maxindex = 46000, maxdev = 0.9618, maxtst = 0.9533
2017-05-09 04:52:01.605933: step 48020, loss = 0.1284, acc = 0.9680 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:52:06.383157: step 48040, loss = 0.1247, acc = 0.9580 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 04:52:12.008800: step 48060, loss = 0.1583, acc = 0.9440 (255.1 examples/sec; 0.251 sec/batch)
2017-05-09 04:52:16.652532: step 48080, loss = 0.1289, acc = 0.9620 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 04:52:21.602713: step 48100, loss = 0.1486, acc = 0.9520 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 04:52:26.218105: step 48120, loss = 0.1371, acc = 0.9600 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 04:52:30.915398: step 48140, loss = 0.1170, acc = 0.9680 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 04:52:35.524575: step 48160, loss = 0.1270, acc = 0.9520 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:52:40.153687: step 48180, loss = 0.1187, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 04:52:44.765641: step 48200, loss = 0.1350, acc = 0.9600 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 04:52:49.599763: step 48220, loss = 0.1528, acc = 0.9440 (226.5 examples/sec; 0.283 sec/batch)
2017-05-09 04:52:54.135295: step 48240, loss = 0.1301, acc = 0.9600 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 04:52:58.700430: step 48260, loss = 0.1206, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 04:53:03.281676: step 48280, loss = 0.1065, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:53:08.013971: step 48300, loss = 0.1254, acc = 0.9580 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 04:53:12.653779: step 48320, loss = 0.1253, acc = 0.9620 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 04:53:17.241748: step 48340, loss = 0.1334, acc = 0.9600 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 04:53:21.960560: step 48360, loss = 0.1168, acc = 0.9700 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 04:53:27.299798: step 48380, loss = 0.1506, acc = 0.9400 (155.2 examples/sec; 0.412 sec/batch)
2017-05-09 04:53:31.923313: step 48400, loss = 0.1196, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 04:53:36.781689: step 48420, loss = 0.1420, acc = 0.9540 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 04:53:41.382743: step 48440, loss = 0.1104, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 04:53:45.903208: step 48460, loss = 0.1242, acc = 0.9620 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 04:53:50.673193: step 48480, loss = 0.1167, acc = 0.9680 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 04:53:55.111981: step 48500, loss = 0.1522, acc = 0.9520 (299.6 examples/sec; 0.214 sec/batch)
2017-05-09 04:53:59.722695: step 48520, loss = 0.1438, acc = 0.9480 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:54:04.283797: step 48540, loss = 0.1064, acc = 0.9680 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 04:54:08.864484: step 48560, loss = 0.1320, acc = 0.9520 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 04:54:13.540824: step 48580, loss = 0.1295, acc = 0.9620 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 04:54:18.105416: step 48600, loss = 0.1235, acc = 0.9560 (301.2 examples/sec; 0.212 sec/batch)
2017-05-09 04:54:22.808634: step 48620, loss = 0.1197, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 04:54:27.406553: step 48640, loss = 0.1041, acc = 0.9700 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 04:54:31.996037: step 48660, loss = 0.1132, acc = 0.9580 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 04:54:36.705687: step 48680, loss = 0.1275, acc = 0.9600 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 04:54:41.519423: step 48700, loss = 0.1261, acc = 0.9580 (251.2 examples/sec; 0.255 sec/batch)
2017-05-09 04:54:46.140913: step 48720, loss = 0.1142, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 04:54:50.961412: step 48740, loss = 0.1366, acc = 0.9600 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:54:55.453894: step 48760, loss = 0.1095, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 04:54:59.965240: step 48780, loss = 0.1180, acc = 0.9540 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 04:55:04.636412: step 48800, loss = 0.1281, acc = 0.9580 (257.5 examples/sec; 0.248 sec/batch)
2017-05-09 04:55:09.167602: step 48820, loss = 0.1000, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 04:55:13.745880: step 48840, loss = 0.1121, acc = 0.9580 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 04:55:18.322867: step 48860, loss = 0.1198, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 04:55:23.048758: step 48880, loss = 0.1337, acc = 0.9580 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 04:55:27.589474: step 48900, loss = 0.1657, acc = 0.9420 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 04:55:32.234520: step 48920, loss = 0.1222, acc = 0.9640 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 04:55:36.863685: step 48940, loss = 0.1194, acc = 0.9580 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:55:41.521127: step 48960, loss = 0.1190, acc = 0.9600 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 04:55:46.065870: step 48980, loss = 0.1276, acc = 0.9640 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 04:55:50.813153: step 49000, loss = 0.1381, acc = 0.9560 (278.1 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 04:56:04.821021: step 49000, acc = 0.9609, f1 = 0.9596
[Test] 2017-05-09 04:56:14.293657: step 49000, acc = 0.9525, f1 = 0.9521
[Status] 2017-05-09 04:56:14.293763: step 49000, maxindex = 46000, maxdev = 0.9618, maxtst = 0.9533
2017-05-09 04:56:18.988265: step 49020, loss = 0.1394, acc = 0.9520 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 04:56:23.602635: step 49040, loss = 0.1274, acc = 0.9500 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 04:56:28.384718: step 49060, loss = 0.1031, acc = 0.9720 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 04:56:33.018107: step 49080, loss = 0.1329, acc = 0.9620 (300.9 examples/sec; 0.213 sec/batch)
2017-05-09 04:56:37.519403: step 49100, loss = 0.1245, acc = 0.9520 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 04:56:42.075417: step 49120, loss = 0.1091, acc = 0.9620 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 04:56:46.660527: step 49140, loss = 0.1417, acc = 0.9540 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 04:56:51.496280: step 49160, loss = 0.1318, acc = 0.9560 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 04:56:56.119601: step 49180, loss = 0.1110, acc = 0.9700 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 04:57:00.766006: step 49200, loss = 0.1329, acc = 0.9520 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 04:57:05.406833: step 49220, loss = 0.1339, acc = 0.9580 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 04:57:09.993280: step 49240, loss = 0.1291, acc = 0.9600 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 04:57:14.552958: step 49260, loss = 0.1279, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 04:57:19.167828: step 49280, loss = 0.1391, acc = 0.9560 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 04:57:23.790957: step 49300, loss = 0.1349, acc = 0.9560 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 04:57:28.384954: step 49320, loss = 0.1010, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 04:57:33.403919: step 49340, loss = 0.1471, acc = 0.9520 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 04:57:37.979157: step 49360, loss = 0.1092, acc = 0.9680 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 04:57:42.404468: step 49380, loss = 0.1371, acc = 0.9520 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 04:57:48.515188: step 49400, loss = 0.1307, acc = 0.9560 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 04:57:53.093324: step 49420, loss = 0.1448, acc = 0.9520 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 04:57:57.609184: step 49440, loss = 0.1066, acc = 0.9720 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 04:58:02.397601: step 49460, loss = 0.1197, acc = 0.9660 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 04:58:06.964889: step 49480, loss = 0.1095, acc = 0.9660 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 04:58:11.473519: step 49500, loss = 0.1149, acc = 0.9620 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 04:58:16.074098: step 49520, loss = 0.1470, acc = 0.9500 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 04:58:20.884836: step 49540, loss = 0.1161, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 04:58:25.425833: step 49560, loss = 0.1202, acc = 0.9660 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:58:30.470305: step 49580, loss = 0.1123, acc = 0.9560 (230.0 examples/sec; 0.278 sec/batch)
2017-05-09 04:58:35.288629: step 49600, loss = 0.1225, acc = 0.9620 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 04:58:39.845612: step 49620, loss = 0.0981, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 04:58:44.455918: step 49640, loss = 0.1174, acc = 0.9700 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 04:58:49.214847: step 49660, loss = 0.1429, acc = 0.9500 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 04:58:53.890468: step 49680, loss = 0.1124, acc = 0.9620 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 04:58:58.386709: step 49700, loss = 0.1075, acc = 0.9680 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 04:59:03.149927: step 49720, loss = 0.1360, acc = 0.9500 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 04:59:07.674633: step 49740, loss = 0.1119, acc = 0.9640 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 04:59:12.264954: step 49760, loss = 0.1153, acc = 0.9580 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 04:59:17.082804: step 49780, loss = 0.1104, acc = 0.9560 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 04:59:21.792869: step 49800, loss = 0.1414, acc = 0.9560 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 04:59:26.383472: step 49820, loss = 0.1256, acc = 0.9620 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 04:59:31.257821: step 49840, loss = 0.1273, acc = 0.9620 (236.0 examples/sec; 0.271 sec/batch)
2017-05-09 04:59:35.872768: step 49860, loss = 0.1124, acc = 0.9740 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 04:59:40.569302: step 49880, loss = 0.1226, acc = 0.9600 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 04:59:45.411671: step 49900, loss = 0.1121, acc = 0.9640 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 04:59:50.036831: step 49920, loss = 0.1200, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 04:59:54.635060: step 49940, loss = 0.1082, acc = 0.9720 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 04:59:59.254683: step 49960, loss = 0.1371, acc = 0.9640 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 05:00:04.084539: step 49980, loss = 0.1085, acc = 0.9660 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 05:00:08.725506: step 50000, loss = 0.1110, acc = 0.9720 (255.5 examples/sec; 0.251 sec/batch)
[Eval] 2017-05-09 05:00:23.144744: step 50000, acc = 0.9612, f1 = 0.9600
[Test] 2017-05-09 05:00:32.771519: step 50000, acc = 0.9530, f1 = 0.9526
[Status] 2017-05-09 05:00:32.771638: step 50000, maxindex = 46000, maxdev = 0.9618, maxtst = 0.9533
2017-05-09 05:00:37.465099: step 50020, loss = 0.0894, acc = 0.9820 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 05:00:42.158040: step 50040, loss = 0.1252, acc = 0.9620 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 05:00:46.874564: step 50060, loss = 0.1430, acc = 0.9580 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 05:00:51.445505: step 50080, loss = 0.1074, acc = 0.9620 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 05:00:56.006500: step 50100, loss = 0.1210, acc = 0.9660 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 05:01:00.571296: step 50120, loss = 0.1308, acc = 0.9620 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 05:01:05.208156: step 50140, loss = 0.1508, acc = 0.9520 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:01:09.763530: step 50160, loss = 0.1368, acc = 0.9600 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 05:01:14.699731: step 50180, loss = 0.1555, acc = 0.9520 (218.8 examples/sec; 0.292 sec/batch)
2017-05-09 05:01:19.387175: step 50200, loss = 0.1158, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 05:01:24.099971: step 50220, loss = 0.1158, acc = 0.9680 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 05:01:28.787837: step 50240, loss = 0.0995, acc = 0.9760 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:01:33.499737: step 50260, loss = 0.1292, acc = 0.9720 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 05:01:38.120641: step 50280, loss = 0.1330, acc = 0.9560 (257.3 examples/sec; 0.249 sec/batch)
2017-05-09 05:01:42.797930: step 50300, loss = 0.1291, acc = 0.9640 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 05:01:47.382697: step 50320, loss = 0.1510, acc = 0.9480 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 05:01:51.922373: step 50340, loss = 0.1418, acc = 0.9540 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 05:01:56.405741: step 50360, loss = 0.1007, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:02:01.071261: step 50380, loss = 0.1289, acc = 0.9540 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 05:02:06.416035: step 50400, loss = 0.1018, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:02:11.059734: step 50420, loss = 0.1286, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 05:02:15.726732: step 50440, loss = 0.1034, acc = 0.9720 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 05:02:20.233046: step 50460, loss = 0.1619, acc = 0.9440 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 05:02:24.719347: step 50480, loss = 0.1237, acc = 0.9600 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 05:02:29.741525: step 50500, loss = 0.1051, acc = 0.9780 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 05:02:34.329973: step 50520, loss = 0.1114, acc = 0.9740 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 05:02:38.936530: step 50540, loss = 0.1491, acc = 0.9500 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 05:02:43.899945: step 50560, loss = 0.1204, acc = 0.9540 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 05:02:48.537498: step 50580, loss = 0.1171, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 05:02:52.953621: step 50600, loss = 0.1118, acc = 0.9700 (291.6 examples/sec; 0.220 sec/batch)
2017-05-09 05:02:57.767714: step 50620, loss = 0.1151, acc = 0.9560 (214.9 examples/sec; 0.298 sec/batch)
2017-05-09 05:03:02.415624: step 50640, loss = 0.1272, acc = 0.9700 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 05:03:06.931479: step 50660, loss = 0.0910, acc = 0.9780 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 05:03:11.608449: step 50680, loss = 0.1071, acc = 0.9660 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 05:03:16.925543: step 50700, loss = 0.1406, acc = 0.9560 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 05:03:21.936910: step 50720, loss = 0.1347, acc = 0.9560 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 05:03:26.697102: step 50740, loss = 0.0931, acc = 0.9680 (232.1 examples/sec; 0.276 sec/batch)
2017-05-09 05:03:31.254800: step 50760, loss = 0.1544, acc = 0.9540 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 05:03:35.985663: step 50780, loss = 0.1103, acc = 0.9740 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 05:03:40.570876: step 50800, loss = 0.1440, acc = 0.9520 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 05:03:45.512244: step 50820, loss = 0.1153, acc = 0.9680 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 05:03:50.152066: step 50840, loss = 0.1255, acc = 0.9620 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 05:03:54.752051: step 50860, loss = 0.1255, acc = 0.9620 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 05:03:59.371965: step 50880, loss = 0.1071, acc = 0.9660 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 05:04:03.853262: step 50900, loss = 0.1322, acc = 0.9520 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 05:04:08.349976: step 50920, loss = 0.1206, acc = 0.9720 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 05:04:13.073310: step 50940, loss = 0.1478, acc = 0.9400 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 05:04:17.640857: step 50960, loss = 0.1167, acc = 0.9680 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 05:04:22.271974: step 50980, loss = 0.1373, acc = 0.9600 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 05:04:27.164003: step 51000, loss = 0.1178, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 05:04:41.204393: step 51000, acc = 0.9625, f1 = 0.9613
[Test] 2017-05-09 05:04:50.571928: step 51000, acc = 0.9535, f1 = 0.9531
[Status] 2017-05-09 05:04:50.572009: step 51000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:04:58.557218: step 51020, loss = 0.1326, acc = 0.9640 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 05:05:03.100793: step 51040, loss = 0.1542, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 05:05:07.838573: step 51060, loss = 0.1133, acc = 0.9660 (250.3 examples/sec; 0.256 sec/batch)
2017-05-09 05:05:12.634560: step 51080, loss = 0.1154, acc = 0.9700 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 05:05:17.178985: step 51100, loss = 0.1180, acc = 0.9600 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 05:05:21.881620: step 51120, loss = 0.1396, acc = 0.9500 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 05:05:26.787580: step 51140, loss = 0.0990, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 05:05:31.262710: step 51160, loss = 0.1261, acc = 0.9560 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:05:35.720789: step 51180, loss = 0.1112, acc = 0.9720 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 05:05:40.511862: step 51200, loss = 0.1032, acc = 0.9720 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 05:05:45.123515: step 51220, loss = 0.1471, acc = 0.9540 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 05:05:49.689314: step 51240, loss = 0.1417, acc = 0.9540 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 05:05:54.590083: step 51260, loss = 0.1205, acc = 0.9640 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 05:05:59.159667: step 51280, loss = 0.1250, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 05:06:03.748319: step 51300, loss = 0.1267, acc = 0.9560 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 05:06:08.389224: step 51320, loss = 0.1053, acc = 0.9680 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 05:06:12.995980: step 51340, loss = 0.1360, acc = 0.9500 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:06:17.546261: step 51360, loss = 0.1329, acc = 0.9500 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 05:06:22.278415: step 51380, loss = 0.1406, acc = 0.9520 (244.1 examples/sec; 0.262 sec/batch)
2017-05-09 05:06:26.767790: step 51400, loss = 0.1556, acc = 0.9420 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:06:32.406431: step 51420, loss = 0.1280, acc = 0.9540 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 05:06:37.143083: step 51440, loss = 0.1387, acc = 0.9540 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:06:41.775060: step 51460, loss = 0.1231, acc = 0.9660 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 05:06:46.376717: step 51480, loss = 0.1073, acc = 0.9600 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 05:06:50.901294: step 51500, loss = 0.1409, acc = 0.9500 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 05:06:55.712543: step 51520, loss = 0.1244, acc = 0.9700 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 05:07:00.201500: step 51540, loss = 0.1523, acc = 0.9480 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 05:07:04.686528: step 51560, loss = 0.1490, acc = 0.9480 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 05:07:09.360881: step 51580, loss = 0.1017, acc = 0.9640 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 05:07:13.816315: step 51600, loss = 0.1170, acc = 0.9640 (299.3 examples/sec; 0.214 sec/batch)
2017-05-09 05:07:18.618588: step 51620, loss = 0.1261, acc = 0.9580 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:07:23.489300: step 51640, loss = 0.1051, acc = 0.9760 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 05:07:28.082588: step 51660, loss = 0.1598, acc = 0.9560 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 05:07:32.711308: step 51680, loss = 0.1091, acc = 0.9660 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 05:07:37.486231: step 51700, loss = 0.1421, acc = 0.9580 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 05:07:42.027419: step 51720, loss = 0.1156, acc = 0.9640 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 05:07:46.661782: step 51740, loss = 0.1675, acc = 0.9440 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 05:07:51.282177: step 51760, loss = 0.1359, acc = 0.9620 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 05:07:55.813163: step 51780, loss = 0.1651, acc = 0.9400 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 05:08:00.396600: step 51800, loss = 0.1349, acc = 0.9480 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 05:08:05.161702: step 51820, loss = 0.1445, acc = 0.9580 (277.7 examples/sec; 0.231 sec/batch)
2017-05-09 05:08:09.765899: step 51840, loss = 0.1227, acc = 0.9480 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 05:08:14.281488: step 51860, loss = 0.1343, acc = 0.9600 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 05:08:19.056836: step 51880, loss = 0.1087, acc = 0.9700 (236.3 examples/sec; 0.271 sec/batch)
2017-05-09 05:08:23.600088: step 51900, loss = 0.1529, acc = 0.9460 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 05:08:28.236418: step 51920, loss = 0.1141, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:08:33.232117: step 51940, loss = 0.1042, acc = 0.9720 (220.6 examples/sec; 0.290 sec/batch)
2017-05-09 05:08:37.860459: step 51960, loss = 0.1397, acc = 0.9620 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 05:08:42.572356: step 51980, loss = 0.1351, acc = 0.9480 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 05:08:47.413682: step 52000, loss = 0.1047, acc = 0.9700 (250.8 examples/sec; 0.255 sec/batch)
[Eval] 2017-05-09 05:09:01.275338: step 52000, acc = 0.9607, f1 = 0.9594
[Test] 2017-05-09 05:09:10.878409: step 52000, acc = 0.9520, f1 = 0.9516
[Status] 2017-05-09 05:09:10.878497: step 52000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:09:15.465958: step 52020, loss = 0.1255, acc = 0.9680 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 05:09:20.244067: step 52040, loss = 0.1313, acc = 0.9660 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 05:09:24.751431: step 52060, loss = 0.1315, acc = 0.9560 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 05:09:29.317781: step 52080, loss = 0.1382, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 05:09:34.050817: step 52100, loss = 0.1408, acc = 0.9520 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 05:09:38.692330: step 52120, loss = 0.1177, acc = 0.9640 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 05:09:43.401840: step 52140, loss = 0.1341, acc = 0.9540 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 05:09:48.234104: step 52160, loss = 0.1259, acc = 0.9600 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:09:52.696879: step 52180, loss = 0.1119, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 05:09:57.324491: step 52200, loss = 0.1244, acc = 0.9580 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 05:10:02.102013: step 52220, loss = 0.1051, acc = 0.9680 (227.0 examples/sec; 0.282 sec/batch)
2017-05-09 05:10:06.589823: step 52240, loss = 0.1113, acc = 0.9580 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 05:10:11.161099: step 52260, loss = 0.1381, acc = 0.9540 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 05:10:15.665862: step 52280, loss = 0.1201, acc = 0.9600 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 05:10:20.289871: step 52300, loss = 0.1366, acc = 0.9500 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:10:24.775875: step 52320, loss = 0.1280, acc = 0.9540 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 05:10:29.185506: step 52340, loss = 0.1372, acc = 0.9480 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:10:33.987749: step 52360, loss = 0.1218, acc = 0.9520 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 05:10:38.572883: step 52380, loss = 0.1020, acc = 0.9760 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 05:10:43.157745: step 52400, loss = 0.1273, acc = 0.9640 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 05:10:48.874463: step 52420, loss = 0.1135, acc = 0.9720 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 05:10:53.475404: step 52440, loss = 0.1141, acc = 0.9660 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:10:58.033400: step 52460, loss = 0.1211, acc = 0.9680 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 05:11:02.691979: step 52480, loss = 0.1487, acc = 0.9480 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 05:11:07.316395: step 52500, loss = 0.1111, acc = 0.9660 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 05:11:11.862781: step 52520, loss = 0.1314, acc = 0.9540 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 05:11:16.685684: step 52540, loss = 0.1229, acc = 0.9600 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:11:21.315188: step 52560, loss = 0.1203, acc = 0.9620 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 05:11:25.940522: step 52580, loss = 0.1116, acc = 0.9680 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 05:11:30.511988: step 52600, loss = 0.1303, acc = 0.9600 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 05:11:35.194034: step 52620, loss = 0.1224, acc = 0.9560 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 05:11:39.730948: step 52640, loss = 0.1134, acc = 0.9660 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 05:11:44.276219: step 52660, loss = 0.1436, acc = 0.9500 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 05:11:49.150433: step 52680, loss = 0.1287, acc = 0.9660 (255.7 examples/sec; 0.250 sec/batch)
2017-05-09 05:11:53.695464: step 52700, loss = 0.1105, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 05:11:58.270140: step 52720, loss = 0.1233, acc = 0.9600 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 05:12:03.142662: step 52740, loss = 0.1281, acc = 0.9580 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 05:12:07.722524: step 52760, loss = 0.1464, acc = 0.9520 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 05:12:12.329110: step 52780, loss = 0.1194, acc = 0.9660 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:12:17.029136: step 52800, loss = 0.1069, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 05:12:21.726093: step 52820, loss = 0.1476, acc = 0.9500 (255.4 examples/sec; 0.251 sec/batch)
2017-05-09 05:12:26.200415: step 52840, loss = 0.1308, acc = 0.9640 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 05:12:30.809520: step 52860, loss = 0.1187, acc = 0.9620 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 05:12:35.619566: step 52880, loss = 0.1206, acc = 0.9540 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 05:12:40.172537: step 52900, loss = 0.1251, acc = 0.9520 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:12:44.619248: step 52920, loss = 0.1507, acc = 0.9620 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 05:12:49.518894: step 52940, loss = 0.1357, acc = 0.9540 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 05:12:54.026409: step 52960, loss = 0.0973, acc = 0.9740 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 05:12:58.615314: step 52980, loss = 0.1278, acc = 0.9600 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 05:13:03.255390: step 53000, loss = 0.1086, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 05:13:17.307300: step 53000, acc = 0.9612, f1 = 0.9600
[Test] 2017-05-09 05:13:26.585078: step 53000, acc = 0.9526, f1 = 0.9522
[Status] 2017-05-09 05:13:26.585154: step 53000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:13:31.149888: step 53020, loss = 0.1337, acc = 0.9600 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 05:13:35.776451: step 53040, loss = 0.1182, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 05:13:40.288393: step 53060, loss = 0.1297, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 05:13:44.987037: step 53080, loss = 0.1345, acc = 0.9500 (234.5 examples/sec; 0.273 sec/batch)
2017-05-09 05:13:49.469375: step 53100, loss = 0.1003, acc = 0.9700 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:13:53.962987: step 53120, loss = 0.1277, acc = 0.9600 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 05:13:58.637685: step 53140, loss = 0.1180, acc = 0.9680 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 05:14:03.208841: step 53160, loss = 0.1335, acc = 0.9580 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 05:14:07.762138: step 53180, loss = 0.1266, acc = 0.9560 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 05:14:12.337415: step 53200, loss = 0.1075, acc = 0.9640 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 05:14:16.976734: step 53220, loss = 0.1328, acc = 0.9500 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 05:14:21.518026: step 53240, loss = 0.1304, acc = 0.9580 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 05:14:26.208522: step 53260, loss = 0.1259, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 05:14:30.966793: step 53280, loss = 0.0913, acc = 0.9800 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 05:14:35.649482: step 53300, loss = 0.1379, acc = 0.9600 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 05:14:40.194448: step 53320, loss = 0.1009, acc = 0.9680 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 05:14:45.177248: step 53340, loss = 0.1184, acc = 0.9640 (203.2 examples/sec; 0.315 sec/batch)
2017-05-09 05:14:49.796212: step 53360, loss = 0.0996, acc = 0.9740 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 05:14:54.422036: step 53380, loss = 0.1265, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:14:59.160198: step 53400, loss = 0.1210, acc = 0.9640 (255.2 examples/sec; 0.251 sec/batch)
2017-05-09 05:15:04.832308: step 53420, loss = 0.1069, acc = 0.9720 (132.0 examples/sec; 0.485 sec/batch)
2017-05-09 05:15:09.299316: step 53440, loss = 0.0928, acc = 0.9780 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 05:15:13.925890: step 53460, loss = 0.1496, acc = 0.9360 (240.4 examples/sec; 0.266 sec/batch)
2017-05-09 05:15:18.617613: step 53480, loss = 0.1089, acc = 0.9620 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 05:15:23.155419: step 53500, loss = 0.1142, acc = 0.9560 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:15:27.836342: step 53520, loss = 0.1348, acc = 0.9540 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:15:32.575866: step 53540, loss = 0.1122, acc = 0.9740 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 05:15:37.194136: step 53560, loss = 0.1480, acc = 0.9440 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:15:41.723106: step 53580, loss = 0.1435, acc = 0.9480 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:15:46.412755: step 53600, loss = 0.1024, acc = 0.9700 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 05:15:51.026218: step 53620, loss = 0.1331, acc = 0.9440 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 05:15:55.642371: step 53640, loss = 0.0868, acc = 0.9740 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 05:16:00.353770: step 53660, loss = 0.1132, acc = 0.9640 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 05:16:04.944625: step 53680, loss = 0.1231, acc = 0.9660 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 05:16:09.590146: step 53700, loss = 0.1179, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:16:14.537754: step 53720, loss = 0.1254, acc = 0.9600 (228.8 examples/sec; 0.280 sec/batch)
2017-05-09 05:16:18.987966: step 53740, loss = 0.1047, acc = 0.9640 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 05:16:23.544990: step 53760, loss = 0.0934, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 05:16:28.147683: step 53780, loss = 0.1273, acc = 0.9620 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 05:16:32.884857: step 53800, loss = 0.1171, acc = 0.9620 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:16:37.435280: step 53820, loss = 0.1334, acc = 0.9580 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 05:16:41.991106: step 53840, loss = 0.1266, acc = 0.9640 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:16:46.876463: step 53860, loss = 0.1087, acc = 0.9700 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 05:16:51.470535: step 53880, loss = 0.1477, acc = 0.9560 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 05:16:55.974429: step 53900, loss = 0.1259, acc = 0.9680 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 05:17:00.614762: step 53920, loss = 0.1219, acc = 0.9600 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 05:17:05.192538: step 53940, loss = 0.1323, acc = 0.9480 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:17:09.862063: step 53960, loss = 0.1230, acc = 0.9620 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 05:17:14.611848: step 53980, loss = 0.1486, acc = 0.9600 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 05:17:19.103578: step 54000, loss = 0.1322, acc = 0.9640 (283.8 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 05:17:33.314606: step 54000, acc = 0.9610, f1 = 0.9598
[Test] 2017-05-09 05:17:42.910696: step 54000, acc = 0.9519, f1 = 0.9515
[Status] 2017-05-09 05:17:42.910768: step 54000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:17:47.413279: step 54020, loss = 0.1182, acc = 0.9560 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 05:17:51.990305: step 54040, loss = 0.1543, acc = 0.9600 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 05:17:56.736811: step 54060, loss = 0.1346, acc = 0.9540 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:18:01.357574: step 54080, loss = 0.1089, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 05:18:06.068588: step 54100, loss = 0.1043, acc = 0.9660 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 05:18:10.537425: step 54120, loss = 0.1237, acc = 0.9620 (292.9 examples/sec; 0.218 sec/batch)
2017-05-09 05:18:15.305058: step 54140, loss = 0.1020, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 05:18:19.862435: step 54160, loss = 0.1394, acc = 0.9500 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:18:24.368749: step 54180, loss = 0.1721, acc = 0.9340 (298.2 examples/sec; 0.215 sec/batch)
2017-05-09 05:18:29.141480: step 54200, loss = 0.1399, acc = 0.9600 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:18:33.644305: step 54220, loss = 0.1083, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 05:18:38.153237: step 54240, loss = 0.0999, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 05:18:42.815672: step 54260, loss = 0.1718, acc = 0.9460 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 05:18:47.323674: step 54280, loss = 0.1351, acc = 0.9580 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 05:18:51.790204: step 54300, loss = 0.1331, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 05:18:56.469239: step 54320, loss = 0.1426, acc = 0.9560 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 05:19:01.114933: step 54340, loss = 0.1651, acc = 0.9400 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 05:19:05.760380: step 54360, loss = 0.1621, acc = 0.9420 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 05:19:10.495573: step 54380, loss = 0.1233, acc = 0.9620 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 05:19:14.968087: step 54400, loss = 0.1130, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 05:19:19.567575: step 54420, loss = 0.1469, acc = 0.9560 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:19:24.729379: step 54440, loss = 0.1173, acc = 0.9660 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 05:19:29.421446: step 54460, loss = 0.1134, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 05:19:34.038522: step 54480, loss = 0.1522, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 05:19:38.995950: step 54500, loss = 0.1142, acc = 0.9640 (201.1 examples/sec; 0.318 sec/batch)
2017-05-09 05:19:43.664959: step 54520, loss = 0.1058, acc = 0.9700 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:19:48.241977: step 54540, loss = 0.1191, acc = 0.9560 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 05:19:52.793587: step 54560, loss = 0.1270, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 05:19:57.548610: step 54580, loss = 0.1204, acc = 0.9540 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 05:20:02.133461: step 54600, loss = 0.1339, acc = 0.9480 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 05:20:06.736422: step 54620, loss = 0.1314, acc = 0.9540 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:20:11.391236: step 54640, loss = 0.1377, acc = 0.9520 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 05:20:15.852075: step 54660, loss = 0.1052, acc = 0.9660 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 05:20:20.447379: step 54680, loss = 0.1392, acc = 0.9560 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 05:20:25.166208: step 54700, loss = 0.1228, acc = 0.9620 (241.2 examples/sec; 0.265 sec/batch)
2017-05-09 05:20:29.927368: step 54720, loss = 0.1399, acc = 0.9480 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 05:20:34.569316: step 54740, loss = 0.1122, acc = 0.9680 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 05:20:39.230914: step 54760, loss = 0.1161, acc = 0.9640 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 05:20:44.066472: step 54780, loss = 0.1336, acc = 0.9420 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 05:20:48.639806: step 54800, loss = 0.1062, acc = 0.9740 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 05:20:53.277095: step 54820, loss = 0.1261, acc = 0.9560 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 05:20:58.094873: step 54840, loss = 0.1000, acc = 0.9760 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 05:21:02.812012: step 54860, loss = 0.1267, acc = 0.9580 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:21:07.670927: step 54880, loss = 0.1153, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 05:21:12.740913: step 54900, loss = 0.1258, acc = 0.9500 (248.7 examples/sec; 0.257 sec/batch)
2017-05-09 05:21:17.388287: step 54920, loss = 0.1173, acc = 0.9540 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 05:21:21.909347: step 54940, loss = 0.1177, acc = 0.9680 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 05:21:26.603410: step 54960, loss = 0.1431, acc = 0.9440 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 05:21:31.385313: step 54980, loss = 0.1273, acc = 0.9620 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 05:21:36.029742: step 55000, loss = 0.1119, acc = 0.9660 (268.2 examples/sec; 0.239 sec/batch)
[Eval] 2017-05-09 05:21:49.970131: step 55000, acc = 0.9620, f1 = 0.9608
[Test] 2017-05-09 05:21:59.806519: step 55000, acc = 0.9539, f1 = 0.9535
[Status] 2017-05-09 05:21:59.806621: step 55000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:22:04.318366: step 55020, loss = 0.1103, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 05:22:08.967241: step 55040, loss = 0.1109, acc = 0.9680 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 05:22:13.513085: step 55060, loss = 0.1160, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 05:22:18.042092: step 55080, loss = 0.1276, acc = 0.9620 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 05:22:22.616034: step 55100, loss = 0.0958, acc = 0.9860 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 05:22:27.295258: step 55120, loss = 0.0848, acc = 0.9800 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 05:22:31.796089: step 55140, loss = 0.1098, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 05:22:36.535479: step 55160, loss = 0.1152, acc = 0.9760 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 05:22:41.210979: step 55180, loss = 0.1292, acc = 0.9600 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:22:45.822030: step 55200, loss = 0.1053, acc = 0.9660 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 05:22:50.555751: step 55220, loss = 0.1131, acc = 0.9640 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 05:22:55.287091: step 55240, loss = 0.1227, acc = 0.9640 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 05:22:59.915103: step 55260, loss = 0.1311, acc = 0.9500 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 05:23:04.489347: step 55280, loss = 0.1190, acc = 0.9700 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 05:23:09.336400: step 55300, loss = 0.1012, acc = 0.9740 (214.4 examples/sec; 0.299 sec/batch)
2017-05-09 05:23:13.797639: step 55320, loss = 0.1089, acc = 0.9600 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:23:18.483086: step 55340, loss = 0.1430, acc = 0.9580 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 05:23:23.201236: step 55360, loss = 0.1473, acc = 0.9560 (232.8 examples/sec; 0.275 sec/batch)
2017-05-09 05:23:27.748784: step 55380, loss = 0.1605, acc = 0.9480 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 05:23:32.313284: step 55400, loss = 0.0852, acc = 0.9720 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:23:36.840900: step 55420, loss = 0.1336, acc = 0.9540 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 05:23:42.650281: step 55440, loss = 0.1013, acc = 0.9640 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:23:47.299560: step 55460, loss = 0.1429, acc = 0.9460 (300.1 examples/sec; 0.213 sec/batch)
2017-05-09 05:23:51.897112: step 55480, loss = 0.1125, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 05:23:56.886251: step 55500, loss = 0.1465, acc = 0.9580 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 05:24:01.440597: step 55520, loss = 0.1061, acc = 0.9620 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 05:24:06.029276: step 55540, loss = 0.1354, acc = 0.9560 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 05:24:11.050871: step 55560, loss = 0.1303, acc = 0.9560 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 05:24:15.583536: step 55580, loss = 0.1245, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:24:20.090700: step 55600, loss = 0.1509, acc = 0.9300 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 05:24:24.883645: step 55620, loss = 0.1378, acc = 0.9520 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 05:24:29.889976: step 55640, loss = 0.1638, acc = 0.9440 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 05:24:34.468061: step 55660, loss = 0.1626, acc = 0.9420 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 05:24:39.240517: step 55680, loss = 0.1282, acc = 0.9540 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 05:24:43.852041: step 55700, loss = 0.1314, acc = 0.9600 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 05:24:48.500262: step 55720, loss = 0.1127, acc = 0.9680 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 05:24:53.051699: step 55740, loss = 0.1407, acc = 0.9520 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 05:24:57.808658: step 55760, loss = 0.1285, acc = 0.9500 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 05:25:02.541035: step 55780, loss = 0.1180, acc = 0.9600 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 05:25:07.138451: step 55800, loss = 0.1232, acc = 0.9660 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:25:11.717681: step 55820, loss = 0.1371, acc = 0.9440 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 05:25:16.303244: step 55840, loss = 0.0978, acc = 0.9720 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 05:25:21.045036: step 55860, loss = 0.0972, acc = 0.9760 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 05:25:25.823276: step 55880, loss = 0.1518, acc = 0.9460 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:25:30.407658: step 55900, loss = 0.1302, acc = 0.9680 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 05:25:34.895650: step 55920, loss = 0.1521, acc = 0.9500 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 05:25:39.554995: step 55940, loss = 0.1130, acc = 0.9600 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:25:44.237729: step 55960, loss = 0.0895, acc = 0.9760 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 05:25:48.902067: step 55980, loss = 0.1348, acc = 0.9600 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 05:25:53.653453: step 56000, loss = 0.1136, acc = 0.9640 (283.5 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 05:26:07.675398: step 56000, acc = 0.9623, f1 = 0.9611
[Test] 2017-05-09 05:26:17.163052: step 56000, acc = 0.9540, f1 = 0.9536
[Status] 2017-05-09 05:26:17.163153: step 56000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:26:21.967759: step 56020, loss = 0.1279, acc = 0.9580 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 05:26:26.577618: step 56040, loss = 0.1064, acc = 0.9700 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 05:26:31.202528: step 56060, loss = 0.1227, acc = 0.9600 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 05:26:35.752192: step 56080, loss = 0.1298, acc = 0.9560 (307.6 examples/sec; 0.208 sec/batch)
2017-05-09 05:26:40.277406: step 56100, loss = 0.1190, acc = 0.9640 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 05:26:44.775805: step 56120, loss = 0.1698, acc = 0.9280 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 05:26:49.392465: step 56140, loss = 0.1260, acc = 0.9640 (247.6 examples/sec; 0.259 sec/batch)
2017-05-09 05:26:54.014122: step 56160, loss = 0.1004, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 05:26:58.563613: step 56180, loss = 0.1237, acc = 0.9660 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 05:27:03.224882: step 56200, loss = 0.1168, acc = 0.9560 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 05:27:07.870854: step 56220, loss = 0.1291, acc = 0.9480 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 05:27:12.394669: step 56240, loss = 0.1151, acc = 0.9600 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 05:27:16.951386: step 56260, loss = 0.1337, acc = 0.9500 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 05:27:21.570937: step 56280, loss = 0.1197, acc = 0.9600 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 05:27:26.070472: step 56300, loss = 0.1298, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 05:27:30.521992: step 56320, loss = 0.1121, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 05:27:35.410779: step 56340, loss = 0.1180, acc = 0.9600 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 05:27:39.943805: step 56360, loss = 0.1196, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 05:27:44.466040: step 56380, loss = 0.1120, acc = 0.9620 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 05:27:49.126191: step 56400, loss = 0.1377, acc = 0.9560 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 05:27:53.592479: step 56420, loss = 0.1167, acc = 0.9700 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 05:27:58.174914: step 56440, loss = 0.1142, acc = 0.9740 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 05:28:03.731298: step 56460, loss = 0.1083, acc = 0.9720 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 05:28:08.313482: step 56480, loss = 0.1261, acc = 0.9640 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 05:28:12.912625: step 56500, loss = 0.1255, acc = 0.9500 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 05:28:17.725395: step 56520, loss = 0.1292, acc = 0.9640 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 05:28:22.359009: step 56540, loss = 0.1082, acc = 0.9660 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 05:28:26.960788: step 56560, loss = 0.0907, acc = 0.9780 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 05:28:31.555531: step 56580, loss = 0.1065, acc = 0.9760 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 05:28:36.289492: step 56600, loss = 0.1131, acc = 0.9720 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 05:28:40.906969: step 56620, loss = 0.1278, acc = 0.9620 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 05:28:45.403849: step 56640, loss = 0.1091, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 05:28:50.095041: step 56660, loss = 0.1238, acc = 0.9540 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 05:28:54.587259: step 56680, loss = 0.1098, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:28:59.411958: step 56700, loss = 0.1190, acc = 0.9580 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 05:29:04.194882: step 56720, loss = 0.1227, acc = 0.9640 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 05:29:08.836384: step 56740, loss = 0.1398, acc = 0.9580 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 05:29:13.416986: step 56760, loss = 0.1567, acc = 0.9440 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 05:29:18.256624: step 56780, loss = 0.1377, acc = 0.9480 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 05:29:22.955832: step 56800, loss = 0.1615, acc = 0.9340 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 05:29:27.560222: step 56820, loss = 0.1297, acc = 0.9640 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 05:29:32.321441: step 56840, loss = 0.1308, acc = 0.9620 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 05:29:36.899349: step 56860, loss = 0.1026, acc = 0.9800 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:29:41.463314: step 56880, loss = 0.1279, acc = 0.9500 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:29:46.295882: step 56900, loss = 0.1147, acc = 0.9680 (231.1 examples/sec; 0.277 sec/batch)
2017-05-09 05:29:50.776151: step 56920, loss = 0.1289, acc = 0.9580 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:29:55.208672: step 56940, loss = 0.1288, acc = 0.9580 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 05:29:59.685706: step 56960, loss = 0.1210, acc = 0.9600 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 05:30:04.626768: step 56980, loss = 0.1503, acc = 0.9540 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 05:30:09.228410: step 57000, loss = 0.1197, acc = 0.9580 (287.1 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 05:30:24.800516: step 57000, acc = 0.9610, f1 = 0.9598
[Test] 2017-05-09 05:30:34.881055: step 57000, acc = 0.9521, f1 = 0.9517
[Status] 2017-05-09 05:30:34.881144: step 57000, maxindex = 51000, maxdev = 0.9625, maxtst = 0.9535
2017-05-09 05:30:39.428446: step 57020, loss = 0.1438, acc = 0.9480 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:30:44.318820: step 57040, loss = 0.1338, acc = 0.9580 (220.5 examples/sec; 0.290 sec/batch)
2017-05-09 05:30:48.901083: step 57060, loss = 0.1311, acc = 0.9520 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 05:30:53.440001: step 57080, loss = 0.1261, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 05:30:57.939213: step 57100, loss = 0.1189, acc = 0.9660 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 05:31:02.641137: step 57120, loss = 0.1296, acc = 0.9600 (255.6 examples/sec; 0.250 sec/batch)
2017-05-09 05:31:07.291539: step 57140, loss = 0.1050, acc = 0.9660 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 05:31:11.982453: step 57160, loss = 0.1060, acc = 0.9680 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 05:31:16.885328: step 57180, loss = 0.1264, acc = 0.9620 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 05:31:21.534082: step 57200, loss = 0.1431, acc = 0.9500 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 05:31:26.086284: step 57220, loss = 0.1629, acc = 0.9420 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 05:31:30.756321: step 57240, loss = 0.1164, acc = 0.9640 (257.9 examples/sec; 0.248 sec/batch)
2017-05-09 05:31:35.316449: step 57260, loss = 0.1050, acc = 0.9760 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 05:31:40.068848: step 57280, loss = 0.1129, acc = 0.9560 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 05:31:44.725412: step 57300, loss = 0.1072, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 05:31:49.244194: step 57320, loss = 0.1195, acc = 0.9580 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 05:31:53.850063: step 57340, loss = 0.1156, acc = 0.9660 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 05:31:58.825512: step 57360, loss = 0.1195, acc = 0.9560 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 05:32:03.420932: step 57380, loss = 0.1462, acc = 0.9600 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 05:32:08.051550: step 57400, loss = 0.1062, acc = 0.9720 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 05:32:12.669497: step 57420, loss = 0.1230, acc = 0.9540 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 05:32:17.669991: step 57440, loss = 0.0887, acc = 0.9780 (248.8 examples/sec; 0.257 sec/batch)
2017-05-09 05:32:23.260312: step 57460, loss = 0.1270, acc = 0.9660 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 05:32:27.819079: step 57480, loss = 0.1135, acc = 0.9680 (301.6 examples/sec; 0.212 sec/batch)
2017-05-09 05:32:32.458029: step 57500, loss = 0.1236, acc = 0.9520 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 05:32:36.988782: step 57520, loss = 0.1275, acc = 0.9580 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 05:32:41.614049: step 57540, loss = 0.1027, acc = 0.9780 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 05:32:46.279242: step 57560, loss = 0.1589, acc = 0.9480 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 05:32:50.821250: step 57580, loss = 0.1235, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 05:32:55.390653: step 57600, loss = 0.1513, acc = 0.9480 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 05:33:00.204197: step 57620, loss = 0.1260, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 05:33:04.710116: step 57640, loss = 0.1071, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 05:33:09.381998: step 57660, loss = 0.1045, acc = 0.9720 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 05:33:13.994771: step 57680, loss = 0.1012, acc = 0.9740 (299.4 examples/sec; 0.214 sec/batch)
2017-05-09 05:33:18.550810: step 57700, loss = 0.1310, acc = 0.9600 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:33:23.176400: step 57720, loss = 0.1336, acc = 0.9520 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 05:33:27.862814: step 57740, loss = 0.1131, acc = 0.9540 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 05:33:32.492840: step 57760, loss = 0.1096, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 05:33:37.166949: step 57780, loss = 0.1166, acc = 0.9640 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 05:33:41.745346: step 57800, loss = 0.1219, acc = 0.9560 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 05:33:46.310021: step 57820, loss = 0.1272, acc = 0.9600 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 05:33:50.900360: step 57840, loss = 0.1419, acc = 0.9540 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 05:33:55.508006: step 57860, loss = 0.1321, acc = 0.9540 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 05:34:00.233994: step 57880, loss = 0.1202, acc = 0.9660 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 05:34:04.790870: step 57900, loss = 0.1383, acc = 0.9620 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:34:09.315157: step 57920, loss = 0.1101, acc = 0.9660 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 05:34:13.982600: step 57940, loss = 0.1316, acc = 0.9620 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 05:34:18.496847: step 57960, loss = 0.1475, acc = 0.9580 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 05:34:23.184132: step 57980, loss = 0.1363, acc = 0.9540 (256.0 examples/sec; 0.250 sec/batch)
2017-05-09 05:34:28.117158: step 58000, loss = 0.1414, acc = 0.9520 (212.0 examples/sec; 0.302 sec/batch)
[Eval] 2017-05-09 05:34:41.880572: step 58000, acc = 0.9626, f1 = 0.9614
[Test] 2017-05-09 05:34:51.421361: step 58000, acc = 0.9541, f1 = 0.9537
[Status] 2017-05-09 05:34:51.421438: step 58000, maxindex = 58000, maxdev = 0.9626, maxtst = 0.9541
2017-05-09 05:35:00.066255: step 58020, loss = 0.1159, acc = 0.9620 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 05:35:04.735190: step 58040, loss = 0.1227, acc = 0.9640 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 05:35:09.242317: step 58060, loss = 0.1149, acc = 0.9700 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 05:35:14.047526: step 58080, loss = 0.1228, acc = 0.9620 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 05:35:18.605035: step 58100, loss = 0.1260, acc = 0.9520 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 05:35:23.133919: step 58120, loss = 0.1844, acc = 0.9400 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 05:35:27.781646: step 58140, loss = 0.1465, acc = 0.9500 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 05:35:32.310982: step 58160, loss = 0.1124, acc = 0.9740 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 05:35:36.803285: step 58180, loss = 0.1182, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:35:41.588721: step 58200, loss = 0.0872, acc = 0.9800 (223.7 examples/sec; 0.286 sec/batch)
2017-05-09 05:35:46.332102: step 58220, loss = 0.0993, acc = 0.9740 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 05:35:50.961832: step 58240, loss = 0.0957, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:35:55.828977: step 58260, loss = 0.1504, acc = 0.9540 (237.1 examples/sec; 0.270 sec/batch)
2017-05-09 05:36:00.694365: step 58280, loss = 0.1427, acc = 0.9480 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 05:36:05.183797: step 58300, loss = 0.1201, acc = 0.9580 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 05:36:09.922369: step 58320, loss = 0.1279, acc = 0.9620 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 05:36:14.774715: step 58340, loss = 0.1056, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:36:19.479508: step 58360, loss = 0.1306, acc = 0.9580 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 05:36:24.190856: step 58380, loss = 0.1120, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 05:36:28.890653: step 58400, loss = 0.1060, acc = 0.9660 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 05:36:33.573690: step 58420, loss = 0.1212, acc = 0.9640 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 05:36:38.286088: step 58440, loss = 0.1160, acc = 0.9700 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 05:36:43.720500: step 58460, loss = 0.1380, acc = 0.9500 (163.2 examples/sec; 0.392 sec/batch)
2017-05-09 05:36:48.256111: step 58480, loss = 0.0994, acc = 0.9740 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 05:36:52.811721: step 58500, loss = 0.1290, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 05:36:57.524207: step 58520, loss = 0.1242, acc = 0.9600 (237.5 examples/sec; 0.269 sec/batch)
2017-05-09 05:37:02.064889: step 58540, loss = 0.1226, acc = 0.9680 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 05:37:06.499339: step 58560, loss = 0.1264, acc = 0.9680 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 05:37:10.992170: step 58580, loss = 0.1353, acc = 0.9540 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 05:37:15.774124: step 58600, loss = 0.1232, acc = 0.9640 (260.7 examples/sec; 0.246 sec/batch)
2017-05-09 05:37:20.515516: step 58620, loss = 0.1068, acc = 0.9620 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 05:37:25.129489: step 58640, loss = 0.1325, acc = 0.9600 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 05:37:29.835728: step 58660, loss = 0.1046, acc = 0.9680 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:37:34.510132: step 58680, loss = 0.1277, acc = 0.9660 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 05:37:39.101020: step 58700, loss = 0.1311, acc = 0.9560 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 05:37:43.953320: step 58720, loss = 0.1258, acc = 0.9620 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:37:48.561404: step 58740, loss = 0.1309, acc = 0.9520 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 05:37:53.215555: step 58760, loss = 0.1398, acc = 0.9560 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 05:37:57.873597: step 58780, loss = 0.1658, acc = 0.9460 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 05:38:02.276674: step 58800, loss = 0.1231, acc = 0.9640 (297.9 examples/sec; 0.215 sec/batch)
2017-05-09 05:38:06.905808: step 58820, loss = 0.0911, acc = 0.9740 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 05:38:11.536875: step 58840, loss = 0.1145, acc = 0.9680 (245.0 examples/sec; 0.261 sec/batch)
2017-05-09 05:38:16.165841: step 58860, loss = 0.1262, acc = 0.9560 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 05:38:20.967973: step 58880, loss = 0.1316, acc = 0.9540 (228.5 examples/sec; 0.280 sec/batch)
2017-05-09 05:38:25.742534: step 58900, loss = 0.1288, acc = 0.9660 (236.9 examples/sec; 0.270 sec/batch)
2017-05-09 05:38:30.281367: step 58920, loss = 0.1489, acc = 0.9600 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 05:38:34.759495: step 58940, loss = 0.1247, acc = 0.9640 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 05:38:39.309614: step 58960, loss = 0.1114, acc = 0.9620 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 05:38:44.074071: step 58980, loss = 0.1306, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 05:38:48.718933: step 59000, loss = 0.1214, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 05:39:02.753068: step 59000, acc = 0.9625, f1 = 0.9613
[Test] 2017-05-09 05:39:12.582499: step 59000, acc = 0.9543, f1 = 0.9539
[Status] 2017-05-09 05:39:12.582602: step 59000, maxindex = 58000, maxdev = 0.9626, maxtst = 0.9541
2017-05-09 05:39:17.159259: step 59020, loss = 0.1176, acc = 0.9720 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 05:39:21.782925: step 59040, loss = 0.1347, acc = 0.9560 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 05:39:26.413038: step 59060, loss = 0.1193, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:39:30.894182: step 59080, loss = 0.1386, acc = 0.9580 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 05:39:35.467492: step 59100, loss = 0.0998, acc = 0.9720 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 05:39:40.356853: step 59120, loss = 0.1231, acc = 0.9700 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 05:39:44.853338: step 59140, loss = 0.1460, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 05:39:49.427872: step 59160, loss = 0.1089, acc = 0.9740 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 05:39:54.284771: step 59180, loss = 0.1110, acc = 0.9600 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 05:39:58.941785: step 59200, loss = 0.1545, acc = 0.9480 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 05:40:03.481092: step 59220, loss = 0.1298, acc = 0.9580 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 05:40:08.194934: step 59240, loss = 0.1610, acc = 0.9460 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 05:40:12.789189: step 59260, loss = 0.1235, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:40:17.396582: step 59280, loss = 0.1148, acc = 0.9600 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 05:40:22.033093: step 59300, loss = 0.1352, acc = 0.9560 (252.4 examples/sec; 0.254 sec/batch)
2017-05-09 05:40:26.638530: step 59320, loss = 0.1256, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 05:40:31.219735: step 59340, loss = 0.1195, acc = 0.9580 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 05:40:35.839644: step 59360, loss = 0.1250, acc = 0.9580 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 05:40:40.528073: step 59380, loss = 0.1006, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 05:40:45.187161: step 59400, loss = 0.0978, acc = 0.9660 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 05:40:49.784617: step 59420, loss = 0.1280, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 05:40:54.346796: step 59440, loss = 0.1369, acc = 0.9480 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 05:40:59.070116: step 59460, loss = 0.1502, acc = 0.9460 (251.7 examples/sec; 0.254 sec/batch)
2017-05-09 05:41:04.650188: step 59480, loss = 0.1436, acc = 0.9480 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 05:41:09.449097: step 59500, loss = 0.1091, acc = 0.9600 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 05:41:14.123345: step 59520, loss = 0.1585, acc = 0.9360 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 05:41:18.667627: step 59540, loss = 0.1457, acc = 0.9500 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 05:41:23.564533: step 59560, loss = 0.1092, acc = 0.9660 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 05:41:28.094466: step 59580, loss = 0.1109, acc = 0.9700 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 05:41:32.755817: step 59600, loss = 0.1114, acc = 0.9640 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 05:41:37.361569: step 59620, loss = 0.1219, acc = 0.9600 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:41:41.945938: step 59640, loss = 0.1048, acc = 0.9760 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 05:41:46.402518: step 59660, loss = 0.1154, acc = 0.9680 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 05:41:51.097096: step 59680, loss = 0.1341, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 05:41:55.638309: step 59700, loss = 0.1119, acc = 0.9640 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 05:42:00.200694: step 59720, loss = 0.1164, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 05:42:04.807240: step 59740, loss = 0.1348, acc = 0.9500 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:42:09.604862: step 59760, loss = 0.1184, acc = 0.9620 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 05:42:14.192178: step 59780, loss = 0.1207, acc = 0.9640 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 05:42:18.814354: step 59800, loss = 0.1373, acc = 0.9520 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 05:42:23.583084: step 59820, loss = 0.1347, acc = 0.9600 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 05:42:28.221888: step 59840, loss = 0.1117, acc = 0.9680 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 05:42:32.801160: step 59860, loss = 0.1236, acc = 0.9560 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 05:42:37.574772: step 59880, loss = 0.1092, acc = 0.9580 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 05:42:42.137509: step 59900, loss = 0.1061, acc = 0.9720 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 05:42:46.892504: step 59920, loss = 0.1573, acc = 0.9440 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 05:42:51.690779: step 59940, loss = 0.1302, acc = 0.9520 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 05:42:56.404588: step 59960, loss = 0.1178, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 05:43:01.029626: step 59980, loss = 0.1166, acc = 0.9680 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 05:43:05.932801: step 60000, loss = 0.1211, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 05:43:20.011020: step 60000, acc = 0.9619, f1 = 0.9608
[Test] 2017-05-09 05:43:29.576960: step 60000, acc = 0.9541, f1 = 0.9538
[Status] 2017-05-09 05:43:29.577053: step 60000, maxindex = 58000, maxdev = 0.9626, maxtst = 0.9541
2017-05-09 05:43:34.335218: step 60020, loss = 0.1229, acc = 0.9580 (247.7 examples/sec; 0.258 sec/batch)
2017-05-09 05:43:38.896044: step 60040, loss = 0.1184, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 05:43:43.502647: step 60060, loss = 0.1262, acc = 0.9600 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 05:43:48.032777: step 60080, loss = 0.1233, acc = 0.9540 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 05:43:53.022709: step 60100, loss = 0.1127, acc = 0.9740 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 05:43:57.746200: step 60120, loss = 0.1298, acc = 0.9600 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 05:44:02.383219: step 60140, loss = 0.1230, acc = 0.9580 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 05:44:07.084347: step 60160, loss = 0.1410, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 05:44:11.666033: step 60180, loss = 0.1254, acc = 0.9600 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 05:44:16.178977: step 60200, loss = 0.1313, acc = 0.9640 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 05:44:20.957184: step 60220, loss = 0.1349, acc = 0.9500 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 05:44:25.614788: step 60240, loss = 0.1219, acc = 0.9500 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 05:44:30.198797: step 60260, loss = 0.1041, acc = 0.9740 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 05:44:34.929091: step 60280, loss = 0.1274, acc = 0.9580 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 05:44:39.468687: step 60300, loss = 0.1406, acc = 0.9560 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 05:44:43.998516: step 60320, loss = 0.0969, acc = 0.9720 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 05:44:48.719679: step 60340, loss = 0.1310, acc = 0.9580 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 05:44:53.307424: step 60360, loss = 0.1094, acc = 0.9680 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 05:44:57.891787: step 60380, loss = 0.1133, acc = 0.9620 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 05:45:02.579690: step 60400, loss = 0.1013, acc = 0.9760 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 05:45:07.406132: step 60420, loss = 0.1048, acc = 0.9680 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 05:45:11.942629: step 60440, loss = 0.1122, acc = 0.9640 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 05:45:16.503309: step 60460, loss = 0.1144, acc = 0.9680 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 05:45:21.929628: step 60480, loss = 0.1186, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:45:26.537428: step 60500, loss = 0.1391, acc = 0.9480 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:45:31.092372: step 60520, loss = 0.1034, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 05:45:35.837940: step 60540, loss = 0.1133, acc = 0.9640 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 05:45:40.386641: step 60560, loss = 0.1321, acc = 0.9580 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:45:44.999615: step 60580, loss = 0.1381, acc = 0.9480 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 05:45:49.752073: step 60600, loss = 0.1200, acc = 0.9660 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:45:54.355489: step 60620, loss = 0.0970, acc = 0.9780 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 05:45:58.875264: step 60640, loss = 0.1447, acc = 0.9600 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 05:46:03.692730: step 60660, loss = 0.1293, acc = 0.9560 (223.3 examples/sec; 0.287 sec/batch)
2017-05-09 05:46:08.448147: step 60680, loss = 0.1403, acc = 0.9580 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 05:46:13.057788: step 60700, loss = 0.1121, acc = 0.9580 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 05:46:17.673794: step 60720, loss = 0.1188, acc = 0.9620 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 05:46:22.319569: step 60740, loss = 0.1228, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 05:46:26.950110: step 60760, loss = 0.1380, acc = 0.9480 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:46:31.498487: step 60780, loss = 0.1312, acc = 0.9500 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 05:46:36.278527: step 60800, loss = 0.1055, acc = 0.9700 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 05:46:40.890966: step 60820, loss = 0.1066, acc = 0.9680 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:46:45.484462: step 60840, loss = 0.1298, acc = 0.9520 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 05:46:50.095762: step 60860, loss = 0.1074, acc = 0.9740 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 05:46:54.730637: step 60880, loss = 0.1240, acc = 0.9620 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:46:59.316628: step 60900, loss = 0.0971, acc = 0.9740 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:47:03.960788: step 60920, loss = 0.1121, acc = 0.9600 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 05:47:08.561904: step 60940, loss = 0.1276, acc = 0.9480 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:47:13.057053: step 60960, loss = 0.1179, acc = 0.9660 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 05:47:17.846151: step 60980, loss = 0.1567, acc = 0.9520 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 05:47:22.460750: step 61000, loss = 0.1125, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 05:47:36.520364: step 61000, acc = 0.9613, f1 = 0.9601
[Test] 2017-05-09 05:47:46.383663: step 61000, acc = 0.9536, f1 = 0.9532
[Status] 2017-05-09 05:47:46.383754: step 61000, maxindex = 58000, maxdev = 0.9626, maxtst = 0.9541
2017-05-09 05:47:50.906028: step 61020, loss = 0.1251, acc = 0.9660 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 05:47:55.507454: step 61040, loss = 0.1162, acc = 0.9600 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 05:48:00.158980: step 61060, loss = 0.1309, acc = 0.9520 (243.4 examples/sec; 0.263 sec/batch)
2017-05-09 05:48:04.730614: step 61080, loss = 0.1312, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 05:48:09.220187: step 61100, loss = 0.1185, acc = 0.9680 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 05:48:13.829070: step 61120, loss = 0.1223, acc = 0.9660 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 05:48:18.565281: step 61140, loss = 0.1027, acc = 0.9700 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:48:23.119738: step 61160, loss = 0.1287, acc = 0.9620 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 05:48:27.803650: step 61180, loss = 0.1324, acc = 0.9600 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 05:48:32.469348: step 61200, loss = 0.1219, acc = 0.9640 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 05:48:37.052551: step 61220, loss = 0.1325, acc = 0.9560 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 05:48:41.681312: step 61240, loss = 0.1013, acc = 0.9660 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 05:48:46.352929: step 61260, loss = 0.1316, acc = 0.9560 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 05:48:50.894421: step 61280, loss = 0.1159, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 05:48:55.502002: step 61300, loss = 0.1197, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 05:49:00.385633: step 61320, loss = 0.1142, acc = 0.9660 (222.2 examples/sec; 0.288 sec/batch)
2017-05-09 05:49:04.968242: step 61340, loss = 0.1442, acc = 0.9420 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:49:09.752017: step 61360, loss = 0.1053, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 05:49:14.380816: step 61380, loss = 0.1159, acc = 0.9600 (252.4 examples/sec; 0.254 sec/batch)
2017-05-09 05:49:19.247377: step 61400, loss = 0.1187, acc = 0.9560 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 05:49:23.909530: step 61420, loss = 0.1131, acc = 0.9580 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 05:49:28.509749: step 61440, loss = 0.1348, acc = 0.9580 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:49:33.147160: step 61460, loss = 0.1208, acc = 0.9700 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 05:49:37.720189: step 61480, loss = 0.1290, acc = 0.9500 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 05:49:43.338000: step 61500, loss = 0.1166, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:49:48.033290: step 61520, loss = 0.1309, acc = 0.9560 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 05:49:52.608240: step 61540, loss = 0.1350, acc = 0.9580 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 05:49:57.136191: step 61560, loss = 0.1378, acc = 0.9520 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 05:50:01.995618: step 61580, loss = 0.1308, acc = 0.9580 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 05:50:06.528223: step 61600, loss = 0.1163, acc = 0.9620 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 05:50:11.454369: step 61620, loss = 0.1283, acc = 0.9640 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 05:50:16.198332: step 61640, loss = 0.1202, acc = 0.9560 (297.9 examples/sec; 0.215 sec/batch)
2017-05-09 05:50:20.720233: step 61660, loss = 0.1319, acc = 0.9580 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 05:50:25.374940: step 61680, loss = 0.1409, acc = 0.9540 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 05:50:30.013196: step 61700, loss = 0.1211, acc = 0.9700 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 05:50:34.881297: step 61720, loss = 0.1201, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 05:50:39.528158: step 61740, loss = 0.1096, acc = 0.9700 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 05:50:44.050585: step 61760, loss = 0.1266, acc = 0.9540 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 05:50:49.016564: step 61780, loss = 0.1225, acc = 0.9680 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 05:50:53.591907: step 61800, loss = 0.1083, acc = 0.9660 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:50:58.172482: step 61820, loss = 0.1055, acc = 0.9680 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 05:51:02.822486: step 61840, loss = 0.1187, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 05:51:07.370641: step 61860, loss = 0.1502, acc = 0.9520 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 05:51:11.844991: step 61880, loss = 0.1122, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 05:51:16.520068: step 61900, loss = 0.1229, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 05:51:21.058032: step 61920, loss = 0.1124, acc = 0.9700 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 05:51:25.666523: step 61940, loss = 0.1219, acc = 0.9480 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 05:51:30.305060: step 61960, loss = 0.1035, acc = 0.9700 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 05:51:34.913497: step 61980, loss = 0.0902, acc = 0.9760 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 05:51:39.482321: step 62000, loss = 0.0857, acc = 0.9840 (280.6 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 05:51:53.537489: step 62000, acc = 0.9628, f1 = 0.9615
[Test] 2017-05-09 05:52:03.593463: step 62000, acc = 0.9543, f1 = 0.9540
[Status] 2017-05-09 05:52:03.593541: step 62000, maxindex = 62000, maxdev = 0.9628, maxtst = 0.9543
2017-05-09 05:52:11.523163: step 62020, loss = 0.1120, acc = 0.9680 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 05:52:16.153303: step 62040, loss = 0.0956, acc = 0.9740 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:52:20.629634: step 62060, loss = 0.1167, acc = 0.9720 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 05:52:25.274815: step 62080, loss = 0.1390, acc = 0.9560 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 05:52:29.982641: step 62100, loss = 0.0962, acc = 0.9700 (295.6 examples/sec; 0.217 sec/batch)
2017-05-09 05:52:34.562313: step 62120, loss = 0.1231, acc = 0.9620 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 05:52:39.209728: step 62140, loss = 0.1321, acc = 0.9600 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 05:52:44.103310: step 62160, loss = 0.1288, acc = 0.9580 (235.7 examples/sec; 0.271 sec/batch)
2017-05-09 05:52:48.711777: step 62180, loss = 0.0971, acc = 0.9660 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 05:52:53.294114: step 62200, loss = 0.1090, acc = 0.9800 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 05:52:57.967505: step 62220, loss = 0.1010, acc = 0.9720 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 05:53:02.793123: step 62240, loss = 0.1240, acc = 0.9500 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 05:53:07.460904: step 62260, loss = 0.1299, acc = 0.9600 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 05:53:12.071334: step 62280, loss = 0.1267, acc = 0.9540 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 05:53:16.983787: step 62300, loss = 0.1143, acc = 0.9740 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 05:53:21.461943: step 62320, loss = 0.0857, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 05:53:25.994150: step 62340, loss = 0.1069, acc = 0.9720 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 05:53:30.984392: step 62360, loss = 0.1278, acc = 0.9580 (249.1 examples/sec; 0.257 sec/batch)
2017-05-09 05:53:35.619006: step 62380, loss = 0.1236, acc = 0.9640 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 05:53:40.197304: step 62400, loss = 0.1393, acc = 0.9580 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 05:53:45.029633: step 62420, loss = 0.1070, acc = 0.9720 (231.5 examples/sec; 0.277 sec/batch)
2017-05-09 05:53:49.734575: step 62440, loss = 0.1162, acc = 0.9700 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 05:53:54.303666: step 62460, loss = 0.1182, acc = 0.9600 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 05:53:59.054616: step 62480, loss = 0.1102, acc = 0.9580 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 05:54:04.193668: step 62500, loss = 0.1246, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 05:54:08.685955: step 62520, loss = 0.1169, acc = 0.9620 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 05:54:13.295191: step 62540, loss = 0.1207, acc = 0.9620 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 05:54:17.969506: step 62560, loss = 0.1211, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 05:54:22.543492: step 62580, loss = 0.1236, acc = 0.9560 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 05:54:27.130170: step 62600, loss = 0.1353, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 05:54:31.876418: step 62620, loss = 0.1342, acc = 0.9580 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 05:54:36.539249: step 62640, loss = 0.1404, acc = 0.9560 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 05:54:41.119525: step 62660, loss = 0.1256, acc = 0.9620 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 05:54:45.946254: step 62680, loss = 0.1231, acc = 0.9620 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 05:54:50.506128: step 62700, loss = 0.1178, acc = 0.9640 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 05:54:54.985327: step 62720, loss = 0.1137, acc = 0.9620 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 05:54:59.660729: step 62740, loss = 0.1164, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 05:55:04.220194: step 62760, loss = 0.0963, acc = 0.9720 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 05:55:08.850746: step 62780, loss = 0.1121, acc = 0.9600 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 05:55:13.568879: step 62800, loss = 0.1146, acc = 0.9540 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 05:55:18.230161: step 62820, loss = 0.1249, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 05:55:22.821406: step 62840, loss = 0.1024, acc = 0.9740 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 05:55:27.462439: step 62860, loss = 0.1004, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:55:32.131962: step 62880, loss = 0.1262, acc = 0.9600 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 05:55:36.589695: step 62900, loss = 0.1073, acc = 0.9720 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 05:55:41.170784: step 62920, loss = 0.1268, acc = 0.9620 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 05:55:45.846888: step 62940, loss = 0.1280, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 05:55:50.479513: step 62960, loss = 0.1014, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 05:55:55.054261: step 62980, loss = 0.1102, acc = 0.9740 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 05:55:59.923552: step 63000, loss = 0.1119, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 05:56:14.107417: step 63000, acc = 0.9603, f1 = 0.9590
[Test] 2017-05-09 05:56:23.585521: step 63000, acc = 0.9513, f1 = 0.9509
[Status] 2017-05-09 05:56:23.585641: step 63000, maxindex = 62000, maxdev = 0.9628, maxtst = 0.9543
2017-05-09 05:56:28.287643: step 63020, loss = 0.1349, acc = 0.9580 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 05:56:32.868289: step 63040, loss = 0.1249, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:56:37.514430: step 63060, loss = 0.1582, acc = 0.9520 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 05:56:42.332187: step 63080, loss = 0.1700, acc = 0.9520 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 05:56:46.895025: step 63100, loss = 0.1092, acc = 0.9800 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 05:56:51.439360: step 63120, loss = 0.1336, acc = 0.9520 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 05:56:56.157816: step 63140, loss = 0.1109, acc = 0.9660 (243.1 examples/sec; 0.263 sec/batch)
2017-05-09 05:57:00.783072: step 63160, loss = 0.1047, acc = 0.9760 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 05:57:05.325349: step 63180, loss = 0.1473, acc = 0.9480 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 05:57:09.922144: step 63200, loss = 0.1008, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 05:57:14.611475: step 63220, loss = 0.1394, acc = 0.9580 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 05:57:19.551261: step 63240, loss = 0.1325, acc = 0.9520 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 05:57:24.383531: step 63260, loss = 0.1105, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 05:57:29.152184: step 63280, loss = 0.1121, acc = 0.9660 (297.0 examples/sec; 0.215 sec/batch)
2017-05-09 05:57:33.790514: step 63300, loss = 0.1158, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 05:57:38.517245: step 63320, loss = 0.1334, acc = 0.9560 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 05:57:43.474802: step 63340, loss = 0.1314, acc = 0.9600 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 05:57:47.994644: step 63360, loss = 0.1085, acc = 0.9680 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 05:57:52.482629: step 63380, loss = 0.1093, acc = 0.9740 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 05:57:57.149699: step 63400, loss = 0.1345, acc = 0.9640 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 05:58:01.694543: step 63420, loss = 0.0894, acc = 0.9820 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 05:58:06.222668: step 63440, loss = 0.1136, acc = 0.9680 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 05:58:10.918931: step 63460, loss = 0.0971, acc = 0.9820 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 05:58:15.513149: step 63480, loss = 0.1302, acc = 0.9600 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 05:58:21.100979: step 63500, loss = 0.1110, acc = 0.9540 (132.5 examples/sec; 0.483 sec/batch)
2017-05-09 05:58:26.042611: step 63520, loss = 0.1169, acc = 0.9660 (231.0 examples/sec; 0.277 sec/batch)
2017-05-09 05:58:30.671451: step 63540, loss = 0.1383, acc = 0.9580 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 05:58:35.373974: step 63560, loss = 0.1383, acc = 0.9520 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 05:58:40.016499: step 63580, loss = 0.1226, acc = 0.9580 (245.0 examples/sec; 0.261 sec/batch)
2017-05-09 05:58:44.582750: step 63600, loss = 0.1115, acc = 0.9640 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 05:58:49.197210: step 63620, loss = 0.1065, acc = 0.9660 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 05:58:53.782007: step 63640, loss = 0.1287, acc = 0.9700 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 05:58:58.457676: step 63660, loss = 0.0975, acc = 0.9800 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 05:59:02.991241: step 63680, loss = 0.1371, acc = 0.9600 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 05:59:07.571564: step 63700, loss = 0.0969, acc = 0.9680 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 05:59:12.214440: step 63720, loss = 0.1301, acc = 0.9480 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 05:59:16.813474: step 63740, loss = 0.1199, acc = 0.9640 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 05:59:21.349498: step 63760, loss = 0.1324, acc = 0.9580 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 05:59:26.006229: step 63780, loss = 0.0952, acc = 0.9800 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 05:59:30.570551: step 63800, loss = 0.1322, acc = 0.9580 (298.3 examples/sec; 0.215 sec/batch)
2017-05-09 05:59:35.124918: step 63820, loss = 0.1083, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 05:59:39.860081: step 63840, loss = 0.1208, acc = 0.9700 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 05:59:44.378931: step 63860, loss = 0.1173, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 05:59:48.893078: step 63880, loss = 0.1060, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 05:59:53.519550: step 63900, loss = 0.1234, acc = 0.9720 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 05:59:58.263571: step 63920, loss = 0.1138, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 06:00:02.863532: step 63940, loss = 0.1199, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 06:00:07.476984: step 63960, loss = 0.1220, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 06:00:12.207473: step 63980, loss = 0.1100, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 06:00:16.834208: step 64000, loss = 0.1626, acc = 0.9500 (271.9 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 06:00:30.865778: step 64000, acc = 0.9613, f1 = 0.9601
[Test] 2017-05-09 06:00:40.510018: step 64000, acc = 0.9533, f1 = 0.9529
[Status] 2017-05-09 06:00:40.510110: step 64000, maxindex = 62000, maxdev = 0.9628, maxtst = 0.9543
2017-05-09 06:00:45.102586: step 64020, loss = 0.1221, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 06:00:49.580346: step 64040, loss = 0.1237, acc = 0.9660 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 06:00:54.351787: step 64060, loss = 0.1264, acc = 0.9500 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 06:00:58.974159: step 64080, loss = 0.1306, acc = 0.9580 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 06:01:03.596258: step 64100, loss = 0.1114, acc = 0.9660 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 06:01:08.189489: step 64120, loss = 0.1128, acc = 0.9660 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 06:01:12.920670: step 64140, loss = 0.1320, acc = 0.9520 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 06:01:17.487992: step 64160, loss = 0.0934, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 06:01:22.166715: step 64180, loss = 0.1343, acc = 0.9620 (301.4 examples/sec; 0.212 sec/batch)
2017-05-09 06:01:26.585905: step 64200, loss = 0.1492, acc = 0.9560 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 06:01:31.285605: step 64220, loss = 0.1228, acc = 0.9640 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 06:01:35.985657: step 64240, loss = 0.0955, acc = 0.9740 (242.6 examples/sec; 0.264 sec/batch)
2017-05-09 06:01:40.545550: step 64260, loss = 0.1427, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 06:01:45.128252: step 64280, loss = 0.1070, acc = 0.9640 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 06:01:49.691254: step 64300, loss = 0.1117, acc = 0.9620 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 06:01:54.475334: step 64320, loss = 0.1253, acc = 0.9700 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 06:01:59.081649: step 64340, loss = 0.1103, acc = 0.9680 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 06:02:03.674015: step 64360, loss = 0.1140, acc = 0.9640 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 06:02:08.365429: step 64380, loss = 0.1367, acc = 0.9480 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 06:02:12.961882: step 64400, loss = 0.1356, acc = 0.9580 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 06:02:17.511778: step 64420, loss = 0.1276, acc = 0.9580 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 06:02:22.369000: step 64440, loss = 0.1229, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 06:02:27.208908: step 64460, loss = 0.1238, acc = 0.9600 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 06:02:31.857402: step 64480, loss = 0.1049, acc = 0.9620 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 06:02:36.590331: step 64500, loss = 0.1280, acc = 0.9620 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 06:02:41.853703: step 64520, loss = 0.1081, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 06:02:46.418220: step 64540, loss = 0.1169, acc = 0.9580 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 06:02:51.166339: step 64560, loss = 0.0954, acc = 0.9780 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 06:02:55.751053: step 64580, loss = 0.0942, acc = 0.9760 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 06:03:00.437975: step 64600, loss = 0.0780, acc = 0.9840 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 06:03:05.053603: step 64620, loss = 0.1210, acc = 0.9580 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 06:03:09.721081: step 64640, loss = 0.1233, acc = 0.9580 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 06:03:14.255480: step 64660, loss = 0.1170, acc = 0.9740 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 06:03:18.939409: step 64680, loss = 0.1134, acc = 0.9660 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 06:03:23.687828: step 64700, loss = 0.1245, acc = 0.9620 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 06:03:28.295889: step 64720, loss = 0.1100, acc = 0.9780 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 06:03:32.860681: step 64740, loss = 0.1580, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:03:37.586894: step 64760, loss = 0.1033, acc = 0.9760 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 06:03:42.226609: step 64780, loss = 0.1113, acc = 0.9680 (250.9 examples/sec; 0.255 sec/batch)
2017-05-09 06:03:46.855633: step 64800, loss = 0.1221, acc = 0.9580 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 06:03:51.540408: step 64820, loss = 0.1080, acc = 0.9640 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 06:03:56.064408: step 64840, loss = 0.1081, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 06:04:00.557399: step 64860, loss = 0.1115, acc = 0.9680 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 06:04:05.109822: step 64880, loss = 0.1121, acc = 0.9700 (255.7 examples/sec; 0.250 sec/batch)
2017-05-09 06:04:09.590189: step 64900, loss = 0.0979, acc = 0.9740 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 06:04:14.189424: step 64920, loss = 0.1221, acc = 0.9580 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 06:04:18.744334: step 64940, loss = 0.1121, acc = 0.9680 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 06:04:23.653813: step 64960, loss = 0.1026, acc = 0.9680 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 06:04:28.344837: step 64980, loss = 0.1328, acc = 0.9560 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 06:04:32.981339: step 65000, loss = 0.0919, acc = 0.9680 (272.6 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 06:04:46.986585: step 65000, acc = 0.9630, f1 = 0.9618
[Test] 2017-05-09 06:04:56.730200: step 65000, acc = 0.9544, f1 = 0.9540
[Status] 2017-05-09 06:04:56.730269: step 65000, maxindex = 65000, maxdev = 0.9630, maxtst = 0.9544
2017-05-09 06:05:04.917406: step 65020, loss = 0.1160, acc = 0.9700 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 06:05:09.661336: step 65040, loss = 0.1166, acc = 0.9680 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 06:05:14.299617: step 65060, loss = 0.1092, acc = 0.9680 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 06:05:19.098089: step 65080, loss = 0.1246, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 06:05:23.726001: step 65100, loss = 0.0891, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 06:05:28.292127: step 65120, loss = 0.1451, acc = 0.9540 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 06:05:32.969716: step 65140, loss = 0.1089, acc = 0.9740 (243.3 examples/sec; 0.263 sec/batch)
2017-05-09 06:05:37.491471: step 65160, loss = 0.1135, acc = 0.9640 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 06:05:42.151895: step 65180, loss = 0.1124, acc = 0.9580 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 06:05:46.844913: step 65200, loss = 0.1416, acc = 0.9460 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 06:05:51.483267: step 65220, loss = 0.1126, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 06:05:56.272319: step 65240, loss = 0.1332, acc = 0.9580 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 06:06:01.017865: step 65260, loss = 0.1099, acc = 0.9700 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 06:06:05.703416: step 65280, loss = 0.1220, acc = 0.9620 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 06:06:10.334742: step 65300, loss = 0.0921, acc = 0.9760 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 06:06:14.932009: step 65320, loss = 0.1290, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:06:19.603594: step 65340, loss = 0.1339, acc = 0.9620 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 06:06:24.146931: step 65360, loss = 0.1101, acc = 0.9720 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 06:06:28.834062: step 65380, loss = 0.1136, acc = 0.9700 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 06:06:33.534152: step 65400, loss = 0.1120, acc = 0.9620 (250.4 examples/sec; 0.256 sec/batch)
2017-05-09 06:06:38.100677: step 65420, loss = 0.1035, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 06:06:42.745714: step 65440, loss = 0.1093, acc = 0.9680 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 06:06:47.599083: step 65460, loss = 0.1151, acc = 0.9580 (227.1 examples/sec; 0.282 sec/batch)
2017-05-09 06:06:52.245374: step 65480, loss = 0.1107, acc = 0.9620 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 06:06:56.847602: step 65500, loss = 0.0939, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 06:07:02.490970: step 65520, loss = 0.1253, acc = 0.9620 (234.8 examples/sec; 0.273 sec/batch)
2017-05-09 06:07:07.017807: step 65540, loss = 0.1220, acc = 0.9660 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:07:11.624706: step 65560, loss = 0.1354, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 06:07:16.262697: step 65580, loss = 0.1303, acc = 0.9580 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 06:07:21.390528: step 65600, loss = 0.1171, acc = 0.9600 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 06:07:25.958452: step 65620, loss = 0.1191, acc = 0.9660 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 06:07:30.649720: step 65640, loss = 0.1043, acc = 0.9720 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 06:07:35.543967: step 65660, loss = 0.1246, acc = 0.9600 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 06:07:40.175267: step 65680, loss = 0.1266, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 06:07:44.775354: step 65700, loss = 0.1349, acc = 0.9600 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 06:07:49.626996: step 65720, loss = 0.1175, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 06:07:54.108671: step 65740, loss = 0.1066, acc = 0.9660 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:07:58.721781: step 65760, loss = 0.1249, acc = 0.9640 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 06:08:03.612515: step 65780, loss = 0.1115, acc = 0.9760 (241.3 examples/sec; 0.265 sec/batch)
2017-05-09 06:08:08.408224: step 65800, loss = 0.1182, acc = 0.9600 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 06:08:13.006409: step 65820, loss = 0.1075, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 06:08:17.530596: step 65840, loss = 0.1090, acc = 0.9700 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 06:08:22.337271: step 65860, loss = 0.1213, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:08:26.904307: step 65880, loss = 0.1198, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 06:08:31.416660: step 65900, loss = 0.0923, acc = 0.9740 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 06:08:36.092540: step 65920, loss = 0.1289, acc = 0.9560 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 06:08:40.693336: step 65940, loss = 0.1531, acc = 0.9420 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 06:08:45.150178: step 65960, loss = 0.1308, acc = 0.9680 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 06:08:49.811438: step 65980, loss = 0.1324, acc = 0.9540 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 06:08:54.485017: step 66000, loss = 0.1063, acc = 0.9640 (289.3 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 06:09:08.496233: step 66000, acc = 0.9628, f1 = 0.9617
[Test] 2017-05-09 06:09:18.099977: step 66000, acc = 0.9547, f1 = 0.9544
[Status] 2017-05-09 06:09:18.100056: step 66000, maxindex = 65000, maxdev = 0.9630, maxtst = 0.9544
2017-05-09 06:09:22.567072: step 66020, loss = 0.1138, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 06:09:27.277822: step 66040, loss = 0.0960, acc = 0.9780 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 06:09:32.054188: step 66060, loss = 0.1220, acc = 0.9680 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 06:09:36.634345: step 66080, loss = 0.1185, acc = 0.9660 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 06:09:41.207445: step 66100, loss = 0.1210, acc = 0.9580 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 06:09:46.008915: step 66120, loss = 0.1030, acc = 0.9700 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 06:09:50.575428: step 66140, loss = 0.1185, acc = 0.9560 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 06:09:55.202499: step 66160, loss = 0.1145, acc = 0.9660 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 06:09:59.895226: step 66180, loss = 0.1381, acc = 0.9540 (240.8 examples/sec; 0.266 sec/batch)
2017-05-09 06:10:04.507046: step 66200, loss = 0.0895, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:10:09.298962: step 66220, loss = 0.0977, acc = 0.9800 (233.6 examples/sec; 0.274 sec/batch)
2017-05-09 06:10:13.856917: step 66240, loss = 0.1288, acc = 0.9600 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 06:10:18.688154: step 66260, loss = 0.1346, acc = 0.9580 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 06:10:23.263532: step 66280, loss = 0.1108, acc = 0.9680 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 06:10:27.861952: step 66300, loss = 0.1369, acc = 0.9540 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 06:10:32.754275: step 66320, loss = 0.1232, acc = 0.9580 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 06:10:37.286678: step 66340, loss = 0.1403, acc = 0.9540 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 06:10:41.971729: step 66360, loss = 0.1012, acc = 0.9700 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 06:10:46.739828: step 66380, loss = 0.1167, acc = 0.9620 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 06:10:51.416642: step 66400, loss = 0.1235, acc = 0.9540 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 06:10:55.991534: step 66420, loss = 0.1356, acc = 0.9600 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 06:11:00.697188: step 66440, loss = 0.1087, acc = 0.9680 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 06:11:05.201417: step 66460, loss = 0.1027, acc = 0.9720 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 06:11:09.739783: step 66480, loss = 0.1129, acc = 0.9660 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 06:11:14.390028: step 66500, loss = 0.1008, acc = 0.9660 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 06:11:19.642000: step 66520, loss = 0.1307, acc = 0.9520 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 06:11:24.990635: step 66540, loss = 0.1226, acc = 0.9720 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 06:11:29.686478: step 66560, loss = 0.1073, acc = 0.9600 (233.1 examples/sec; 0.275 sec/batch)
2017-05-09 06:11:34.305733: step 66580, loss = 0.1057, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 06:11:39.007379: step 66600, loss = 0.1156, acc = 0.9620 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 06:11:43.631057: step 66620, loss = 0.0867, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 06:11:48.371501: step 66640, loss = 0.1447, acc = 0.9480 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 06:11:53.000151: step 66660, loss = 0.1567, acc = 0.9540 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 06:11:57.645939: step 66680, loss = 0.1339, acc = 0.9700 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 06:12:02.524012: step 66700, loss = 0.0832, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:12:07.086975: step 66720, loss = 0.1140, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:12:11.638524: step 66740, loss = 0.1047, acc = 0.9760 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 06:12:16.133644: step 66760, loss = 0.1246, acc = 0.9700 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 06:12:20.802841: step 66780, loss = 0.0977, acc = 0.9720 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 06:12:25.549073: step 66800, loss = 0.1259, acc = 0.9620 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 06:12:30.131395: step 66820, loss = 0.1432, acc = 0.9460 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 06:12:34.955015: step 66840, loss = 0.1269, acc = 0.9540 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 06:12:39.521447: step 66860, loss = 0.1129, acc = 0.9740 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 06:12:44.273395: step 66880, loss = 0.1615, acc = 0.9460 (251.0 examples/sec; 0.255 sec/batch)
2017-05-09 06:12:48.954548: step 66900, loss = 0.1274, acc = 0.9540 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:12:53.523205: step 66920, loss = 0.1080, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 06:12:58.240775: step 66940, loss = 0.0926, acc = 0.9720 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 06:13:02.927212: step 66960, loss = 0.1033, acc = 0.9680 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 06:13:07.434120: step 66980, loss = 0.1366, acc = 0.9660 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 06:13:12.093524: step 67000, loss = 0.1208, acc = 0.9560 (275.5 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 06:13:26.022584: step 67000, acc = 0.9630, f1 = 0.9617
[Test] 2017-05-09 06:13:35.683147: step 67000, acc = 0.9546, f1 = 0.9542
[Status] 2017-05-09 06:13:35.683242: step 67000, maxindex = 65000, maxdev = 0.9630, maxtst = 0.9544
2017-05-09 06:13:40.349135: step 67020, loss = 0.1322, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 06:13:45.212734: step 67040, loss = 0.1231, acc = 0.9640 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 06:13:49.758404: step 67060, loss = 0.1467, acc = 0.9480 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 06:13:54.420334: step 67080, loss = 0.1150, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 06:13:59.064242: step 67100, loss = 0.0986, acc = 0.9700 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 06:14:03.606238: step 67120, loss = 0.1101, acc = 0.9680 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 06:14:08.159903: step 67140, loss = 0.1139, acc = 0.9660 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 06:14:12.733620: step 67160, loss = 0.1279, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 06:14:17.465947: step 67180, loss = 0.0891, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 06:14:22.118365: step 67200, loss = 0.1046, acc = 0.9660 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 06:14:26.692127: step 67220, loss = 0.0926, acc = 0.9740 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 06:14:31.610916: step 67240, loss = 0.1194, acc = 0.9680 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 06:14:36.282834: step 67260, loss = 0.1201, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 06:14:40.819635: step 67280, loss = 0.1188, acc = 0.9720 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 06:14:45.612962: step 67300, loss = 0.1315, acc = 0.9620 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 06:14:50.175732: step 67320, loss = 0.1361, acc = 0.9620 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 06:14:54.751548: step 67340, loss = 0.1214, acc = 0.9560 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 06:14:59.459175: step 67360, loss = 0.1136, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 06:15:04.007161: step 67380, loss = 0.1314, acc = 0.9580 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 06:15:08.753850: step 67400, loss = 0.0879, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 06:15:13.491114: step 67420, loss = 0.1348, acc = 0.9580 (229.4 examples/sec; 0.279 sec/batch)
2017-05-09 06:15:18.120115: step 67440, loss = 0.1034, acc = 0.9760 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 06:15:22.675209: step 67460, loss = 0.1011, acc = 0.9720 (259.1 examples/sec; 0.247 sec/batch)
2017-05-09 06:15:27.161811: step 67480, loss = 0.1132, acc = 0.9620 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 06:15:31.928426: step 67500, loss = 0.1165, acc = 0.9640 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 06:15:36.724175: step 67520, loss = 0.1292, acc = 0.9520 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 06:15:42.437922: step 67540, loss = 0.1179, acc = 0.9660 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 06:15:47.221769: step 67560, loss = 0.1033, acc = 0.9780 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 06:15:51.771314: step 67580, loss = 0.1261, acc = 0.9620 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:15:56.563721: step 67600, loss = 0.1141, acc = 0.9760 (239.1 examples/sec; 0.268 sec/batch)
2017-05-09 06:16:01.105659: step 67620, loss = 0.1060, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 06:16:05.558097: step 67640, loss = 0.1389, acc = 0.9620 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 06:16:10.773244: step 67660, loss = 0.0886, acc = 0.9760 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 06:16:15.680848: step 67680, loss = 0.1112, acc = 0.9680 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 06:16:20.341101: step 67700, loss = 0.1013, acc = 0.9720 (265.0 examples/sec; 0.241 sec/batch)
2017-05-09 06:16:24.946771: step 67720, loss = 0.1085, acc = 0.9640 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 06:16:29.646851: step 67740, loss = 0.1161, acc = 0.9700 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 06:16:34.194158: step 67760, loss = 0.1170, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 06:16:38.903116: step 67780, loss = 0.1379, acc = 0.9560 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 06:16:43.781591: step 67800, loss = 0.1061, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:16:48.371975: step 67820, loss = 0.1004, acc = 0.9680 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 06:16:52.925235: step 67840, loss = 0.1136, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 06:16:57.886646: step 67860, loss = 0.1068, acc = 0.9680 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:17:02.579476: step 67880, loss = 0.1349, acc = 0.9460 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 06:17:07.264120: step 67900, loss = 0.1467, acc = 0.9500 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 06:17:12.062154: step 67920, loss = 0.0977, acc = 0.9720 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 06:17:16.729232: step 67940, loss = 0.1076, acc = 0.9660 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 06:17:21.294569: step 67960, loss = 0.1031, acc = 0.9640 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 06:17:26.105602: step 67980, loss = 0.0936, acc = 0.9760 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 06:17:30.729189: step 68000, loss = 0.1096, acc = 0.9680 (271.9 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 06:17:44.759317: step 68000, acc = 0.9632, f1 = 0.9621
[Test] 2017-05-09 06:17:54.392004: step 68000, acc = 0.9550, f1 = 0.9546
[Status] 2017-05-09 06:17:54.392094: step 68000, maxindex = 68000, maxdev = 0.9632, maxtst = 0.9550
2017-05-09 06:18:02.302828: step 68020, loss = 0.1249, acc = 0.9640 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 06:18:06.872495: step 68040, loss = 0.1332, acc = 0.9500 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 06:18:11.551270: step 68060, loss = 0.1170, acc = 0.9600 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 06:18:16.138157: step 68080, loss = 0.1319, acc = 0.9500 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 06:18:20.792341: step 68100, loss = 0.1154, acc = 0.9660 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 06:18:25.510950: step 68120, loss = 0.1111, acc = 0.9700 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 06:18:29.963373: step 68140, loss = 0.1022, acc = 0.9780 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 06:18:34.509408: step 68160, loss = 0.0925, acc = 0.9740 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 06:18:39.190606: step 68180, loss = 0.0794, acc = 0.9840 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 06:18:43.646800: step 68200, loss = 0.1092, acc = 0.9700 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 06:18:48.112094: step 68220, loss = 0.0964, acc = 0.9700 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 06:18:52.948322: step 68240, loss = 0.1142, acc = 0.9620 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 06:18:57.631578: step 68260, loss = 0.1187, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 06:19:02.357941: step 68280, loss = 0.0981, acc = 0.9680 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 06:19:07.124894: step 68300, loss = 0.1166, acc = 0.9520 (246.5 examples/sec; 0.260 sec/batch)
2017-05-09 06:19:11.965658: step 68320, loss = 0.1168, acc = 0.9660 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 06:19:16.693695: step 68340, loss = 0.1054, acc = 0.9640 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 06:19:21.290904: step 68360, loss = 0.1554, acc = 0.9560 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 06:19:26.077311: step 68380, loss = 0.0829, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 06:19:30.643149: step 68400, loss = 0.1089, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 06:19:35.244981: step 68420, loss = 0.1153, acc = 0.9660 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 06:19:40.181803: step 68440, loss = 0.1204, acc = 0.9720 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 06:19:44.899112: step 68460, loss = 0.1010, acc = 0.9620 (256.0 examples/sec; 0.250 sec/batch)
2017-05-09 06:19:49.455139: step 68480, loss = 0.1224, acc = 0.9680 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 06:19:54.178915: step 68500, loss = 0.1028, acc = 0.9680 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 06:19:58.738014: step 68520, loss = 0.1057, acc = 0.9700 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 06:20:04.210740: step 68540, loss = 0.0977, acc = 0.9740 (159.4 examples/sec; 0.401 sec/batch)
2017-05-09 06:20:09.132640: step 68560, loss = 0.1003, acc = 0.9720 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 06:20:13.720613: step 68580, loss = 0.1076, acc = 0.9620 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 06:20:18.351861: step 68600, loss = 0.1315, acc = 0.9660 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:20:23.084820: step 68620, loss = 0.1369, acc = 0.9520 (226.9 examples/sec; 0.282 sec/batch)
2017-05-09 06:20:27.603334: step 68640, loss = 0.1291, acc = 0.9640 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 06:20:32.196524: step 68660, loss = 0.1063, acc = 0.9720 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 06:20:36.881434: step 68680, loss = 0.0944, acc = 0.9740 (241.8 examples/sec; 0.265 sec/batch)
2017-05-09 06:20:41.495581: step 68700, loss = 0.1007, acc = 0.9780 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 06:20:46.037647: step 68720, loss = 0.1265, acc = 0.9600 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 06:20:50.713001: step 68740, loss = 0.1091, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 06:20:55.493593: step 68760, loss = 0.1090, acc = 0.9640 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 06:21:00.160597: step 68780, loss = 0.1138, acc = 0.9740 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 06:21:04.655207: step 68800, loss = 0.1037, acc = 0.9740 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 06:21:09.408526: step 68820, loss = 0.1163, acc = 0.9700 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 06:21:13.947252: step 68840, loss = 0.0980, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 06:21:18.568665: step 68860, loss = 0.0989, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 06:21:23.194159: step 68880, loss = 0.1181, acc = 0.9620 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 06:21:27.830530: step 68900, loss = 0.1123, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 06:21:32.491362: step 68920, loss = 0.1214, acc = 0.9660 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 06:21:37.120050: step 68940, loss = 0.1513, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 06:21:41.639607: step 68960, loss = 0.0984, acc = 0.9720 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:21:46.088279: step 68980, loss = 0.0968, acc = 0.9780 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 06:21:50.873063: step 69000, loss = 0.1027, acc = 0.9720 (221.2 examples/sec; 0.289 sec/batch)
[Eval] 2017-05-09 06:22:04.820299: step 69000, acc = 0.9629, f1 = 0.9618
[Test] 2017-05-09 06:22:14.164762: step 69000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 06:22:14.164853: step 69000, maxindex = 68000, maxdev = 0.9632, maxtst = 0.9550
2017-05-09 06:22:18.734981: step 69020, loss = 0.1068, acc = 0.9720 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 06:22:23.658589: step 69040, loss = 0.1096, acc = 0.9700 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 06:22:28.336022: step 69060, loss = 0.1137, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 06:22:32.936758: step 69080, loss = 0.1110, acc = 0.9600 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 06:22:37.588198: step 69100, loss = 0.1401, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 06:22:42.123140: step 69120, loss = 0.1119, acc = 0.9680 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 06:22:46.696649: step 69140, loss = 0.1003, acc = 0.9700 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:22:51.411764: step 69160, loss = 0.1129, acc = 0.9740 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 06:22:56.049428: step 69180, loss = 0.1368, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:23:00.766916: step 69200, loss = 0.1066, acc = 0.9700 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 06:23:05.403802: step 69220, loss = 0.1481, acc = 0.9520 (297.7 examples/sec; 0.215 sec/batch)
2017-05-09 06:23:09.959994: step 69240, loss = 0.1328, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 06:23:14.490597: step 69260, loss = 0.1041, acc = 0.9660 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 06:23:19.073936: step 69280, loss = 0.1519, acc = 0.9500 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 06:23:23.876944: step 69300, loss = 0.1188, acc = 0.9720 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 06:23:28.454033: step 69320, loss = 0.1452, acc = 0.9500 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 06:23:32.971706: step 69340, loss = 0.1119, acc = 0.9760 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 06:23:37.763755: step 69360, loss = 0.1315, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 06:23:42.332850: step 69380, loss = 0.1104, acc = 0.9560 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 06:23:47.051386: step 69400, loss = 0.0918, acc = 0.9820 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 06:23:51.876010: step 69420, loss = 0.1369, acc = 0.9440 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 06:23:56.548372: step 69440, loss = 0.1087, acc = 0.9700 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 06:24:01.174878: step 69460, loss = 0.1188, acc = 0.9560 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 06:24:05.858710: step 69480, loss = 0.1101, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 06:24:10.515871: step 69500, loss = 0.1016, acc = 0.9740 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 06:24:15.017998: step 69520, loss = 0.1420, acc = 0.9520 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 06:24:19.891116: step 69540, loss = 0.1274, acc = 0.9600 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 06:24:25.587485: step 69560, loss = 0.1307, acc = 0.9560 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 06:24:30.164050: step 69580, loss = 0.1147, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:24:34.818414: step 69600, loss = 0.1492, acc = 0.9520 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 06:24:39.415069: step 69620, loss = 0.1085, acc = 0.9660 (255.6 examples/sec; 0.250 sec/batch)
2017-05-09 06:24:44.030856: step 69640, loss = 0.1142, acc = 0.9620 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 06:24:48.859464: step 69660, loss = 0.1213, acc = 0.9580 (226.7 examples/sec; 0.282 sec/batch)
2017-05-09 06:24:53.552388: step 69680, loss = 0.1048, acc = 0.9620 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 06:24:58.121467: step 69700, loss = 0.1034, acc = 0.9620 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 06:25:02.801040: step 69720, loss = 0.1053, acc = 0.9720 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 06:25:07.675309: step 69740, loss = 0.1151, acc = 0.9640 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 06:25:12.263963: step 69760, loss = 0.1402, acc = 0.9620 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 06:25:16.930517: step 69780, loss = 0.1200, acc = 0.9600 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 06:25:21.557259: step 69800, loss = 0.1181, acc = 0.9560 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 06:25:26.107151: step 69820, loss = 0.0923, acc = 0.9760 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 06:25:30.773455: step 69840, loss = 0.1107, acc = 0.9660 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 06:25:35.488266: step 69860, loss = 0.0898, acc = 0.9820 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 06:25:40.160238: step 69880, loss = 0.1137, acc = 0.9600 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 06:25:44.694254: step 69900, loss = 0.0811, acc = 0.9840 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 06:25:49.783709: step 69920, loss = 0.0989, acc = 0.9740 (237.2 examples/sec; 0.270 sec/batch)
2017-05-09 06:25:54.390317: step 69940, loss = 0.1159, acc = 0.9640 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 06:25:58.955417: step 69960, loss = 0.1371, acc = 0.9600 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 06:26:03.386268: step 69980, loss = 0.1164, acc = 0.9580 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 06:26:08.364272: step 70000, loss = 0.1107, acc = 0.9740 (290.0 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 06:26:22.303269: step 70000, acc = 0.9629, f1 = 0.9618
[Test] 2017-05-09 06:26:31.499547: step 70000, acc = 0.9554, f1 = 0.9551
[Status] 2017-05-09 06:26:31.499640: step 70000, maxindex = 68000, maxdev = 0.9632, maxtst = 0.9550
2017-05-09 06:26:36.195460: step 70020, loss = 0.1229, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 06:26:40.826949: step 70040, loss = 0.1051, acc = 0.9740 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 06:26:45.500593: step 70060, loss = 0.1027, acc = 0.9780 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 06:26:50.172331: step 70080, loss = 0.1168, acc = 0.9680 (296.3 examples/sec; 0.216 sec/batch)
2017-05-09 06:26:54.737146: step 70100, loss = 0.1250, acc = 0.9640 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:26:59.312581: step 70120, loss = 0.1019, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 06:27:04.112430: step 70140, loss = 0.1066, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 06:27:08.660017: step 70160, loss = 0.1226, acc = 0.9720 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 06:27:13.226913: step 70180, loss = 0.1185, acc = 0.9720 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 06:27:18.006722: step 70200, loss = 0.1174, acc = 0.9580 (242.2 examples/sec; 0.264 sec/batch)
2017-05-09 06:27:22.597340: step 70220, loss = 0.1375, acc = 0.9580 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 06:27:27.183212: step 70240, loss = 0.1134, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 06:27:31.970745: step 70260, loss = 0.1134, acc = 0.9700 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 06:27:36.560039: step 70280, loss = 0.1352, acc = 0.9600 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 06:27:41.141999: step 70300, loss = 0.1423, acc = 0.9620 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 06:27:45.874639: step 70320, loss = 0.1340, acc = 0.9500 (248.1 examples/sec; 0.258 sec/batch)
2017-05-09 06:27:50.644185: step 70340, loss = 0.1209, acc = 0.9680 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 06:27:55.260945: step 70360, loss = 0.1295, acc = 0.9640 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 06:27:59.818798: step 70380, loss = 0.1263, acc = 0.9540 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 06:28:04.511127: step 70400, loss = 0.1171, acc = 0.9640 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 06:28:09.117397: step 70420, loss = 0.1334, acc = 0.9580 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 06:28:13.730773: step 70440, loss = 0.0844, acc = 0.9820 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 06:28:18.496761: step 70460, loss = 0.0959, acc = 0.9720 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 06:28:23.056271: step 70480, loss = 0.1142, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:28:27.745290: step 70500, loss = 0.1186, acc = 0.9640 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 06:28:32.553382: step 70520, loss = 0.1097, acc = 0.9720 (239.5 examples/sec; 0.267 sec/batch)
2017-05-09 06:28:37.211653: step 70540, loss = 0.1337, acc = 0.9600 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 06:28:42.595839: step 70560, loss = 0.1245, acc = 0.9600 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 06:28:47.280819: step 70580, loss = 0.1302, acc = 0.9560 (250.3 examples/sec; 0.256 sec/batch)
2017-05-09 06:28:51.848835: step 70600, loss = 0.1252, acc = 0.9640 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 06:28:56.510753: step 70620, loss = 0.1143, acc = 0.9720 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 06:29:01.098401: step 70640, loss = 0.1284, acc = 0.9500 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 06:29:06.030142: step 70660, loss = 0.1203, acc = 0.9640 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 06:29:10.643701: step 70680, loss = 0.1099, acc = 0.9680 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 06:29:15.211217: step 70700, loss = 0.1059, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:29:19.893191: step 70720, loss = 0.1323, acc = 0.9520 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 06:29:24.441589: step 70740, loss = 0.1458, acc = 0.9620 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 06:29:29.053834: step 70760, loss = 0.1118, acc = 0.9640 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 06:29:33.722823: step 70780, loss = 0.1002, acc = 0.9720 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 06:29:38.282932: step 70800, loss = 0.1144, acc = 0.9600 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 06:29:42.847928: step 70820, loss = 0.1171, acc = 0.9680 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 06:29:47.536651: step 70840, loss = 0.1087, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 06:29:52.113639: step 70860, loss = 0.1137, acc = 0.9640 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 06:29:56.689487: step 70880, loss = 0.1087, acc = 0.9680 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 06:30:01.509510: step 70900, loss = 0.1145, acc = 0.9640 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 06:30:06.079780: step 70920, loss = 0.1117, acc = 0.9580 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 06:30:10.699909: step 70940, loss = 0.1133, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 06:30:15.241133: step 70960, loss = 0.1631, acc = 0.9420 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 06:30:20.032967: step 70980, loss = 0.1412, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 06:30:24.681987: step 71000, loss = 0.1171, acc = 0.9620 (275.7 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 06:30:38.678484: step 71000, acc = 0.9622, f1 = 0.9610
[Test] 2017-05-09 06:30:48.290996: step 71000, acc = 0.9542, f1 = 0.9539
[Status] 2017-05-09 06:30:48.291092: step 71000, maxindex = 68000, maxdev = 0.9632, maxtst = 0.9550
2017-05-09 06:30:53.115127: step 71020, loss = 0.1051, acc = 0.9740 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 06:30:57.771632: step 71040, loss = 0.1273, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 06:31:02.640188: step 71060, loss = 0.0901, acc = 0.9760 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 06:31:07.363344: step 71080, loss = 0.1188, acc = 0.9540 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 06:31:11.854534: step 71100, loss = 0.1260, acc = 0.9560 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 06:31:16.718741: step 71120, loss = 0.1560, acc = 0.9420 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 06:31:21.254036: step 71140, loss = 0.1170, acc = 0.9640 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 06:31:25.936917: step 71160, loss = 0.1205, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:31:30.693353: step 71180, loss = 0.1129, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 06:31:35.310401: step 71200, loss = 0.1068, acc = 0.9720 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 06:31:39.901709: step 71220, loss = 0.1162, acc = 0.9720 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 06:31:44.588876: step 71240, loss = 0.1023, acc = 0.9740 (244.4 examples/sec; 0.262 sec/batch)
2017-05-09 06:31:49.269700: step 71260, loss = 0.1048, acc = 0.9740 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 06:31:53.889677: step 71280, loss = 0.1395, acc = 0.9480 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 06:31:58.457185: step 71300, loss = 0.1396, acc = 0.9560 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 06:32:03.328333: step 71320, loss = 0.1213, acc = 0.9580 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 06:32:07.843142: step 71340, loss = 0.1196, acc = 0.9660 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 06:32:12.377439: step 71360, loss = 0.1045, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:32:17.142555: step 71380, loss = 0.0990, acc = 0.9720 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 06:32:21.712113: step 71400, loss = 0.1488, acc = 0.9460 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 06:32:26.313951: step 71420, loss = 0.1426, acc = 0.9420 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 06:32:30.784662: step 71440, loss = 0.1180, acc = 0.9740 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 06:32:35.423953: step 71460, loss = 0.1260, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 06:32:39.954873: step 71480, loss = 0.1156, acc = 0.9700 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 06:32:44.452447: step 71500, loss = 0.1270, acc = 0.9600 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 06:32:49.044649: step 71520, loss = 0.1243, acc = 0.9600 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 06:32:53.631206: step 71540, loss = 0.1323, acc = 0.9560 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 06:32:58.300902: step 71560, loss = 0.1188, acc = 0.9640 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 06:33:03.836237: step 71580, loss = 0.1174, acc = 0.9620 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 06:33:08.345762: step 71600, loss = 0.1161, acc = 0.9620 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 06:33:13.008625: step 71620, loss = 0.1186, acc = 0.9640 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 06:33:17.533720: step 71640, loss = 0.0900, acc = 0.9780 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 06:33:22.075847: step 71660, loss = 0.1060, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 06:33:26.654850: step 71680, loss = 0.1112, acc = 0.9660 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 06:33:31.296779: step 71700, loss = 0.1203, acc = 0.9520 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 06:33:35.914347: step 71720, loss = 0.1176, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 06:33:40.941004: step 71740, loss = 0.1402, acc = 0.9540 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 06:33:45.643183: step 71760, loss = 0.1265, acc = 0.9600 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 06:33:50.225071: step 71780, loss = 0.1253, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 06:33:54.941680: step 71800, loss = 0.1398, acc = 0.9580 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 06:33:59.555201: step 71820, loss = 0.0965, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 06:34:04.136275: step 71840, loss = 0.1232, acc = 0.9560 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 06:34:09.048283: step 71860, loss = 0.1153, acc = 0.9680 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 06:34:13.559155: step 71880, loss = 0.1232, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 06:34:18.149494: step 71900, loss = 0.1115, acc = 0.9700 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 06:34:23.152982: step 71920, loss = 0.1380, acc = 0.9600 (223.5 examples/sec; 0.286 sec/batch)
2017-05-09 06:34:27.707881: step 71940, loss = 0.1188, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 06:34:32.555911: step 71960, loss = 0.1232, acc = 0.9600 (204.6 examples/sec; 0.313 sec/batch)
2017-05-09 06:34:37.396038: step 71980, loss = 0.1315, acc = 0.9600 (214.2 examples/sec; 0.299 sec/batch)
2017-05-09 06:34:41.979248: step 72000, loss = 0.1062, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 06:34:55.943529: step 72000, acc = 0.9627, f1 = 0.9615
[Test] 2017-05-09 06:35:05.537453: step 72000, acc = 0.9554, f1 = 0.9551
[Status] 2017-05-09 06:35:05.537523: step 72000, maxindex = 68000, maxdev = 0.9632, maxtst = 0.9550
2017-05-09 06:35:10.159016: step 72020, loss = 0.1021, acc = 0.9720 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 06:35:14.691314: step 72040, loss = 0.1368, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 06:35:19.152149: step 72060, loss = 0.1108, acc = 0.9680 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 06:35:23.917836: step 72080, loss = 0.1328, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 06:35:28.546021: step 72100, loss = 0.1669, acc = 0.9500 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 06:35:33.160796: step 72120, loss = 0.1143, acc = 0.9680 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 06:35:37.860987: step 72140, loss = 0.1044, acc = 0.9700 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 06:35:42.599127: step 72160, loss = 0.1362, acc = 0.9660 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 06:35:47.224591: step 72180, loss = 0.1092, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 06:35:51.907923: step 72200, loss = 0.1107, acc = 0.9660 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 06:35:56.424841: step 72220, loss = 0.1269, acc = 0.9660 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 06:36:00.997417: step 72240, loss = 0.1078, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 06:36:05.812184: step 72260, loss = 0.1540, acc = 0.9540 (245.1 examples/sec; 0.261 sec/batch)
2017-05-09 06:36:10.524576: step 72280, loss = 0.1156, acc = 0.9700 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 06:36:15.064056: step 72300, loss = 0.1180, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 06:36:19.614472: step 72320, loss = 0.1285, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 06:36:24.468564: step 72340, loss = 0.1462, acc = 0.9600 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 06:36:29.014228: step 72360, loss = 0.0949, acc = 0.9680 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 06:36:33.534540: step 72380, loss = 0.1138, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 06:36:38.147154: step 72400, loss = 0.1181, acc = 0.9600 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 06:36:42.618402: step 72420, loss = 0.1209, acc = 0.9620 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 06:36:47.161661: step 72440, loss = 0.1080, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 06:36:52.099247: step 72460, loss = 0.1131, acc = 0.9700 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 06:36:56.798894: step 72480, loss = 0.1228, acc = 0.9600 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 06:37:01.412299: step 72500, loss = 0.0909, acc = 0.9740 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 06:37:06.125886: step 72520, loss = 0.1118, acc = 0.9680 (245.2 examples/sec; 0.261 sec/batch)
2017-05-09 06:37:10.655491: step 72540, loss = 0.1075, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 06:37:15.119455: step 72560, loss = 0.1407, acc = 0.9540 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 06:37:20.441926: step 72580, loss = 0.1115, acc = 0.9740 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 06:37:24.985453: step 72600, loss = 0.1075, acc = 0.9620 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:37:29.676510: step 72620, loss = 0.1173, acc = 0.9660 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 06:37:34.260024: step 72640, loss = 0.1514, acc = 0.9580 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 06:37:39.012929: step 72660, loss = 0.1162, acc = 0.9620 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 06:37:43.555020: step 72680, loss = 0.1247, acc = 0.9600 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 06:37:48.243791: step 72700, loss = 0.1080, acc = 0.9660 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 06:37:53.163704: step 72720, loss = 0.1221, acc = 0.9640 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 06:37:57.887844: step 72740, loss = 0.1443, acc = 0.9500 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 06:38:02.484496: step 72760, loss = 0.1222, acc = 0.9600 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 06:38:07.099526: step 72780, loss = 0.0997, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 06:38:12.213816: step 72800, loss = 0.1203, acc = 0.9660 (176.4 examples/sec; 0.363 sec/batch)
2017-05-09 06:38:16.756462: step 72820, loss = 0.1213, acc = 0.9580 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 06:38:21.591835: step 72840, loss = 0.1047, acc = 0.9700 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 06:38:26.189525: step 72860, loss = 0.1113, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 06:38:30.791941: step 72880, loss = 0.1141, acc = 0.9660 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 06:38:35.671754: step 72900, loss = 0.1073, acc = 0.9700 (222.0 examples/sec; 0.288 sec/batch)
2017-05-09 06:38:40.271607: step 72920, loss = 0.1312, acc = 0.9620 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 06:38:44.918653: step 72940, loss = 0.1010, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 06:38:49.545460: step 72960, loss = 0.1039, acc = 0.9740 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 06:38:54.263751: step 72980, loss = 0.1076, acc = 0.9620 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 06:38:58.809500: step 73000, loss = 0.1241, acc = 0.9560 (294.1 examples/sec; 0.218 sec/batch)
[Eval] 2017-05-09 06:39:12.950050: step 73000, acc = 0.9634, f1 = 0.9622
[Test] 2017-05-09 06:39:22.749325: step 73000, acc = 0.9549, f1 = 0.9545
[Status] 2017-05-09 06:39:22.749406: step 73000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 06:39:30.713447: step 73020, loss = 0.1277, acc = 0.9540 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 06:39:35.387206: step 73040, loss = 0.1148, acc = 0.9640 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 06:39:39.822441: step 73060, loss = 0.1153, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 06:39:44.359711: step 73080, loss = 0.1191, acc = 0.9580 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 06:39:49.222650: step 73100, loss = 0.1445, acc = 0.9480 (218.3 examples/sec; 0.293 sec/batch)
2017-05-09 06:39:53.846340: step 73120, loss = 0.1283, acc = 0.9640 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 06:39:58.565419: step 73140, loss = 0.1199, acc = 0.9560 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 06:40:03.154179: step 73160, loss = 0.1021, acc = 0.9680 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 06:40:07.888276: step 73180, loss = 0.1078, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 06:40:12.445481: step 73200, loss = 0.1102, acc = 0.9660 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 06:40:16.963266: step 73220, loss = 0.1278, acc = 0.9640 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 06:40:21.827504: step 73240, loss = 0.0977, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 06:40:26.417120: step 73260, loss = 0.0929, acc = 0.9760 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 06:40:30.948103: step 73280, loss = 0.1263, acc = 0.9600 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 06:40:35.708573: step 73300, loss = 0.1310, acc = 0.9480 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 06:40:40.254983: step 73320, loss = 0.1093, acc = 0.9760 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 06:40:44.878847: step 73340, loss = 0.0961, acc = 0.9700 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 06:40:49.727322: step 73360, loss = 0.1086, acc = 0.9640 (232.0 examples/sec; 0.276 sec/batch)
2017-05-09 06:40:54.447053: step 73380, loss = 0.1058, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 06:40:59.071245: step 73400, loss = 0.1122, acc = 0.9700 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 06:41:03.841116: step 73420, loss = 0.0953, acc = 0.9700 (250.4 examples/sec; 0.256 sec/batch)
2017-05-09 06:41:08.418565: step 73440, loss = 0.1136, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:41:12.969011: step 73460, loss = 0.1000, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 06:41:17.713127: step 73480, loss = 0.1112, acc = 0.9700 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 06:41:22.562443: step 73500, loss = 0.1291, acc = 0.9640 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:41:27.154276: step 73520, loss = 0.1087, acc = 0.9660 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 06:41:31.724868: step 73540, loss = 0.1108, acc = 0.9600 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 06:41:36.485451: step 73560, loss = 0.1092, acc = 0.9700 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 06:41:42.063702: step 73580, loss = 0.1386, acc = 0.9480 (130.4 examples/sec; 0.491 sec/batch)
2017-05-09 06:41:46.681005: step 73600, loss = 0.1337, acc = 0.9520 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 06:41:51.447682: step 73620, loss = 0.1227, acc = 0.9620 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 06:41:56.032798: step 73640, loss = 0.1190, acc = 0.9520 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 06:42:00.631336: step 73660, loss = 0.1020, acc = 0.9620 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 06:42:05.221872: step 73680, loss = 0.1150, acc = 0.9660 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 06:42:10.115707: step 73700, loss = 0.1398, acc = 0.9560 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 06:42:14.800174: step 73720, loss = 0.1125, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 06:42:19.363983: step 73740, loss = 0.1205, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 06:42:24.128245: step 73760, loss = 0.1146, acc = 0.9620 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 06:42:28.699781: step 73780, loss = 0.1176, acc = 0.9580 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 06:42:33.322228: step 73800, loss = 0.1026, acc = 0.9660 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 06:42:38.126946: step 73820, loss = 0.1202, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 06:42:42.613140: step 73840, loss = 0.0999, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 06:42:47.158035: step 73860, loss = 0.1392, acc = 0.9600 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 06:42:52.023056: step 73880, loss = 0.1011, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 06:42:56.589248: step 73900, loss = 0.1161, acc = 0.9660 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 06:43:01.207809: step 73920, loss = 0.1203, acc = 0.9680 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 06:43:05.971086: step 73940, loss = 0.1262, acc = 0.9640 (237.5 examples/sec; 0.270 sec/batch)
2017-05-09 06:43:10.583577: step 73960, loss = 0.1056, acc = 0.9600 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 06:43:15.163474: step 73980, loss = 0.1135, acc = 0.9620 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 06:43:19.873781: step 74000, loss = 0.1149, acc = 0.9700 (257.7 examples/sec; 0.248 sec/batch)
[Eval] 2017-05-09 06:43:33.884900: step 74000, acc = 0.9627, f1 = 0.9615
[Test] 2017-05-09 06:43:43.498941: step 74000, acc = 0.9555, f1 = 0.9551
[Status] 2017-05-09 06:43:43.499025: step 74000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 06:43:48.159162: step 74020, loss = 0.1097, acc = 0.9720 (247.4 examples/sec; 0.259 sec/batch)
2017-05-09 06:43:52.733002: step 74040, loss = 0.1092, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 06:43:57.250614: step 74060, loss = 0.1251, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 06:44:01.878656: step 74080, loss = 0.1079, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 06:44:06.789034: step 74100, loss = 0.1402, acc = 0.9540 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 06:44:11.374008: step 74120, loss = 0.1151, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 06:44:16.006450: step 74140, loss = 0.1096, acc = 0.9600 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 06:44:20.771639: step 74160, loss = 0.1254, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 06:44:25.440263: step 74180, loss = 0.1277, acc = 0.9520 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 06:44:30.151635: step 74200, loss = 0.1153, acc = 0.9640 (259.6 examples/sec; 0.246 sec/batch)
2017-05-09 06:44:34.970631: step 74220, loss = 0.1345, acc = 0.9580 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 06:44:39.748576: step 74240, loss = 0.0996, acc = 0.9720 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 06:44:44.370779: step 74260, loss = 0.1126, acc = 0.9620 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 06:44:49.078227: step 74280, loss = 0.1257, acc = 0.9680 (236.9 examples/sec; 0.270 sec/batch)
2017-05-09 06:44:53.992273: step 74300, loss = 0.1060, acc = 0.9780 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 06:44:58.938975: step 74320, loss = 0.1237, acc = 0.9580 (263.9 examples/sec; 0.242 sec/batch)
2017-05-09 06:45:03.410801: step 74340, loss = 0.1345, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 06:45:08.186139: step 74360, loss = 0.1173, acc = 0.9560 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 06:45:12.720667: step 74380, loss = 0.1204, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 06:45:17.294385: step 74400, loss = 0.1376, acc = 0.9600 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 06:45:22.162936: step 74420, loss = 0.1046, acc = 0.9740 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 06:45:26.825063: step 74440, loss = 0.1275, acc = 0.9540 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 06:45:31.470424: step 74460, loss = 0.1091, acc = 0.9600 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 06:45:36.115203: step 74480, loss = 0.1209, acc = 0.9520 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 06:45:40.683181: step 74500, loss = 0.1338, acc = 0.9540 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 06:45:45.290699: step 74520, loss = 0.1079, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 06:45:50.174897: step 74540, loss = 0.1003, acc = 0.9780 (238.6 examples/sec; 0.268 sec/batch)
2017-05-09 06:45:54.755256: step 74560, loss = 0.0995, acc = 0.9740 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:45:59.392004: step 74580, loss = 0.1195, acc = 0.9660 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 06:46:04.819324: step 74600, loss = 0.1011, acc = 0.9700 (237.8 examples/sec; 0.269 sec/batch)
2017-05-09 06:46:09.525750: step 74620, loss = 0.0954, acc = 0.9720 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 06:46:14.115688: step 74640, loss = 0.1435, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 06:46:18.796767: step 74660, loss = 0.0972, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 06:46:23.467419: step 74680, loss = 0.1059, acc = 0.9680 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 06:46:27.961278: step 74700, loss = 0.1329, acc = 0.9500 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 06:46:32.447822: step 74720, loss = 0.1304, acc = 0.9500 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 06:46:37.108313: step 74740, loss = 0.0972, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 06:46:41.811398: step 74760, loss = 0.1549, acc = 0.9600 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 06:46:46.453711: step 74780, loss = 0.1087, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 06:46:51.065479: step 74800, loss = 0.1249, acc = 0.9620 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:46:55.732407: step 74820, loss = 0.1038, acc = 0.9660 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 06:47:00.309478: step 74840, loss = 0.1123, acc = 0.9640 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 06:47:04.983201: step 74860, loss = 0.0997, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:47:09.792232: step 74880, loss = 0.0953, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 06:47:14.360252: step 74900, loss = 0.1164, acc = 0.9600 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 06:47:19.063355: step 74920, loss = 0.1014, acc = 0.9700 (240.0 examples/sec; 0.267 sec/batch)
2017-05-09 06:47:23.594934: step 74940, loss = 0.1234, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 06:47:28.184694: step 74960, loss = 0.1076, acc = 0.9660 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 06:47:32.780122: step 74980, loss = 0.1451, acc = 0.9480 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 06:47:37.639620: step 75000, loss = 0.0899, acc = 0.9820 (296.0 examples/sec; 0.216 sec/batch)
[Eval] 2017-05-09 06:47:51.538571: step 75000, acc = 0.9631, f1 = 0.9619
[Test] 2017-05-09 06:48:00.775839: step 75000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 06:48:00.775931: step 75000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 06:48:05.750936: step 75020, loss = 0.1231, acc = 0.9620 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 06:48:10.462712: step 75040, loss = 0.0988, acc = 0.9700 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 06:48:15.067692: step 75060, loss = 0.1080, acc = 0.9720 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 06:48:19.750670: step 75080, loss = 0.1076, acc = 0.9640 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 06:48:24.289880: step 75100, loss = 0.1236, acc = 0.9560 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 06:48:28.875157: step 75120, loss = 0.1421, acc = 0.9580 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 06:48:33.572248: step 75140, loss = 0.1144, acc = 0.9580 (226.7 examples/sec; 0.282 sec/batch)
2017-05-09 06:48:38.125446: step 75160, loss = 0.1300, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 06:48:42.595724: step 75180, loss = 0.1285, acc = 0.9580 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 06:48:47.401167: step 75200, loss = 0.1341, acc = 0.9580 (239.2 examples/sec; 0.268 sec/batch)
2017-05-09 06:48:51.880059: step 75220, loss = 0.0969, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 06:48:56.441580: step 75240, loss = 0.1091, acc = 0.9660 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 06:49:01.108442: step 75260, loss = 0.1129, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 06:49:05.816133: step 75280, loss = 0.1253, acc = 0.9540 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 06:49:10.322793: step 75300, loss = 0.1111, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 06:49:14.916854: step 75320, loss = 0.1448, acc = 0.9360 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 06:49:19.702756: step 75340, loss = 0.1308, acc = 0.9560 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 06:49:24.167816: step 75360, loss = 0.1244, acc = 0.9720 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 06:49:28.782504: step 75380, loss = 0.0931, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 06:49:33.496644: step 75400, loss = 0.1095, acc = 0.9700 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 06:49:38.056335: step 75420, loss = 0.1268, acc = 0.9580 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 06:49:42.634496: step 75440, loss = 0.1284, acc = 0.9540 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 06:49:47.296572: step 75460, loss = 0.1055, acc = 0.9580 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 06:49:51.959509: step 75480, loss = 0.1441, acc = 0.9620 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 06:49:56.532980: step 75500, loss = 0.1365, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 06:50:01.331365: step 75520, loss = 0.0978, acc = 0.9720 (222.5 examples/sec; 0.288 sec/batch)
2017-05-09 06:50:05.882928: step 75540, loss = 0.1037, acc = 0.9640 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 06:50:10.409126: step 75560, loss = 0.0912, acc = 0.9720 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 06:50:15.247591: step 75580, loss = 0.1053, acc = 0.9720 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 06:50:20.789734: step 75600, loss = 0.1181, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 06:50:25.433235: step 75620, loss = 0.1093, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 06:50:30.180251: step 75640, loss = 0.1302, acc = 0.9680 (227.1 examples/sec; 0.282 sec/batch)
2017-05-09 06:50:34.669190: step 75660, loss = 0.1402, acc = 0.9560 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 06:50:39.197074: step 75680, loss = 0.1157, acc = 0.9620 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 06:50:43.828907: step 75700, loss = 0.1293, acc = 0.9560 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 06:50:48.593868: step 75720, loss = 0.1109, acc = 0.9660 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 06:50:53.262307: step 75740, loss = 0.1224, acc = 0.9560 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 06:50:57.824661: step 75760, loss = 0.0882, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 06:51:02.595391: step 75780, loss = 0.0950, acc = 0.9740 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 06:51:07.246999: step 75800, loss = 0.1118, acc = 0.9620 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 06:51:11.844765: step 75820, loss = 0.1052, acc = 0.9660 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 06:51:16.485218: step 75840, loss = 0.1312, acc = 0.9560 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 06:51:20.986010: step 75860, loss = 0.1039, acc = 0.9640 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 06:51:25.519420: step 75880, loss = 0.0987, acc = 0.9700 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 06:51:30.267228: step 75900, loss = 0.1013, acc = 0.9760 (219.1 examples/sec; 0.292 sec/batch)
2017-05-09 06:51:34.844948: step 75920, loss = 0.0886, acc = 0.9780 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 06:51:39.360699: step 75940, loss = 0.0993, acc = 0.9720 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 06:51:43.903211: step 75960, loss = 0.1133, acc = 0.9640 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 06:51:48.711192: step 75980, loss = 0.1091, acc = 0.9700 (249.9 examples/sec; 0.256 sec/batch)
2017-05-09 06:51:53.298334: step 76000, loss = 0.0848, acc = 0.9820 (270.7 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 06:52:07.540053: step 76000, acc = 0.9631, f1 = 0.9619
[Test] 2017-05-09 06:52:17.293593: step 76000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 06:52:17.293665: step 76000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 06:52:21.790981: step 76020, loss = 0.1101, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 06:52:26.436186: step 76040, loss = 0.1111, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 06:52:31.122461: step 76060, loss = 0.1250, acc = 0.9560 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 06:52:35.857568: step 76080, loss = 0.1063, acc = 0.9720 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 06:52:40.409791: step 76100, loss = 0.1124, acc = 0.9620 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 06:52:45.106392: step 76120, loss = 0.1481, acc = 0.9620 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 06:52:49.756994: step 76140, loss = 0.1344, acc = 0.9580 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 06:52:54.330781: step 76160, loss = 0.1052, acc = 0.9740 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 06:52:59.116498: step 76180, loss = 0.1282, acc = 0.9520 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 06:53:03.630419: step 76200, loss = 0.1009, acc = 0.9760 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 06:53:08.209427: step 76220, loss = 0.0969, acc = 0.9740 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 06:53:12.819759: step 76240, loss = 0.1187, acc = 0.9640 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 06:53:17.540422: step 76260, loss = 0.1020, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 06:53:22.330235: step 76280, loss = 0.0990, acc = 0.9680 (249.9 examples/sec; 0.256 sec/batch)
2017-05-09 06:53:26.883527: step 76300, loss = 0.1107, acc = 0.9640 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 06:53:31.578439: step 76320, loss = 0.1036, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 06:53:36.158821: step 76340, loss = 0.0992, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 06:53:40.757629: step 76360, loss = 0.1072, acc = 0.9720 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 06:53:45.529086: step 76380, loss = 0.1117, acc = 0.9720 (227.6 examples/sec; 0.281 sec/batch)
2017-05-09 06:53:50.190376: step 76400, loss = 0.1271, acc = 0.9620 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 06:53:54.876987: step 76420, loss = 0.1122, acc = 0.9620 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 06:53:59.424515: step 76440, loss = 0.1213, acc = 0.9560 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 06:54:04.284923: step 76460, loss = 0.1205, acc = 0.9600 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 06:54:08.968138: step 76480, loss = 0.1483, acc = 0.9480 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 06:54:13.513310: step 76500, loss = 0.0978, acc = 0.9760 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 06:54:18.116553: step 76520, loss = 0.1358, acc = 0.9560 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 06:54:22.589891: step 76540, loss = 0.1078, acc = 0.9720 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 06:54:27.306948: step 76560, loss = 0.1132, acc = 0.9680 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 06:54:31.987704: step 76580, loss = 0.0940, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 06:54:36.608945: step 76600, loss = 0.1065, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 06:54:41.955117: step 76620, loss = 0.1089, acc = 0.9640 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 06:54:46.755826: step 76640, loss = 0.0915, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 06:54:51.315737: step 76660, loss = 0.1061, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 06:54:55.941873: step 76680, loss = 0.1273, acc = 0.9540 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 06:55:00.713208: step 76700, loss = 0.1178, acc = 0.9660 (235.9 examples/sec; 0.271 sec/batch)
2017-05-09 06:55:05.292271: step 76720, loss = 0.1189, acc = 0.9640 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 06:55:09.974408: step 76740, loss = 0.1274, acc = 0.9580 (262.0 examples/sec; 0.244 sec/batch)
2017-05-09 06:55:14.621159: step 76760, loss = 0.1262, acc = 0.9600 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 06:55:19.432480: step 76780, loss = 0.1256, acc = 0.9480 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 06:55:24.045897: step 76800, loss = 0.1230, acc = 0.9540 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 06:55:28.615080: step 76820, loss = 0.0885, acc = 0.9820 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 06:55:33.504498: step 76840, loss = 0.1077, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 06:55:38.079758: step 76860, loss = 0.1129, acc = 0.9720 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 06:55:42.656872: step 76880, loss = 0.0940, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 06:55:47.517133: step 76900, loss = 0.0993, acc = 0.9640 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 06:55:52.061497: step 76920, loss = 0.1316, acc = 0.9500 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 06:55:56.715238: step 76940, loss = 0.1336, acc = 0.9620 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 06:56:01.398987: step 76960, loss = 0.1236, acc = 0.9660 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 06:56:06.170610: step 76980, loss = 0.1027, acc = 0.9660 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 06:56:10.731236: step 77000, loss = 0.1021, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 06:56:24.686238: step 77000, acc = 0.9632, f1 = 0.9620
[Test] 2017-05-09 06:56:34.319426: step 77000, acc = 0.9555, f1 = 0.9551
[Status] 2017-05-09 06:56:34.319518: step 77000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 06:56:38.877239: step 77020, loss = 0.1117, acc = 0.9660 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 06:56:43.659430: step 77040, loss = 0.1008, acc = 0.9680 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 06:56:48.233775: step 77060, loss = 0.1401, acc = 0.9520 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 06:56:52.774943: step 77080, loss = 0.1451, acc = 0.9520 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 06:56:57.475792: step 77100, loss = 0.1120, acc = 0.9640 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 06:57:02.073963: step 77120, loss = 0.1442, acc = 0.9460 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 06:57:06.659225: step 77140, loss = 0.1150, acc = 0.9680 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 06:57:11.298689: step 77160, loss = 0.1310, acc = 0.9580 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 06:57:15.918705: step 77180, loss = 0.1024, acc = 0.9780 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 06:57:20.426161: step 77200, loss = 0.1334, acc = 0.9620 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 06:57:24.933595: step 77220, loss = 0.1273, acc = 0.9560 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 06:57:29.621069: step 77240, loss = 0.1094, acc = 0.9680 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 06:57:34.152614: step 77260, loss = 0.1286, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 06:57:38.660820: step 77280, loss = 0.1348, acc = 0.9640 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 06:57:43.724042: step 77300, loss = 0.1253, acc = 0.9680 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 06:57:48.287051: step 77320, loss = 0.0948, acc = 0.9700 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 06:57:52.855887: step 77340, loss = 0.1164, acc = 0.9620 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 06:57:57.651998: step 77360, loss = 0.1044, acc = 0.9580 (230.5 examples/sec; 0.278 sec/batch)
2017-05-09 06:58:02.128597: step 77380, loss = 0.1420, acc = 0.9500 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 06:58:06.679088: step 77400, loss = 0.1126, acc = 0.9560 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 06:58:11.280072: step 77420, loss = 0.1253, acc = 0.9600 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 06:58:15.737727: step 77440, loss = 0.1275, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 06:58:20.402513: step 77460, loss = 0.1218, acc = 0.9620 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 06:58:24.922395: step 77480, loss = 0.1451, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 06:58:29.822308: step 77500, loss = 0.1295, acc = 0.9600 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 06:58:34.435950: step 77520, loss = 0.1048, acc = 0.9700 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 06:58:39.090712: step 77540, loss = 0.1236, acc = 0.9620 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 06:58:43.813396: step 77560, loss = 0.1113, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 06:58:48.391568: step 77580, loss = 0.0933, acc = 0.9760 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 06:58:53.183132: step 77600, loss = 0.1169, acc = 0.9540 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 06:58:59.097827: step 77620, loss = 0.0963, acc = 0.9720 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 06:59:03.789311: step 77640, loss = 0.1043, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 06:59:08.380889: step 77660, loss = 0.1171, acc = 0.9580 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 06:59:13.109344: step 77680, loss = 0.1050, acc = 0.9740 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 06:59:17.746314: step 77700, loss = 0.1344, acc = 0.9540 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 06:59:22.363524: step 77720, loss = 0.1342, acc = 0.9520 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 06:59:27.306640: step 77740, loss = 0.1029, acc = 0.9780 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 06:59:31.949367: step 77760, loss = 0.1114, acc = 0.9700 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 06:59:36.659561: step 77780, loss = 0.1195, acc = 0.9660 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 06:59:41.420453: step 77800, loss = 0.1272, acc = 0.9640 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 06:59:46.079890: step 77820, loss = 0.1275, acc = 0.9600 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 06:59:50.745599: step 77840, loss = 0.1125, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 06:59:55.330936: step 77860, loss = 0.0913, acc = 0.9780 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 07:00:00.242490: step 77880, loss = 0.0954, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 07:00:04.882036: step 77900, loss = 0.1032, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 07:00:09.525987: step 77920, loss = 0.0975, acc = 0.9720 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 07:00:14.395845: step 77940, loss = 0.1108, acc = 0.9620 (252.8 examples/sec; 0.253 sec/batch)
2017-05-09 07:00:19.057694: step 77960, loss = 0.1095, acc = 0.9680 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:00:23.632436: step 77980, loss = 0.1339, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:00:28.623143: step 78000, loss = 0.1074, acc = 0.9660 (267.9 examples/sec; 0.239 sec/batch)
[Eval] 2017-05-09 07:00:42.657297: step 78000, acc = 0.9631, f1 = 0.9620
[Test] 2017-05-09 07:00:52.139455: step 78000, acc = 0.9554, f1 = 0.9551
[Status] 2017-05-09 07:00:52.139553: step 78000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:00:56.901617: step 78020, loss = 0.1013, acc = 0.9740 (232.8 examples/sec; 0.275 sec/batch)
2017-05-09 07:01:01.542598: step 78040, loss = 0.1063, acc = 0.9740 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 07:01:06.107460: step 78060, loss = 0.0963, acc = 0.9800 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:01:10.867328: step 78080, loss = 0.1321, acc = 0.9640 (246.9 examples/sec; 0.259 sec/batch)
2017-05-09 07:01:15.395647: step 78100, loss = 0.1160, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:01:20.129767: step 78120, loss = 0.1122, acc = 0.9680 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 07:01:24.870749: step 78140, loss = 0.1108, acc = 0.9740 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 07:01:29.584355: step 78160, loss = 0.1104, acc = 0.9680 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 07:01:34.092285: step 78180, loss = 0.1005, acc = 0.9720 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 07:01:39.096976: step 78200, loss = 0.1269, acc = 0.9660 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 07:01:43.814634: step 78220, loss = 0.1078, acc = 0.9680 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 07:01:48.312189: step 78240, loss = 0.1419, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:01:52.956758: step 78260, loss = 0.1228, acc = 0.9620 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 07:01:57.751966: step 78280, loss = 0.1090, acc = 0.9720 (251.2 examples/sec; 0.255 sec/batch)
2017-05-09 07:02:02.273058: step 78300, loss = 0.1034, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 07:02:06.785661: step 78320, loss = 0.0803, acc = 0.9840 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 07:02:11.693664: step 78340, loss = 0.1102, acc = 0.9720 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 07:02:16.223601: step 78360, loss = 0.1021, acc = 0.9760 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 07:02:20.833190: step 78380, loss = 0.0981, acc = 0.9700 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:02:25.593367: step 78400, loss = 0.1276, acc = 0.9560 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 07:02:30.200529: step 78420, loss = 0.1047, acc = 0.9720 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:02:34.864450: step 78440, loss = 0.1012, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 07:02:39.565635: step 78460, loss = 0.1188, acc = 0.9560 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 07:02:44.273587: step 78480, loss = 0.0936, acc = 0.9780 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 07:02:48.847849: step 78500, loss = 0.1326, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:02:53.350538: step 78520, loss = 0.1426, acc = 0.9540 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 07:02:58.019404: step 78540, loss = 0.1266, acc = 0.9580 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 07:03:02.602467: step 78560, loss = 0.1023, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:03:07.217115: step 78580, loss = 0.1367, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 07:03:12.007134: step 78600, loss = 0.1275, acc = 0.9600 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 07:03:17.238297: step 78620, loss = 0.1139, acc = 0.9740 (161.0 examples/sec; 0.398 sec/batch)
2017-05-09 07:03:21.802092: step 78640, loss = 0.1111, acc = 0.9640 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 07:03:26.539645: step 78660, loss = 0.1313, acc = 0.9540 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 07:03:31.211787: step 78680, loss = 0.1163, acc = 0.9680 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 07:03:35.852336: step 78700, loss = 0.1097, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 07:03:40.545182: step 78720, loss = 0.1300, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 07:03:45.517504: step 78740, loss = 0.1108, acc = 0.9720 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:03:50.222221: step 78760, loss = 0.1195, acc = 0.9480 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 07:03:54.857724: step 78780, loss = 0.0740, acc = 0.9860 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 07:03:59.542973: step 78800, loss = 0.1395, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:04:04.064923: step 78820, loss = 0.1216, acc = 0.9560 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 07:04:08.926502: step 78840, loss = 0.1482, acc = 0.9500 (236.9 examples/sec; 0.270 sec/batch)
2017-05-09 07:04:13.309830: step 78860, loss = 0.1189, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 07:04:17.906682: step 78880, loss = 0.1060, acc = 0.9760 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:04:22.452203: step 78900, loss = 0.1571, acc = 0.9520 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 07:04:26.971641: step 78920, loss = 0.1208, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 07:04:31.565616: step 78940, loss = 0.1513, acc = 0.9480 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 07:04:36.365227: step 78960, loss = 0.0953, acc = 0.9780 (228.7 examples/sec; 0.280 sec/batch)
2017-05-09 07:04:40.912468: step 78980, loss = 0.1318, acc = 0.9580 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 07:04:45.523911: step 79000, loss = 0.1018, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 07:04:59.713291: step 79000, acc = 0.9613, f1 = 0.9599
[Test] 2017-05-09 07:05:09.400299: step 79000, acc = 0.9520, f1 = 0.9516
[Status] 2017-05-09 07:05:09.400379: step 79000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:05:13.985905: step 79020, loss = 0.1769, acc = 0.9340 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 07:05:18.594799: step 79040, loss = 0.1037, acc = 0.9760 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 07:05:23.488496: step 79060, loss = 0.0976, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:05:28.105940: step 79080, loss = 0.0902, acc = 0.9720 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 07:05:32.902260: step 79100, loss = 0.0981, acc = 0.9700 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 07:05:37.775163: step 79120, loss = 0.1193, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 07:05:42.305784: step 79140, loss = 0.1311, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 07:05:46.898160: step 79160, loss = 0.1127, acc = 0.9720 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 07:05:51.655377: step 79180, loss = 0.1029, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 07:05:56.153167: step 79200, loss = 0.1015, acc = 0.9640 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 07:06:00.778859: step 79220, loss = 0.1148, acc = 0.9620 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 07:06:05.557014: step 79240, loss = 0.1005, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 07:06:10.022116: step 79260, loss = 0.1111, acc = 0.9680 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 07:06:14.672114: step 79280, loss = 0.1052, acc = 0.9680 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 07:06:19.502823: step 79300, loss = 0.1100, acc = 0.9760 (232.0 examples/sec; 0.276 sec/batch)
2017-05-09 07:06:23.977780: step 79320, loss = 0.1149, acc = 0.9580 (300.7 examples/sec; 0.213 sec/batch)
2017-05-09 07:06:28.545194: step 79340, loss = 0.1336, acc = 0.9560 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 07:06:33.326916: step 79360, loss = 0.1348, acc = 0.9520 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 07:06:38.225214: step 79380, loss = 0.1350, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 07:06:42.822117: step 79400, loss = 0.1047, acc = 0.9640 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 07:06:47.513458: step 79420, loss = 0.0990, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 07:06:52.226780: step 79440, loss = 0.0978, acc = 0.9740 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 07:06:57.075935: step 79460, loss = 0.1149, acc = 0.9580 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 07:07:01.863646: step 79480, loss = 0.1033, acc = 0.9580 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 07:07:06.444812: step 79500, loss = 0.1112, acc = 0.9700 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 07:07:10.901310: step 79520, loss = 0.1246, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 07:07:15.457130: step 79540, loss = 0.1010, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 07:07:20.232167: step 79560, loss = 0.1202, acc = 0.9640 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 07:07:24.765712: step 79580, loss = 0.1253, acc = 0.9460 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 07:07:29.347284: step 79600, loss = 0.1215, acc = 0.9580 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 07:07:34.005473: step 79620, loss = 0.1068, acc = 0.9740 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 07:07:39.713025: step 79640, loss = 0.0972, acc = 0.9700 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:07:44.428833: step 79660, loss = 0.1047, acc = 0.9700 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 07:07:49.319244: step 79680, loss = 0.0936, acc = 0.9800 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 07:07:53.936471: step 79700, loss = 0.1077, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 07:07:58.552100: step 79720, loss = 0.1305, acc = 0.9560 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 07:08:03.325166: step 79740, loss = 0.1110, acc = 0.9660 (218.8 examples/sec; 0.292 sec/batch)
2017-05-09 07:08:07.976410: step 79760, loss = 0.1128, acc = 0.9720 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 07:08:12.569587: step 79780, loss = 0.1085, acc = 0.9700 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 07:08:17.097949: step 79800, loss = 0.1176, acc = 0.9660 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 07:08:21.850733: step 79820, loss = 0.0964, acc = 0.9740 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 07:08:26.441076: step 79840, loss = 0.1207, acc = 0.9620 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:08:31.198271: step 79860, loss = 0.1102, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 07:08:35.904486: step 79880, loss = 0.1142, acc = 0.9680 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 07:08:40.425863: step 79900, loss = 0.1529, acc = 0.9540 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 07:08:45.021841: step 79920, loss = 0.0994, acc = 0.9780 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 07:08:49.703870: step 79940, loss = 0.1085, acc = 0.9660 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 07:08:54.358101: step 79960, loss = 0.1260, acc = 0.9660 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 07:08:59.030299: step 79980, loss = 0.1145, acc = 0.9620 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 07:09:03.719876: step 80000, loss = 0.1213, acc = 0.9600 (282.6 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 07:09:17.805331: step 80000, acc = 0.9625, f1 = 0.9612
[Test] 2017-05-09 07:09:27.103286: step 80000, acc = 0.9536, f1 = 0.9532
[Status] 2017-05-09 07:09:27.103372: step 80000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:09:31.742626: step 80020, loss = 0.1097, acc = 0.9620 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 07:09:36.663516: step 80040, loss = 0.1069, acc = 0.9760 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 07:09:41.317420: step 80060, loss = 0.1171, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 07:09:45.985446: step 80080, loss = 0.0882, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 07:09:50.829784: step 80100, loss = 0.1134, acc = 0.9700 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 07:09:55.519106: step 80120, loss = 0.1321, acc = 0.9440 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 07:10:00.088832: step 80140, loss = 0.1013, acc = 0.9640 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 07:10:04.828967: step 80160, loss = 0.1199, acc = 0.9600 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 07:10:09.266503: step 80180, loss = 0.0964, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:10:13.753645: step 80200, loss = 0.1167, acc = 0.9720 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 07:10:18.582720: step 80220, loss = 0.1323, acc = 0.9580 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 07:10:23.097439: step 80240, loss = 0.1175, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 07:10:27.598052: step 80260, loss = 0.1349, acc = 0.9500 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 07:10:32.218354: step 80280, loss = 0.0981, acc = 0.9680 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 07:10:36.767689: step 80300, loss = 0.1056, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 07:10:41.398055: step 80320, loss = 0.0989, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:10:46.164530: step 80340, loss = 0.1070, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 07:10:50.677375: step 80360, loss = 0.1149, acc = 0.9620 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 07:10:55.284967: step 80380, loss = 0.0918, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 07:11:00.071959: step 80400, loss = 0.1238, acc = 0.9660 (236.4 examples/sec; 0.271 sec/batch)
2017-05-09 07:11:04.631994: step 80420, loss = 0.1097, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 07:11:09.297285: step 80440, loss = 0.1252, acc = 0.9620 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 07:11:13.856892: step 80460, loss = 0.0898, acc = 0.9760 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 07:11:18.602384: step 80480, loss = 0.0866, acc = 0.9700 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 07:11:23.161208: step 80500, loss = 0.1216, acc = 0.9620 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 07:11:27.883579: step 80520, loss = 0.0955, acc = 0.9720 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 07:11:32.526733: step 80540, loss = 0.1006, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:11:37.100865: step 80560, loss = 0.1154, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 07:11:41.687752: step 80580, loss = 0.1205, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 07:11:46.508160: step 80600, loss = 0.1071, acc = 0.9640 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 07:11:51.057475: step 80620, loss = 0.0989, acc = 0.9760 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 07:11:56.476525: step 80640, loss = 0.0749, acc = 0.9840 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 07:12:01.163087: step 80660, loss = 0.1103, acc = 0.9640 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 07:12:05.813178: step 80680, loss = 0.1102, acc = 0.9680 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:12:10.481813: step 80700, loss = 0.1343, acc = 0.9480 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 07:12:15.145393: step 80720, loss = 0.0866, acc = 0.9780 (246.6 examples/sec; 0.260 sec/batch)
2017-05-09 07:12:19.724786: step 80740, loss = 0.0933, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:12:24.265936: step 80760, loss = 0.1320, acc = 0.9640 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:12:28.935858: step 80780, loss = 0.1409, acc = 0.9560 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 07:12:33.662749: step 80800, loss = 0.1270, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 07:12:38.302045: step 80820, loss = 0.0868, acc = 0.9800 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 07:12:42.836579: step 80840, loss = 0.1291, acc = 0.9620 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 07:12:47.535421: step 80860, loss = 0.1268, acc = 0.9600 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 07:12:52.214933: step 80880, loss = 0.0907, acc = 0.9720 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 07:12:57.002562: step 80900, loss = 0.1137, acc = 0.9620 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 07:13:01.672043: step 80920, loss = 0.1102, acc = 0.9740 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 07:13:06.183429: step 80940, loss = 0.0965, acc = 0.9700 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 07:13:10.802972: step 80960, loss = 0.1304, acc = 0.9620 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 07:13:15.602551: step 80980, loss = 0.1187, acc = 0.9660 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 07:13:20.147888: step 81000, loss = 0.1106, acc = 0.9740 (276.6 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 07:13:34.187394: step 81000, acc = 0.9633, f1 = 0.9621
[Test] 2017-05-09 07:13:43.863283: step 81000, acc = 0.9556, f1 = 0.9552
[Status] 2017-05-09 07:13:43.863349: step 81000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:13:48.535091: step 81020, loss = 0.1213, acc = 0.9680 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 07:13:53.046896: step 81040, loss = 0.1077, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 07:13:57.616382: step 81060, loss = 0.1087, acc = 0.9640 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 07:14:02.355586: step 81080, loss = 0.1106, acc = 0.9660 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 07:14:06.801193: step 81100, loss = 0.1078, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 07:14:11.263412: step 81120, loss = 0.1297, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 07:14:16.152212: step 81140, loss = 0.1059, acc = 0.9780 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 07:14:20.587624: step 81160, loss = 0.1084, acc = 0.9640 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 07:14:25.101505: step 81180, loss = 0.1130, acc = 0.9620 (303.3 examples/sec; 0.211 sec/batch)
2017-05-09 07:14:29.699758: step 81200, loss = 0.0858, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 07:14:34.301907: step 81220, loss = 0.1278, acc = 0.9500 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 07:14:38.900712: step 81240, loss = 0.1261, acc = 0.9540 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 07:14:43.529250: step 81260, loss = 0.1193, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 07:14:47.955302: step 81280, loss = 0.0992, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:14:52.650984: step 81300, loss = 0.1135, acc = 0.9620 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 07:14:57.387364: step 81320, loss = 0.1252, acc = 0.9520 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 07:15:01.957286: step 81340, loss = 0.1248, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 07:15:06.518944: step 81360, loss = 0.1113, acc = 0.9660 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 07:15:11.238624: step 81380, loss = 0.1122, acc = 0.9660 (242.3 examples/sec; 0.264 sec/batch)
2017-05-09 07:15:15.927575: step 81400, loss = 0.1112, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 07:15:20.590771: step 81420, loss = 0.1245, acc = 0.9640 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 07:15:25.077869: step 81440, loss = 0.1209, acc = 0.9600 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 07:15:29.890184: step 81460, loss = 0.1389, acc = 0.9500 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 07:15:34.615465: step 81480, loss = 0.1138, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 07:15:39.132768: step 81500, loss = 0.0983, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 07:15:43.889629: step 81520, loss = 0.1217, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 07:15:48.556248: step 81540, loss = 0.1253, acc = 0.9600 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 07:15:53.137736: step 81560, loss = 0.1126, acc = 0.9680 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:15:58.053015: step 81580, loss = 0.1003, acc = 0.9680 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:16:02.674142: step 81600, loss = 0.1046, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 07:16:07.155206: step 81620, loss = 0.1209, acc = 0.9620 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 07:16:11.893572: step 81640, loss = 0.1278, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 07:16:17.406568: step 81660, loss = 0.1032, acc = 0.9680 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 07:16:21.916835: step 81680, loss = 0.1208, acc = 0.9680 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 07:16:26.613789: step 81700, loss = 0.0958, acc = 0.9720 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 07:16:31.193819: step 81720, loss = 0.0901, acc = 0.9760 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:16:35.755674: step 81740, loss = 0.1009, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:16:40.469509: step 81760, loss = 0.1219, acc = 0.9620 (238.6 examples/sec; 0.268 sec/batch)
2017-05-09 07:16:45.093086: step 81780, loss = 0.1205, acc = 0.9580 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:16:49.695101: step 81800, loss = 0.1047, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 07:16:54.290084: step 81820, loss = 0.1251, acc = 0.9480 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:16:58.955768: step 81840, loss = 0.1267, acc = 0.9620 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 07:17:03.523487: step 81860, loss = 0.0983, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 07:17:08.065547: step 81880, loss = 0.0822, acc = 0.9840 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 07:17:12.651966: step 81900, loss = 0.1279, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 07:17:17.234260: step 81920, loss = 0.1116, acc = 0.9700 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 07:17:21.779204: step 81940, loss = 0.1202, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:17:26.398044: step 81960, loss = 0.1169, acc = 0.9680 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 07:17:30.980884: step 81980, loss = 0.1274, acc = 0.9580 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 07:17:35.796676: step 82000, loss = 0.1194, acc = 0.9600 (256.6 examples/sec; 0.249 sec/batch)
[Eval] 2017-05-09 07:17:49.699120: step 82000, acc = 0.9632, f1 = 0.9619
[Test] 2017-05-09 07:17:59.383810: step 82000, acc = 0.9552, f1 = 0.9549
[Status] 2017-05-09 07:17:59.383890: step 82000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:18:03.958290: step 82020, loss = 0.1022, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 07:18:08.674013: step 82040, loss = 0.1615, acc = 0.9360 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 07:18:13.415249: step 82060, loss = 0.1091, acc = 0.9680 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 07:18:18.073296: step 82080, loss = 0.1136, acc = 0.9660 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 07:18:22.603641: step 82100, loss = 0.1458, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 07:18:27.340045: step 82120, loss = 0.1040, acc = 0.9620 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:18:32.164366: step 82140, loss = 0.1165, acc = 0.9660 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 07:18:36.744104: step 82160, loss = 0.1313, acc = 0.9620 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 07:18:41.688332: step 82180, loss = 0.1238, acc = 0.9620 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 07:18:46.176288: step 82200, loss = 0.1255, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 07:18:50.781280: step 82220, loss = 0.1226, acc = 0.9620 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 07:18:55.445225: step 82240, loss = 0.1188, acc = 0.9700 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 07:19:00.012284: step 82260, loss = 0.1078, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 07:19:04.621401: step 82280, loss = 0.1019, acc = 0.9760 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 07:19:09.215161: step 82300, loss = 0.1129, acc = 0.9560 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:19:13.995765: step 82320, loss = 0.1054, acc = 0.9680 (255.3 examples/sec; 0.251 sec/batch)
2017-05-09 07:19:18.601663: step 82340, loss = 0.1431, acc = 0.9540 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 07:19:23.275691: step 82360, loss = 0.0935, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 07:19:27.929634: step 82380, loss = 0.1314, acc = 0.9620 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 07:19:32.608648: step 82400, loss = 0.1033, acc = 0.9700 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 07:19:37.260933: step 82420, loss = 0.1166, acc = 0.9600 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 07:19:42.010874: step 82440, loss = 0.1024, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 07:19:46.643426: step 82460, loss = 0.1188, acc = 0.9700 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:19:51.155604: step 82480, loss = 0.0898, acc = 0.9740 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 07:19:55.886450: step 82500, loss = 0.1236, acc = 0.9600 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 07:20:00.493094: step 82520, loss = 0.0985, acc = 0.9660 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 07:20:05.115634: step 82540, loss = 0.1216, acc = 0.9640 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 07:20:09.833834: step 82560, loss = 0.1275, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 07:20:14.319307: step 82580, loss = 0.0854, acc = 0.9780 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 07:20:18.837219: step 82600, loss = 0.1073, acc = 0.9680 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 07:20:23.537286: step 82620, loss = 0.1225, acc = 0.9640 (234.9 examples/sec; 0.272 sec/batch)
2017-05-09 07:20:28.171979: step 82640, loss = 0.1385, acc = 0.9540 (297.1 examples/sec; 0.215 sec/batch)
2017-05-09 07:20:33.296537: step 82660, loss = 0.1147, acc = 0.9680 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 07:20:37.875001: step 82680, loss = 0.1107, acc = 0.9620 (246.0 examples/sec; 0.260 sec/batch)
2017-05-09 07:20:42.468333: step 82700, loss = 0.1087, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 07:20:47.255658: step 82720, loss = 0.0898, acc = 0.9880 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 07:20:51.844389: step 82740, loss = 0.1097, acc = 0.9620 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 07:20:56.595246: step 82760, loss = 0.1406, acc = 0.9620 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 07:21:01.164725: step 82780, loss = 0.1247, acc = 0.9620 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 07:21:05.783872: step 82800, loss = 0.1131, acc = 0.9700 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 07:21:10.570471: step 82820, loss = 0.0920, acc = 0.9780 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 07:21:15.282776: step 82840, loss = 0.1306, acc = 0.9580 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 07:21:19.921531: step 82860, loss = 0.0932, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 07:21:24.700123: step 82880, loss = 0.1159, acc = 0.9600 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:21:29.161791: step 82900, loss = 0.1429, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 07:21:33.657706: step 82920, loss = 0.0965, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 07:21:38.548282: step 82940, loss = 0.0773, acc = 0.9820 (224.4 examples/sec; 0.285 sec/batch)
2017-05-09 07:21:43.129416: step 82960, loss = 0.0891, acc = 0.9760 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 07:21:47.782628: step 82980, loss = 0.1119, acc = 0.9540 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 07:21:52.409198: step 83000, loss = 0.1182, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 07:22:06.431540: step 83000, acc = 0.9631, f1 = 0.9619
[Test] 2017-05-09 07:22:16.071847: step 83000, acc = 0.9549, f1 = 0.9545
[Status] 2017-05-09 07:22:16.071944: step 83000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:22:20.693517: step 83020, loss = 0.0979, acc = 0.9740 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:22:25.217761: step 83040, loss = 0.1056, acc = 0.9780 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 07:22:29.844088: step 83060, loss = 0.1207, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 07:22:34.492150: step 83080, loss = 0.1163, acc = 0.9640 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 07:22:39.393047: step 83100, loss = 0.1324, acc = 0.9460 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 07:22:44.031477: step 83120, loss = 0.0894, acc = 0.9780 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 07:22:48.671559: step 83140, loss = 0.1047, acc = 0.9720 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 07:22:53.621986: step 83160, loss = 0.1040, acc = 0.9700 (211.6 examples/sec; 0.302 sec/batch)
2017-05-09 07:22:58.239859: step 83180, loss = 0.0930, acc = 0.9680 (262.8 examples/sec; 0.243 sec/batch)
2017-05-09 07:23:02.961073: step 83200, loss = 0.1061, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 07:23:07.726152: step 83220, loss = 0.0901, acc = 0.9760 (227.8 examples/sec; 0.281 sec/batch)
2017-05-09 07:23:12.271621: step 83240, loss = 0.1002, acc = 0.9740 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 07:23:16.816864: step 83260, loss = 0.1128, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 07:23:21.575730: step 83280, loss = 0.1185, acc = 0.9680 (249.3 examples/sec; 0.257 sec/batch)
2017-05-09 07:23:26.167801: step 83300, loss = 0.1171, acc = 0.9600 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 07:23:30.807018: step 83320, loss = 0.0980, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 07:23:35.415614: step 83340, loss = 0.1079, acc = 0.9760 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 07:23:40.146572: step 83360, loss = 0.0949, acc = 0.9760 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 07:23:44.656838: step 83380, loss = 0.0818, acc = 0.9820 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 07:23:49.260971: step 83400, loss = 0.1170, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 07:23:54.000753: step 83420, loss = 0.1085, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:23:58.503401: step 83440, loss = 0.1285, acc = 0.9640 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 07:24:03.057447: step 83460, loss = 0.1242, acc = 0.9500 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 07:24:07.787492: step 83480, loss = 0.0929, acc = 0.9760 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 07:24:12.389163: step 83500, loss = 0.1283, acc = 0.9600 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 07:24:17.187820: step 83520, loss = 0.0841, acc = 0.9820 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:24:21.806854: step 83540, loss = 0.1082, acc = 0.9660 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 07:24:26.624815: step 83560, loss = 0.1080, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 07:24:31.254040: step 83580, loss = 0.1238, acc = 0.9580 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:24:35.868809: step 83600, loss = 0.1239, acc = 0.9540 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 07:24:40.541303: step 83620, loss = 0.1416, acc = 0.9560 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 07:24:45.077426: step 83640, loss = 0.1016, acc = 0.9720 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 07:24:50.899747: step 83660, loss = 0.1151, acc = 0.9620 (123.2 examples/sec; 0.520 sec/batch)
2017-05-09 07:24:55.795414: step 83680, loss = 0.1108, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 07:25:00.241744: step 83700, loss = 0.1159, acc = 0.9460 (296.6 examples/sec; 0.216 sec/batch)
2017-05-09 07:25:04.843319: step 83720, loss = 0.1115, acc = 0.9640 (248.9 examples/sec; 0.257 sec/batch)
2017-05-09 07:25:09.317468: step 83740, loss = 0.1335, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:25:14.012673: step 83760, loss = 0.1088, acc = 0.9680 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 07:25:18.514528: step 83780, loss = 0.0877, acc = 0.9820 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 07:25:23.388503: step 83800, loss = 0.0928, acc = 0.9700 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 07:25:28.131728: step 83820, loss = 0.1417, acc = 0.9420 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 07:25:32.787864: step 83840, loss = 0.1031, acc = 0.9740 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 07:25:37.621825: step 83860, loss = 0.1144, acc = 0.9500 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 07:25:42.227788: step 83880, loss = 0.1264, acc = 0.9520 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 07:25:46.942185: step 83900, loss = 0.1150, acc = 0.9640 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 07:25:51.558184: step 83920, loss = 0.1157, acc = 0.9680 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 07:25:55.997121: step 83940, loss = 0.1012, acc = 0.9700 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 07:26:00.579067: step 83960, loss = 0.1254, acc = 0.9580 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 07:26:05.293191: step 83980, loss = 0.0975, acc = 0.9800 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 07:26:09.787512: step 84000, loss = 0.1108, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 07:26:23.726068: step 84000, acc = 0.9633, f1 = 0.9621
[Test] 2017-05-09 07:26:33.077019: step 84000, acc = 0.9560, f1 = 0.9556
[Status] 2017-05-09 07:26:33.077134: step 84000, maxindex = 73000, maxdev = 0.9634, maxtst = 0.9549
2017-05-09 07:26:37.756560: step 84020, loss = 0.1106, acc = 0.9640 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 07:26:42.402883: step 84040, loss = 0.1082, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 07:26:46.915096: step 84060, loss = 0.1115, acc = 0.9720 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 07:26:51.711596: step 84080, loss = 0.1023, acc = 0.9640 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 07:26:56.268917: step 84100, loss = 0.0905, acc = 0.9740 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 07:27:00.838609: step 84120, loss = 0.1241, acc = 0.9580 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 07:27:05.516080: step 84140, loss = 0.1147, acc = 0.9620 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:27:10.265394: step 84160, loss = 0.0939, acc = 0.9720 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 07:27:14.819857: step 84180, loss = 0.0948, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 07:27:19.619971: step 84200, loss = 0.0980, acc = 0.9740 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 07:27:24.257776: step 84220, loss = 0.1151, acc = 0.9740 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 07:27:28.833236: step 84240, loss = 0.1082, acc = 0.9680 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 07:27:33.429768: step 84260, loss = 0.1241, acc = 0.9580 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 07:27:38.110423: step 84280, loss = 0.1241, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:27:42.719970: step 84300, loss = 0.0990, acc = 0.9780 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 07:27:47.356461: step 84320, loss = 0.1149, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:27:52.222639: step 84340, loss = 0.1271, acc = 0.9580 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 07:27:56.782252: step 84360, loss = 0.1009, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:28:01.389784: step 84380, loss = 0.0930, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 07:28:06.319136: step 84400, loss = 0.1030, acc = 0.9660 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 07:28:10.925975: step 84420, loss = 0.0776, acc = 0.9860 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 07:28:15.546077: step 84440, loss = 0.1099, acc = 0.9740 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 07:28:20.385463: step 84460, loss = 0.1375, acc = 0.9540 (229.4 examples/sec; 0.279 sec/batch)
2017-05-09 07:28:24.890747: step 84480, loss = 0.1352, acc = 0.9600 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 07:28:29.449842: step 84500, loss = 0.1291, acc = 0.9520 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 07:28:34.059115: step 84520, loss = 0.1028, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 07:28:38.785229: step 84540, loss = 0.1082, acc = 0.9720 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 07:28:43.316724: step 84560, loss = 0.1116, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 07:28:47.886846: step 84580, loss = 0.1082, acc = 0.9640 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:28:52.772439: step 84600, loss = 0.1380, acc = 0.9540 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 07:28:57.331881: step 84620, loss = 0.1002, acc = 0.9760 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 07:29:02.013834: step 84640, loss = 0.0965, acc = 0.9680 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 07:29:06.745356: step 84660, loss = 0.1228, acc = 0.9640 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 07:29:11.922158: step 84680, loss = 0.1089, acc = 0.9680 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 07:29:16.568360: step 84700, loss = 0.1097, acc = 0.9660 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:29:21.185274: step 84720, loss = 0.0863, acc = 0.9760 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 07:29:25.719641: step 84740, loss = 0.1379, acc = 0.9600 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 07:29:30.309231: step 84760, loss = 0.1244, acc = 0.9600 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 07:29:35.159497: step 84780, loss = 0.1093, acc = 0.9660 (223.4 examples/sec; 0.286 sec/batch)
2017-05-09 07:29:39.766856: step 84800, loss = 0.0892, acc = 0.9740 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 07:29:44.357497: step 84820, loss = 0.1080, acc = 0.9640 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 07:29:49.018377: step 84840, loss = 0.1053, acc = 0.9700 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 07:29:53.698441: step 84860, loss = 0.1170, acc = 0.9660 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 07:29:58.383532: step 84880, loss = 0.1163, acc = 0.9680 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 07:30:02.981528: step 84900, loss = 0.1161, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:30:07.793043: step 84920, loss = 0.0783, acc = 0.9820 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 07:30:12.335853: step 84940, loss = 0.1072, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 07:30:16.986711: step 84960, loss = 0.1222, acc = 0.9580 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 07:30:21.775028: step 84980, loss = 0.1060, acc = 0.9720 (295.8 examples/sec; 0.216 sec/batch)
2017-05-09 07:30:26.343439: step 85000, loss = 0.0909, acc = 0.9780 (288.4 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 07:30:40.597119: step 85000, acc = 0.9639, f1 = 0.9627
[Test] 2017-05-09 07:30:50.246204: step 85000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 07:30:50.246346: step 85000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:30:58.246246: step 85020, loss = 0.0976, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 07:31:02.817277: step 85040, loss = 0.1040, acc = 0.9720 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 07:31:07.413292: step 85060, loss = 0.1077, acc = 0.9720 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 07:31:11.899519: step 85080, loss = 0.1054, acc = 0.9760 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 07:31:16.536761: step 85100, loss = 0.0982, acc = 0.9760 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 07:31:21.576691: step 85120, loss = 0.1213, acc = 0.9620 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 07:31:26.196264: step 85140, loss = 0.1360, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 07:31:30.833608: step 85160, loss = 0.0931, acc = 0.9740 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 07:31:35.665513: step 85180, loss = 0.0868, acc = 0.9780 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 07:31:40.205303: step 85200, loss = 0.1126, acc = 0.9700 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 07:31:44.804881: step 85220, loss = 0.1025, acc = 0.9720 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:31:49.735201: step 85240, loss = 0.1029, acc = 0.9660 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 07:31:54.470984: step 85260, loss = 0.0929, acc = 0.9760 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 07:31:58.972194: step 85280, loss = 0.0992, acc = 0.9740 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 07:32:03.997748: step 85300, loss = 0.1241, acc = 0.9600 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 07:32:08.645113: step 85320, loss = 0.1115, acc = 0.9600 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 07:32:13.251691: step 85340, loss = 0.1068, acc = 0.9680 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 07:32:18.078690: step 85360, loss = 0.1160, acc = 0.9640 (217.2 examples/sec; 0.295 sec/batch)
2017-05-09 07:32:22.701184: step 85380, loss = 0.1070, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 07:32:27.414309: step 85400, loss = 0.1175, acc = 0.9560 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 07:32:32.087121: step 85420, loss = 0.1091, acc = 0.9640 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 07:32:36.906477: step 85440, loss = 0.1287, acc = 0.9560 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 07:32:41.495860: step 85460, loss = 0.1101, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:32:46.095575: step 85480, loss = 0.1029, acc = 0.9660 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 07:32:50.971413: step 85500, loss = 0.1104, acc = 0.9680 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 07:32:55.879153: step 85520, loss = 0.0894, acc = 0.9760 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 07:33:00.455588: step 85540, loss = 0.0902, acc = 0.9800 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 07:33:05.412821: step 85560, loss = 0.1147, acc = 0.9680 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 07:33:09.987165: step 85580, loss = 0.1049, acc = 0.9640 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 07:33:14.549733: step 85600, loss = 0.0975, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 07:33:19.217339: step 85620, loss = 0.1095, acc = 0.9700 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 07:33:24.014048: step 85640, loss = 0.1402, acc = 0.9600 (248.9 examples/sec; 0.257 sec/batch)
2017-05-09 07:33:28.630881: step 85660, loss = 0.0876, acc = 0.9760 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 07:33:34.469354: step 85680, loss = 0.1692, acc = 0.9400 (230.8 examples/sec; 0.277 sec/batch)
2017-05-09 07:33:39.008915: step 85700, loss = 0.0880, acc = 0.9740 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 07:33:43.561610: step 85720, loss = 0.1278, acc = 0.9660 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 07:33:48.210768: step 85740, loss = 0.1147, acc = 0.9680 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 07:33:52.752328: step 85760, loss = 0.1090, acc = 0.9720 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 07:33:57.351851: step 85780, loss = 0.0994, acc = 0.9760 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 07:34:01.876807: step 85800, loss = 0.0951, acc = 0.9760 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 07:34:06.502453: step 85820, loss = 0.0996, acc = 0.9660 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 07:34:11.171047: step 85840, loss = 0.0991, acc = 0.9680 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 07:34:15.920181: step 85860, loss = 0.0892, acc = 0.9840 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 07:34:20.684538: step 85880, loss = 0.1268, acc = 0.9700 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 07:34:25.231636: step 85900, loss = 0.1302, acc = 0.9680 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 07:34:29.835741: step 85920, loss = 0.1041, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 07:34:34.632657: step 85940, loss = 0.1160, acc = 0.9600 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 07:34:39.199068: step 85960, loss = 0.1054, acc = 0.9640 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 07:34:43.747082: step 85980, loss = 0.0925, acc = 0.9780 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 07:34:48.224978: step 86000, loss = 0.1029, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 07:35:04.774552: step 86000, acc = 0.9635, f1 = 0.9624
[Test] 2017-05-09 07:35:16.514706: step 86000, acc = 0.9557, f1 = 0.9554
[Status] 2017-05-09 07:35:16.514821: step 86000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:35:21.120966: step 86020, loss = 0.0864, acc = 0.9780 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 07:35:25.773527: step 86040, loss = 0.0950, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 07:35:30.246915: step 86060, loss = 0.1083, acc = 0.9660 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 07:35:34.995928: step 86080, loss = 0.1482, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 07:35:39.536058: step 86100, loss = 0.0933, acc = 0.9700 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:35:44.074473: step 86120, loss = 0.1261, acc = 0.9580 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 07:35:48.975568: step 86140, loss = 0.0985, acc = 0.9680 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 07:35:53.571868: step 86160, loss = 0.1034, acc = 0.9720 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 07:35:58.267584: step 86180, loss = 0.1092, acc = 0.9640 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 07:36:02.905119: step 86200, loss = 0.0984, acc = 0.9700 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 07:36:07.511691: step 86220, loss = 0.1022, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 07:36:12.111750: step 86240, loss = 0.1237, acc = 0.9620 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:36:16.878261: step 86260, loss = 0.1104, acc = 0.9660 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 07:36:21.470052: step 86280, loss = 0.1252, acc = 0.9560 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 07:36:26.054988: step 86300, loss = 0.1092, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 07:36:30.605497: step 86320, loss = 0.1165, acc = 0.9640 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:36:35.436717: step 86340, loss = 0.1068, acc = 0.9720 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 07:36:40.090750: step 86360, loss = 0.1049, acc = 0.9760 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 07:36:44.855899: step 86380, loss = 0.1249, acc = 0.9620 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 07:36:49.677444: step 86400, loss = 0.0930, acc = 0.9720 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 07:36:54.294301: step 86420, loss = 0.1168, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 07:36:59.137526: step 86440, loss = 0.1053, acc = 0.9560 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 07:37:03.983476: step 86460, loss = 0.0944, acc = 0.9760 (242.0 examples/sec; 0.264 sec/batch)
2017-05-09 07:37:08.539770: step 86480, loss = 0.1491, acc = 0.9500 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 07:37:13.211816: step 86500, loss = 0.1146, acc = 0.9720 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 07:37:17.743949: step 86520, loss = 0.1209, acc = 0.9580 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 07:37:22.635821: step 86540, loss = 0.1260, acc = 0.9620 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 07:37:27.258277: step 86560, loss = 0.1447, acc = 0.9620 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 07:37:31.937777: step 86580, loss = 0.0993, acc = 0.9700 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 07:37:36.783056: step 86600, loss = 0.1382, acc = 0.9540 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 07:37:41.419928: step 86620, loss = 0.0918, acc = 0.9740 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 07:37:45.972172: step 86640, loss = 0.0900, acc = 0.9800 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 07:37:50.585747: step 86660, loss = 0.1333, acc = 0.9560 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 07:37:55.363527: step 86680, loss = 0.1143, acc = 0.9600 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 07:38:00.610492: step 86700, loss = 0.1081, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 07:38:05.335409: step 86720, loss = 0.1188, acc = 0.9580 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 07:38:09.843213: step 86740, loss = 0.0894, acc = 0.9800 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 07:38:14.424492: step 86760, loss = 0.1109, acc = 0.9700 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 07:38:19.018113: step 86780, loss = 0.1061, acc = 0.9740 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 07:38:23.692157: step 86800, loss = 0.1028, acc = 0.9720 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 07:38:28.281408: step 86820, loss = 0.0966, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 07:38:32.859854: step 86840, loss = 0.1134, acc = 0.9580 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 07:38:37.771800: step 86860, loss = 0.1363, acc = 0.9620 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 07:38:42.307165: step 86880, loss = 0.1232, acc = 0.9560 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 07:38:46.957225: step 86900, loss = 0.1574, acc = 0.9420 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 07:38:51.674125: step 86920, loss = 0.1231, acc = 0.9500 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 07:38:56.315581: step 86940, loss = 0.1140, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 07:39:00.919831: step 86960, loss = 0.0810, acc = 0.9860 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 07:39:05.637078: step 86980, loss = 0.0993, acc = 0.9720 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:39:10.324593: step 87000, loss = 0.1140, acc = 0.9620 (261.8 examples/sec; 0.244 sec/batch)
[Eval] 2017-05-09 07:39:24.435649: step 87000, acc = 0.9633, f1 = 0.9621
[Test] 2017-05-09 07:39:34.318934: step 87000, acc = 0.9563, f1 = 0.9559
[Status] 2017-05-09 07:39:34.319035: step 87000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:39:38.871861: step 87020, loss = 0.1175, acc = 0.9640 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 07:39:43.630771: step 87040, loss = 0.1042, acc = 0.9700 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 07:39:48.432234: step 87060, loss = 0.0969, acc = 0.9740 (225.4 examples/sec; 0.284 sec/batch)
2017-05-09 07:39:53.042172: step 87080, loss = 0.0913, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 07:39:57.612804: step 87100, loss = 0.1312, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 07:40:02.230077: step 87120, loss = 0.0993, acc = 0.9780 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 07:40:07.024051: step 87140, loss = 0.1050, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 07:40:11.536434: step 87160, loss = 0.1089, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 07:40:16.047501: step 87180, loss = 0.1226, acc = 0.9540 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 07:40:20.681754: step 87200, loss = 0.0855, acc = 0.9800 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 07:40:25.246600: step 87220, loss = 0.1111, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 07:40:29.755655: step 87240, loss = 0.0914, acc = 0.9740 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 07:40:34.482861: step 87260, loss = 0.1363, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 07:40:39.089303: step 87280, loss = 0.0978, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 07:40:43.655009: step 87300, loss = 0.1061, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 07:40:48.532445: step 87320, loss = 0.1142, acc = 0.9740 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:40:53.144103: step 87340, loss = 0.1144, acc = 0.9620 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 07:40:57.802224: step 87360, loss = 0.1095, acc = 0.9680 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 07:41:02.515644: step 87380, loss = 0.1278, acc = 0.9560 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 07:41:07.130130: step 87400, loss = 0.0996, acc = 0.9700 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 07:41:11.773419: step 87420, loss = 0.1302, acc = 0.9660 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:41:16.429494: step 87440, loss = 0.1183, acc = 0.9600 (241.8 examples/sec; 0.265 sec/batch)
2017-05-09 07:41:21.067355: step 87460, loss = 0.1551, acc = 0.9480 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 07:41:25.536241: step 87480, loss = 0.1337, acc = 0.9600 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 07:41:29.992039: step 87500, loss = 0.1231, acc = 0.9640 (308.7 examples/sec; 0.207 sec/batch)
2017-05-09 07:41:34.626526: step 87520, loss = 0.0969, acc = 0.9720 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 07:41:39.170785: step 87540, loss = 0.1397, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 07:41:43.729623: step 87560, loss = 0.0973, acc = 0.9800 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 07:41:48.479828: step 87580, loss = 0.1086, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 07:41:53.119042: step 87600, loss = 0.1135, acc = 0.9660 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 07:41:57.634651: step 87620, loss = 0.1209, acc = 0.9640 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 07:42:02.417273: step 87640, loss = 0.1399, acc = 0.9460 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 07:42:06.939194: step 87660, loss = 0.0969, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 07:42:11.484836: step 87680, loss = 0.1107, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 07:42:17.595503: step 87700, loss = 0.1232, acc = 0.9540 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 07:42:22.181099: step 87720, loss = 0.1171, acc = 0.9580 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 07:42:26.798773: step 87740, loss = 0.1232, acc = 0.9560 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 07:42:31.510176: step 87760, loss = 0.0942, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 07:42:36.098387: step 87780, loss = 0.1106, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 07:42:40.638914: step 87800, loss = 0.1151, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 07:42:45.399394: step 87820, loss = 0.1062, acc = 0.9580 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 07:42:49.962435: step 87840, loss = 0.1116, acc = 0.9640 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 07:42:54.573107: step 87860, loss = 0.1343, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 07:42:59.303944: step 87880, loss = 0.1082, acc = 0.9760 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 07:43:03.861989: step 87900, loss = 0.1069, acc = 0.9700 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 07:43:08.392062: step 87920, loss = 0.1257, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 07:43:12.898088: step 87940, loss = 0.1089, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 07:43:17.592234: step 87960, loss = 0.1054, acc = 0.9680 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 07:43:23.066224: step 87980, loss = 0.1334, acc = 0.9540 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 07:43:27.637647: step 88000, loss = 0.0847, acc = 0.9740 (271.9 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 07:43:41.658699: step 88000, acc = 0.9634, f1 = 0.9623
[Test] 2017-05-09 07:43:51.299902: step 88000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 07:43:51.299988: step 88000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:43:55.758954: step 88020, loss = 0.1030, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 07:44:00.479779: step 88040, loss = 0.1491, acc = 0.9660 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 07:44:05.054591: step 88060, loss = 0.1243, acc = 0.9560 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:44:09.851099: step 88080, loss = 0.1184, acc = 0.9760 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 07:44:14.530768: step 88100, loss = 0.1218, acc = 0.9660 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 07:44:19.035503: step 88120, loss = 0.1194, acc = 0.9660 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 07:44:23.607717: step 88140, loss = 0.1259, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 07:44:28.544337: step 88160, loss = 0.1173, acc = 0.9660 (226.0 examples/sec; 0.283 sec/batch)
2017-05-09 07:44:33.114913: step 88180, loss = 0.0892, acc = 0.9780 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 07:44:37.649220: step 88200, loss = 0.1192, acc = 0.9620 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 07:44:42.364985: step 88220, loss = 0.1107, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 07:44:47.289849: step 88240, loss = 0.1123, acc = 0.9660 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 07:44:51.840894: step 88260, loss = 0.1130, acc = 0.9620 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 07:44:56.479004: step 88280, loss = 0.0810, acc = 0.9840 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 07:45:01.187447: step 88300, loss = 0.1211, acc = 0.9560 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 07:45:05.737327: step 88320, loss = 0.1009, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 07:45:10.358421: step 88340, loss = 0.1095, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 07:45:14.935097: step 88360, loss = 0.0917, acc = 0.9740 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 07:45:19.648849: step 88380, loss = 0.1385, acc = 0.9500 (253.0 examples/sec; 0.253 sec/batch)
2017-05-09 07:45:24.101849: step 88400, loss = 0.1016, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:45:28.800871: step 88420, loss = 0.1218, acc = 0.9800 (234.3 examples/sec; 0.273 sec/batch)
2017-05-09 07:45:33.338594: step 88440, loss = 0.1169, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 07:45:37.930445: step 88460, loss = 0.0988, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 07:45:42.591427: step 88480, loss = 0.1076, acc = 0.9620 (242.9 examples/sec; 0.263 sec/batch)
2017-05-09 07:45:47.176076: step 88500, loss = 0.0966, acc = 0.9760 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 07:45:51.768244: step 88520, loss = 0.1063, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:45:56.362523: step 88540, loss = 0.0943, acc = 0.9740 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 07:46:01.076862: step 88560, loss = 0.0946, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 07:46:05.699151: step 88580, loss = 0.1069, acc = 0.9640 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 07:46:10.271568: step 88600, loss = 0.1287, acc = 0.9580 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:46:14.979793: step 88620, loss = 0.1287, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 07:46:19.503516: step 88640, loss = 0.1005, acc = 0.9720 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 07:46:24.221694: step 88660, loss = 0.0987, acc = 0.9720 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 07:46:28.949443: step 88680, loss = 0.0847, acc = 0.9800 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 07:46:34.256564: step 88700, loss = 0.1151, acc = 0.9620 (156.9 examples/sec; 0.408 sec/batch)
2017-05-09 07:46:38.735025: step 88720, loss = 0.0942, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 07:46:43.392296: step 88740, loss = 0.1124, acc = 0.9640 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 07:46:47.975338: step 88760, loss = 0.0960, acc = 0.9760 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 07:46:52.538451: step 88780, loss = 0.1062, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 07:46:57.490394: step 88800, loss = 0.1101, acc = 0.9660 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 07:47:02.170455: step 88820, loss = 0.1098, acc = 0.9660 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 07:47:06.796871: step 88840, loss = 0.1116, acc = 0.9600 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 07:47:11.364795: step 88860, loss = 0.0969, acc = 0.9680 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 07:47:16.024310: step 88880, loss = 0.0997, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:47:20.685776: step 88900, loss = 0.1091, acc = 0.9660 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 07:47:25.266824: step 88920, loss = 0.0870, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 07:47:29.908667: step 88940, loss = 0.1069, acc = 0.9600 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 07:47:34.431489: step 88960, loss = 0.1193, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 07:47:38.939994: step 88980, loss = 0.1261, acc = 0.9520 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:47:43.628336: step 89000, loss = 0.1062, acc = 0.9720 (291.9 examples/sec; 0.219 sec/batch)
[Eval] 2017-05-09 07:47:57.680252: step 89000, acc = 0.9635, f1 = 0.9623
[Test] 2017-05-09 07:48:06.942443: step 89000, acc = 0.9553, f1 = 0.9550
[Status] 2017-05-09 07:48:06.942508: step 89000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:48:11.754716: step 89020, loss = 0.1160, acc = 0.9700 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 07:48:16.294846: step 89040, loss = 0.0906, acc = 0.9680 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 07:48:20.933934: step 89060, loss = 0.1060, acc = 0.9780 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 07:48:25.586687: step 89080, loss = 0.1445, acc = 0.9480 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:48:30.015632: step 89100, loss = 0.1340, acc = 0.9600 (298.2 examples/sec; 0.215 sec/batch)
2017-05-09 07:48:34.519245: step 89120, loss = 0.1073, acc = 0.9620 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 07:48:39.164892: step 89140, loss = 0.1299, acc = 0.9660 (241.1 examples/sec; 0.265 sec/batch)
2017-05-09 07:48:43.864897: step 89160, loss = 0.1201, acc = 0.9660 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 07:48:48.383657: step 89180, loss = 0.1136, acc = 0.9700 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 07:48:53.038820: step 89200, loss = 0.1002, acc = 0.9700 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 07:48:57.774367: step 89220, loss = 0.0889, acc = 0.9740 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 07:49:02.324636: step 89240, loss = 0.0832, acc = 0.9800 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 07:49:07.006278: step 89260, loss = 0.1058, acc = 0.9640 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 07:49:11.642673: step 89280, loss = 0.1131, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 07:49:16.427555: step 89300, loss = 0.1070, acc = 0.9640 (214.6 examples/sec; 0.298 sec/batch)
2017-05-09 07:49:21.142896: step 89320, loss = 0.1111, acc = 0.9720 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 07:49:26.003301: step 89340, loss = 0.1067, acc = 0.9720 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 07:49:30.579150: step 89360, loss = 0.0844, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 07:49:35.129212: step 89380, loss = 0.1157, acc = 0.9640 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 07:49:39.883870: step 89400, loss = 0.1057, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 07:49:44.452820: step 89420, loss = 0.0968, acc = 0.9720 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 07:49:49.011515: step 89440, loss = 0.0925, acc = 0.9740 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 07:49:53.751718: step 89460, loss = 0.0906, acc = 0.9780 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 07:49:58.317965: step 89480, loss = 0.0961, acc = 0.9760 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 07:50:02.902028: step 89500, loss = 0.1272, acc = 0.9620 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 07:50:07.559279: step 89520, loss = 0.0955, acc = 0.9720 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 07:50:12.286826: step 89540, loss = 0.0837, acc = 0.9780 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:50:16.865071: step 89560, loss = 0.1170, acc = 0.9640 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 07:50:21.457581: step 89580, loss = 0.1289, acc = 0.9680 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 07:50:26.131511: step 89600, loss = 0.0842, acc = 0.9800 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:50:30.694239: step 89620, loss = 0.0972, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 07:50:35.247210: step 89640, loss = 0.0910, acc = 0.9740 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 07:50:39.909321: step 89660, loss = 0.0888, acc = 0.9760 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 07:50:44.374121: step 89680, loss = 0.0991, acc = 0.9760 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 07:50:48.865927: step 89700, loss = 0.0924, acc = 0.9800 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 07:50:54.652317: step 89720, loss = 0.1294, acc = 0.9580 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 07:50:59.222508: step 89740, loss = 0.1277, acc = 0.9600 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 07:51:03.695881: step 89760, loss = 0.1374, acc = 0.9500 (301.0 examples/sec; 0.213 sec/batch)
2017-05-09 07:51:08.470036: step 89780, loss = 0.0950, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:51:12.964834: step 89800, loss = 0.1107, acc = 0.9600 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 07:51:17.958229: step 89820, loss = 0.1310, acc = 0.9580 (258.6 examples/sec; 0.247 sec/batch)
2017-05-09 07:51:22.714151: step 89840, loss = 0.1015, acc = 0.9700 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 07:51:27.142142: step 89860, loss = 0.1230, acc = 0.9520 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 07:51:31.838479: step 89880, loss = 0.0990, acc = 0.9720 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 07:51:36.522507: step 89900, loss = 0.0937, acc = 0.9740 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 07:51:41.448042: step 89920, loss = 0.1250, acc = 0.9540 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 07:51:46.075532: step 89940, loss = 0.0934, acc = 0.9720 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 07:51:50.726324: step 89960, loss = 0.1331, acc = 0.9620 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 07:51:55.307622: step 89980, loss = 0.1161, acc = 0.9680 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 07:52:00.032220: step 90000, loss = 0.1079, acc = 0.9660 (261.0 examples/sec; 0.245 sec/batch)
[Eval] 2017-05-09 07:52:14.137515: step 90000, acc = 0.9620, f1 = 0.9607
[Test] 2017-05-09 07:52:23.821152: step 90000, acc = 0.9529, f1 = 0.9525
[Status] 2017-05-09 07:52:23.821239: step 90000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:52:28.477543: step 90020, loss = 0.1103, acc = 0.9600 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 07:52:33.180363: step 90040, loss = 0.1233, acc = 0.9640 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 07:52:38.075223: step 90060, loss = 0.1058, acc = 0.9580 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 07:52:42.574946: step 90080, loss = 0.0947, acc = 0.9780 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:52:47.246857: step 90100, loss = 0.0799, acc = 0.9860 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 07:52:51.971618: step 90120, loss = 0.1053, acc = 0.9640 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 07:52:56.486739: step 90140, loss = 0.1084, acc = 0.9700 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 07:53:01.083401: step 90160, loss = 0.1215, acc = 0.9600 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 07:53:05.871635: step 90180, loss = 0.1191, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 07:53:10.564497: step 90200, loss = 0.1121, acc = 0.9580 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 07:53:15.144080: step 90220, loss = 0.0911, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 07:53:19.852943: step 90240, loss = 0.1197, acc = 0.9680 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 07:53:24.523960: step 90260, loss = 0.0914, acc = 0.9780 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 07:53:29.208252: step 90280, loss = 0.1095, acc = 0.9640 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 07:53:33.744306: step 90300, loss = 0.0794, acc = 0.9840 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 07:53:38.529340: step 90320, loss = 0.0953, acc = 0.9720 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 07:53:43.153481: step 90340, loss = 0.1150, acc = 0.9620 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 07:53:47.841952: step 90360, loss = 0.1264, acc = 0.9620 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 07:53:52.621957: step 90380, loss = 0.1050, acc = 0.9700 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 07:53:57.274658: step 90400, loss = 0.1014, acc = 0.9700 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 07:54:01.831766: step 90420, loss = 0.1164, acc = 0.9640 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 07:54:06.577737: step 90440, loss = 0.0920, acc = 0.9780 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 07:54:11.202468: step 90460, loss = 0.1247, acc = 0.9620 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 07:54:15.683934: step 90480, loss = 0.1238, acc = 0.9540 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:54:20.303744: step 90500, loss = 0.1102, acc = 0.9700 (303.7 examples/sec; 0.211 sec/batch)
2017-05-09 07:54:24.992710: step 90520, loss = 0.1312, acc = 0.9700 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 07:54:29.833838: step 90540, loss = 0.0961, acc = 0.9800 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 07:54:34.595330: step 90560, loss = 0.1013, acc = 0.9780 (235.4 examples/sec; 0.272 sec/batch)
2017-05-09 07:54:39.264956: step 90580, loss = 0.1041, acc = 0.9700 (247.1 examples/sec; 0.259 sec/batch)
2017-05-09 07:54:43.871479: step 90600, loss = 0.0897, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 07:54:48.483099: step 90620, loss = 0.0947, acc = 0.9660 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 07:54:53.154092: step 90640, loss = 0.1443, acc = 0.9540 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 07:54:57.784962: step 90660, loss = 0.1079, acc = 0.9700 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 07:55:02.364335: step 90680, loss = 0.1010, acc = 0.9720 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 07:55:07.149714: step 90700, loss = 0.1092, acc = 0.9580 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 07:55:12.555039: step 90720, loss = 0.1437, acc = 0.9500 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 07:55:17.134598: step 90740, loss = 0.1300, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 07:55:21.805464: step 90760, loss = 0.1041, acc = 0.9700 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 07:55:26.437055: step 90780, loss = 0.0900, acc = 0.9800 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 07:55:30.990627: step 90800, loss = 0.1160, acc = 0.9620 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 07:55:35.750917: step 90820, loss = 0.1286, acc = 0.9640 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 07:55:40.248461: step 90840, loss = 0.0910, acc = 0.9740 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 07:55:44.932870: step 90860, loss = 0.1254, acc = 0.9640 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 07:55:49.582731: step 90880, loss = 0.0939, acc = 0.9800 (297.7 examples/sec; 0.215 sec/batch)
2017-05-09 07:55:54.205497: step 90900, loss = 0.1162, acc = 0.9740 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 07:55:58.803809: step 90920, loss = 0.1309, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 07:56:03.535259: step 90940, loss = 0.1115, acc = 0.9660 (230.2 examples/sec; 0.278 sec/batch)
2017-05-09 07:56:08.092457: step 90960, loss = 0.0920, acc = 0.9700 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 07:56:12.733477: step 90980, loss = 0.1312, acc = 0.9660 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 07:56:17.371675: step 91000, loss = 0.1172, acc = 0.9660 (249.6 examples/sec; 0.256 sec/batch)
[Eval] 2017-05-09 07:56:30.984428: step 91000, acc = 0.9615, f1 = 0.9602
[Test] 2017-05-09 07:56:40.511774: step 91000, acc = 0.9523, f1 = 0.9519
[Status] 2017-05-09 07:56:40.511839: step 91000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 07:56:45.154687: step 91020, loss = 0.1029, acc = 0.9720 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 07:56:49.907534: step 91040, loss = 0.1300, acc = 0.9600 (299.5 examples/sec; 0.214 sec/batch)
2017-05-09 07:56:54.561590: step 91060, loss = 0.1354, acc = 0.9580 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 07:56:59.207845: step 91080, loss = 0.0958, acc = 0.9760 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 07:57:04.100430: step 91100, loss = 0.1103, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 07:57:08.602910: step 91120, loss = 0.1109, acc = 0.9640 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 07:57:13.104249: step 91140, loss = 0.0736, acc = 0.9840 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 07:57:17.932121: step 91160, loss = 0.1609, acc = 0.9360 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 07:57:22.531015: step 91180, loss = 0.1119, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 07:57:27.144153: step 91200, loss = 0.0849, acc = 0.9820 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 07:57:32.045408: step 91220, loss = 0.1299, acc = 0.9620 (233.1 examples/sec; 0.275 sec/batch)
2017-05-09 07:57:36.535733: step 91240, loss = 0.1113, acc = 0.9700 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 07:57:41.010269: step 91260, loss = 0.1050, acc = 0.9560 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 07:57:45.760865: step 91280, loss = 0.1083, acc = 0.9620 (239.3 examples/sec; 0.267 sec/batch)
2017-05-09 07:57:50.335217: step 91300, loss = 0.1200, acc = 0.9660 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 07:57:55.008084: step 91320, loss = 0.0891, acc = 0.9740 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 07:57:59.791125: step 91340, loss = 0.0926, acc = 0.9740 (242.8 examples/sec; 0.264 sec/batch)
2017-05-09 07:58:04.325117: step 91360, loss = 0.0940, acc = 0.9800 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 07:58:08.907194: step 91380, loss = 0.0992, acc = 0.9680 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 07:58:13.660099: step 91400, loss = 0.0980, acc = 0.9700 (235.8 examples/sec; 0.271 sec/batch)
2017-05-09 07:58:18.204218: step 91420, loss = 0.1018, acc = 0.9680 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 07:58:22.816044: step 91440, loss = 0.0976, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 07:58:27.384616: step 91460, loss = 0.1247, acc = 0.9600 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 07:58:31.981399: step 91480, loss = 0.1081, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 07:58:36.572204: step 91500, loss = 0.0877, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 07:58:41.108788: step 91520, loss = 0.0976, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 07:58:45.951077: step 91540, loss = 0.1090, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 07:58:50.624275: step 91560, loss = 0.0878, acc = 0.9740 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 07:58:55.716639: step 91580, loss = 0.1031, acc = 0.9720 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 07:59:00.344262: step 91600, loss = 0.0967, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 07:59:05.076357: step 91620, loss = 0.0960, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 07:59:09.578863: step 91640, loss = 0.1084, acc = 0.9680 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 07:59:14.272172: step 91660, loss = 0.1123, acc = 0.9680 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 07:59:18.839258: step 91680, loss = 0.1164, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 07:59:24.339576: step 91700, loss = 0.1216, acc = 0.9620 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 07:59:28.989043: step 91720, loss = 0.0990, acc = 0.9640 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 07:59:34.623464: step 91740, loss = 0.0957, acc = 0.9760 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 07:59:39.163504: step 91760, loss = 0.0955, acc = 0.9740 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 07:59:44.130963: step 91780, loss = 0.1294, acc = 0.9500 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 07:59:48.687205: step 91800, loss = 0.1361, acc = 0.9540 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 07:59:53.322540: step 91820, loss = 0.1273, acc = 0.9620 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 07:59:57.986341: step 91840, loss = 0.1178, acc = 0.9540 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 08:00:02.591400: step 91860, loss = 0.0895, acc = 0.9760 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 08:00:07.139060: step 91880, loss = 0.0970, acc = 0.9700 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 08:00:11.877218: step 91900, loss = 0.1220, acc = 0.9600 (244.7 examples/sec; 0.262 sec/batch)
2017-05-09 08:00:16.477386: step 91920, loss = 0.0933, acc = 0.9740 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:00:20.996310: step 91940, loss = 0.1264, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:00:25.513501: step 91960, loss = 0.1352, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 08:00:30.333119: step 91980, loss = 0.1221, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 08:00:34.937113: step 92000, loss = 0.1155, acc = 0.9660 (277.4 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 08:00:49.172943: step 92000, acc = 0.9635, f1 = 0.9624
[Test] 2017-05-09 08:00:59.063519: step 92000, acc = 0.9554, f1 = 0.9551
[Status] 2017-05-09 08:00:59.063634: step 92000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 08:01:03.656299: step 92020, loss = 0.1133, acc = 0.9720 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 08:01:08.308800: step 92040, loss = 0.0911, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 08:01:13.039406: step 92060, loss = 0.1221, acc = 0.9640 (240.5 examples/sec; 0.266 sec/batch)
2017-05-09 08:01:17.797469: step 92080, loss = 0.1370, acc = 0.9500 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 08:01:22.406401: step 92100, loss = 0.0836, acc = 0.9800 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:01:27.007112: step 92120, loss = 0.1013, acc = 0.9700 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:01:31.745308: step 92140, loss = 0.0921, acc = 0.9700 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 08:01:36.418421: step 92160, loss = 0.1026, acc = 0.9780 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 08:01:40.980650: step 92180, loss = 0.0900, acc = 0.9760 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 08:01:45.746542: step 92200, loss = 0.0838, acc = 0.9760 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 08:01:50.290093: step 92220, loss = 0.1121, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 08:01:54.876326: step 92240, loss = 0.1025, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 08:01:59.596519: step 92260, loss = 0.1121, acc = 0.9720 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 08:02:04.158176: step 92280, loss = 0.1077, acc = 0.9740 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 08:02:08.665098: step 92300, loss = 0.1041, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 08:02:13.401926: step 92320, loss = 0.0978, acc = 0.9760 (237.1 examples/sec; 0.270 sec/batch)
2017-05-09 08:02:17.995027: step 92340, loss = 0.0958, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 08:02:22.625495: step 92360, loss = 0.1081, acc = 0.9660 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 08:02:27.178451: step 92380, loss = 0.1086, acc = 0.9720 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 08:02:31.955783: step 92400, loss = 0.0877, acc = 0.9740 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 08:02:36.534913: step 92420, loss = 0.0942, acc = 0.9840 (255.2 examples/sec; 0.251 sec/batch)
2017-05-09 08:02:41.068785: step 92440, loss = 0.1034, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 08:02:45.796325: step 92460, loss = 0.1037, acc = 0.9660 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 08:02:50.362558: step 92480, loss = 0.1243, acc = 0.9620 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 08:02:54.956477: step 92500, loss = 0.1020, acc = 0.9720 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 08:02:59.689064: step 92520, loss = 0.1233, acc = 0.9520 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:03:04.270784: step 92540, loss = 0.1058, acc = 0.9620 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 08:03:08.878658: step 92560, loss = 0.0894, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:03:13.746493: step 92580, loss = 0.0989, acc = 0.9740 (229.0 examples/sec; 0.279 sec/batch)
2017-05-09 08:03:18.330430: step 92600, loss = 0.1249, acc = 0.9560 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 08:03:22.876076: step 92620, loss = 0.0844, acc = 0.9760 (296.2 examples/sec; 0.216 sec/batch)
2017-05-09 08:03:27.574590: step 92640, loss = 0.1122, acc = 0.9640 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 08:03:32.435023: step 92660, loss = 0.1057, acc = 0.9740 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 08:03:37.076453: step 92680, loss = 0.1111, acc = 0.9740 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 08:03:41.594077: step 92700, loss = 0.1217, acc = 0.9600 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 08:03:46.238849: step 92720, loss = 0.1267, acc = 0.9620 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:03:51.424805: step 92740, loss = 0.1288, acc = 0.9660 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 08:03:55.919827: step 92760, loss = 0.1207, acc = 0.9680 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 08:04:00.556453: step 92780, loss = 0.0923, acc = 0.9720 (259.0 examples/sec; 0.247 sec/batch)
2017-05-09 08:04:05.111371: step 92800, loss = 0.1088, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 08:04:09.686807: step 92820, loss = 0.1045, acc = 0.9800 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:04:14.353608: step 92840, loss = 0.1216, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 08:04:18.888364: step 92860, loss = 0.1206, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 08:04:23.579161: step 92880, loss = 0.0767, acc = 0.9800 (250.0 examples/sec; 0.256 sec/batch)
2017-05-09 08:04:28.349392: step 92900, loss = 0.1202, acc = 0.9600 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 08:04:33.051432: step 92920, loss = 0.1124, acc = 0.9680 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 08:04:37.756791: step 92940, loss = 0.1000, acc = 0.9720 (256.7 examples/sec; 0.249 sec/batch)
2017-05-09 08:04:42.575593: step 92960, loss = 0.1004, acc = 0.9700 (240.1 examples/sec; 0.267 sec/batch)
2017-05-09 08:04:47.167987: step 92980, loss = 0.1179, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:04:51.795185: step 93000, loss = 0.1242, acc = 0.9580 (279.1 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 08:05:05.863878: step 93000, acc = 0.9609, f1 = 0.9597
[Test] 2017-05-09 08:05:15.594921: step 93000, acc = 0.9533, f1 = 0.9529
[Status] 2017-05-09 08:05:15.595011: step 93000, maxindex = 85000, maxdev = 0.9639, maxtst = 0.9563
2017-05-09 08:05:20.148955: step 93020, loss = 0.1154, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 08:05:24.715871: step 93040, loss = 0.0889, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 08:05:29.461147: step 93060, loss = 0.1267, acc = 0.9620 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 08:05:34.007310: step 93080, loss = 0.0950, acc = 0.9680 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 08:05:38.682122: step 93100, loss = 0.1112, acc = 0.9680 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 08:05:43.368687: step 93120, loss = 0.0813, acc = 0.9840 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 08:05:47.882423: step 93140, loss = 0.1173, acc = 0.9620 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 08:05:52.465351: step 93160, loss = 0.1271, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 08:05:57.445451: step 93180, loss = 0.1340, acc = 0.9680 (207.0 examples/sec; 0.309 sec/batch)
2017-05-09 08:06:02.187809: step 93200, loss = 0.1124, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 08:06:07.119969: step 93220, loss = 0.1127, acc = 0.9620 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:06:11.818636: step 93240, loss = 0.1189, acc = 0.9660 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 08:06:16.691431: step 93260, loss = 0.1232, acc = 0.9540 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:06:21.368183: step 93280, loss = 0.1086, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 08:06:25.950684: step 93300, loss = 0.1069, acc = 0.9640 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 08:06:30.685981: step 93320, loss = 0.1134, acc = 0.9620 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 08:06:35.223020: step 93340, loss = 0.1059, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 08:06:39.860891: step 93360, loss = 0.0935, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 08:06:44.619216: step 93380, loss = 0.0995, acc = 0.9800 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 08:06:49.197780: step 93400, loss = 0.1100, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 08:06:53.825436: step 93420, loss = 0.1015, acc = 0.9700 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 08:06:58.551003: step 93440, loss = 0.1395, acc = 0.9560 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 08:07:03.102538: step 93460, loss = 0.1039, acc = 0.9640 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 08:07:07.705987: step 93480, loss = 0.0798, acc = 0.9880 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 08:07:12.316366: step 93500, loss = 0.0789, acc = 0.9840 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 08:07:17.111835: step 93520, loss = 0.1024, acc = 0.9720 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 08:07:21.729596: step 93540, loss = 0.1356, acc = 0.9560 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 08:07:26.296423: step 93560, loss = 0.1237, acc = 0.9680 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 08:07:31.094145: step 93580, loss = 0.1178, acc = 0.9640 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 08:07:35.741504: step 93600, loss = 0.1045, acc = 0.9720 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 08:07:40.380834: step 93620, loss = 0.1166, acc = 0.9600 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 08:07:44.916890: step 93640, loss = 0.1206, acc = 0.9620 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 08:07:49.421234: step 93660, loss = 0.1041, acc = 0.9660 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 08:07:53.963023: step 93680, loss = 0.1184, acc = 0.9600 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:07:58.727406: step 93700, loss = 0.1052, acc = 0.9640 (226.4 examples/sec; 0.283 sec/batch)
2017-05-09 08:08:03.302932: step 93720, loss = 0.0835, acc = 0.9820 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 08:08:08.874600: step 93740, loss = 0.1128, acc = 0.9660 (135.7 examples/sec; 0.472 sec/batch)
2017-05-09 08:08:13.691004: step 93760, loss = 0.1109, acc = 0.9720 (243.4 examples/sec; 0.263 sec/batch)
2017-05-09 08:08:18.271528: step 93780, loss = 0.1117, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 08:08:22.884938: step 93800, loss = 0.1017, acc = 0.9580 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 08:08:27.433183: step 93820, loss = 0.0903, acc = 0.9780 (305.4 examples/sec; 0.210 sec/batch)
2017-05-09 08:08:32.085115: step 93840, loss = 0.1180, acc = 0.9640 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 08:08:36.645170: step 93860, loss = 0.1202, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 08:08:41.314876: step 93880, loss = 0.1186, acc = 0.9720 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 08:08:45.893984: step 93900, loss = 0.1176, acc = 0.9640 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 08:08:50.611517: step 93920, loss = 0.1163, acc = 0.9660 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 08:08:55.119952: step 93940, loss = 0.1165, acc = 0.9700 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 08:08:59.752989: step 93960, loss = 0.1108, acc = 0.9560 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 08:09:04.292764: step 93980, loss = 0.0995, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 08:09:08.817731: step 94000, loss = 0.0907, acc = 0.9900 (276.1 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 08:09:22.995629: step 94000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 08:09:32.603410: step 94000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 08:09:32.603503: step 94000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:09:40.211239: step 94020, loss = 0.1326, acc = 0.9600 (297.1 examples/sec; 0.215 sec/batch)
2017-05-09 08:09:44.948976: step 94040, loss = 0.1047, acc = 0.9760 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 08:09:49.557471: step 94060, loss = 0.0980, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:09:54.290282: step 94080, loss = 0.1032, acc = 0.9680 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 08:09:59.028061: step 94100, loss = 0.1198, acc = 0.9640 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 08:10:03.724475: step 94120, loss = 0.1244, acc = 0.9580 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 08:10:08.364711: step 94140, loss = 0.1183, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 08:10:13.034359: step 94160, loss = 0.0943, acc = 0.9700 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 08:10:17.725345: step 94180, loss = 0.1318, acc = 0.9600 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 08:10:22.348247: step 94200, loss = 0.0982, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 08:10:26.959705: step 94220, loss = 0.0835, acc = 0.9860 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 08:10:31.891633: step 94240, loss = 0.1133, acc = 0.9580 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 08:10:36.464502: step 94260, loss = 0.1195, acc = 0.9600 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 08:10:41.124211: step 94280, loss = 0.1060, acc = 0.9600 (303.0 examples/sec; 0.211 sec/batch)
2017-05-09 08:10:45.753965: step 94300, loss = 0.1119, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 08:10:50.306400: step 94320, loss = 0.1147, acc = 0.9720 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 08:10:54.852239: step 94340, loss = 0.1053, acc = 0.9680 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:10:59.621518: step 94360, loss = 0.0884, acc = 0.9780 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 08:11:04.189797: step 94380, loss = 0.1103, acc = 0.9740 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 08:11:08.874710: step 94400, loss = 0.1281, acc = 0.9660 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 08:11:13.539135: step 94420, loss = 0.0899, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 08:11:18.115799: step 94440, loss = 0.1160, acc = 0.9680 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 08:11:22.668441: step 94460, loss = 0.1080, acc = 0.9680 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 08:11:27.449096: step 94480, loss = 0.1282, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 08:11:31.919245: step 94500, loss = 0.1314, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 08:11:36.438501: step 94520, loss = 0.0872, acc = 0.9800 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 08:11:41.145347: step 94540, loss = 0.0979, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 08:11:45.654626: step 94560, loss = 0.0982, acc = 0.9720 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 08:11:50.260791: step 94580, loss = 0.1185, acc = 0.9760 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 08:11:55.162292: step 94600, loss = 0.1004, acc = 0.9760 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 08:11:59.821654: step 94620, loss = 0.1257, acc = 0.9600 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 08:12:04.566972: step 94640, loss = 0.1063, acc = 0.9700 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 08:12:09.301360: step 94660, loss = 0.1066, acc = 0.9620 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 08:12:13.869256: step 94680, loss = 0.1255, acc = 0.9560 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 08:12:18.492263: step 94700, loss = 0.1051, acc = 0.9640 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 08:12:23.020027: step 94720, loss = 0.0953, acc = 0.9700 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 08:12:27.702681: step 94740, loss = 0.0845, acc = 0.9800 (255.8 examples/sec; 0.250 sec/batch)
2017-05-09 08:12:32.890486: step 94760, loss = 0.1315, acc = 0.9560 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 08:12:37.469029: step 94780, loss = 0.1073, acc = 0.9700 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 08:12:42.027846: step 94800, loss = 0.1350, acc = 0.9600 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 08:12:46.695412: step 94820, loss = 0.1286, acc = 0.9580 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 08:12:51.139076: step 94840, loss = 0.0907, acc = 0.9700 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 08:12:55.722655: step 94860, loss = 0.1087, acc = 0.9700 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 08:13:00.341659: step 94880, loss = 0.0913, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 08:13:04.787039: step 94900, loss = 0.1046, acc = 0.9680 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:13:09.512791: step 94920, loss = 0.1095, acc = 0.9760 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 08:13:14.475050: step 94940, loss = 0.1091, acc = 0.9680 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 08:13:19.189256: step 94960, loss = 0.1026, acc = 0.9680 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 08:13:23.878915: step 94980, loss = 0.0952, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 08:13:28.815956: step 95000, loss = 0.0917, acc = 0.9740 (290.3 examples/sec; 0.220 sec/batch)
[Eval] 2017-05-09 08:13:42.904883: step 95000, acc = 0.9640, f1 = 0.9629
[Test] 2017-05-09 08:13:52.388865: step 95000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 08:13:52.388970: step 95000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:13:57.159682: step 95020, loss = 0.0958, acc = 0.9760 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 08:14:01.811878: step 95040, loss = 0.0990, acc = 0.9720 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 08:14:06.404767: step 95060, loss = 0.1260, acc = 0.9600 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 08:14:10.975365: step 95080, loss = 0.1083, acc = 0.9660 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 08:14:15.757824: step 95100, loss = 0.1031, acc = 0.9660 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 08:14:20.442208: step 95120, loss = 0.1232, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:14:25.062534: step 95140, loss = 0.1036, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 08:14:29.937692: step 95160, loss = 0.1214, acc = 0.9700 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 08:14:34.480470: step 95180, loss = 0.1390, acc = 0.9480 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 08:14:39.110661: step 95200, loss = 0.1307, acc = 0.9600 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 08:14:43.867404: step 95220, loss = 0.1314, acc = 0.9480 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 08:14:48.432846: step 95240, loss = 0.1005, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:14:52.983779: step 95260, loss = 0.1256, acc = 0.9640 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 08:14:57.674480: step 95280, loss = 0.1295, acc = 0.9560 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 08:15:02.287555: step 95300, loss = 0.1071, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 08:15:06.856683: step 95320, loss = 0.1013, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 08:15:11.747383: step 95340, loss = 0.1188, acc = 0.9720 (224.6 examples/sec; 0.285 sec/batch)
2017-05-09 08:15:16.271319: step 95360, loss = 0.1012, acc = 0.9700 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 08:15:20.854338: step 95380, loss = 0.1090, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 08:15:25.494575: step 95400, loss = 0.1185, acc = 0.9600 (234.3 examples/sec; 0.273 sec/batch)
2017-05-09 08:15:30.077338: step 95420, loss = 0.1233, acc = 0.9660 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 08:15:34.633467: step 95440, loss = 0.1037, acc = 0.9680 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 08:15:39.401184: step 95460, loss = 0.0914, acc = 0.9780 (246.3 examples/sec; 0.260 sec/batch)
2017-05-09 08:15:44.386335: step 95480, loss = 0.1089, acc = 0.9660 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 08:15:48.945295: step 95500, loss = 0.1295, acc = 0.9520 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:15:53.443725: step 95520, loss = 0.1301, acc = 0.9620 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 08:15:58.090986: step 95540, loss = 0.1054, acc = 0.9680 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 08:16:02.543749: step 95560, loss = 0.1132, acc = 0.9680 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 08:16:07.082783: step 95580, loss = 0.1041, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:16:11.766107: step 95600, loss = 0.1333, acc = 0.9640 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 08:16:16.344594: step 95620, loss = 0.1305, acc = 0.9600 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 08:16:20.899978: step 95640, loss = 0.1076, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 08:16:25.753904: step 95660, loss = 0.1178, acc = 0.9620 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 08:16:30.307129: step 95680, loss = 0.0897, acc = 0.9800 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 08:16:34.871304: step 95700, loss = 0.0941, acc = 0.9760 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 08:16:39.618264: step 95720, loss = 0.1028, acc = 0.9780 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 08:16:44.242918: step 95740, loss = 0.1051, acc = 0.9640 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 08:16:49.703677: step 95760, loss = 0.1099, acc = 0.9700 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 08:16:54.574997: step 95780, loss = 0.1126, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 08:16:59.187659: step 95800, loss = 0.0925, acc = 0.9800 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 08:17:03.790158: step 95820, loss = 0.1167, acc = 0.9640 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 08:17:08.645597: step 95840, loss = 0.1098, acc = 0.9640 (238.0 examples/sec; 0.269 sec/batch)
2017-05-09 08:17:13.297930: step 95860, loss = 0.0953, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 08:17:17.849230: step 95880, loss = 0.1111, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 08:17:22.475262: step 95900, loss = 0.1067, acc = 0.9680 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:17:27.291585: step 95920, loss = 0.0932, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 08:17:31.836371: step 95940, loss = 0.1042, acc = 0.9680 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 08:17:36.393429: step 95960, loss = 0.0886, acc = 0.9720 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 08:17:41.281931: step 95980, loss = 0.0880, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 08:17:45.958122: step 96000, loss = 0.0769, acc = 0.9840 (269.3 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 08:18:00.012018: step 96000, acc = 0.9640, f1 = 0.9628
[Test] 2017-05-09 08:18:10.177379: step 96000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 08:18:10.177467: step 96000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:18:14.863616: step 96020, loss = 0.1081, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 08:18:19.461830: step 96040, loss = 0.0923, acc = 0.9760 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 08:18:24.129822: step 96060, loss = 0.1188, acc = 0.9600 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 08:18:28.683620: step 96080, loss = 0.1322, acc = 0.9600 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 08:18:33.286490: step 96100, loss = 0.1023, acc = 0.9640 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 08:18:38.168276: step 96120, loss = 0.0918, acc = 0.9740 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 08:18:42.752239: step 96140, loss = 0.1231, acc = 0.9700 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 08:18:47.398806: step 96160, loss = 0.1029, acc = 0.9800 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 08:18:52.230644: step 96180, loss = 0.1134, acc = 0.9660 (229.9 examples/sec; 0.278 sec/batch)
2017-05-09 08:18:56.818065: step 96200, loss = 0.1062, acc = 0.9700 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 08:19:01.382968: step 96220, loss = 0.1088, acc = 0.9640 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 08:19:05.904109: step 96240, loss = 0.0973, acc = 0.9640 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 08:19:10.668015: step 96260, loss = 0.1238, acc = 0.9640 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 08:19:15.663614: step 96280, loss = 0.0996, acc = 0.9680 (207.6 examples/sec; 0.308 sec/batch)
2017-05-09 08:19:20.402159: step 96300, loss = 0.1052, acc = 0.9720 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 08:19:25.105024: step 96320, loss = 0.0925, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 08:19:29.677726: step 96340, loss = 0.0985, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:19:34.354144: step 96360, loss = 0.1000, acc = 0.9740 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 08:19:39.195426: step 96380, loss = 0.1063, acc = 0.9700 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 08:19:43.669816: step 96400, loss = 0.0956, acc = 0.9800 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 08:19:48.224983: step 96420, loss = 0.1641, acc = 0.9440 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 08:19:53.040119: step 96440, loss = 0.1075, acc = 0.9700 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 08:19:57.648721: step 96460, loss = 0.1089, acc = 0.9720 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 08:20:02.254058: step 96480, loss = 0.1029, acc = 0.9700 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 08:20:06.895082: step 96500, loss = 0.1189, acc = 0.9600 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 08:20:11.392164: step 96520, loss = 0.1021, acc = 0.9720 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 08:20:15.892105: step 96540, loss = 0.1234, acc = 0.9680 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 08:20:20.514116: step 96560, loss = 0.1238, acc = 0.9600 (245.7 examples/sec; 0.261 sec/batch)
2017-05-09 08:20:25.172817: step 96580, loss = 0.0924, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 08:20:29.744047: step 96600, loss = 0.1011, acc = 0.9720 (302.7 examples/sec; 0.211 sec/batch)
2017-05-09 08:20:34.748873: step 96620, loss = 0.1019, acc = 0.9640 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 08:20:39.524059: step 96640, loss = 0.0968, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 08:20:44.129013: step 96660, loss = 0.1251, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 08:20:48.777236: step 96680, loss = 0.1243, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 08:20:53.585893: step 96700, loss = 0.1299, acc = 0.9640 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 08:20:58.257138: step 96720, loss = 0.0940, acc = 0.9820 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 08:21:03.036342: step 96740, loss = 0.0966, acc = 0.9780 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 08:21:07.902628: step 96760, loss = 0.1053, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 08:21:13.270219: step 96780, loss = 0.1212, acc = 0.9520 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 08:21:17.900522: step 96800, loss = 0.0806, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 08:21:22.513528: step 96820, loss = 0.1170, acc = 0.9680 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 08:21:27.149527: step 96840, loss = 0.1111, acc = 0.9540 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 08:21:31.652157: step 96860, loss = 0.1277, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:21:36.505793: step 96880, loss = 0.1185, acc = 0.9620 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 08:21:41.152033: step 96900, loss = 0.1282, acc = 0.9660 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 08:21:45.754524: step 96920, loss = 0.1011, acc = 0.9840 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 08:21:50.401162: step 96940, loss = 0.1423, acc = 0.9500 (295.6 examples/sec; 0.217 sec/batch)
2017-05-09 08:21:54.986496: step 96960, loss = 0.1177, acc = 0.9680 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 08:21:59.471419: step 96980, loss = 0.0947, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 08:22:04.000756: step 97000, loss = 0.0974, acc = 0.9640 (275.0 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 08:22:17.899185: step 97000, acc = 0.9628, f1 = 0.9615
[Test] 2017-05-09 08:22:27.584507: step 97000, acc = 0.9545, f1 = 0.9541
[Status] 2017-05-09 08:22:27.584582: step 97000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:22:32.112847: step 97020, loss = 0.0896, acc = 0.9760 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 08:22:36.866106: step 97040, loss = 0.1037, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 08:22:41.335263: step 97060, loss = 0.0871, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 08:22:45.931154: step 97080, loss = 0.1050, acc = 0.9780 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 08:22:50.770737: step 97100, loss = 0.0926, acc = 0.9800 (219.5 examples/sec; 0.292 sec/batch)
2017-05-09 08:22:55.424755: step 97120, loss = 0.1221, acc = 0.9560 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 08:23:00.045460: step 97140, loss = 0.1244, acc = 0.9560 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 08:23:04.587092: step 97160, loss = 0.1366, acc = 0.9460 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 08:23:09.448756: step 97180, loss = 0.0862, acc = 0.9800 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 08:23:14.010460: step 97200, loss = 0.1154, acc = 0.9720 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 08:23:18.577069: step 97220, loss = 0.0947, acc = 0.9760 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 08:23:23.158900: step 97240, loss = 0.1060, acc = 0.9720 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 08:23:27.791132: step 97260, loss = 0.1432, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 08:23:32.425050: step 97280, loss = 0.1224, acc = 0.9600 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 08:23:37.221970: step 97300, loss = 0.0986, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:23:41.781118: step 97320, loss = 0.0996, acc = 0.9720 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 08:23:46.379085: step 97340, loss = 0.1100, acc = 0.9740 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 08:23:51.190194: step 97360, loss = 0.1014, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 08:23:55.819986: step 97380, loss = 0.0965, acc = 0.9760 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:24:00.441814: step 97400, loss = 0.1068, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 08:24:05.263804: step 97420, loss = 0.1158, acc = 0.9680 (235.7 examples/sec; 0.272 sec/batch)
2017-05-09 08:24:09.803968: step 97440, loss = 0.1096, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:24:14.377142: step 97460, loss = 0.0908, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 08:24:19.085351: step 97480, loss = 0.0972, acc = 0.9680 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 08:24:23.780613: step 97500, loss = 0.1096, acc = 0.9680 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 08:24:28.272632: step 97520, loss = 0.0906, acc = 0.9780 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 08:24:32.807753: step 97540, loss = 0.1125, acc = 0.9720 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 08:24:37.574858: step 97560, loss = 0.1059, acc = 0.9700 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:24:42.293840: step 97580, loss = 0.1090, acc = 0.9620 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 08:24:46.877078: step 97600, loss = 0.1104, acc = 0.9680 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 08:24:51.613800: step 97620, loss = 0.1131, acc = 0.9680 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 08:24:56.214704: step 97640, loss = 0.1034, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 08:25:00.771572: step 97660, loss = 0.0923, acc = 0.9880 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 08:25:05.489375: step 97680, loss = 0.1296, acc = 0.9540 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 08:25:10.048462: step 97700, loss = 0.1306, acc = 0.9660 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 08:25:14.835998: step 97720, loss = 0.0923, acc = 0.9760 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 08:25:19.523329: step 97740, loss = 0.0933, acc = 0.9720 (248.7 examples/sec; 0.257 sec/batch)
2017-05-09 08:25:24.018246: step 97760, loss = 0.1165, acc = 0.9580 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 08:25:29.674778: step 97780, loss = 0.0941, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 08:25:34.510005: step 97800, loss = 0.1008, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 08:25:39.116923: step 97820, loss = 0.1193, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 08:25:43.813749: step 97840, loss = 0.1083, acc = 0.9600 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 08:25:48.798815: step 97860, loss = 0.1013, acc = 0.9720 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 08:25:53.396729: step 97880, loss = 0.1139, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 08:25:57.908619: step 97900, loss = 0.1112, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 08:26:02.553088: step 97920, loss = 0.1077, acc = 0.9620 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 08:26:07.315943: step 97940, loss = 0.1168, acc = 0.9640 (243.1 examples/sec; 0.263 sec/batch)
2017-05-09 08:26:11.897089: step 97960, loss = 0.0985, acc = 0.9760 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 08:26:16.456250: step 97980, loss = 0.1068, acc = 0.9660 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 08:26:21.167641: step 98000, loss = 0.1190, acc = 0.9760 (291.9 examples/sec; 0.219 sec/batch)
[Eval] 2017-05-09 08:26:35.585283: step 98000, acc = 0.9638, f1 = 0.9627
[Test] 2017-05-09 08:26:44.874636: step 98000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 08:26:44.874729: step 98000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:26:49.624538: step 98020, loss = 0.1386, acc = 0.9560 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 08:26:54.277065: step 98040, loss = 0.1117, acc = 0.9660 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 08:26:58.820677: step 98060, loss = 0.1287, acc = 0.9600 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:27:03.702941: step 98080, loss = 0.0948, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 08:27:08.223997: step 98100, loss = 0.0811, acc = 0.9800 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 08:27:12.894076: step 98120, loss = 0.1100, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 08:27:17.522998: step 98140, loss = 0.0944, acc = 0.9820 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 08:27:22.181178: step 98160, loss = 0.1190, acc = 0.9700 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 08:27:26.858218: step 98180, loss = 0.1111, acc = 0.9640 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 08:27:31.654860: step 98200, loss = 0.1110, acc = 0.9700 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 08:27:36.255563: step 98220, loss = 0.0859, acc = 0.9780 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 08:27:40.955764: step 98240, loss = 0.0954, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 08:27:45.573997: step 98260, loss = 0.0980, acc = 0.9740 (299.3 examples/sec; 0.214 sec/batch)
2017-05-09 08:27:50.239033: step 98280, loss = 0.1224, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 08:27:54.848858: step 98300, loss = 0.0959, acc = 0.9740 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 08:27:59.517983: step 98320, loss = 0.1376, acc = 0.9560 (257.9 examples/sec; 0.248 sec/batch)
2017-05-09 08:28:04.180245: step 98340, loss = 0.1124, acc = 0.9680 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 08:28:08.861229: step 98360, loss = 0.1049, acc = 0.9700 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 08:28:13.466423: step 98380, loss = 0.1094, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 08:28:18.218557: step 98400, loss = 0.1156, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 08:28:22.879980: step 98420, loss = 0.1055, acc = 0.9660 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 08:28:27.413158: step 98440, loss = 0.1163, acc = 0.9600 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 08:28:32.154299: step 98460, loss = 0.1186, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 08:28:36.848335: step 98480, loss = 0.1017, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 08:28:41.370677: step 98500, loss = 0.1309, acc = 0.9620 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 08:28:45.995919: step 98520, loss = 0.1235, acc = 0.9500 (252.1 examples/sec; 0.254 sec/batch)
2017-05-09 08:28:50.558570: step 98540, loss = 0.1291, acc = 0.9520 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 08:28:55.111355: step 98560, loss = 0.1065, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 08:28:59.761536: step 98580, loss = 0.0859, acc = 0.9800 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 08:29:04.588997: step 98600, loss = 0.0939, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 08:29:09.311948: step 98620, loss = 0.1367, acc = 0.9700 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 08:29:13.997271: step 98640, loss = 0.1130, acc = 0.9720 (252.1 examples/sec; 0.254 sec/batch)
2017-05-09 08:29:18.852413: step 98660, loss = 0.1037, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 08:29:23.748141: step 98680, loss = 0.0863, acc = 0.9840 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 08:29:28.438910: step 98700, loss = 0.0986, acc = 0.9740 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 08:29:33.240880: step 98720, loss = 0.0997, acc = 0.9660 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 08:29:37.898762: step 98740, loss = 0.1188, acc = 0.9500 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 08:29:42.565606: step 98760, loss = 0.1057, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 08:29:47.959243: step 98780, loss = 0.1078, acc = 0.9680 (154.7 examples/sec; 0.414 sec/batch)
2017-05-09 08:29:52.636733: step 98800, loss = 0.1152, acc = 0.9640 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 08:29:57.234824: step 98820, loss = 0.1079, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 08:30:01.915306: step 98840, loss = 0.1096, acc = 0.9720 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 08:30:06.493021: step 98860, loss = 0.0970, acc = 0.9720 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 08:30:11.160406: step 98880, loss = 0.0672, acc = 0.9920 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:30:15.792254: step 98900, loss = 0.0911, acc = 0.9760 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 08:30:20.608957: step 98920, loss = 0.1018, acc = 0.9780 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 08:30:25.278039: step 98940, loss = 0.1168, acc = 0.9660 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 08:30:29.861198: step 98960, loss = 0.1084, acc = 0.9760 (299.7 examples/sec; 0.214 sec/batch)
2017-05-09 08:30:34.418140: step 98980, loss = 0.0953, acc = 0.9740 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:30:39.120366: step 99000, loss = 0.1169, acc = 0.9560 (272.6 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 08:30:53.109946: step 99000, acc = 0.9625, f1 = 0.9612
[Test] 2017-05-09 08:31:02.650892: step 99000, acc = 0.9542, f1 = 0.9538
[Status] 2017-05-09 08:31:02.650996: step 99000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:31:07.271354: step 99020, loss = 0.1308, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 08:31:11.933260: step 99040, loss = 0.1088, acc = 0.9660 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 08:31:16.757294: step 99060, loss = 0.1127, acc = 0.9680 (235.8 examples/sec; 0.271 sec/batch)
2017-05-09 08:31:21.331813: step 99080, loss = 0.0925, acc = 0.9720 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 08:31:25.986884: step 99100, loss = 0.1331, acc = 0.9500 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 08:31:30.735575: step 99120, loss = 0.0961, acc = 0.9780 (237.0 examples/sec; 0.270 sec/batch)
2017-05-09 08:31:35.918142: step 99140, loss = 0.1331, acc = 0.9720 (207.2 examples/sec; 0.309 sec/batch)
2017-05-09 08:31:40.482128: step 99160, loss = 0.0728, acc = 0.9820 (291.6 examples/sec; 0.220 sec/batch)
2017-05-09 08:31:45.132167: step 99180, loss = 0.1201, acc = 0.9520 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 08:31:49.841581: step 99200, loss = 0.1043, acc = 0.9660 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 08:31:54.396050: step 99220, loss = 0.0850, acc = 0.9840 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 08:31:58.942382: step 99240, loss = 0.1222, acc = 0.9700 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:32:03.627041: step 99260, loss = 0.1168, acc = 0.9680 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 08:32:08.225286: step 99280, loss = 0.1081, acc = 0.9720 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:32:12.867738: step 99300, loss = 0.1373, acc = 0.9480 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 08:32:17.596772: step 99320, loss = 0.0878, acc = 0.9820 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:32:22.127801: step 99340, loss = 0.0909, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 08:32:26.567202: step 99360, loss = 0.1117, acc = 0.9620 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 08:32:31.359203: step 99380, loss = 0.0932, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 08:32:35.916897: step 99400, loss = 0.1112, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 08:32:40.437837: step 99420, loss = 0.1063, acc = 0.9660 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 08:32:45.141421: step 99440, loss = 0.1349, acc = 0.9600 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 08:32:49.688654: step 99460, loss = 0.0881, acc = 0.9860 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 08:32:54.307170: step 99480, loss = 0.0856, acc = 0.9760 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 08:32:59.134644: step 99500, loss = 0.1020, acc = 0.9680 (240.3 examples/sec; 0.266 sec/batch)
2017-05-09 08:33:03.684882: step 99520, loss = 0.1005, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:33:08.348662: step 99540, loss = 0.0970, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 08:33:12.879052: step 99560, loss = 0.1010, acc = 0.9700 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 08:33:17.605648: step 99580, loss = 0.1292, acc = 0.9580 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 08:33:22.357177: step 99600, loss = 0.1351, acc = 0.9620 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 08:33:26.928474: step 99620, loss = 0.1097, acc = 0.9720 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 08:33:31.746348: step 99640, loss = 0.1222, acc = 0.9660 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 08:33:36.231571: step 99660, loss = 0.0953, acc = 0.9800 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 08:33:40.831727: step 99680, loss = 0.1227, acc = 0.9540 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 08:33:45.531764: step 99700, loss = 0.0926, acc = 0.9780 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 08:33:50.160999: step 99720, loss = 0.0971, acc = 0.9700 (252.4 examples/sec; 0.254 sec/batch)
2017-05-09 08:33:54.790747: step 99740, loss = 0.1304, acc = 0.9580 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 08:33:59.691389: step 99760, loss = 0.1016, acc = 0.9660 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 08:34:04.142120: step 99780, loss = 0.1314, acc = 0.9600 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 08:34:09.771707: step 99800, loss = 0.0985, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 08:34:14.447701: step 99820, loss = 0.1174, acc = 0.9660 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 08:34:18.965685: step 99840, loss = 0.1095, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 08:34:23.537732: step 99860, loss = 0.1192, acc = 0.9660 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 08:34:28.081841: step 99880, loss = 0.1365, acc = 0.9480 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 08:34:33.014834: step 99900, loss = 0.1266, acc = 0.9640 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 08:34:37.636844: step 99920, loss = 0.1273, acc = 0.9520 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 08:34:42.286611: step 99940, loss = 0.1090, acc = 0.9680 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 08:34:46.945736: step 99960, loss = 0.1397, acc = 0.9500 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 08:34:51.433284: step 99980, loss = 0.1224, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 08:34:55.887365: step 100000, loss = 0.1163, acc = 0.9680 (287.5 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 08:35:10.294298: step 100000, acc = 0.9638, f1 = 0.9626
[Test] 2017-05-09 08:35:19.927410: step 100000, acc = 0.9556, f1 = 0.9552
[Status] 2017-05-09 08:35:19.927493: step 100000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:35:24.571132: step 100020, loss = 0.1012, acc = 0.9740 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 08:35:29.283818: step 100040, loss = 0.1213, acc = 0.9600 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 08:35:33.733451: step 100060, loss = 0.1190, acc = 0.9580 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 08:35:38.190307: step 100080, loss = 0.0741, acc = 0.9840 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 08:35:42.978822: step 100100, loss = 0.1075, acc = 0.9720 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 08:35:47.658708: step 100120, loss = 0.1320, acc = 0.9540 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 08:35:52.235070: step 100140, loss = 0.0911, acc = 0.9800 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 08:35:57.046217: step 100160, loss = 0.1132, acc = 0.9720 (225.6 examples/sec; 0.284 sec/batch)
2017-05-09 08:36:01.602271: step 100180, loss = 0.1277, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 08:36:06.298184: step 100200, loss = 0.1099, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 08:36:10.789494: step 100220, loss = 0.1320, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 08:36:15.496834: step 100240, loss = 0.1228, acc = 0.9660 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 08:36:20.065419: step 100260, loss = 0.1006, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 08:36:24.623354: step 100280, loss = 0.1116, acc = 0.9640 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:36:29.345327: step 100300, loss = 0.1094, acc = 0.9640 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 08:36:33.938495: step 100320, loss = 0.1077, acc = 0.9620 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 08:36:38.509201: step 100340, loss = 0.0910, acc = 0.9820 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 08:36:43.403615: step 100360, loss = 0.0849, acc = 0.9840 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 08:36:47.916959: step 100380, loss = 0.1141, acc = 0.9780 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 08:36:52.487739: step 100400, loss = 0.1005, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 08:36:57.030193: step 100420, loss = 0.0784, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 08:37:01.516547: step 100440, loss = 0.1061, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 08:37:06.158991: step 100460, loss = 0.1002, acc = 0.9700 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 08:37:10.738758: step 100480, loss = 0.1393, acc = 0.9520 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 08:37:15.364467: step 100500, loss = 0.1034, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 08:37:19.999169: step 100520, loss = 0.1055, acc = 0.9740 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 08:37:24.632496: step 100540, loss = 0.0857, acc = 0.9820 (260.7 examples/sec; 0.246 sec/batch)
2017-05-09 08:37:29.174636: step 100560, loss = 0.0780, acc = 0.9880 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 08:37:33.785105: step 100580, loss = 0.1147, acc = 0.9740 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 08:37:38.577369: step 100600, loss = 0.1269, acc = 0.9620 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 08:37:43.243810: step 100620, loss = 0.1071, acc = 0.9660 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 08:37:47.813414: step 100640, loss = 0.1398, acc = 0.9580 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 08:37:52.781319: step 100660, loss = 0.1332, acc = 0.9600 (257.1 examples/sec; 0.249 sec/batch)
2017-05-09 08:37:57.353096: step 100680, loss = 0.1221, acc = 0.9600 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 08:38:02.030560: step 100700, loss = 0.1261, acc = 0.9640 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 08:38:06.699316: step 100720, loss = 0.1009, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 08:38:11.492859: step 100740, loss = 0.0899, acc = 0.9760 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 08:38:15.989976: step 100760, loss = 0.0799, acc = 0.9880 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 08:38:20.659443: step 100780, loss = 0.1005, acc = 0.9620 (257.0 examples/sec; 0.249 sec/batch)
2017-05-09 08:38:26.287596: step 100800, loss = 0.0987, acc = 0.9720 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:38:30.804874: step 100820, loss = 0.1534, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:38:35.480259: step 100840, loss = 0.1091, acc = 0.9740 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 08:38:40.310833: step 100860, loss = 0.1494, acc = 0.9520 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 08:38:44.942028: step 100880, loss = 0.0947, acc = 0.9780 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 08:38:49.659415: step 100900, loss = 0.1050, acc = 0.9720 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 08:38:54.332293: step 100920, loss = 0.1222, acc = 0.9660 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:38:59.364344: step 100940, loss = 0.0961, acc = 0.9780 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 08:39:03.909653: step 100960, loss = 0.1086, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 08:39:08.735888: step 100980, loss = 0.1083, acc = 0.9560 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:39:13.361235: step 101000, loss = 0.0891, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 08:39:27.874230: step 101000, acc = 0.9639, f1 = 0.9627
[Test] 2017-05-09 08:39:37.621058: step 101000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 08:39:37.621145: step 101000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:39:42.368035: step 101020, loss = 0.1408, acc = 0.9500 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 08:39:47.096090: step 101040, loss = 0.1088, acc = 0.9660 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 08:39:51.991694: step 101060, loss = 0.1172, acc = 0.9580 (230.2 examples/sec; 0.278 sec/batch)
2017-05-09 08:39:56.533100: step 101080, loss = 0.1151, acc = 0.9580 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 08:40:01.114487: step 101100, loss = 0.1174, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 08:40:05.565937: step 101120, loss = 0.1221, acc = 0.9600 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 08:40:10.314319: step 101140, loss = 0.1034, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 08:40:14.945275: step 101160, loss = 0.1020, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 08:40:19.668559: step 101180, loss = 0.1164, acc = 0.9660 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 08:40:24.298734: step 101200, loss = 0.0784, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 08:40:28.841327: step 101220, loss = 0.0913, acc = 0.9720 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 08:40:33.401067: step 101240, loss = 0.1070, acc = 0.9660 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 08:40:38.097379: step 101260, loss = 0.1014, acc = 0.9740 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 08:40:42.678789: step 101280, loss = 0.1134, acc = 0.9700 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 08:40:47.236489: step 101300, loss = 0.1018, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 08:40:51.957671: step 101320, loss = 0.0829, acc = 0.9740 (233.0 examples/sec; 0.275 sec/batch)
2017-05-09 08:40:56.713026: step 101340, loss = 0.0956, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:41:01.369291: step 101360, loss = 0.0810, acc = 0.9760 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 08:41:06.078596: step 101380, loss = 0.1070, acc = 0.9620 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 08:41:10.888100: step 101400, loss = 0.1301, acc = 0.9660 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 08:41:15.565188: step 101420, loss = 0.1153, acc = 0.9680 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 08:41:20.135055: step 101440, loss = 0.1068, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 08:41:24.887860: step 101460, loss = 0.1378, acc = 0.9580 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 08:41:29.588006: step 101480, loss = 0.1414, acc = 0.9600 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 08:41:34.186875: step 101500, loss = 0.1104, acc = 0.9740 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:41:39.040950: step 101520, loss = 0.1091, acc = 0.9640 (238.1 examples/sec; 0.269 sec/batch)
2017-05-09 08:41:44.012589: step 101540, loss = 0.1280, acc = 0.9600 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 08:41:48.543621: step 101560, loss = 0.0920, acc = 0.9740 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 08:41:53.147312: step 101580, loss = 0.1065, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 08:41:57.964998: step 101600, loss = 0.1005, acc = 0.9680 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 08:42:02.652294: step 101620, loss = 0.1030, acc = 0.9720 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 08:42:07.318522: step 101640, loss = 0.0903, acc = 0.9760 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 08:42:12.180387: step 101660, loss = 0.1055, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 08:42:16.725801: step 101680, loss = 0.1403, acc = 0.9520 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:42:21.255587: step 101700, loss = 0.1142, acc = 0.9680 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 08:42:26.151604: step 101720, loss = 0.1166, acc = 0.9620 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 08:42:30.736600: step 101740, loss = 0.0909, acc = 0.9720 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 08:42:35.254444: step 101760, loss = 0.1032, acc = 0.9640 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 08:42:40.009750: step 101780, loss = 0.0879, acc = 0.9840 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:42:44.661170: step 101800, loss = 0.0850, acc = 0.9760 (263.9 examples/sec; 0.242 sec/batch)
2017-05-09 08:42:50.384363: step 101820, loss = 0.1073, acc = 0.9640 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 08:42:55.290719: step 101840, loss = 0.1086, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 08:42:59.930064: step 101860, loss = 0.1361, acc = 0.9540 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 08:43:04.516452: step 101880, loss = 0.0921, acc = 0.9740 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 08:43:09.395962: step 101900, loss = 0.1155, acc = 0.9680 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 08:43:13.996746: step 101920, loss = 0.0936, acc = 0.9800 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 08:43:18.773492: step 101940, loss = 0.1102, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 08:43:23.366779: step 101960, loss = 0.0985, acc = 0.9660 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 08:43:28.082089: step 101980, loss = 0.1377, acc = 0.9620 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 08:43:32.850468: step 102000, loss = 0.1005, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 08:43:47.352676: step 102000, acc = 0.9602, f1 = 0.9588
[Test] 2017-05-09 08:43:57.312040: step 102000, acc = 0.9489, f1 = 0.9485
[Status] 2017-05-09 08:43:57.312130: step 102000, maxindex = 94000, maxdev = 0.9642, maxtst = 0.9563
2017-05-09 08:44:01.891387: step 102020, loss = 0.0768, acc = 0.9860 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:44:06.496326: step 102040, loss = 0.1042, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 08:44:11.413696: step 102060, loss = 0.1000, acc = 0.9720 (298.2 examples/sec; 0.215 sec/batch)
2017-05-09 08:44:16.019819: step 102080, loss = 0.1084, acc = 0.9680 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 08:44:20.581442: step 102100, loss = 0.1039, acc = 0.9600 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 08:44:25.407129: step 102120, loss = 0.1131, acc = 0.9540 (210.6 examples/sec; 0.304 sec/batch)
2017-05-09 08:44:30.067724: step 102140, loss = 0.1223, acc = 0.9580 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 08:44:34.643546: step 102160, loss = 0.0880, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 08:44:39.277179: step 102180, loss = 0.1083, acc = 0.9580 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 08:44:44.000966: step 102200, loss = 0.0932, acc = 0.9680 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 08:44:48.561129: step 102220, loss = 0.0943, acc = 0.9800 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 08:44:53.092568: step 102240, loss = 0.0944, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:44:57.896524: step 102260, loss = 0.1018, acc = 0.9760 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 08:45:02.496905: step 102280, loss = 0.0960, acc = 0.9740 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 08:45:07.021435: step 102300, loss = 0.1050, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 08:45:11.707122: step 102320, loss = 0.0898, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 08:45:16.315707: step 102340, loss = 0.1015, acc = 0.9680 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 08:45:21.040863: step 102360, loss = 0.1037, acc = 0.9660 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 08:45:25.866236: step 102380, loss = 0.1097, acc = 0.9680 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 08:45:30.483309: step 102400, loss = 0.1202, acc = 0.9560 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 08:45:34.965006: step 102420, loss = 0.1163, acc = 0.9620 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 08:45:39.693858: step 102440, loss = 0.0943, acc = 0.9740 (257.3 examples/sec; 0.249 sec/batch)
2017-05-09 08:45:44.352197: step 102460, loss = 0.1065, acc = 0.9740 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 08:45:48.874588: step 102480, loss = 0.0922, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 08:45:53.531949: step 102500, loss = 0.1079, acc = 0.9620 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 08:45:58.158346: step 102520, loss = 0.1041, acc = 0.9660 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 08:46:02.782617: step 102540, loss = 0.0832, acc = 0.9820 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 08:46:07.440757: step 102560, loss = 0.0899, acc = 0.9660 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 08:46:12.140008: step 102580, loss = 0.0979, acc = 0.9680 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 08:46:16.705316: step 102600, loss = 0.1145, acc = 0.9660 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 08:46:21.396705: step 102620, loss = 0.1058, acc = 0.9660 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 08:46:26.150419: step 102640, loss = 0.1036, acc = 0.9660 (214.4 examples/sec; 0.299 sec/batch)
2017-05-09 08:46:30.750979: step 102660, loss = 0.0991, acc = 0.9700 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 08:46:35.411403: step 102680, loss = 0.1059, acc = 0.9700 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 08:46:40.016140: step 102700, loss = 0.1042, acc = 0.9700 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 08:46:44.798310: step 102720, loss = 0.0929, acc = 0.9700 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 08:46:49.442045: step 102740, loss = 0.1172, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 08:46:54.084204: step 102760, loss = 0.1059, acc = 0.9720 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 08:46:58.846254: step 102780, loss = 0.1254, acc = 0.9620 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 08:47:03.420033: step 102800, loss = 0.1118, acc = 0.9680 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 08:47:08.714494: step 102820, loss = 0.1273, acc = 0.9480 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 08:47:13.411708: step 102840, loss = 0.0938, acc = 0.9800 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 08:47:18.234681: step 102860, loss = 0.1180, acc = 0.9600 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 08:47:22.889163: step 102880, loss = 0.0846, acc = 0.9800 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 08:47:27.842697: step 102900, loss = 0.0965, acc = 0.9740 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 08:47:32.487299: step 102920, loss = 0.1267, acc = 0.9680 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 08:47:37.284264: step 102940, loss = 0.1128, acc = 0.9600 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 08:47:41.888772: step 102960, loss = 0.1245, acc = 0.9600 (249.5 examples/sec; 0.257 sec/batch)
2017-05-09 08:47:46.515565: step 102980, loss = 0.0879, acc = 0.9860 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 08:47:51.140760: step 103000, loss = 0.0884, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 08:48:05.263353: step 103000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 08:48:15.135294: step 103000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 08:48:15.135399: step 103000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 08:48:23.235543: step 103020, loss = 0.1042, acc = 0.9700 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:48:27.816284: step 103040, loss = 0.1087, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 08:48:32.485098: step 103060, loss = 0.1183, acc = 0.9600 (250.4 examples/sec; 0.256 sec/batch)
2017-05-09 08:48:37.078093: step 103080, loss = 0.0910, acc = 0.9760 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 08:48:42.159928: step 103100, loss = 0.0941, acc = 0.9760 (228.3 examples/sec; 0.280 sec/batch)
2017-05-09 08:48:46.730566: step 103120, loss = 0.1200, acc = 0.9740 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 08:48:51.364446: step 103140, loss = 0.1118, acc = 0.9640 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:48:56.124196: step 103160, loss = 0.1061, acc = 0.9620 (237.5 examples/sec; 0.269 sec/batch)
2017-05-09 08:49:00.703250: step 103180, loss = 0.1019, acc = 0.9720 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 08:49:05.280875: step 103200, loss = 0.0984, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:49:09.936468: step 103220, loss = 0.1178, acc = 0.9680 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 08:49:14.575301: step 103240, loss = 0.1266, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 08:49:19.142855: step 103260, loss = 0.1114, acc = 0.9760 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 08:49:23.673655: step 103280, loss = 0.0832, acc = 0.9780 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 08:49:28.377048: step 103300, loss = 0.0996, acc = 0.9780 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 08:49:32.989919: step 103320, loss = 0.0997, acc = 0.9700 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 08:49:37.542457: step 103340, loss = 0.1084, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 08:49:42.193314: step 103360, loss = 0.1031, acc = 0.9660 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 08:49:46.669232: step 103380, loss = 0.1156, acc = 0.9700 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 08:49:51.144576: step 103400, loss = 0.0961, acc = 0.9780 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 08:49:55.787267: step 103420, loss = 0.0962, acc = 0.9720 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 08:50:00.478758: step 103440, loss = 0.1012, acc = 0.9700 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 08:50:05.098565: step 103460, loss = 0.0978, acc = 0.9820 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 08:50:09.633902: step 103480, loss = 0.1092, acc = 0.9560 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 08:50:14.486349: step 103500, loss = 0.0834, acc = 0.9820 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 08:50:19.097265: step 103520, loss = 0.0998, acc = 0.9720 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 08:50:23.687197: step 103540, loss = 0.0994, acc = 0.9760 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 08:50:28.447494: step 103560, loss = 0.0924, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 08:50:33.031884: step 103580, loss = 0.1147, acc = 0.9600 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 08:50:37.705769: step 103600, loss = 0.0937, acc = 0.9760 (255.3 examples/sec; 0.251 sec/batch)
2017-05-09 08:50:42.366322: step 103620, loss = 0.0858, acc = 0.9800 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 08:50:46.985078: step 103640, loss = 0.0916, acc = 0.9840 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:50:51.691374: step 103660, loss = 0.1124, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 08:50:56.610896: step 103680, loss = 0.1222, acc = 0.9640 (226.5 examples/sec; 0.283 sec/batch)
2017-05-09 08:51:01.207713: step 103700, loss = 0.1068, acc = 0.9660 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 08:51:05.786708: step 103720, loss = 0.1051, acc = 0.9760 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 08:51:10.411633: step 103740, loss = 0.1062, acc = 0.9640 (243.2 examples/sec; 0.263 sec/batch)
2017-05-09 08:51:15.049920: step 103760, loss = 0.0916, acc = 0.9700 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 08:51:19.672965: step 103780, loss = 0.1082, acc = 0.9680 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 08:51:24.191154: step 103800, loss = 0.0950, acc = 0.9720 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 08:51:29.989091: step 103820, loss = 0.1119, acc = 0.9700 (128.9 examples/sec; 0.496 sec/batch)
2017-05-09 08:51:34.507649: step 103840, loss = 0.1478, acc = 0.9560 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 08:51:39.098019: step 103860, loss = 0.0852, acc = 0.9740 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 08:51:43.841329: step 103880, loss = 0.0978, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 08:51:48.473208: step 103900, loss = 0.1118, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 08:51:53.188733: step 103920, loss = 0.1111, acc = 0.9660 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 08:51:57.793048: step 103940, loss = 0.1001, acc = 0.9760 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 08:52:02.426873: step 103960, loss = 0.0966, acc = 0.9640 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 08:52:07.050299: step 103980, loss = 0.0992, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 08:52:11.884088: step 104000, loss = 0.0885, acc = 0.9780 (292.6 examples/sec; 0.219 sec/batch)
[Eval] 2017-05-09 08:52:25.903600: step 104000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 08:52:35.251193: step 104000, acc = 0.9566, f1 = 0.9563
[Status] 2017-05-09 08:52:35.251282: step 104000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 08:52:40.010783: step 104020, loss = 0.1046, acc = 0.9680 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 08:52:44.584057: step 104040, loss = 0.0792, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:52:49.218029: step 104060, loss = 0.0966, acc = 0.9740 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 08:52:53.806529: step 104080, loss = 0.1027, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 08:52:58.328018: step 104100, loss = 0.1272, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 08:53:02.950047: step 104120, loss = 0.1168, acc = 0.9600 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 08:53:07.870740: step 104140, loss = 0.1169, acc = 0.9680 (206.3 examples/sec; 0.310 sec/batch)
2017-05-09 08:53:12.547442: step 104160, loss = 0.0991, acc = 0.9720 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 08:53:17.285584: step 104180, loss = 0.1113, acc = 0.9640 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 08:53:21.891217: step 104200, loss = 0.1175, acc = 0.9640 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 08:53:26.840744: step 104220, loss = 0.1075, acc = 0.9660 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 08:53:31.808709: step 104240, loss = 0.0948, acc = 0.9720 (192.5 examples/sec; 0.332 sec/batch)
2017-05-09 08:53:36.442947: step 104260, loss = 0.1064, acc = 0.9740 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 08:53:41.133963: step 104280, loss = 0.1174, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 08:53:45.682166: step 104300, loss = 0.1030, acc = 0.9740 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 08:53:50.370400: step 104320, loss = 0.1393, acc = 0.9660 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 08:53:55.071073: step 104340, loss = 0.1074, acc = 0.9620 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 08:53:59.749394: step 104360, loss = 0.1093, acc = 0.9700 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 08:54:04.300486: step 104380, loss = 0.0821, acc = 0.9820 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 08:54:09.251270: step 104400, loss = 0.0779, acc = 0.9880 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 08:54:14.049473: step 104420, loss = 0.1062, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 08:54:18.707012: step 104440, loss = 0.1141, acc = 0.9620 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 08:54:23.359137: step 104460, loss = 0.1035, acc = 0.9740 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 08:54:27.970361: step 104480, loss = 0.1074, acc = 0.9720 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 08:54:32.499363: step 104500, loss = 0.1014, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 08:54:37.045285: step 104520, loss = 0.0818, acc = 0.9860 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 08:54:41.678577: step 104540, loss = 0.0771, acc = 0.9820 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 08:54:46.274097: step 104560, loss = 0.0962, acc = 0.9720 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 08:54:50.850542: step 104580, loss = 0.0777, acc = 0.9840 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 08:54:55.622114: step 104600, loss = 0.1113, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:55:00.340291: step 104620, loss = 0.0977, acc = 0.9780 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 08:55:05.003251: step 104640, loss = 0.0928, acc = 0.9760 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 08:55:09.803616: step 104660, loss = 0.0924, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 08:55:14.472025: step 104680, loss = 0.1040, acc = 0.9680 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 08:55:19.155380: step 104700, loss = 0.0925, acc = 0.9700 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 08:55:24.184219: step 104720, loss = 0.0850, acc = 0.9820 (223.7 examples/sec; 0.286 sec/batch)
2017-05-09 08:55:28.829718: step 104740, loss = 0.1083, acc = 0.9640 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 08:55:33.364259: step 104760, loss = 0.1016, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 08:55:38.008124: step 104780, loss = 0.1286, acc = 0.9620 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 08:55:42.775253: step 104800, loss = 0.0963, acc = 0.9680 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 08:55:47.412368: step 104820, loss = 0.1024, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 08:55:52.723654: step 104840, loss = 0.0934, acc = 0.9720 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 08:55:57.642300: step 104860, loss = 0.1183, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 08:56:02.289395: step 104880, loss = 0.1156, acc = 0.9640 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 08:56:06.952540: step 104900, loss = 0.1066, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 08:56:11.659094: step 104920, loss = 0.1098, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 08:56:16.288806: step 104940, loss = 0.0824, acc = 0.9720 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 08:56:20.850211: step 104960, loss = 0.1059, acc = 0.9640 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 08:56:25.585632: step 104980, loss = 0.1118, acc = 0.9620 (229.7 examples/sec; 0.279 sec/batch)
2017-05-09 08:56:30.102335: step 105000, loss = 0.0939, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 08:56:44.165686: step 105000, acc = 0.9638, f1 = 0.9627
[Test] 2017-05-09 08:56:54.175260: step 105000, acc = 0.9552, f1 = 0.9548
[Status] 2017-05-09 08:56:54.175361: step 105000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 08:56:58.720544: step 105020, loss = 0.0949, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 08:57:03.244903: step 105040, loss = 0.1105, acc = 0.9680 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 08:57:07.993022: step 105060, loss = 0.1012, acc = 0.9740 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 08:57:12.860452: step 105080, loss = 0.1030, acc = 0.9660 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 08:57:17.602421: step 105100, loss = 0.0935, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 08:57:22.240019: step 105120, loss = 0.1074, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 08:57:26.898361: step 105140, loss = 0.1054, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 08:57:31.527876: step 105160, loss = 0.1386, acc = 0.9560 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 08:57:36.028704: step 105180, loss = 0.0989, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 08:57:40.831254: step 105200, loss = 0.1043, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:57:45.422035: step 105220, loss = 0.1088, acc = 0.9660 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 08:57:50.037713: step 105240, loss = 0.1144, acc = 0.9760 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 08:57:54.759206: step 105260, loss = 0.1027, acc = 0.9660 (245.1 examples/sec; 0.261 sec/batch)
2017-05-09 08:57:59.384781: step 105280, loss = 0.1122, acc = 0.9700 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 08:58:03.888920: step 105300, loss = 0.1144, acc = 0.9660 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 08:58:08.464409: step 105320, loss = 0.1106, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 08:58:13.314867: step 105340, loss = 0.0986, acc = 0.9620 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 08:58:17.872574: step 105360, loss = 0.0919, acc = 0.9780 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 08:58:22.444279: step 105380, loss = 0.0933, acc = 0.9680 (295.8 examples/sec; 0.216 sec/batch)
2017-05-09 08:58:27.255441: step 105400, loss = 0.0963, acc = 0.9780 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 08:58:31.902881: step 105420, loss = 0.1101, acc = 0.9520 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 08:58:36.555632: step 105440, loss = 0.1105, acc = 0.9740 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 08:58:41.358816: step 105460, loss = 0.1095, acc = 0.9660 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 08:58:45.920176: step 105480, loss = 0.1068, acc = 0.9620 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 08:58:50.676087: step 105500, loss = 0.1067, acc = 0.9660 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 08:58:55.635675: step 105520, loss = 0.1029, acc = 0.9700 (216.6 examples/sec; 0.295 sec/batch)
2017-05-09 08:59:00.310520: step 105540, loss = 0.1178, acc = 0.9680 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 08:59:04.977325: step 105560, loss = 0.0936, acc = 0.9760 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 08:59:09.722531: step 105580, loss = 0.1058, acc = 0.9720 (241.7 examples/sec; 0.265 sec/batch)
2017-05-09 08:59:14.194617: step 105600, loss = 0.1249, acc = 0.9560 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 08:59:18.830086: step 105620, loss = 0.1279, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 08:59:23.263458: step 105640, loss = 0.1091, acc = 0.9740 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 08:59:27.857867: step 105660, loss = 0.1038, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 08:59:32.429122: step 105680, loss = 0.0989, acc = 0.9780 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 08:59:36.981192: step 105700, loss = 0.0982, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 08:59:41.831007: step 105720, loss = 0.1158, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 08:59:46.349621: step 105740, loss = 0.0887, acc = 0.9780 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 08:59:50.992284: step 105760, loss = 0.0736, acc = 0.9840 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 08:59:55.727175: step 105780, loss = 0.0891, acc = 0.9740 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 09:00:00.308628: step 105800, loss = 0.1149, acc = 0.9640 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 09:00:04.973324: step 105820, loss = 0.0963, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 09:00:10.769714: step 105840, loss = 0.1135, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 09:00:15.615061: step 105860, loss = 0.0854, acc = 0.9780 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 09:00:20.193293: step 105880, loss = 0.1061, acc = 0.9760 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 09:00:24.852696: step 105900, loss = 0.0999, acc = 0.9740 (256.5 examples/sec; 0.249 sec/batch)
2017-05-09 09:00:29.460042: step 105920, loss = 0.1231, acc = 0.9560 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 09:00:34.006952: step 105940, loss = 0.1002, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:00:38.586932: step 105960, loss = 0.1010, acc = 0.9740 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 09:00:43.555155: step 105980, loss = 0.1108, acc = 0.9620 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 09:00:48.110257: step 106000, loss = 0.0992, acc = 0.9640 (290.2 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 09:01:02.239321: step 106000, acc = 0.9639, f1 = 0.9628
[Test] 2017-05-09 09:01:12.047452: step 106000, acc = 0.9563, f1 = 0.9559
[Status] 2017-05-09 09:01:12.047541: step 106000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:01:16.626239: step 106020, loss = 0.1105, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 09:01:21.247409: step 106040, loss = 0.1080, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:01:25.886740: step 106060, loss = 0.1130, acc = 0.9640 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 09:01:30.440693: step 106080, loss = 0.1102, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 09:01:35.045317: step 106100, loss = 0.1166, acc = 0.9640 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 09:01:39.831754: step 106120, loss = 0.1274, acc = 0.9620 (229.2 examples/sec; 0.279 sec/batch)
2017-05-09 09:01:44.325605: step 106140, loss = 0.0905, acc = 0.9760 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 09:01:48.888510: step 106160, loss = 0.0904, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 09:01:53.533983: step 106180, loss = 0.1055, acc = 0.9720 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 09:01:58.648359: step 106200, loss = 0.1033, acc = 0.9700 (259.0 examples/sec; 0.247 sec/batch)
2017-05-09 09:02:03.231146: step 106220, loss = 0.0858, acc = 0.9780 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 09:02:07.837538: step 106240, loss = 0.1048, acc = 0.9680 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 09:02:12.646676: step 106260, loss = 0.1047, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 09:02:17.245732: step 106280, loss = 0.0832, acc = 0.9740 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 09:02:21.791533: step 106300, loss = 0.0844, acc = 0.9800 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 09:02:26.667646: step 106320, loss = 0.1039, acc = 0.9680 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 09:02:31.208724: step 106340, loss = 0.0878, acc = 0.9740 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 09:02:35.742031: step 106360, loss = 0.1197, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 09:02:40.359450: step 106380, loss = 0.1236, acc = 0.9580 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 09:02:44.842789: step 106400, loss = 0.1032, acc = 0.9680 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 09:02:49.496512: step 106420, loss = 0.1154, acc = 0.9640 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 09:02:54.390608: step 106440, loss = 0.0856, acc = 0.9800 (236.6 examples/sec; 0.271 sec/batch)
2017-05-09 09:02:59.427571: step 106460, loss = 0.1187, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 09:03:04.071919: step 106480, loss = 0.1015, acc = 0.9640 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 09:03:08.658624: step 106500, loss = 0.0972, acc = 0.9740 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 09:03:13.363723: step 106520, loss = 0.1053, acc = 0.9660 (295.6 examples/sec; 0.216 sec/batch)
2017-05-09 09:03:17.953086: step 106540, loss = 0.1073, acc = 0.9660 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 09:03:22.491641: step 106560, loss = 0.0816, acc = 0.9700 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 09:03:27.147870: step 106580, loss = 0.0790, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 09:03:31.794579: step 106600, loss = 0.1125, acc = 0.9660 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 09:03:36.346821: step 106620, loss = 0.1083, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 09:03:40.977075: step 106640, loss = 0.0974, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 09:03:45.597794: step 106660, loss = 0.1316, acc = 0.9540 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 09:03:50.216862: step 106680, loss = 0.1082, acc = 0.9720 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 09:03:55.016192: step 106700, loss = 0.1004, acc = 0.9680 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 09:03:59.664399: step 106720, loss = 0.0923, acc = 0.9840 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 09:04:04.268388: step 106740, loss = 0.1210, acc = 0.9620 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:04:08.887192: step 106760, loss = 0.1074, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 09:04:13.687123: step 106780, loss = 0.1101, acc = 0.9660 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 09:04:18.284116: step 106800, loss = 0.1070, acc = 0.9660 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 09:04:22.888189: step 106820, loss = 0.1115, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:04:27.670719: step 106840, loss = 0.1062, acc = 0.9620 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 09:04:32.991019: step 106860, loss = 0.1150, acc = 0.9640 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 09:04:37.585626: step 106880, loss = 0.0932, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 09:04:42.406220: step 106900, loss = 0.0961, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 09:04:47.057942: step 106920, loss = 0.1002, acc = 0.9720 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:04:51.674684: step 106940, loss = 0.1086, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 09:04:56.794559: step 106960, loss = 0.0912, acc = 0.9720 (246.4 examples/sec; 0.260 sec/batch)
2017-05-09 09:05:01.251583: step 106980, loss = 0.1036, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 09:05:05.928318: step 107000, loss = 0.0902, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 09:05:19.870211: step 107000, acc = 0.9634, f1 = 0.9621
[Test] 2017-05-09 09:05:29.649089: step 107000, acc = 0.9550, f1 = 0.9547
[Status] 2017-05-09 09:05:29.649179: step 107000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:05:34.221225: step 107020, loss = 0.0951, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:05:38.780111: step 107040, loss = 0.0890, acc = 0.9820 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 09:05:43.544706: step 107060, loss = 0.0961, acc = 0.9740 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 09:05:48.086498: step 107080, loss = 0.1288, acc = 0.9600 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 09:05:52.684636: step 107100, loss = 0.1052, acc = 0.9620 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 09:05:57.312587: step 107120, loss = 0.0963, acc = 0.9680 (309.7 examples/sec; 0.207 sec/batch)
2017-05-09 09:06:01.831529: step 107140, loss = 0.1118, acc = 0.9660 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 09:06:06.508062: step 107160, loss = 0.1107, acc = 0.9720 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 09:06:11.522590: step 107180, loss = 0.0915, acc = 0.9800 (242.3 examples/sec; 0.264 sec/batch)
2017-05-09 09:06:16.100166: step 107200, loss = 0.1218, acc = 0.9600 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 09:06:20.623195: step 107220, loss = 0.0847, acc = 0.9780 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 09:06:25.257917: step 107240, loss = 0.1040, acc = 0.9600 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 09:06:30.145636: step 107260, loss = 0.0911, acc = 0.9700 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 09:06:34.695640: step 107280, loss = 0.1023, acc = 0.9720 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 09:06:39.372250: step 107300, loss = 0.0986, acc = 0.9780 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 09:06:44.002885: step 107320, loss = 0.1000, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 09:06:48.478278: step 107340, loss = 0.1200, acc = 0.9640 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 09:06:53.102630: step 107360, loss = 0.1246, acc = 0.9600 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:06:57.749237: step 107380, loss = 0.1013, acc = 0.9740 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 09:07:02.284794: step 107400, loss = 0.0888, acc = 0.9800 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:07:06.831951: step 107420, loss = 0.1322, acc = 0.9620 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 09:07:11.673472: step 107440, loss = 0.0866, acc = 0.9740 (230.2 examples/sec; 0.278 sec/batch)
2017-05-09 09:07:16.309365: step 107460, loss = 0.1183, acc = 0.9640 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 09:07:20.946325: step 107480, loss = 0.0925, acc = 0.9740 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 09:07:25.432035: step 107500, loss = 0.0898, acc = 0.9720 (295.6 examples/sec; 0.217 sec/batch)
2017-05-09 09:07:30.150312: step 107520, loss = 0.1112, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 09:07:34.851581: step 107540, loss = 0.1132, acc = 0.9740 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 09:07:39.450836: step 107560, loss = 0.1084, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 09:07:44.175919: step 107580, loss = 0.1108, acc = 0.9760 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 09:07:48.792328: step 107600, loss = 0.0999, acc = 0.9720 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 09:07:53.437686: step 107620, loss = 0.0979, acc = 0.9720 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 09:07:58.086900: step 107640, loss = 0.1029, acc = 0.9680 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 09:08:02.789824: step 107660, loss = 0.0915, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 09:08:07.416978: step 107680, loss = 0.1000, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 09:08:12.234921: step 107700, loss = 0.1134, acc = 0.9660 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 09:08:16.809413: step 107720, loss = 0.1112, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 09:08:21.325382: step 107740, loss = 0.0921, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 09:08:25.894438: step 107760, loss = 0.1125, acc = 0.9660 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 09:08:30.732022: step 107780, loss = 0.1005, acc = 0.9700 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 09:08:35.389507: step 107800, loss = 0.1084, acc = 0.9620 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 09:08:40.037656: step 107820, loss = 0.0871, acc = 0.9820 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 09:08:44.789921: step 107840, loss = 0.0881, acc = 0.9740 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 09:08:50.346465: step 107860, loss = 0.0816, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 09:08:55.078670: step 107880, loss = 0.1341, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 09:08:59.913658: step 107900, loss = 0.1196, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 09:09:04.502547: step 107920, loss = 0.1082, acc = 0.9640 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 09:09:09.139096: step 107940, loss = 0.1078, acc = 0.9720 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:09:13.844050: step 107960, loss = 0.1070, acc = 0.9720 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 09:09:18.383235: step 107980, loss = 0.0960, acc = 0.9760 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 09:09:22.987506: step 108000, loss = 0.1361, acc = 0.9480 (269.6 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 09:09:36.871928: step 108000, acc = 0.9603, f1 = 0.9589
[Test] 2017-05-09 09:09:46.865719: step 108000, acc = 0.9491, f1 = 0.9487
[Status] 2017-05-09 09:09:46.865817: step 108000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:09:51.570551: step 108020, loss = 0.0999, acc = 0.9740 (259.1 examples/sec; 0.247 sec/batch)
2017-05-09 09:09:56.227371: step 108040, loss = 0.1128, acc = 0.9660 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 09:10:00.845901: step 108060, loss = 0.1162, acc = 0.9640 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:10:05.403357: step 108080, loss = 0.1152, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 09:10:10.168770: step 108100, loss = 0.0818, acc = 0.9740 (249.4 examples/sec; 0.257 sec/batch)
2017-05-09 09:10:14.681205: step 108120, loss = 0.1006, acc = 0.9660 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:10:19.168864: step 108140, loss = 0.0831, acc = 0.9860 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 09:10:23.671380: step 108160, loss = 0.0949, acc = 0.9780 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 09:10:28.348484: step 108180, loss = 0.0760, acc = 0.9840 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 09:10:32.874097: step 108200, loss = 0.1030, acc = 0.9700 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 09:10:37.580197: step 108220, loss = 0.1018, acc = 0.9760 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 09:10:42.140017: step 108240, loss = 0.0895, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 09:10:46.627783: step 108260, loss = 0.1161, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 09:10:51.199885: step 108280, loss = 0.0874, acc = 0.9800 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 09:10:55.875558: step 108300, loss = 0.1330, acc = 0.9600 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 09:11:00.352303: step 108320, loss = 0.1080, acc = 0.9660 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 09:11:04.845847: step 108340, loss = 0.1040, acc = 0.9780 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 09:11:09.730816: step 108360, loss = 0.0930, acc = 0.9720 (226.4 examples/sec; 0.283 sec/batch)
2017-05-09 09:11:14.272939: step 108380, loss = 0.1060, acc = 0.9660 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 09:11:18.945296: step 108400, loss = 0.1277, acc = 0.9520 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 09:11:23.621036: step 108420, loss = 0.1235, acc = 0.9660 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 09:11:28.455649: step 108440, loss = 0.1093, acc = 0.9640 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 09:11:33.156931: step 108460, loss = 0.0962, acc = 0.9740 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 09:11:37.908226: step 108480, loss = 0.0932, acc = 0.9780 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 09:11:42.734495: step 108500, loss = 0.1061, acc = 0.9600 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 09:11:47.383510: step 108520, loss = 0.1337, acc = 0.9580 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 09:11:51.938643: step 108540, loss = 0.0956, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 09:11:56.613447: step 108560, loss = 0.1012, acc = 0.9760 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 09:12:01.257008: step 108580, loss = 0.1035, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 09:12:05.867582: step 108600, loss = 0.1191, acc = 0.9660 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:12:10.541615: step 108620, loss = 0.1141, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:12:15.087419: step 108640, loss = 0.1018, acc = 0.9720 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 09:12:19.725289: step 108660, loss = 0.1316, acc = 0.9520 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 09:12:24.503920: step 108680, loss = 0.1027, acc = 0.9680 (231.2 examples/sec; 0.277 sec/batch)
2017-05-09 09:12:29.108461: step 108700, loss = 0.0859, acc = 0.9720 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 09:12:33.797653: step 108720, loss = 0.0994, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 09:12:38.286563: step 108740, loss = 0.1269, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 09:12:43.280776: step 108760, loss = 0.1252, acc = 0.9620 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 09:12:47.970984: step 108780, loss = 0.1299, acc = 0.9560 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 09:12:52.614627: step 108800, loss = 0.0900, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 09:12:57.359841: step 108820, loss = 0.0830, acc = 0.9720 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 09:13:01.957345: step 108840, loss = 0.1248, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 09:13:07.193746: step 108860, loss = 0.1062, acc = 0.9700 (164.0 examples/sec; 0.390 sec/batch)
2017-05-09 09:13:11.828559: step 108880, loss = 0.0982, acc = 0.9680 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 09:13:16.368832: step 108900, loss = 0.0918, acc = 0.9780 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 09:13:21.152944: step 108920, loss = 0.1062, acc = 0.9760 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 09:13:25.951173: step 108940, loss = 0.0980, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 09:13:30.614805: step 108960, loss = 0.0895, acc = 0.9800 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 09:13:35.216823: step 108980, loss = 0.0939, acc = 0.9760 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 09:13:39.973179: step 109000, loss = 0.0924, acc = 0.9820 (279.7 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 09:13:54.065322: step 109000, acc = 0.9641, f1 = 0.9629
[Test] 2017-05-09 09:14:03.692411: step 109000, acc = 0.9562, f1 = 0.9559
[Status] 2017-05-09 09:14:03.692497: step 109000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:14:08.469374: step 109020, loss = 0.1127, acc = 0.9660 (233.7 examples/sec; 0.274 sec/batch)
2017-05-09 09:14:12.964916: step 109040, loss = 0.0998, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 09:14:17.766859: step 109060, loss = 0.1019, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 09:14:22.404455: step 109080, loss = 0.1072, acc = 0.9700 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 09:14:27.432805: step 109100, loss = 0.1009, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 09:14:32.010561: step 109120, loss = 0.1020, acc = 0.9680 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 09:14:36.642871: step 109140, loss = 0.1057, acc = 0.9720 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:14:41.404257: step 109160, loss = 0.0952, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 09:14:45.938981: step 109180, loss = 0.1003, acc = 0.9700 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 09:14:50.495887: step 109200, loss = 0.0852, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 09:14:55.183238: step 109220, loss = 0.0963, acc = 0.9760 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 09:14:59.744976: step 109240, loss = 0.1006, acc = 0.9720 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 09:15:04.370086: step 109260, loss = 0.1079, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 09:15:09.100146: step 109280, loss = 0.0813, acc = 0.9860 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 09:15:14.035668: step 109300, loss = 0.1306, acc = 0.9580 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 09:15:18.551660: step 109320, loss = 0.0999, acc = 0.9740 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 09:15:23.303209: step 109340, loss = 0.1036, acc = 0.9700 (234.3 examples/sec; 0.273 sec/batch)
2017-05-09 09:15:27.999667: step 109360, loss = 0.0835, acc = 0.9760 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 09:15:32.576379: step 109380, loss = 0.0942, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 09:15:37.189230: step 109400, loss = 0.0947, acc = 0.9780 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 09:15:41.771135: step 109420, loss = 0.1164, acc = 0.9600 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 09:15:46.329775: step 109440, loss = 0.0911, acc = 0.9680 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 09:15:50.890894: step 109460, loss = 0.1074, acc = 0.9680 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 09:15:55.639657: step 109480, loss = 0.0853, acc = 0.9800 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 09:16:00.224033: step 109500, loss = 0.0897, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 09:16:04.858634: step 109520, loss = 0.0923, acc = 0.9720 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 09:16:09.925039: step 109540, loss = 0.1104, acc = 0.9600 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 09:16:14.516285: step 109560, loss = 0.0934, acc = 0.9700 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 09:16:19.095415: step 109580, loss = 0.0972, acc = 0.9660 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 09:16:23.820559: step 109600, loss = 0.0938, acc = 0.9680 (233.1 examples/sec; 0.275 sec/batch)
2017-05-09 09:16:28.317989: step 109620, loss = 0.1055, acc = 0.9680 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 09:16:32.958441: step 109640, loss = 0.1101, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 09:16:37.672173: step 109660, loss = 0.1219, acc = 0.9700 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 09:16:42.471257: step 109680, loss = 0.1147, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 09:16:47.113093: step 109700, loss = 0.1021, acc = 0.9620 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 09:16:51.703747: step 109720, loss = 0.1168, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:16:56.416633: step 109740, loss = 0.1025, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 09:17:01.046810: step 109760, loss = 0.1032, acc = 0.9660 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 09:17:05.680138: step 109780, loss = 0.1008, acc = 0.9740 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 09:17:10.406597: step 109800, loss = 0.0955, acc = 0.9720 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 09:17:14.976608: step 109820, loss = 0.0855, acc = 0.9840 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:17:19.615346: step 109840, loss = 0.0920, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:17:24.404759: step 109860, loss = 0.0976, acc = 0.9720 (232.9 examples/sec; 0.275 sec/batch)
2017-05-09 09:17:29.999406: step 109880, loss = 0.1121, acc = 0.9700 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 09:17:34.627368: step 109900, loss = 0.1272, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:17:39.344830: step 109920, loss = 0.0936, acc = 0.9780 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 09:17:44.238209: step 109940, loss = 0.0930, acc = 0.9800 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 09:17:48.917847: step 109960, loss = 0.1124, acc = 0.9680 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 09:17:53.532641: step 109980, loss = 0.0884, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 09:17:58.360735: step 110000, loss = 0.1254, acc = 0.9560 (283.4 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 09:18:12.478894: step 110000, acc = 0.9633, f1 = 0.9621
[Test] 2017-05-09 09:18:22.073186: step 110000, acc = 0.9543, f1 = 0.9540
[Status] 2017-05-09 09:18:22.073277: step 110000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:18:26.696750: step 110020, loss = 0.0805, acc = 0.9780 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 09:18:31.280029: step 110040, loss = 0.0917, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:18:35.947908: step 110060, loss = 0.1141, acc = 0.9640 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 09:18:40.658049: step 110080, loss = 0.0830, acc = 0.9800 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 09:18:45.144860: step 110100, loss = 0.1179, acc = 0.9820 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 09:18:49.816887: step 110120, loss = 0.0816, acc = 0.9800 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 09:18:54.449737: step 110140, loss = 0.1032, acc = 0.9660 (240.7 examples/sec; 0.266 sec/batch)
2017-05-09 09:18:58.973333: step 110160, loss = 0.1013, acc = 0.9680 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 09:19:03.487599: step 110180, loss = 0.0930, acc = 0.9760 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 09:19:08.025139: step 110200, loss = 0.1380, acc = 0.9540 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 09:19:12.729492: step 110220, loss = 0.1096, acc = 0.9640 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 09:19:17.391199: step 110240, loss = 0.1240, acc = 0.9580 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 09:19:21.961881: step 110260, loss = 0.0954, acc = 0.9700 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:19:26.758392: step 110280, loss = 0.1034, acc = 0.9700 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:19:31.332054: step 110300, loss = 0.1130, acc = 0.9580 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 09:19:35.813591: step 110320, loss = 0.0956, acc = 0.9800 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 09:19:40.572774: step 110340, loss = 0.0965, acc = 0.9720 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 09:19:45.254964: step 110360, loss = 0.1138, acc = 0.9600 (251.3 examples/sec; 0.255 sec/batch)
2017-05-09 09:19:49.860241: step 110380, loss = 0.1154, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 09:19:54.542263: step 110400, loss = 0.1067, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 09:19:59.351105: step 110420, loss = 0.1215, acc = 0.9640 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 09:20:04.040520: step 110440, loss = 0.1120, acc = 0.9660 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 09:20:08.663958: step 110460, loss = 0.0909, acc = 0.9700 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 09:20:13.556707: step 110480, loss = 0.1182, acc = 0.9680 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 09:20:18.208899: step 110500, loss = 0.1104, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:20:22.744587: step 110520, loss = 0.1078, acc = 0.9640 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 09:20:27.676563: step 110540, loss = 0.1173, acc = 0.9580 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 09:20:32.613752: step 110560, loss = 0.1079, acc = 0.9720 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 09:20:37.216055: step 110580, loss = 0.1059, acc = 0.9700 (258.6 examples/sec; 0.247 sec/batch)
2017-05-09 09:20:41.991249: step 110600, loss = 0.1066, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 09:20:46.589409: step 110620, loss = 0.0953, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:20:51.343018: step 110640, loss = 0.0846, acc = 0.9800 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 09:20:56.173395: step 110660, loss = 0.1225, acc = 0.9640 (224.2 examples/sec; 0.285 sec/batch)
2017-05-09 09:21:00.754101: step 110680, loss = 0.1088, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 09:21:05.491367: step 110700, loss = 0.1283, acc = 0.9660 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 09:21:10.310529: step 110720, loss = 0.0922, acc = 0.9740 (229.8 examples/sec; 0.279 sec/batch)
2017-05-09 09:21:15.093106: step 110740, loss = 0.0945, acc = 0.9720 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 09:21:19.892258: step 110760, loss = 0.1047, acc = 0.9720 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 09:21:24.613806: step 110780, loss = 0.0970, acc = 0.9740 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 09:21:29.269211: step 110800, loss = 0.0869, acc = 0.9780 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 09:21:33.853577: step 110820, loss = 0.1125, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 09:21:38.425936: step 110840, loss = 0.0985, acc = 0.9660 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 09:21:42.978943: step 110860, loss = 0.0988, acc = 0.9680 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:21:48.189127: step 110880, loss = 0.1121, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 09:21:52.761695: step 110900, loss = 0.1124, acc = 0.9680 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 09:21:57.386022: step 110920, loss = 0.0914, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 09:22:02.058329: step 110940, loss = 0.1112, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 09:22:06.585803: step 110960, loss = 0.1070, acc = 0.9700 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 09:22:11.471163: step 110980, loss = 0.1134, acc = 0.9760 (248.8 examples/sec; 0.257 sec/batch)
2017-05-09 09:22:16.033317: step 111000, loss = 0.1257, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 09:22:29.964370: step 111000, acc = 0.9632, f1 = 0.9621
[Test] 2017-05-09 09:22:40.239796: step 111000, acc = 0.9552, f1 = 0.9548
[Status] 2017-05-09 09:22:40.239856: step 111000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:22:44.791266: step 111020, loss = 0.1202, acc = 0.9640 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 09:22:49.379554: step 111040, loss = 0.1049, acc = 0.9740 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 09:22:54.133846: step 111060, loss = 0.0982, acc = 0.9700 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 09:22:58.792435: step 111080, loss = 0.1197, acc = 0.9620 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 09:23:03.394443: step 111100, loss = 0.0988, acc = 0.9680 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 09:23:07.995495: step 111120, loss = 0.0799, acc = 0.9760 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 09:23:12.903851: step 111140, loss = 0.1073, acc = 0.9660 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 09:23:17.426998: step 111160, loss = 0.0954, acc = 0.9780 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 09:23:22.207884: step 111180, loss = 0.1080, acc = 0.9700 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 09:23:27.028811: step 111200, loss = 0.1389, acc = 0.9500 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 09:23:31.619393: step 111220, loss = 0.0850, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 09:23:36.186003: step 111240, loss = 0.1173, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 09:23:40.992794: step 111260, loss = 0.1095, acc = 0.9680 (234.0 examples/sec; 0.274 sec/batch)
2017-05-09 09:23:45.643283: step 111280, loss = 0.1192, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 09:23:50.734305: step 111300, loss = 0.0799, acc = 0.9840 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:23:55.588942: step 111320, loss = 0.0919, acc = 0.9680 (220.0 examples/sec; 0.291 sec/batch)
2017-05-09 09:24:00.054164: step 111340, loss = 0.1037, acc = 0.9760 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 09:24:04.694038: step 111360, loss = 0.0855, acc = 0.9680 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 09:24:09.341756: step 111380, loss = 0.1100, acc = 0.9600 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 09:24:14.206451: step 111400, loss = 0.1135, acc = 0.9500 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 09:24:18.883629: step 111420, loss = 0.1061, acc = 0.9700 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 09:24:23.454398: step 111440, loss = 0.1042, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:24:28.146503: step 111460, loss = 0.0798, acc = 0.9860 (301.0 examples/sec; 0.213 sec/batch)
2017-05-09 09:24:32.711757: step 111480, loss = 0.1088, acc = 0.9520 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:24:37.630799: step 111500, loss = 0.1104, acc = 0.9580 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 09:24:42.343637: step 111520, loss = 0.0977, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 09:24:46.923660: step 111540, loss = 0.1439, acc = 0.9460 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 09:24:51.457932: step 111560, loss = 0.0952, acc = 0.9780 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 09:24:56.156786: step 111580, loss = 0.0915, acc = 0.9760 (251.0 examples/sec; 0.255 sec/batch)
2017-05-09 09:25:00.783620: step 111600, loss = 0.0917, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 09:25:05.441946: step 111620, loss = 0.0975, acc = 0.9640 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 09:25:10.230567: step 111640, loss = 0.0921, acc = 0.9760 (236.9 examples/sec; 0.270 sec/batch)
2017-05-09 09:25:14.695757: step 111660, loss = 0.0923, acc = 0.9780 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 09:25:19.307369: step 111680, loss = 0.0852, acc = 0.9740 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 09:25:24.028237: step 111700, loss = 0.0884, acc = 0.9740 (253.0 examples/sec; 0.253 sec/batch)
2017-05-09 09:25:28.467430: step 111720, loss = 0.1237, acc = 0.9640 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 09:25:32.967349: step 111740, loss = 0.1213, acc = 0.9600 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:25:37.434346: step 111760, loss = 0.1122, acc = 0.9640 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 09:25:41.973393: step 111780, loss = 0.1005, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 09:25:46.464409: step 111800, loss = 0.1358, acc = 0.9580 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 09:25:50.869007: step 111820, loss = 0.1075, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 09:25:55.679510: step 111840, loss = 0.1042, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 09:26:00.294432: step 111860, loss = 0.0893, acc = 0.9740 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 09:26:04.861891: step 111880, loss = 0.1022, acc = 0.9680 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 09:26:10.680396: step 111900, loss = 0.1070, acc = 0.9700 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 09:26:15.177459: step 111920, loss = 0.1146, acc = 0.9740 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 09:26:19.764194: step 111940, loss = 0.0956, acc = 0.9720 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:26:24.460700: step 111960, loss = 0.1174, acc = 0.9660 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 09:26:29.196512: step 111980, loss = 0.0854, acc = 0.9740 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 09:26:33.824438: step 112000, loss = 0.0876, acc = 0.9820 (274.7 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 09:26:47.888992: step 112000, acc = 0.9631, f1 = 0.9620
[Test] 2017-05-09 09:26:57.655211: step 112000, acc = 0.9554, f1 = 0.9550
[Status] 2017-05-09 09:26:57.655281: step 112000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:27:02.272076: step 112020, loss = 0.0864, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 09:27:06.810118: step 112040, loss = 0.1147, acc = 0.9640 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 09:27:11.483649: step 112060, loss = 0.1037, acc = 0.9740 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:27:16.091761: step 112080, loss = 0.1037, acc = 0.9720 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 09:27:20.662825: step 112100, loss = 0.1036, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 09:27:25.566824: step 112120, loss = 0.1120, acc = 0.9640 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 09:27:30.117496: step 112140, loss = 0.1302, acc = 0.9620 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 09:27:34.620920: step 112160, loss = 0.1216, acc = 0.9640 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 09:27:39.445691: step 112180, loss = 0.1096, acc = 0.9680 (228.6 examples/sec; 0.280 sec/batch)
2017-05-09 09:27:43.999542: step 112200, loss = 0.0760, acc = 0.9900 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 09:27:48.680635: step 112220, loss = 0.0910, acc = 0.9740 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 09:27:53.204263: step 112240, loss = 0.0915, acc = 0.9760 (304.0 examples/sec; 0.211 sec/batch)
2017-05-09 09:27:57.987245: step 112260, loss = 0.1003, acc = 0.9660 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 09:28:02.633236: step 112280, loss = 0.1503, acc = 0.9540 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 09:28:07.199525: step 112300, loss = 0.0960, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 09:28:11.870357: step 112320, loss = 0.0826, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:28:16.412571: step 112340, loss = 0.1142, acc = 0.9600 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 09:28:21.003617: step 112360, loss = 0.1156, acc = 0.9640 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 09:28:25.738121: step 112380, loss = 0.1018, acc = 0.9720 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 09:28:30.470893: step 112400, loss = 0.1065, acc = 0.9600 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 09:28:35.053521: step 112420, loss = 0.0889, acc = 0.9720 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 09:28:39.808518: step 112440, loss = 0.0859, acc = 0.9820 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 09:28:44.383113: step 112460, loss = 0.1075, acc = 0.9640 (292.9 examples/sec; 0.219 sec/batch)
2017-05-09 09:28:48.963527: step 112480, loss = 0.1096, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 09:28:53.619554: step 112500, loss = 0.0843, acc = 0.9780 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 09:28:58.299762: step 112520, loss = 0.1173, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 09:29:02.906769: step 112540, loss = 0.1260, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 09:29:07.417446: step 112560, loss = 0.0793, acc = 0.9760 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 09:29:12.202781: step 112580, loss = 0.1022, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 09:29:16.764264: step 112600, loss = 0.0982, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:29:21.450431: step 112620, loss = 0.0962, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 09:29:26.115800: step 112640, loss = 0.1159, acc = 0.9620 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 09:29:30.704536: step 112660, loss = 0.1135, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 09:29:35.431555: step 112680, loss = 0.1101, acc = 0.9740 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 09:29:40.195749: step 112700, loss = 0.1063, acc = 0.9680 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 09:29:44.846160: step 112720, loss = 0.0911, acc = 0.9740 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 09:29:49.414476: step 112740, loss = 0.1021, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 09:29:54.072620: step 112760, loss = 0.1131, acc = 0.9640 (249.5 examples/sec; 0.257 sec/batch)
2017-05-09 09:29:58.721521: step 112780, loss = 0.1041, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 09:30:03.317374: step 112800, loss = 0.0976, acc = 0.9640 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 09:30:07.975413: step 112820, loss = 0.0998, acc = 0.9700 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 09:30:12.622900: step 112840, loss = 0.0828, acc = 0.9820 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 09:30:17.168283: step 112860, loss = 0.0992, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 09:30:21.725395: step 112880, loss = 0.0933, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 09:30:27.152102: step 112900, loss = 0.0842, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 09:30:31.675253: step 112920, loss = 0.1039, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:30:36.248256: step 112940, loss = 0.1105, acc = 0.9580 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 09:30:41.011144: step 112960, loss = 0.0974, acc = 0.9800 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 09:30:45.650521: step 112980, loss = 0.0960, acc = 0.9660 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 09:30:50.368299: step 113000, loss = 0.0866, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 09:31:04.651421: step 113000, acc = 0.9634, f1 = 0.9622
[Test] 2017-05-09 09:31:14.393578: step 113000, acc = 0.9558, f1 = 0.9554
[Status] 2017-05-09 09:31:14.393664: step 113000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:31:19.146118: step 113020, loss = 0.1019, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 09:31:23.787318: step 113040, loss = 0.0888, acc = 0.9820 (235.8 examples/sec; 0.271 sec/batch)
2017-05-09 09:31:28.389492: step 113060, loss = 0.1101, acc = 0.9660 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 09:31:32.964262: step 113080, loss = 0.1013, acc = 0.9660 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:31:37.498687: step 113100, loss = 0.0927, acc = 0.9780 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 09:31:42.194282: step 113120, loss = 0.0991, acc = 0.9680 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 09:31:46.803528: step 113140, loss = 0.1078, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 09:31:51.336846: step 113160, loss = 0.1291, acc = 0.9660 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 09:31:56.046246: step 113180, loss = 0.0980, acc = 0.9780 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 09:32:00.558210: step 113200, loss = 0.0959, acc = 0.9760 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 09:32:05.041020: step 113220, loss = 0.1290, acc = 0.9600 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 09:32:09.834010: step 113240, loss = 0.1118, acc = 0.9660 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 09:32:14.240011: step 113260, loss = 0.1137, acc = 0.9600 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 09:32:18.829735: step 113280, loss = 0.0905, acc = 0.9700 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:32:23.324623: step 113300, loss = 0.0837, acc = 0.9800 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 09:32:28.247134: step 113320, loss = 0.0920, acc = 0.9760 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:32:32.963340: step 113340, loss = 0.0839, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:32:37.524667: step 113360, loss = 0.1053, acc = 0.9720 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:32:42.185646: step 113380, loss = 0.1025, acc = 0.9700 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 09:32:46.876634: step 113400, loss = 0.1041, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 09:32:51.769422: step 113420, loss = 0.0954, acc = 0.9720 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 09:32:56.358528: step 113440, loss = 0.0999, acc = 0.9680 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 09:33:00.959470: step 113460, loss = 0.0871, acc = 0.9800 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 09:33:05.416781: step 113480, loss = 0.1065, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 09:33:10.145096: step 113500, loss = 0.0980, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:33:14.805130: step 113520, loss = 0.1039, acc = 0.9620 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 09:33:19.427130: step 113540, loss = 0.0874, acc = 0.9840 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 09:33:23.982431: step 113560, loss = 0.0958, acc = 0.9660 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 09:33:28.516181: step 113580, loss = 0.1031, acc = 0.9740 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 09:33:33.220375: step 113600, loss = 0.1024, acc = 0.9620 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 09:33:37.896691: step 113620, loss = 0.0883, acc = 0.9780 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 09:33:42.814814: step 113640, loss = 0.0941, acc = 0.9700 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 09:33:47.494188: step 113660, loss = 0.1114, acc = 0.9660 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 09:33:52.087900: step 113680, loss = 0.1122, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 09:33:56.819929: step 113700, loss = 0.0805, acc = 0.9820 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 09:34:01.490380: step 113720, loss = 0.1172, acc = 0.9560 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 09:34:06.039625: step 113740, loss = 0.1080, acc = 0.9740 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 09:34:10.701412: step 113760, loss = 0.0946, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 09:34:15.240037: step 113780, loss = 0.1043, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 09:34:19.849481: step 113800, loss = 0.1341, acc = 0.9560 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 09:34:24.501544: step 113820, loss = 0.0865, acc = 0.9800 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 09:34:29.285514: step 113840, loss = 0.0948, acc = 0.9720 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 09:34:33.935454: step 113860, loss = 0.0964, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 09:34:38.634166: step 113880, loss = 0.1075, acc = 0.9640 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 09:34:44.525231: step 113900, loss = 0.1359, acc = 0.9480 (130.8 examples/sec; 0.489 sec/batch)
2017-05-09 09:34:49.045290: step 113920, loss = 0.0720, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 09:34:53.957223: step 113940, loss = 0.0876, acc = 0.9720 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 09:34:58.692131: step 113960, loss = 0.1086, acc = 0.9620 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 09:35:03.279341: step 113980, loss = 0.1212, acc = 0.9700 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:35:07.868631: step 114000, loss = 0.0929, acc = 0.9760 (288.7 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 09:35:22.248926: step 114000, acc = 0.9633, f1 = 0.9621
[Test] 2017-05-09 09:35:32.098545: step 114000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 09:35:32.098651: step 114000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:35:36.595374: step 114020, loss = 0.0913, acc = 0.9780 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 09:35:41.419402: step 114040, loss = 0.0959, acc = 0.9760 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 09:35:46.146323: step 114060, loss = 0.0824, acc = 0.9780 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 09:35:50.728923: step 114080, loss = 0.0907, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:35:55.493139: step 114100, loss = 0.1087, acc = 0.9600 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 09:36:00.132003: step 114120, loss = 0.1181, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 09:36:04.735383: step 114140, loss = 0.0969, acc = 0.9760 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 09:36:09.429284: step 114160, loss = 0.1054, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 09:36:14.077911: step 114180, loss = 0.0873, acc = 0.9800 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 09:36:18.686114: step 114200, loss = 0.1046, acc = 0.9700 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:36:23.378933: step 114220, loss = 0.1286, acc = 0.9540 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 09:36:28.028939: step 114240, loss = 0.0994, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 09:36:32.739377: step 114260, loss = 0.0971, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:36:37.403308: step 114280, loss = 0.1112, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 09:36:42.212563: step 114300, loss = 0.0959, acc = 0.9780 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 09:36:46.731232: step 114320, loss = 0.1123, acc = 0.9740 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 09:36:51.350513: step 114340, loss = 0.0946, acc = 0.9660 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 09:36:56.044490: step 114360, loss = 0.1071, acc = 0.9700 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 09:37:00.748569: step 114380, loss = 0.1188, acc = 0.9600 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 09:37:05.353944: step 114400, loss = 0.1227, acc = 0.9660 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 09:37:10.226248: step 114420, loss = 0.0917, acc = 0.9740 (226.3 examples/sec; 0.283 sec/batch)
2017-05-09 09:37:14.806719: step 114440, loss = 0.1128, acc = 0.9780 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 09:37:19.368398: step 114460, loss = 0.1113, acc = 0.9620 (297.6 examples/sec; 0.215 sec/batch)
2017-05-09 09:37:24.139710: step 114480, loss = 0.1138, acc = 0.9660 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 09:37:28.815253: step 114500, loss = 0.0993, acc = 0.9680 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 09:37:33.723605: step 114520, loss = 0.0916, acc = 0.9740 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 09:37:38.322152: step 114540, loss = 0.1262, acc = 0.9600 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 09:37:43.012768: step 114560, loss = 0.0939, acc = 0.9720 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 09:37:47.606381: step 114580, loss = 0.0921, acc = 0.9780 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 09:37:52.203515: step 114600, loss = 0.1139, acc = 0.9540 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 09:37:57.026886: step 114620, loss = 0.1095, acc = 0.9720 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 09:38:01.577974: step 114640, loss = 0.0851, acc = 0.9760 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 09:38:06.168548: step 114660, loss = 0.0905, acc = 0.9800 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 09:38:10.801000: step 114680, loss = 0.0942, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 09:38:15.330943: step 114700, loss = 0.0917, acc = 0.9740 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 09:38:19.996705: step 114720, loss = 0.0878, acc = 0.9760 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 09:38:24.634328: step 114740, loss = 0.1163, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 09:38:29.212752: step 114760, loss = 0.0899, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 09:38:33.882871: step 114780, loss = 0.0936, acc = 0.9680 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 09:38:38.633198: step 114800, loss = 0.0896, acc = 0.9820 (217.6 examples/sec; 0.294 sec/batch)
2017-05-09 09:38:43.301047: step 114820, loss = 0.0820, acc = 0.9880 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 09:38:47.918733: step 114840, loss = 0.1086, acc = 0.9640 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 09:38:52.533505: step 114860, loss = 0.0759, acc = 0.9860 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 09:38:57.276797: step 114880, loss = 0.1241, acc = 0.9600 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 09:39:01.961115: step 114900, loss = 0.1423, acc = 0.9540 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 09:39:07.352194: step 114920, loss = 0.0832, acc = 0.9800 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 09:39:12.227179: step 114940, loss = 0.1220, acc = 0.9560 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 09:39:16.849589: step 114960, loss = 0.0895, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 09:39:21.473059: step 114980, loss = 0.1094, acc = 0.9740 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 09:39:26.279104: step 115000, loss = 0.0932, acc = 0.9760 (279.3 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 09:39:40.658231: step 115000, acc = 0.9634, f1 = 0.9623
[Test] 2017-05-09 09:39:50.206736: step 115000, acc = 0.9552, f1 = 0.9548
[Status] 2017-05-09 09:39:50.206838: step 115000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:39:55.010029: step 115020, loss = 0.1244, acc = 0.9580 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 09:39:59.601821: step 115040, loss = 0.0973, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:40:04.126127: step 115060, loss = 0.0992, acc = 0.9640 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 09:40:08.839382: step 115080, loss = 0.0820, acc = 0.9780 (257.0 examples/sec; 0.249 sec/batch)
2017-05-09 09:40:13.505172: step 115100, loss = 0.0980, acc = 0.9660 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 09:40:18.097340: step 115120, loss = 0.1262, acc = 0.9560 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 09:40:22.741977: step 115140, loss = 0.1098, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 09:40:27.186778: step 115160, loss = 0.0902, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 09:40:31.762269: step 115180, loss = 0.0943, acc = 0.9740 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:40:36.480076: step 115200, loss = 0.0923, acc = 0.9760 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 09:40:40.955993: step 115220, loss = 0.1425, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 09:40:45.557377: step 115240, loss = 0.0960, acc = 0.9760 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 09:40:50.411518: step 115260, loss = 0.1420, acc = 0.9540 (245.7 examples/sec; 0.261 sec/batch)
2017-05-09 09:40:54.972255: step 115280, loss = 0.1156, acc = 0.9660 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 09:40:59.581674: step 115300, loss = 0.1243, acc = 0.9600 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 09:41:04.170691: step 115320, loss = 0.1053, acc = 0.9780 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 09:41:08.845268: step 115340, loss = 0.0856, acc = 0.9780 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 09:41:13.408022: step 115360, loss = 0.0845, acc = 0.9760 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:41:18.077484: step 115380, loss = 0.1327, acc = 0.9600 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 09:41:22.792928: step 115400, loss = 0.1328, acc = 0.9560 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 09:41:27.478657: step 115420, loss = 0.1171, acc = 0.9680 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 09:41:32.089936: step 115440, loss = 0.0928, acc = 0.9720 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 09:41:36.775231: step 115460, loss = 0.1277, acc = 0.9580 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 09:41:41.357886: step 115480, loss = 0.1063, acc = 0.9780 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 09:41:45.974293: step 115500, loss = 0.0934, acc = 0.9760 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 09:41:50.551992: step 115520, loss = 0.1099, acc = 0.9660 (297.1 examples/sec; 0.215 sec/batch)
2017-05-09 09:41:55.240936: step 115540, loss = 0.1110, acc = 0.9620 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 09:41:59.896773: step 115560, loss = 0.0961, acc = 0.9720 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 09:42:04.526870: step 115580, loss = 0.0900, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:42:09.415660: step 115600, loss = 0.1118, acc = 0.9740 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 09:42:14.024438: step 115620, loss = 0.0840, acc = 0.9760 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 09:42:18.608194: step 115640, loss = 0.1059, acc = 0.9600 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:42:23.663565: step 115660, loss = 0.1137, acc = 0.9680 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 09:42:28.267936: step 115680, loss = 0.1053, acc = 0.9600 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 09:42:32.796976: step 115700, loss = 0.1237, acc = 0.9560 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 09:42:37.489717: step 115720, loss = 0.0979, acc = 0.9700 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 09:42:42.161261: step 115740, loss = 0.1694, acc = 0.9540 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 09:42:46.776091: step 115760, loss = 0.1148, acc = 0.9620 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 09:42:51.579122: step 115780, loss = 0.1130, acc = 0.9760 (230.3 examples/sec; 0.278 sec/batch)
2017-05-09 09:42:56.420001: step 115800, loss = 0.1080, acc = 0.9680 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 09:43:01.076226: step 115820, loss = 0.0982, acc = 0.9800 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 09:43:05.670090: step 115840, loss = 0.1170, acc = 0.9680 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 09:43:10.692899: step 115860, loss = 0.1248, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 09:43:15.304209: step 115880, loss = 0.0869, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 09:43:19.936470: step 115900, loss = 0.1046, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 09:43:25.728382: step 115920, loss = 0.1160, acc = 0.9600 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 09:43:30.312565: step 115940, loss = 0.0977, acc = 0.9680 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 09:43:34.943158: step 115960, loss = 0.0900, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 09:43:39.659172: step 115980, loss = 0.0816, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 09:43:44.243566: step 116000, loss = 0.1131, acc = 0.9620 (279.0 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 09:43:58.671263: step 116000, acc = 0.9642, f1 = 0.9631
[Test] 2017-05-09 09:44:08.452529: step 116000, acc = 0.9558, f1 = 0.9555
[Status] 2017-05-09 09:44:08.452655: step 116000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:44:13.120294: step 116020, loss = 0.1186, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 09:44:17.766453: step 116040, loss = 0.0894, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 09:44:22.604653: step 116060, loss = 0.1184, acc = 0.9640 (246.1 examples/sec; 0.260 sec/batch)
2017-05-09 09:44:27.328600: step 116080, loss = 0.1065, acc = 0.9620 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 09:44:31.944555: step 116100, loss = 0.0989, acc = 0.9660 (256.1 examples/sec; 0.250 sec/batch)
2017-05-09 09:44:36.388024: step 116120, loss = 0.1477, acc = 0.9480 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 09:44:41.097938: step 116140, loss = 0.1087, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 09:44:45.761746: step 116160, loss = 0.0845, acc = 0.9740 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 09:44:50.237893: step 116180, loss = 0.0859, acc = 0.9840 (301.8 examples/sec; 0.212 sec/batch)
2017-05-09 09:44:55.130032: step 116200, loss = 0.1125, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 09:44:59.659827: step 116220, loss = 0.0951, acc = 0.9620 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 09:45:04.177358: step 116240, loss = 0.0923, acc = 0.9800 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 09:45:08.810525: step 116260, loss = 0.0970, acc = 0.9760 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 09:45:13.368248: step 116280, loss = 0.0954, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 09:45:18.000621: step 116300, loss = 0.0925, acc = 0.9760 (262.0 examples/sec; 0.244 sec/batch)
2017-05-09 09:45:22.896967: step 116320, loss = 0.1004, acc = 0.9720 (219.6 examples/sec; 0.291 sec/batch)
2017-05-09 09:45:27.580152: step 116340, loss = 0.1095, acc = 0.9640 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 09:45:32.112301: step 116360, loss = 0.0837, acc = 0.9800 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 09:45:36.777640: step 116380, loss = 0.0923, acc = 0.9760 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 09:45:41.429060: step 116400, loss = 0.0891, acc = 0.9760 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 09:45:45.986556: step 116420, loss = 0.1042, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 09:45:50.548207: step 116440, loss = 0.0898, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 09:45:55.291163: step 116460, loss = 0.1038, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 09:45:59.866983: step 116480, loss = 0.1062, acc = 0.9680 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 09:46:04.591045: step 116500, loss = 0.0993, acc = 0.9660 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 09:46:09.346152: step 116520, loss = 0.1035, acc = 0.9720 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 09:46:13.917340: step 116540, loss = 0.0877, acc = 0.9800 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 09:46:18.467442: step 116560, loss = 0.0898, acc = 0.9820 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 09:46:23.163588: step 116580, loss = 0.0926, acc = 0.9760 (236.0 examples/sec; 0.271 sec/batch)
2017-05-09 09:46:27.666853: step 116600, loss = 0.1203, acc = 0.9620 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 09:46:32.231998: step 116620, loss = 0.1089, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 09:46:36.688404: step 116640, loss = 0.1042, acc = 0.9720 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 09:46:41.442287: step 116660, loss = 0.0987, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:46:46.053873: step 116680, loss = 0.0973, acc = 0.9780 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 09:46:50.540347: step 116700, loss = 0.0733, acc = 0.9840 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 09:46:55.348330: step 116720, loss = 0.1022, acc = 0.9680 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 09:46:59.950357: step 116740, loss = 0.0773, acc = 0.9840 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 09:47:04.565365: step 116760, loss = 0.1280, acc = 0.9640 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 09:47:09.347171: step 116780, loss = 0.1224, acc = 0.9680 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 09:47:13.847794: step 116800, loss = 0.0987, acc = 0.9740 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 09:47:18.385347: step 116820, loss = 0.1311, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 09:47:23.126201: step 116840, loss = 0.1027, acc = 0.9780 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 09:47:27.712055: step 116860, loss = 0.1192, acc = 0.9580 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 09:47:32.197143: step 116880, loss = 0.0937, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:47:37.146845: step 116900, loss = 0.0828, acc = 0.9780 (227.4 examples/sec; 0.281 sec/batch)
2017-05-09 09:47:41.668040: step 116920, loss = 0.0956, acc = 0.9660 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 09:47:47.142852: step 116940, loss = 0.1110, acc = 0.9700 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 09:47:51.752234: step 116960, loss = 0.0929, acc = 0.9760 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 09:47:56.316981: step 116980, loss = 0.1082, acc = 0.9620 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 09:48:00.983850: step 117000, loss = 0.0996, acc = 0.9640 (276.2 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 09:48:15.416460: step 117000, acc = 0.9637, f1 = 0.9625
[Test] 2017-05-09 09:48:25.339333: step 117000, acc = 0.9556, f1 = 0.9552
[Status] 2017-05-09 09:48:25.339419: step 117000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:48:30.107959: step 117020, loss = 0.0987, acc = 0.9740 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 09:48:34.580295: step 117040, loss = 0.1107, acc = 0.9640 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 09:48:39.263748: step 117060, loss = 0.0948, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 09:48:43.936367: step 117080, loss = 0.0832, acc = 0.9880 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 09:48:48.551742: step 117100, loss = 0.0909, acc = 0.9760 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:48:53.363404: step 117120, loss = 0.0906, acc = 0.9720 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 09:48:57.973590: step 117140, loss = 0.0841, acc = 0.9840 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 09:49:02.661673: step 117160, loss = 0.0988, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 09:49:07.523493: step 117180, loss = 0.0850, acc = 0.9800 (255.2 examples/sec; 0.251 sec/batch)
2017-05-09 09:49:12.183087: step 117200, loss = 0.1021, acc = 0.9720 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 09:49:16.983226: step 117220, loss = 0.0924, acc = 0.9760 (237.8 examples/sec; 0.269 sec/batch)
2017-05-09 09:49:21.946948: step 117240, loss = 0.1005, acc = 0.9720 (221.8 examples/sec; 0.289 sec/batch)
2017-05-09 09:49:26.579704: step 117260, loss = 0.0841, acc = 0.9860 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 09:49:31.196734: step 117280, loss = 0.0988, acc = 0.9680 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 09:49:35.762813: step 117300, loss = 0.0898, acc = 0.9780 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 09:49:40.602111: step 117320, loss = 0.1134, acc = 0.9640 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 09:49:45.280793: step 117340, loss = 0.0859, acc = 0.9800 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 09:49:50.063178: step 117360, loss = 0.1150, acc = 0.9660 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 09:49:54.624752: step 117380, loss = 0.1119, acc = 0.9780 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 09:49:59.195541: step 117400, loss = 0.1153, acc = 0.9680 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 09:50:03.813694: step 117420, loss = 0.1477, acc = 0.9460 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 09:50:08.494046: step 117440, loss = 0.1059, acc = 0.9720 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 09:50:13.269766: step 117460, loss = 0.0880, acc = 0.9760 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 09:50:17.954775: step 117480, loss = 0.1140, acc = 0.9640 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 09:50:22.648526: step 117500, loss = 0.0736, acc = 0.9800 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 09:50:27.177413: step 117520, loss = 0.0928, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 09:50:31.704302: step 117540, loss = 0.0839, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 09:50:36.641452: step 117560, loss = 0.1108, acc = 0.9660 (223.7 examples/sec; 0.286 sec/batch)
2017-05-09 09:50:41.276223: step 117580, loss = 0.1262, acc = 0.9620 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 09:50:45.847745: step 117600, loss = 0.0998, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 09:50:50.538730: step 117620, loss = 0.1251, acc = 0.9640 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 09:50:55.140802: step 117640, loss = 0.1227, acc = 0.9620 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 09:50:59.643650: step 117660, loss = 0.0847, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 09:51:04.148882: step 117680, loss = 0.1231, acc = 0.9640 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 09:51:08.783868: step 117700, loss = 0.1171, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 09:51:13.395165: step 117720, loss = 0.1015, acc = 0.9680 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 09:51:17.920566: step 117740, loss = 0.0950, acc = 0.9760 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 09:51:22.741236: step 117760, loss = 0.1133, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 09:51:27.392202: step 117780, loss = 0.0955, acc = 0.9740 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 09:51:31.997509: step 117800, loss = 0.0898, acc = 0.9720 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 09:51:36.876472: step 117820, loss = 0.1043, acc = 0.9680 (217.5 examples/sec; 0.294 sec/batch)
2017-05-09 09:51:41.453769: step 117840, loss = 0.1095, acc = 0.9620 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 09:51:46.017181: step 117860, loss = 0.1211, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 09:51:50.758494: step 117880, loss = 0.0947, acc = 0.9800 (225.6 examples/sec; 0.284 sec/batch)
2017-05-09 09:51:55.147968: step 117900, loss = 0.1143, acc = 0.9640 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 09:51:59.708544: step 117920, loss = 0.0917, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 09:52:05.279128: step 117940, loss = 0.0778, acc = 0.9820 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 09:52:10.086367: step 117960, loss = 0.1080, acc = 0.9640 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 09:52:14.566387: step 117980, loss = 0.0862, acc = 0.9800 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 09:52:19.110996: step 118000, loss = 0.0769, acc = 0.9840 (285.5 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 09:52:33.136165: step 118000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 09:52:42.603039: step 118000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 09:52:42.603125: step 118000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:52:47.147767: step 118020, loss = 0.1037, acc = 0.9720 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 09:52:51.736956: step 118040, loss = 0.0767, acc = 0.9840 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 09:52:56.364586: step 118060, loss = 0.1156, acc = 0.9680 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 09:53:00.961510: step 118080, loss = 0.1008, acc = 0.9720 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 09:53:05.691836: step 118100, loss = 0.0995, acc = 0.9740 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 09:53:10.373030: step 118120, loss = 0.1096, acc = 0.9760 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 09:53:14.934546: step 118140, loss = 0.1142, acc = 0.9760 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 09:53:19.741116: step 118160, loss = 0.0910, acc = 0.9780 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 09:53:24.344698: step 118180, loss = 0.0864, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 09:53:28.969502: step 118200, loss = 0.1037, acc = 0.9660 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 09:53:33.655106: step 118220, loss = 0.0858, acc = 0.9800 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 09:53:38.647893: step 118240, loss = 0.0944, acc = 0.9660 (245.2 examples/sec; 0.261 sec/batch)
2017-05-09 09:53:43.271267: step 118260, loss = 0.1168, acc = 0.9600 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 09:53:47.756557: step 118280, loss = 0.0796, acc = 0.9780 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 09:53:52.630322: step 118300, loss = 0.1034, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 09:53:57.163098: step 118320, loss = 0.1093, acc = 0.9760 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 09:54:01.843489: step 118340, loss = 0.0915, acc = 0.9780 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 09:54:06.732235: step 118360, loss = 0.0862, acc = 0.9780 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 09:54:11.357235: step 118380, loss = 0.0886, acc = 0.9840 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:54:15.867014: step 118400, loss = 0.1069, acc = 0.9660 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 09:54:20.647142: step 118420, loss = 0.1119, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 09:54:25.128237: step 118440, loss = 0.1078, acc = 0.9760 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 09:54:29.751651: step 118460, loss = 0.0816, acc = 0.9800 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 09:54:34.561424: step 118480, loss = 0.0915, acc = 0.9760 (241.6 examples/sec; 0.265 sec/batch)
2017-05-09 09:54:39.221341: step 118500, loss = 0.1097, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 09:54:43.781620: step 118520, loss = 0.1030, acc = 0.9600 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 09:54:48.592635: step 118540, loss = 0.1170, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 09:54:53.390955: step 118560, loss = 0.0912, acc = 0.9760 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 09:54:57.954103: step 118580, loss = 0.1010, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 09:55:02.652774: step 118600, loss = 0.0949, acc = 0.9620 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 09:55:07.679007: step 118620, loss = 0.0908, acc = 0.9760 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 09:55:12.267253: step 118640, loss = 0.0947, acc = 0.9740 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 09:55:16.843659: step 118660, loss = 0.1211, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 09:55:21.512354: step 118680, loss = 0.1186, acc = 0.9640 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 09:55:26.162358: step 118700, loss = 0.1177, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:55:30.763727: step 118720, loss = 0.0907, acc = 0.9820 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 09:55:35.585541: step 118740, loss = 0.0754, acc = 0.9840 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 09:55:40.123592: step 118760, loss = 0.1016, acc = 0.9680 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 09:55:44.936704: step 118780, loss = 0.1153, acc = 0.9660 (232.4 examples/sec; 0.275 sec/batch)
2017-05-09 09:55:49.577298: step 118800, loss = 0.0944, acc = 0.9760 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 09:55:54.236250: step 118820, loss = 0.1032, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 09:55:58.881082: step 118840, loss = 0.0924, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 09:56:03.745586: step 118860, loss = 0.0915, acc = 0.9800 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 09:56:08.287865: step 118880, loss = 0.1169, acc = 0.9540 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 09:56:12.928617: step 118900, loss = 0.0974, acc = 0.9780 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 09:56:17.674345: step 118920, loss = 0.0840, acc = 0.9780 (243.3 examples/sec; 0.263 sec/batch)
2017-05-09 09:56:23.069366: step 118940, loss = 0.0872, acc = 0.9780 (159.5 examples/sec; 0.401 sec/batch)
2017-05-09 09:56:27.583503: step 118960, loss = 0.0872, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 09:56:32.231676: step 118980, loss = 0.1209, acc = 0.9580 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 09:56:37.086161: step 119000, loss = 0.1010, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 09:56:51.179085: step 119000, acc = 0.9628, f1 = 0.9615
[Test] 2017-05-09 09:57:00.922002: step 119000, acc = 0.9537, f1 = 0.9533
[Status] 2017-05-09 09:57:00.922288: step 119000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 09:57:05.647052: step 119020, loss = 0.0844, acc = 0.9800 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 09:57:10.136368: step 119040, loss = 0.0920, acc = 0.9660 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 09:57:14.546020: step 119060, loss = 0.0949, acc = 0.9740 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 09:57:19.579463: step 119080, loss = 0.1103, acc = 0.9680 (252.5 examples/sec; 0.253 sec/batch)
2017-05-09 09:57:24.094583: step 119100, loss = 0.0983, acc = 0.9740 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 09:57:28.727525: step 119120, loss = 0.0814, acc = 0.9820 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 09:57:33.524852: step 119140, loss = 0.1208, acc = 0.9580 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 09:57:38.145269: step 119160, loss = 0.0797, acc = 0.9840 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 09:57:42.766831: step 119180, loss = 0.0897, acc = 0.9840 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 09:57:47.524937: step 119200, loss = 0.1086, acc = 0.9640 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:57:52.100906: step 119220, loss = 0.1006, acc = 0.9780 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 09:57:56.680459: step 119240, loss = 0.1382, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 09:58:01.565392: step 119260, loss = 0.0827, acc = 0.9760 (215.4 examples/sec; 0.297 sec/batch)
2017-05-09 09:58:06.201213: step 119280, loss = 0.0944, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 09:58:10.758947: step 119300, loss = 0.0944, acc = 0.9720 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 09:58:15.358652: step 119320, loss = 0.0986, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 09:58:20.009341: step 119340, loss = 0.0955, acc = 0.9700 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 09:58:24.741864: step 119360, loss = 0.1060, acc = 0.9700 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 09:58:29.261587: step 119380, loss = 0.0834, acc = 0.9780 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 09:58:33.979628: step 119400, loss = 0.0828, acc = 0.9800 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 09:58:38.371054: step 119420, loss = 0.1065, acc = 0.9780 (296.2 examples/sec; 0.216 sec/batch)
2017-05-09 09:58:42.914979: step 119440, loss = 0.1194, acc = 0.9640 (300.5 examples/sec; 0.213 sec/batch)
2017-05-09 09:58:47.703742: step 119460, loss = 0.1085, acc = 0.9660 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 09:58:52.674345: step 119480, loss = 0.1036, acc = 0.9680 (178.2 examples/sec; 0.359 sec/batch)
2017-05-09 09:58:57.289589: step 119500, loss = 0.1270, acc = 0.9640 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 09:59:01.972928: step 119520, loss = 0.0807, acc = 0.9800 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 09:59:06.606156: step 119540, loss = 0.1168, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 09:59:11.268875: step 119560, loss = 0.1372, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 09:59:16.094598: step 119580, loss = 0.0946, acc = 0.9760 (227.7 examples/sec; 0.281 sec/batch)
2017-05-09 09:59:20.544482: step 119600, loss = 0.1081, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 09:59:25.169870: step 119620, loss = 0.0966, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 09:59:29.930482: step 119640, loss = 0.1178, acc = 0.9660 (237.9 examples/sec; 0.269 sec/batch)
2017-05-09 09:59:34.306519: step 119660, loss = 0.0990, acc = 0.9780 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 09:59:38.881030: step 119680, loss = 0.0961, acc = 0.9800 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 09:59:43.393049: step 119700, loss = 0.1400, acc = 0.9540 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 09:59:48.033978: step 119720, loss = 0.1104, acc = 0.9600 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 09:59:52.633480: step 119740, loss = 0.1180, acc = 0.9700 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 09:59:57.223442: step 119760, loss = 0.0984, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 10:00:01.865998: step 119780, loss = 0.0968, acc = 0.9740 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 10:00:06.419607: step 119800, loss = 0.1141, acc = 0.9640 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 10:00:10.956573: step 119820, loss = 0.0876, acc = 0.9800 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 10:00:15.645679: step 119840, loss = 0.1162, acc = 0.9600 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 10:00:20.361102: step 119860, loss = 0.1049, acc = 0.9620 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 10:00:25.054405: step 119880, loss = 0.1009, acc = 0.9680 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 10:00:29.793667: step 119900, loss = 0.1164, acc = 0.9600 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 10:00:34.587279: step 119920, loss = 0.1082, acc = 0.9700 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 10:00:39.288536: step 119940, loss = 0.0981, acc = 0.9760 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 10:00:44.819926: step 119960, loss = 0.0910, acc = 0.9740 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 10:00:49.621462: step 119980, loss = 0.0849, acc = 0.9740 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 10:00:54.375883: step 120000, loss = 0.0821, acc = 0.9800 (262.8 examples/sec; 0.244 sec/batch)
[Eval] 2017-05-09 10:01:08.484403: step 120000, acc = 0.9642, f1 = 0.9631
[Test] 2017-05-09 10:01:18.181437: step 120000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 10:01:18.181515: step 120000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 10:01:22.730544: step 120020, loss = 0.1133, acc = 0.9640 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 10:01:27.201360: step 120040, loss = 0.1367, acc = 0.9580 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 10:01:31.878923: step 120060, loss = 0.1093, acc = 0.9640 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 10:01:36.563202: step 120080, loss = 0.1160, acc = 0.9640 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 10:01:41.203692: step 120100, loss = 0.0831, acc = 0.9800 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:01:45.926195: step 120120, loss = 0.0878, acc = 0.9760 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 10:01:50.515487: step 120140, loss = 0.0978, acc = 0.9780 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 10:01:54.979159: step 120160, loss = 0.1073, acc = 0.9640 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 10:01:59.976223: step 120180, loss = 0.0985, acc = 0.9620 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 10:02:04.587953: step 120200, loss = 0.1303, acc = 0.9540 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 10:02:09.128395: step 120220, loss = 0.1021, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 10:02:14.060213: step 120240, loss = 0.0928, acc = 0.9800 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 10:02:18.551336: step 120260, loss = 0.0711, acc = 0.9800 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 10:02:23.138608: step 120280, loss = 0.1073, acc = 0.9620 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 10:02:27.865809: step 120300, loss = 0.0952, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:02:32.513032: step 120320, loss = 0.0899, acc = 0.9800 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 10:02:37.114145: step 120340, loss = 0.1014, acc = 0.9620 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 10:02:41.747947: step 120360, loss = 0.1147, acc = 0.9700 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 10:02:46.254178: step 120380, loss = 0.0789, acc = 0.9860 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 10:02:50.707124: step 120400, loss = 0.1038, acc = 0.9720 (298.7 examples/sec; 0.214 sec/batch)
2017-05-09 10:02:55.667705: step 120420, loss = 0.1042, acc = 0.9680 (232.7 examples/sec; 0.275 sec/batch)
2017-05-09 10:03:00.273624: step 120440, loss = 0.1221, acc = 0.9580 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 10:03:04.833549: step 120460, loss = 0.1135, acc = 0.9680 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 10:03:09.487314: step 120480, loss = 0.0925, acc = 0.9760 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 10:03:13.936261: step 120500, loss = 0.1023, acc = 0.9780 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:03:18.471383: step 120520, loss = 0.0938, acc = 0.9740 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 10:03:23.124146: step 120540, loss = 0.1262, acc = 0.9620 (227.7 examples/sec; 0.281 sec/batch)
2017-05-09 10:03:27.647197: step 120560, loss = 0.1085, acc = 0.9660 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:03:32.219983: step 120580, loss = 0.0978, acc = 0.9700 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 10:03:36.825194: step 120600, loss = 0.0934, acc = 0.9780 (298.9 examples/sec; 0.214 sec/batch)
2017-05-09 10:03:41.619124: step 120620, loss = 0.0992, acc = 0.9640 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 10:03:46.205233: step 120640, loss = 0.1053, acc = 0.9720 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 10:03:50.775184: step 120660, loss = 0.1163, acc = 0.9620 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 10:03:55.543206: step 120680, loss = 0.0862, acc = 0.9760 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 10:04:00.068066: step 120700, loss = 0.0973, acc = 0.9760 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 10:04:04.641903: step 120720, loss = 0.1077, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 10:04:09.461991: step 120740, loss = 0.1494, acc = 0.9500 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 10:04:13.914091: step 120760, loss = 0.0891, acc = 0.9740 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 10:04:18.427126: step 120780, loss = 0.1071, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 10:04:23.178532: step 120800, loss = 0.0978, acc = 0.9680 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:04:27.944339: step 120820, loss = 0.1070, acc = 0.9680 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 10:04:32.465178: step 120840, loss = 0.0936, acc = 0.9780 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 10:04:37.074897: step 120860, loss = 0.0982, acc = 0.9720 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 10:04:41.713135: step 120880, loss = 0.1114, acc = 0.9700 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 10:04:46.455885: step 120900, loss = 0.1036, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 10:04:51.092727: step 120920, loss = 0.1244, acc = 0.9480 (245.8 examples/sec; 0.260 sec/batch)
2017-05-09 10:04:55.717018: step 120940, loss = 0.1055, acc = 0.9720 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 10:05:01.020274: step 120960, loss = 0.0815, acc = 0.9840 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 10:05:05.645587: step 120980, loss = 0.0921, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 10:05:10.376407: step 121000, loss = 0.0956, acc = 0.9700 (286.7 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 10:05:24.342854: step 121000, acc = 0.9643, f1 = 0.9632
[Test] 2017-05-09 10:05:33.984577: step 121000, acc = 0.9564, f1 = 0.9561
[Status] 2017-05-09 10:05:33.984652: step 121000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 10:05:38.567007: step 121020, loss = 0.0930, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 10:05:43.124592: step 121040, loss = 0.0980, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 10:05:47.642049: step 121060, loss = 0.1383, acc = 0.9520 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 10:05:52.255860: step 121080, loss = 0.1196, acc = 0.9660 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 10:05:56.858878: step 121100, loss = 0.0887, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 10:06:01.443408: step 121120, loss = 0.0954, acc = 0.9740 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 10:06:06.181116: step 121140, loss = 0.0987, acc = 0.9820 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 10:06:10.723638: step 121160, loss = 0.1099, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 10:06:15.353374: step 121180, loss = 0.1036, acc = 0.9720 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 10:06:20.007613: step 121200, loss = 0.0829, acc = 0.9740 (247.7 examples/sec; 0.258 sec/batch)
2017-05-09 10:06:24.559132: step 121220, loss = 0.0819, acc = 0.9800 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 10:06:29.122459: step 121240, loss = 0.1166, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 10:06:33.687213: step 121260, loss = 0.0839, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 10:06:38.517370: step 121280, loss = 0.0802, acc = 0.9820 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 10:06:43.093156: step 121300, loss = 0.0879, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 10:06:47.655367: step 121320, loss = 0.0904, acc = 0.9780 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 10:06:52.432992: step 121340, loss = 0.1113, acc = 0.9640 (260.9 examples/sec; 0.245 sec/batch)
2017-05-09 10:06:57.291547: step 121360, loss = 0.1111, acc = 0.9640 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:07:01.928500: step 121380, loss = 0.1118, acc = 0.9700 (255.5 examples/sec; 0.251 sec/batch)
2017-05-09 10:07:06.772603: step 121400, loss = 0.0925, acc = 0.9680 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:07:11.428827: step 121420, loss = 0.1232, acc = 0.9600 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 10:07:15.999442: step 121440, loss = 0.0943, acc = 0.9720 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 10:07:20.773687: step 121460, loss = 0.1095, acc = 0.9680 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 10:07:25.579129: step 121480, loss = 0.0901, acc = 0.9740 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 10:07:30.257584: step 121500, loss = 0.1213, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 10:07:35.146376: step 121520, loss = 0.0904, acc = 0.9720 (215.3 examples/sec; 0.297 sec/batch)
2017-05-09 10:07:39.663535: step 121540, loss = 0.1312, acc = 0.9580 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 10:07:44.309589: step 121560, loss = 0.0831, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 10:07:48.888522: step 121580, loss = 0.1136, acc = 0.9660 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 10:07:53.534539: step 121600, loss = 0.1102, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 10:07:58.184556: step 121620, loss = 0.1063, acc = 0.9660 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 10:08:02.914600: step 121640, loss = 0.1133, acc = 0.9680 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 10:08:07.713438: step 121660, loss = 0.1067, acc = 0.9700 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:08:12.243990: step 121680, loss = 0.0893, acc = 0.9820 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 10:08:16.886225: step 121700, loss = 0.0963, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 10:08:21.554276: step 121720, loss = 0.1197, acc = 0.9620 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 10:08:26.168946: step 121740, loss = 0.1233, acc = 0.9640 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 10:08:30.811084: step 121760, loss = 0.1344, acc = 0.9560 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 10:08:35.296794: step 121780, loss = 0.1044, acc = 0.9740 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 10:08:39.849063: step 121800, loss = 0.0833, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 10:08:44.510665: step 121820, loss = 0.1109, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 10:08:49.138578: step 121840, loss = 0.1067, acc = 0.9640 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 10:08:54.041879: step 121860, loss = 0.0848, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 10:08:58.584736: step 121880, loss = 0.1018, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 10:09:03.216718: step 121900, loss = 0.0817, acc = 0.9780 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 10:09:08.131100: step 121920, loss = 0.0840, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 10:09:12.674772: step 121940, loss = 0.1002, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 10:09:17.328623: step 121960, loss = 0.0998, acc = 0.9720 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:09:23.061603: step 121980, loss = 0.1146, acc = 0.9700 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 10:09:27.814439: step 122000, loss = 0.0854, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 10:09:41.804904: step 122000, acc = 0.9637, f1 = 0.9626
[Test] 2017-05-09 10:09:51.385629: step 122000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 10:09:51.385717: step 122000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 10:09:55.853807: step 122020, loss = 0.0850, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 10:10:00.343838: step 122040, loss = 0.1088, acc = 0.9700 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 10:10:05.112691: step 122060, loss = 0.1003, acc = 0.9720 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 10:10:09.632848: step 122080, loss = 0.0929, acc = 0.9680 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:10:14.321553: step 122100, loss = 0.1179, acc = 0.9560 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 10:10:18.918626: step 122120, loss = 0.1135, acc = 0.9720 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 10:10:23.817678: step 122140, loss = 0.1001, acc = 0.9680 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 10:10:28.380148: step 122160, loss = 0.1101, acc = 0.9640 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 10:10:34.019866: step 122180, loss = 0.0951, acc = 0.9740 (195.9 examples/sec; 0.327 sec/batch)
2017-05-09 10:10:38.813740: step 122200, loss = 0.0879, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:10:43.537123: step 122220, loss = 0.0864, acc = 0.9800 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 10:10:48.252872: step 122240, loss = 0.0786, acc = 0.9840 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 10:10:52.781392: step 122260, loss = 0.1043, acc = 0.9640 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 10:10:57.337877: step 122280, loss = 0.0815, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:11:02.055278: step 122300, loss = 0.0692, acc = 0.9900 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 10:11:06.737269: step 122320, loss = 0.1049, acc = 0.9660 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 10:11:11.292103: step 122340, loss = 0.1143, acc = 0.9580 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 10:11:15.998755: step 122360, loss = 0.1219, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 10:11:20.840891: step 122380, loss = 0.1085, acc = 0.9760 (232.7 examples/sec; 0.275 sec/batch)
2017-05-09 10:11:25.366717: step 122400, loss = 0.1224, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 10:11:30.062517: step 122420, loss = 0.0826, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 10:11:34.595850: step 122440, loss = 0.1293, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 10:11:39.489585: step 122460, loss = 0.0949, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 10:11:44.089298: step 122480, loss = 0.1145, acc = 0.9620 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 10:11:48.702559: step 122500, loss = 0.0983, acc = 0.9740 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 10:11:53.411199: step 122520, loss = 0.0998, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 10:11:58.207956: step 122540, loss = 0.1180, acc = 0.9680 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 10:12:02.787630: step 122560, loss = 0.0970, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:12:07.738831: step 122580, loss = 0.1057, acc = 0.9720 (200.7 examples/sec; 0.319 sec/batch)
2017-05-09 10:12:12.289790: step 122600, loss = 0.1055, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:12:16.882631: step 122620, loss = 0.1287, acc = 0.9640 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:12:21.634037: step 122640, loss = 0.1057, acc = 0.9680 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 10:12:26.299178: step 122660, loss = 0.0865, acc = 0.9780 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 10:12:30.883276: step 122680, loss = 0.1209, acc = 0.9640 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 10:12:35.566257: step 122700, loss = 0.0992, acc = 0.9680 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 10:12:40.122134: step 122720, loss = 0.1213, acc = 0.9480 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 10:12:44.669390: step 122740, loss = 0.1077, acc = 0.9680 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:12:49.549398: step 122760, loss = 0.1080, acc = 0.9720 (220.7 examples/sec; 0.290 sec/batch)
2017-05-09 10:12:54.173245: step 122780, loss = 0.1026, acc = 0.9700 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 10:12:58.632054: step 122800, loss = 0.1070, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:13:03.270311: step 122820, loss = 0.1175, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 10:13:08.176200: step 122840, loss = 0.0944, acc = 0.9700 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 10:13:12.781770: step 122860, loss = 0.0916, acc = 0.9680 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 10:13:17.397073: step 122880, loss = 0.1092, acc = 0.9680 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 10:13:22.066933: step 122900, loss = 0.1016, acc = 0.9720 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 10:13:26.576155: step 122920, loss = 0.0906, acc = 0.9740 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 10:13:31.071100: step 122940, loss = 0.1027, acc = 0.9780 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 10:13:35.917189: step 122960, loss = 0.0882, acc = 0.9780 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 10:13:41.481554: step 122980, loss = 0.1044, acc = 0.9620 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 10:13:46.076737: step 123000, loss = 0.1052, acc = 0.9700 (289.2 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 10:14:00.491565: step 123000, acc = 0.9644, f1 = 0.9633
[Test] 2017-05-09 10:14:10.236768: step 123000, acc = 0.9564, f1 = 0.9561
[Status] 2017-05-09 10:14:10.236880: step 123000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 10:14:14.794112: step 123020, loss = 0.0834, acc = 0.9780 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 10:14:19.487338: step 123040, loss = 0.1065, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 10:14:24.042501: step 123060, loss = 0.0941, acc = 0.9680 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 10:14:28.604388: step 123080, loss = 0.1183, acc = 0.9680 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 10:14:33.373308: step 123100, loss = 0.0980, acc = 0.9720 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 10:14:37.952174: step 123120, loss = 0.0972, acc = 0.9680 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 10:14:42.632006: step 123140, loss = 0.1055, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 10:14:47.399408: step 123160, loss = 0.1058, acc = 0.9700 (248.9 examples/sec; 0.257 sec/batch)
2017-05-09 10:14:52.155749: step 123180, loss = 0.1137, acc = 0.9560 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 10:14:56.854833: step 123200, loss = 0.0843, acc = 0.9760 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 10:15:01.463437: step 123220, loss = 0.0939, acc = 0.9840 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:15:06.276020: step 123240, loss = 0.0864, acc = 0.9760 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 10:15:10.944315: step 123260, loss = 0.0955, acc = 0.9720 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 10:15:15.643939: step 123280, loss = 0.1246, acc = 0.9660 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 10:15:20.507700: step 123300, loss = 0.0863, acc = 0.9740 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 10:15:25.101301: step 123320, loss = 0.1036, acc = 0.9620 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 10:15:29.616832: step 123340, loss = 0.1321, acc = 0.9600 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 10:15:34.261022: step 123360, loss = 0.0958, acc = 0.9740 (246.8 examples/sec; 0.259 sec/batch)
2017-05-09 10:15:38.750211: step 123380, loss = 0.0969, acc = 0.9740 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 10:15:43.237272: step 123400, loss = 0.0956, acc = 0.9760 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:15:48.225722: step 123420, loss = 0.0965, acc = 0.9760 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 10:15:52.867139: step 123440, loss = 0.1023, acc = 0.9760 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 10:15:57.612906: step 123460, loss = 0.0779, acc = 0.9820 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 10:16:02.475228: step 123480, loss = 0.1044, acc = 0.9780 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 10:16:07.227026: step 123500, loss = 0.1047, acc = 0.9720 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 10:16:11.965129: step 123520, loss = 0.1142, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 10:16:16.576682: step 123540, loss = 0.0967, acc = 0.9700 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 10:16:21.268827: step 123560, loss = 0.1001, acc = 0.9780 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 10:16:25.948743: step 123580, loss = 0.1037, acc = 0.9700 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 10:16:30.525495: step 123600, loss = 0.0975, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 10:16:35.431443: step 123620, loss = 0.1299, acc = 0.9660 (210.9 examples/sec; 0.303 sec/batch)
2017-05-09 10:16:40.068399: step 123640, loss = 0.1108, acc = 0.9620 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 10:16:44.699134: step 123660, loss = 0.1163, acc = 0.9720 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 10:16:49.223276: step 123680, loss = 0.0871, acc = 0.9740 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 10:16:53.916032: step 123700, loss = 0.1189, acc = 0.9620 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 10:16:58.568671: step 123720, loss = 0.0979, acc = 0.9680 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 10:17:03.130175: step 123740, loss = 0.1243, acc = 0.9620 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 10:17:07.908711: step 123760, loss = 0.0934, acc = 0.9760 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 10:17:12.569940: step 123780, loss = 0.0964, acc = 0.9680 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 10:17:17.100665: step 123800, loss = 0.1057, acc = 0.9760 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 10:17:21.954455: step 123820, loss = 0.1268, acc = 0.9560 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 10:17:26.549707: step 123840, loss = 0.0782, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 10:17:31.080422: step 123860, loss = 0.1169, acc = 0.9620 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 10:17:35.823617: step 123880, loss = 0.1002, acc = 0.9640 (250.3 examples/sec; 0.256 sec/batch)
2017-05-09 10:17:40.358586: step 123900, loss = 0.0806, acc = 0.9840 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 10:17:44.917837: step 123920, loss = 0.1035, acc = 0.9560 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 10:17:49.488431: step 123940, loss = 0.1344, acc = 0.9640 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 10:17:54.206997: step 123960, loss = 0.1227, acc = 0.9620 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 10:17:59.881213: step 123980, loss = 0.0860, acc = 0.9800 (134.4 examples/sec; 0.476 sec/batch)
2017-05-09 10:18:04.591607: step 124000, loss = 0.0949, acc = 0.9620 (276.3 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 10:18:19.008972: step 124000, acc = 0.9634, f1 = 0.9621
[Test] 2017-05-09 10:18:28.733166: step 124000, acc = 0.9546, f1 = 0.9542
[Status] 2017-05-09 10:18:28.733254: step 124000, maxindex = 103000, maxdev = 0.9645, maxtst = 0.9561
2017-05-09 10:18:33.415604: step 124020, loss = 0.1076, acc = 0.9600 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 10:18:38.049184: step 124040, loss = 0.0898, acc = 0.9740 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 10:18:42.555347: step 124060, loss = 0.0931, acc = 0.9800 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 10:18:47.165605: step 124080, loss = 0.1379, acc = 0.9700 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 10:18:51.847159: step 124100, loss = 0.1140, acc = 0.9680 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 10:18:56.537778: step 124120, loss = 0.1002, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 10:19:01.133481: step 124140, loss = 0.1001, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 10:19:05.963857: step 124160, loss = 0.0890, acc = 0.9680 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 10:19:10.626211: step 124180, loss = 0.0971, acc = 0.9700 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 10:19:15.231810: step 124200, loss = 0.0934, acc = 0.9780 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 10:19:19.816940: step 124220, loss = 0.1089, acc = 0.9700 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 10:19:24.489800: step 124240, loss = 0.0846, acc = 0.9760 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:19:29.059314: step 124260, loss = 0.0860, acc = 0.9820 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 10:19:33.627532: step 124280, loss = 0.0710, acc = 0.9820 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 10:19:38.621248: step 124300, loss = 0.0942, acc = 0.9740 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 10:19:43.164559: step 124320, loss = 0.1039, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:19:47.790922: step 124340, loss = 0.0902, acc = 0.9740 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 10:19:52.509449: step 124360, loss = 0.0794, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:19:57.091877: step 124380, loss = 0.1076, acc = 0.9720 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:20:01.673846: step 124400, loss = 0.1033, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:20:06.374149: step 124420, loss = 0.0969, acc = 0.9760 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:20:11.335309: step 124440, loss = 0.0998, acc = 0.9780 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 10:20:15.945274: step 124460, loss = 0.0762, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 10:20:20.625439: step 124480, loss = 0.1324, acc = 0.9580 (245.2 examples/sec; 0.261 sec/batch)
2017-05-09 10:20:25.192346: step 124500, loss = 0.1020, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 10:20:29.841581: step 124520, loss = 0.0974, acc = 0.9740 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 10:20:34.368118: step 124540, loss = 0.0856, acc = 0.9740 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 10:20:39.065518: step 124560, loss = 0.1234, acc = 0.9560 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:20:43.640828: step 124580, loss = 0.0964, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 10:20:48.270500: step 124600, loss = 0.0951, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 10:20:52.989065: step 124620, loss = 0.1039, acc = 0.9660 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 10:20:57.517356: step 124640, loss = 0.1057, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 10:21:02.235906: step 124660, loss = 0.0820, acc = 0.9820 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 10:21:06.999230: step 124680, loss = 0.1187, acc = 0.9680 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 10:21:11.490214: step 124700, loss = 0.0800, acc = 0.9820 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 10:21:16.111490: step 124720, loss = 0.0934, acc = 0.9720 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 10:21:21.086310: step 124740, loss = 0.1095, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 10:21:25.755534: step 124760, loss = 0.1078, acc = 0.9680 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 10:21:30.361014: step 124780, loss = 0.0881, acc = 0.9780 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 10:21:35.118633: step 124800, loss = 0.0890, acc = 0.9700 (243.0 examples/sec; 0.263 sec/batch)
2017-05-09 10:21:39.797462: step 124820, loss = 0.0843, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 10:21:44.444321: step 124840, loss = 0.0972, acc = 0.9680 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 10:21:48.992911: step 124860, loss = 0.0917, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:21:53.742819: step 124880, loss = 0.0762, acc = 0.9840 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 10:21:58.225530: step 124900, loss = 0.1103, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 10:22:02.800809: step 124920, loss = 0.1161, acc = 0.9680 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 10:22:07.513281: step 124940, loss = 0.0895, acc = 0.9820 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 10:22:12.226630: step 124960, loss = 0.0941, acc = 0.9780 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 10:22:16.849877: step 124980, loss = 0.0966, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 10:22:22.247722: step 125000, loss = 0.1040, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 10:22:36.809051: step 125000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 10:22:46.391910: step 125000, acc = 0.9562, f1 = 0.9559
[Status] 2017-05-09 10:22:46.391999: step 125000, maxindex = 125000, maxdev = 0.9646, maxtst = 0.9562
2017-05-09 10:22:54.124635: step 125020, loss = 0.1031, acc = 0.9700 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:22:58.669074: step 125040, loss = 0.0915, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 10:23:03.242157: step 125060, loss = 0.0997, acc = 0.9700 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 10:23:07.998623: step 125080, loss = 0.0988, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:23:12.611006: step 125100, loss = 0.1066, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 10:23:17.180417: step 125120, loss = 0.0950, acc = 0.9760 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 10:23:22.027608: step 125140, loss = 0.1034, acc = 0.9720 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:23:26.597365: step 125160, loss = 0.0988, acc = 0.9760 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 10:23:31.183962: step 125180, loss = 0.1078, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 10:23:35.816308: step 125200, loss = 0.0996, acc = 0.9720 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 10:23:40.335189: step 125220, loss = 0.1007, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 10:23:44.914550: step 125240, loss = 0.1150, acc = 0.9700 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 10:23:49.679100: step 125260, loss = 0.1013, acc = 0.9700 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 10:23:54.293055: step 125280, loss = 0.0869, acc = 0.9800 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 10:23:58.926802: step 125300, loss = 0.0749, acc = 0.9800 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 10:24:03.572966: step 125320, loss = 0.1067, acc = 0.9660 (299.8 examples/sec; 0.213 sec/batch)
2017-05-09 10:24:08.121251: step 125340, loss = 0.0860, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 10:24:12.754366: step 125360, loss = 0.0934, acc = 0.9780 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 10:24:17.329420: step 125380, loss = 0.1160, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 10:24:22.009986: step 125400, loss = 0.1121, acc = 0.9620 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 10:24:26.623239: step 125420, loss = 0.0928, acc = 0.9760 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 10:24:31.231851: step 125440, loss = 0.1330, acc = 0.9580 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 10:24:35.970864: step 125460, loss = 0.0993, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:24:40.616660: step 125480, loss = 0.1140, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 10:24:45.354427: step 125500, loss = 0.0878, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 10:24:50.126225: step 125520, loss = 0.0828, acc = 0.9800 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 10:24:54.709069: step 125540, loss = 0.1036, acc = 0.9720 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 10:24:59.242247: step 125560, loss = 0.0897, acc = 0.9760 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 10:25:03.929468: step 125580, loss = 0.1091, acc = 0.9680 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 10:25:08.477798: step 125600, loss = 0.1000, acc = 0.9740 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 10:25:13.156775: step 125620, loss = 0.1002, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 10:25:17.957142: step 125640, loss = 0.1229, acc = 0.9660 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 10:25:22.552695: step 125660, loss = 0.1163, acc = 0.9620 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:25:27.075431: step 125680, loss = 0.0859, acc = 0.9820 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 10:25:31.813212: step 125700, loss = 0.0908, acc = 0.9820 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 10:25:36.499725: step 125720, loss = 0.1061, acc = 0.9740 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 10:25:41.130265: step 125740, loss = 0.1057, acc = 0.9660 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 10:25:45.764374: step 125760, loss = 0.0936, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 10:25:50.641805: step 125780, loss = 0.1355, acc = 0.9500 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 10:25:55.115768: step 125800, loss = 0.1164, acc = 0.9600 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 10:25:59.580212: step 125820, loss = 0.1578, acc = 0.9500 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 10:26:04.406637: step 125840, loss = 0.1114, acc = 0.9620 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:26:09.053808: step 125860, loss = 0.0967, acc = 0.9760 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 10:26:13.555383: step 125880, loss = 0.1054, acc = 0.9780 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:26:18.501290: step 125900, loss = 0.0959, acc = 0.9640 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 10:26:23.091015: step 125920, loss = 0.1196, acc = 0.9660 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 10:26:27.661811: step 125940, loss = 0.0958, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 10:26:32.343750: step 125960, loss = 0.1354, acc = 0.9640 (247.1 examples/sec; 0.259 sec/batch)
2017-05-09 10:26:36.840094: step 125980, loss = 0.0830, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 10:26:42.467450: step 126000, loss = 0.0925, acc = 0.9740 (280.1 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 10:26:56.507241: step 126000, acc = 0.9641, f1 = 0.9629
[Test] 2017-05-09 10:27:06.440573: step 126000, acc = 0.9565, f1 = 0.9561
[Status] 2017-05-09 10:27:06.440658: step 126000, maxindex = 125000, maxdev = 0.9646, maxtst = 0.9562
2017-05-09 10:27:11.182155: step 126020, loss = 0.0981, acc = 0.9740 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 10:27:15.737424: step 126040, loss = 0.0921, acc = 0.9740 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:27:20.568188: step 126060, loss = 0.0967, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 10:27:25.059459: step 126080, loss = 0.0766, acc = 0.9860 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:27:29.754125: step 126100, loss = 0.0907, acc = 0.9780 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 10:27:34.423434: step 126120, loss = 0.0714, acc = 0.9840 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 10:27:39.035198: step 126140, loss = 0.0968, acc = 0.9780 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 10:27:43.621651: step 126160, loss = 0.1171, acc = 0.9600 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 10:27:48.284993: step 126180, loss = 0.0988, acc = 0.9700 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 10:27:52.836197: step 126200, loss = 0.1232, acc = 0.9540 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 10:27:57.404652: step 126220, loss = 0.0892, acc = 0.9820 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 10:28:02.269996: step 126240, loss = 0.1206, acc = 0.9680 (232.0 examples/sec; 0.276 sec/batch)
2017-05-09 10:28:06.834007: step 126260, loss = 0.1087, acc = 0.9640 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 10:28:11.511749: step 126280, loss = 0.1121, acc = 0.9720 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 10:28:16.202024: step 126300, loss = 0.0932, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 10:28:20.916250: step 126320, loss = 0.1009, acc = 0.9760 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 10:28:25.540172: step 126340, loss = 0.1043, acc = 0.9680 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 10:28:30.158009: step 126360, loss = 0.0911, acc = 0.9720 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 10:28:34.979081: step 126380, loss = 0.1012, acc = 0.9640 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 10:28:39.780115: step 126400, loss = 0.0851, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 10:28:44.499146: step 126420, loss = 0.0987, acc = 0.9640 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 10:28:49.141653: step 126440, loss = 0.0828, acc = 0.9780 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 10:28:53.729759: step 126460, loss = 0.1003, acc = 0.9600 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 10:28:58.459856: step 126480, loss = 0.0995, acc = 0.9780 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 10:29:03.286565: step 126500, loss = 0.1131, acc = 0.9660 (228.2 examples/sec; 0.280 sec/batch)
2017-05-09 10:29:07.831868: step 126520, loss = 0.0930, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:29:12.450738: step 126540, loss = 0.1420, acc = 0.9500 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 10:29:16.981754: step 126560, loss = 0.0842, acc = 0.9720 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 10:29:21.614495: step 126580, loss = 0.0910, acc = 0.9740 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 10:29:26.266494: step 126600, loss = 0.0988, acc = 0.9720 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 10:29:30.903157: step 126620, loss = 0.0878, acc = 0.9740 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 10:29:35.573972: step 126640, loss = 0.0813, acc = 0.9800 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 10:29:40.431109: step 126660, loss = 0.0945, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 10:29:45.148634: step 126680, loss = 0.1008, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 10:29:50.025883: step 126700, loss = 0.1009, acc = 0.9700 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 10:29:54.537105: step 126720, loss = 0.0835, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 10:29:59.061006: step 126740, loss = 0.1096, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 10:30:03.759741: step 126760, loss = 0.0835, acc = 0.9780 (299.5 examples/sec; 0.214 sec/batch)
2017-05-09 10:30:08.684766: step 126780, loss = 0.1036, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 10:30:13.244410: step 126800, loss = 0.0927, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 10:30:18.071345: step 126820, loss = 0.1035, acc = 0.9620 (229.4 examples/sec; 0.279 sec/batch)
2017-05-09 10:30:22.602586: step 126840, loss = 0.0943, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 10:30:27.221112: step 126860, loss = 0.0965, acc = 0.9660 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 10:30:31.880187: step 126880, loss = 0.0978, acc = 0.9740 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 10:30:36.639916: step 126900, loss = 0.0982, acc = 0.9740 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 10:30:41.202576: step 126920, loss = 0.0743, acc = 0.9860 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 10:30:45.739884: step 126940, loss = 0.0817, acc = 0.9800 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 10:30:50.454430: step 126960, loss = 0.1217, acc = 0.9660 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 10:30:55.260585: step 126980, loss = 0.0992, acc = 0.9660 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 10:31:00.113070: step 127000, loss = 0.0869, acc = 0.9780 (257.9 examples/sec; 0.248 sec/batch)
[Eval] 2017-05-09 10:31:14.160277: step 127000, acc = 0.9619, f1 = 0.9607
[Test] 2017-05-09 10:31:24.013189: step 127000, acc = 0.9536, f1 = 0.9532
[Status] 2017-05-09 10:31:24.013290: step 127000, maxindex = 125000, maxdev = 0.9646, maxtst = 0.9562
2017-05-09 10:31:29.212663: step 127020, loss = 0.0898, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:31:34.026304: step 127040, loss = 0.1149, acc = 0.9620 (258.6 examples/sec; 0.247 sec/batch)
2017-05-09 10:31:38.643827: step 127060, loss = 0.0771, acc = 0.9840 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 10:31:43.550030: step 127080, loss = 0.0969, acc = 0.9760 (210.9 examples/sec; 0.304 sec/batch)
2017-05-09 10:31:48.256329: step 127100, loss = 0.0773, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 10:31:52.781631: step 127120, loss = 0.1140, acc = 0.9640 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:31:57.458448: step 127140, loss = 0.1096, acc = 0.9720 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 10:32:02.196484: step 127160, loss = 0.0938, acc = 0.9780 (237.2 examples/sec; 0.270 sec/batch)
2017-05-09 10:32:06.843469: step 127180, loss = 0.0821, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 10:32:11.434963: step 127200, loss = 0.1007, acc = 0.9680 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 10:32:16.167341: step 127220, loss = 0.1107, acc = 0.9700 (233.2 examples/sec; 0.274 sec/batch)
2017-05-09 10:32:20.683459: step 127240, loss = 0.0897, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 10:32:25.250334: step 127260, loss = 0.1172, acc = 0.9600 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 10:32:29.821767: step 127280, loss = 0.1251, acc = 0.9660 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 10:32:34.616904: step 127300, loss = 0.0956, acc = 0.9740 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 10:32:39.305019: step 127320, loss = 0.0932, acc = 0.9700 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 10:32:43.925339: step 127340, loss = 0.1025, acc = 0.9720 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 10:32:48.763751: step 127360, loss = 0.1322, acc = 0.9700 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 10:32:53.393730: step 127380, loss = 0.1171, acc = 0.9740 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 10:32:58.112913: step 127400, loss = 0.0978, acc = 0.9680 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 10:33:02.885879: step 127420, loss = 0.1022, acc = 0.9680 (226.6 examples/sec; 0.282 sec/batch)
2017-05-09 10:33:07.394267: step 127440, loss = 0.0966, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 10:33:11.980797: step 127460, loss = 0.0797, acc = 0.9840 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:33:16.690255: step 127480, loss = 0.1076, acc = 0.9720 (246.9 examples/sec; 0.259 sec/batch)
2017-05-09 10:33:21.504040: step 127500, loss = 0.1048, acc = 0.9780 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 10:33:26.083908: step 127520, loss = 0.0975, acc = 0.9660 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 10:33:30.704743: step 127540, loss = 0.0843, acc = 0.9720 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:33:35.397452: step 127560, loss = 0.0782, acc = 0.9860 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:33:39.993757: step 127580, loss = 0.1070, acc = 0.9660 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 10:33:44.538724: step 127600, loss = 0.0957, acc = 0.9740 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 10:33:49.323280: step 127620, loss = 0.0997, acc = 0.9740 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 10:33:53.897028: step 127640, loss = 0.0868, acc = 0.9820 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 10:33:58.425785: step 127660, loss = 0.1020, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 10:34:03.332290: step 127680, loss = 0.1126, acc = 0.9680 (219.7 examples/sec; 0.291 sec/batch)
2017-05-09 10:34:08.129062: step 127700, loss = 0.0833, acc = 0.9820 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 10:34:12.766121: step 127720, loss = 0.1170, acc = 0.9580 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 10:34:17.439916: step 127740, loss = 0.0893, acc = 0.9800 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:34:22.123459: step 127760, loss = 0.0954, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 10:34:26.701122: step 127780, loss = 0.0884, acc = 0.9780 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 10:34:31.269421: step 127800, loss = 0.0872, acc = 0.9820 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 10:34:35.958165: step 127820, loss = 0.1303, acc = 0.9580 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 10:34:40.651221: step 127840, loss = 0.1065, acc = 0.9680 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 10:34:45.262523: step 127860, loss = 0.0903, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 10:34:50.060679: step 127880, loss = 0.0860, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 10:34:54.440806: step 127900, loss = 0.0957, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:34:58.992240: step 127920, loss = 0.1064, acc = 0.9600 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 10:35:03.662540: step 127940, loss = 0.0747, acc = 0.9840 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 10:35:08.391010: step 127960, loss = 0.0973, acc = 0.9700 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 10:35:12.963421: step 127980, loss = 0.0970, acc = 0.9760 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 10:35:17.494954: step 128000, loss = 0.1074, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 10:35:31.472015: step 128000, acc = 0.9623, f1 = 0.9611
[Test] 2017-05-09 10:35:41.223247: step 128000, acc = 0.9535, f1 = 0.9531
[Status] 2017-05-09 10:35:41.223350: step 128000, maxindex = 125000, maxdev = 0.9646, maxtst = 0.9562
2017-05-09 10:35:46.977521: step 128020, loss = 0.0870, acc = 0.9780 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 10:35:51.511660: step 128040, loss = 0.0868, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:35:56.070960: step 128060, loss = 0.0841, acc = 0.9740 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 10:36:00.711169: step 128080, loss = 0.0873, acc = 0.9780 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 10:36:05.441974: step 128100, loss = 0.0868, acc = 0.9740 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:36:09.996806: step 128120, loss = 0.1080, acc = 0.9660 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:36:14.594660: step 128140, loss = 0.1122, acc = 0.9640 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 10:36:19.314250: step 128160, loss = 0.1061, acc = 0.9600 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 10:36:23.905015: step 128180, loss = 0.1010, acc = 0.9740 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 10:36:28.503261: step 128200, loss = 0.1141, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 10:36:33.304107: step 128220, loss = 0.1281, acc = 0.9600 (235.0 examples/sec; 0.272 sec/batch)
2017-05-09 10:36:37.977659: step 128240, loss = 0.0906, acc = 0.9660 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 10:36:42.538288: step 128260, loss = 0.0950, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 10:36:47.271372: step 128280, loss = 0.1188, acc = 0.9620 (250.1 examples/sec; 0.256 sec/batch)
2017-05-09 10:36:51.964916: step 128300, loss = 0.0943, acc = 0.9720 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 10:36:56.654216: step 128320, loss = 0.0849, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 10:37:01.150345: step 128340, loss = 0.1011, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 10:37:05.836949: step 128360, loss = 0.0947, acc = 0.9640 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 10:37:10.450979: step 128380, loss = 0.0944, acc = 0.9720 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 10:37:15.030022: step 128400, loss = 0.0899, acc = 0.9740 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 10:37:19.758436: step 128420, loss = 0.0848, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 10:37:24.439572: step 128440, loss = 0.0921, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 10:37:29.145732: step 128460, loss = 0.0824, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 10:37:33.920805: step 128480, loss = 0.0853, acc = 0.9740 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 10:37:38.557196: step 128500, loss = 0.1176, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 10:37:43.163877: step 128520, loss = 0.0921, acc = 0.9760 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 10:37:47.910020: step 128540, loss = 0.0763, acc = 0.9800 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 10:37:52.506875: step 128560, loss = 0.0914, acc = 0.9720 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 10:37:57.174017: step 128580, loss = 0.0948, acc = 0.9780 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 10:38:01.992687: step 128600, loss = 0.0986, acc = 0.9760 (242.3 examples/sec; 0.264 sec/batch)
2017-05-09 10:38:06.603010: step 128620, loss = 0.1085, acc = 0.9680 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 10:38:11.188423: step 128640, loss = 0.1049, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 10:38:15.690894: step 128660, loss = 0.1075, acc = 0.9580 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 10:38:20.377181: step 128680, loss = 0.0828, acc = 0.9840 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 10:38:25.037673: step 128700, loss = 0.1015, acc = 0.9740 (255.8 examples/sec; 0.250 sec/batch)
2017-05-09 10:38:29.607278: step 128720, loss = 0.0925, acc = 0.9720 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 10:38:34.420985: step 128740, loss = 0.0850, acc = 0.9740 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 10:38:39.005342: step 128760, loss = 0.0970, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 10:38:43.570590: step 128780, loss = 0.0900, acc = 0.9700 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:38:48.703557: step 128800, loss = 0.0883, acc = 0.9800 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 10:38:53.295012: step 128820, loss = 0.0914, acc = 0.9780 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 10:38:57.842254: step 128840, loss = 0.0946, acc = 0.9740 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 10:39:02.702184: step 128860, loss = 0.0747, acc = 0.9820 (219.1 examples/sec; 0.292 sec/batch)
2017-05-09 10:39:07.345286: step 128880, loss = 0.1177, acc = 0.9680 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 10:39:11.852731: step 128900, loss = 0.1048, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:39:16.442509: step 128920, loss = 0.1138, acc = 0.9580 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 10:39:21.331259: step 128940, loss = 0.1013, acc = 0.9740 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 10:39:25.883036: step 128960, loss = 0.0885, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 10:39:30.551982: step 128980, loss = 0.1039, acc = 0.9700 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 10:39:35.114931: step 129000, loss = 0.0922, acc = 0.9780 (277.5 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 10:39:49.166039: step 129000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 10:39:58.371557: step 129000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 10:39:58.371642: step 129000, maxindex = 125000, maxdev = 0.9646, maxtst = 0.9562
2017-05-09 10:40:03.861021: step 129020, loss = 0.0739, acc = 0.9900 (151.2 examples/sec; 0.423 sec/batch)
2017-05-09 10:40:08.529109: step 129040, loss = 0.0797, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 10:40:13.429295: step 129060, loss = 0.0899, acc = 0.9740 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 10:40:18.142160: step 129080, loss = 0.0937, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 10:40:22.740528: step 129100, loss = 0.1005, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:40:27.438445: step 129120, loss = 0.0952, acc = 0.9680 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 10:40:32.146240: step 129140, loss = 0.1119, acc = 0.9620 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 10:40:36.739237: step 129160, loss = 0.0731, acc = 0.9820 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 10:40:41.356615: step 129180, loss = 0.0955, acc = 0.9640 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 10:40:45.914084: step 129200, loss = 0.0991, acc = 0.9720 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 10:40:50.557545: step 129220, loss = 0.0932, acc = 0.9740 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 10:40:55.143932: step 129240, loss = 0.1263, acc = 0.9580 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 10:40:59.716752: step 129260, loss = 0.1389, acc = 0.9500 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:41:04.456174: step 129280, loss = 0.0722, acc = 0.9860 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 10:41:09.071838: step 129300, loss = 0.0701, acc = 0.9900 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 10:41:13.682665: step 129320, loss = 0.0898, acc = 0.9800 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 10:41:18.598319: step 129340, loss = 0.0895, acc = 0.9800 (248.2 examples/sec; 0.258 sec/batch)
2017-05-09 10:41:23.188171: step 129360, loss = 0.1044, acc = 0.9640 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 10:41:27.885802: step 129380, loss = 0.1081, acc = 0.9680 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 10:41:32.598429: step 129400, loss = 0.1138, acc = 0.9620 (243.0 examples/sec; 0.263 sec/batch)
2017-05-09 10:41:37.069548: step 129420, loss = 0.0877, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 10:41:41.592010: step 129440, loss = 0.1150, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 10:41:46.242099: step 129460, loss = 0.1047, acc = 0.9740 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 10:41:51.064973: step 129480, loss = 0.0671, acc = 0.9880 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 10:41:55.716842: step 129500, loss = 0.1017, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 10:42:00.415395: step 129520, loss = 0.1064, acc = 0.9700 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 10:42:05.217748: step 129540, loss = 0.0932, acc = 0.9820 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 10:42:10.349493: step 129560, loss = 0.1074, acc = 0.9660 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:42:14.966864: step 129580, loss = 0.1140, acc = 0.9740 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 10:42:19.890394: step 129600, loss = 0.0920, acc = 0.9860 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 10:42:24.546832: step 129620, loss = 0.1007, acc = 0.9660 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 10:42:29.103532: step 129640, loss = 0.0834, acc = 0.9800 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 10:42:33.923030: step 129660, loss = 0.0941, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 10:42:38.465478: step 129680, loss = 0.0883, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 10:42:43.026623: step 129700, loss = 0.1203, acc = 0.9600 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 10:42:47.715423: step 129720, loss = 0.1188, acc = 0.9640 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 10:42:52.271147: step 129740, loss = 0.1112, acc = 0.9720 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 10:42:57.039231: step 129760, loss = 0.1193, acc = 0.9680 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 10:43:01.667671: step 129780, loss = 0.1192, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:43:06.220619: step 129800, loss = 0.0943, acc = 0.9780 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 10:43:10.752520: step 129820, loss = 0.1068, acc = 0.9640 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 10:43:15.483490: step 129840, loss = 0.0838, acc = 0.9780 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 10:43:20.085546: step 129860, loss = 0.1078, acc = 0.9660 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 10:43:24.544059: step 129880, loss = 0.0906, acc = 0.9740 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 10:43:29.322776: step 129900, loss = 0.0915, acc = 0.9800 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 10:43:33.879261: step 129920, loss = 0.1069, acc = 0.9660 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 10:43:38.454194: step 129940, loss = 0.1142, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 10:43:43.217310: step 129960, loss = 0.0950, acc = 0.9740 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 10:43:47.808934: step 129980, loss = 0.1188, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 10:43:52.399072: step 130000, loss = 0.0923, acc = 0.9740 (263.5 examples/sec; 0.243 sec/batch)
[Eval] 2017-05-09 10:44:06.404104: step 130000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 10:44:16.154420: step 130000, acc = 0.9563, f1 = 0.9559
[Status] 2017-05-09 10:44:16.154498: step 130000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 10:44:24.140531: step 130020, loss = 0.0889, acc = 0.9780 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 10:44:29.790599: step 130040, loss = 0.1066, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 10:44:34.332888: step 130060, loss = 0.1100, acc = 0.9640 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 10:44:38.926902: step 130080, loss = 0.0848, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 10:44:43.688732: step 130100, loss = 0.1183, acc = 0.9640 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 10:44:48.143635: step 130120, loss = 0.0766, acc = 0.9740 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 10:44:52.840938: step 130140, loss = 0.0950, acc = 0.9740 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 10:44:57.552472: step 130160, loss = 0.1199, acc = 0.9640 (239.7 examples/sec; 0.267 sec/batch)
2017-05-09 10:45:02.075131: step 130180, loss = 0.0917, acc = 0.9800 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 10:45:06.657289: step 130200, loss = 0.0991, acc = 0.9700 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 10:45:11.238791: step 130220, loss = 0.0809, acc = 0.9800 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 10:45:15.879570: step 130240, loss = 0.0919, acc = 0.9720 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 10:45:20.463618: step 130260, loss = 0.0945, acc = 0.9760 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 10:45:25.014080: step 130280, loss = 0.1078, acc = 0.9660 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 10:45:29.883896: step 130300, loss = 0.0967, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 10:45:34.359653: step 130320, loss = 0.0949, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:45:38.972215: step 130340, loss = 0.0777, acc = 0.9840 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 10:45:43.656108: step 130360, loss = 0.1086, acc = 0.9700 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 10:45:48.252329: step 130380, loss = 0.0881, acc = 0.9780 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 10:45:52.811351: step 130400, loss = 0.1005, acc = 0.9700 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 10:45:57.543062: step 130420, loss = 0.1128, acc = 0.9720 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 10:46:02.103101: step 130440, loss = 0.0884, acc = 0.9780 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 10:46:06.595916: step 130460, loss = 0.0963, acc = 0.9700 (297.0 examples/sec; 0.215 sec/batch)
2017-05-09 10:46:11.490228: step 130480, loss = 0.1288, acc = 0.9580 (225.7 examples/sec; 0.284 sec/batch)
2017-05-09 10:46:15.986613: step 130500, loss = 0.1095, acc = 0.9700 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:46:20.589096: step 130520, loss = 0.1019, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 10:46:25.110159: step 130540, loss = 0.1052, acc = 0.9660 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:46:29.822240: step 130560, loss = 0.0991, acc = 0.9740 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:46:34.317369: step 130580, loss = 0.0811, acc = 0.9820 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 10:46:38.807090: step 130600, loss = 0.0917, acc = 0.9700 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 10:46:43.790605: step 130620, loss = 0.0986, acc = 0.9740 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 10:46:48.341495: step 130640, loss = 0.0850, acc = 0.9780 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 10:46:52.984140: step 130660, loss = 0.0932, acc = 0.9740 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 10:46:57.815826: step 130680, loss = 0.1185, acc = 0.9720 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 10:47:02.409389: step 130700, loss = 0.1318, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 10:47:07.074407: step 130720, loss = 0.1061, acc = 0.9780 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 10:47:11.761389: step 130740, loss = 0.0868, acc = 0.9740 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 10:47:16.388787: step 130760, loss = 0.0999, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 10:47:21.004779: step 130780, loss = 0.0737, acc = 0.9820 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 10:47:25.580364: step 130800, loss = 0.0950, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 10:47:30.061377: step 130820, loss = 0.0937, acc = 0.9780 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 10:47:34.597310: step 130840, loss = 0.1012, acc = 0.9660 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 10:47:39.099195: step 130860, loss = 0.1322, acc = 0.9580 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 10:47:44.192181: step 130880, loss = 0.0994, acc = 0.9700 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 10:47:48.764129: step 130900, loss = 0.1014, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 10:47:53.253389: step 130920, loss = 0.1186, acc = 0.9660 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 10:47:57.978050: step 130940, loss = 0.1182, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:48:02.485975: step 130960, loss = 0.0779, acc = 0.9880 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 10:48:07.038213: step 130980, loss = 0.1032, acc = 0.9660 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 10:48:11.733297: step 131000, loss = 0.1069, acc = 0.9640 (286.7 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 10:48:25.673223: step 131000, acc = 0.9573, f1 = 0.9558
[Test] 2017-05-09 10:48:35.046290: step 131000, acc = 0.9452, f1 = 0.9447
[Status] 2017-05-09 10:48:35.046365: step 131000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 10:48:39.701056: step 131020, loss = 0.1100, acc = 0.9560 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 10:48:45.034728: step 131040, loss = 0.1068, acc = 0.9740 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 10:48:49.542338: step 131060, loss = 0.1058, acc = 0.9740 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 10:48:54.213901: step 131080, loss = 0.1031, acc = 0.9680 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 10:48:58.613518: step 131100, loss = 0.0980, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 10:49:03.130040: step 131120, loss = 0.0814, acc = 0.9860 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 10:49:07.829097: step 131140, loss = 0.1032, acc = 0.9680 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 10:49:12.542361: step 131160, loss = 0.0822, acc = 0.9720 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 10:49:17.077578: step 131180, loss = 0.0969, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 10:49:21.770995: step 131200, loss = 0.1251, acc = 0.9560 (254.6 examples/sec; 0.251 sec/batch)
2017-05-09 10:49:26.314270: step 131220, loss = 0.0924, acc = 0.9720 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 10:49:30.821697: step 131240, loss = 0.1026, acc = 0.9660 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 10:49:35.357602: step 131260, loss = 0.1004, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 10:49:40.100593: step 131280, loss = 0.1058, acc = 0.9720 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 10:49:44.931518: step 131300, loss = 0.1048, acc = 0.9740 (219.8 examples/sec; 0.291 sec/batch)
2017-05-09 10:49:49.597789: step 131320, loss = 0.0799, acc = 0.9780 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 10:49:54.301927: step 131340, loss = 0.0870, acc = 0.9780 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:49:58.992886: step 131360, loss = 0.1164, acc = 0.9640 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 10:50:03.765922: step 131380, loss = 0.0809, acc = 0.9740 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 10:50:08.508625: step 131400, loss = 0.0829, acc = 0.9740 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 10:50:13.110725: step 131420, loss = 0.1021, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 10:50:17.625290: step 131440, loss = 0.1061, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 10:50:22.363524: step 131460, loss = 0.0803, acc = 0.9760 (239.2 examples/sec; 0.268 sec/batch)
2017-05-09 10:50:26.961877: step 131480, loss = 0.0839, acc = 0.9820 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:50:31.686973: step 131500, loss = 0.0984, acc = 0.9760 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 10:50:36.296434: step 131520, loss = 0.0885, acc = 0.9780 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 10:50:41.102521: step 131540, loss = 0.0888, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 10:50:45.720423: step 131560, loss = 0.0820, acc = 0.9820 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 10:50:50.367273: step 131580, loss = 0.1184, acc = 0.9660 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 10:50:55.060207: step 131600, loss = 0.0882, acc = 0.9680 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 10:50:59.727931: step 131620, loss = 0.1080, acc = 0.9620 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 10:51:04.368017: step 131640, loss = 0.0975, acc = 0.9700 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 10:51:09.055251: step 131660, loss = 0.0848, acc = 0.9800 (303.5 examples/sec; 0.211 sec/batch)
2017-05-09 10:51:13.578578: step 131680, loss = 0.0942, acc = 0.9720 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 10:51:18.157370: step 131700, loss = 0.0853, acc = 0.9780 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 10:51:23.001762: step 131720, loss = 0.1020, acc = 0.9680 (238.6 examples/sec; 0.268 sec/batch)
2017-05-09 10:51:27.582336: step 131740, loss = 0.1119, acc = 0.9680 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 10:51:32.112825: step 131760, loss = 0.1151, acc = 0.9600 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 10:51:36.772507: step 131780, loss = 0.1074, acc = 0.9600 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 10:51:41.417498: step 131800, loss = 0.1468, acc = 0.9440 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 10:51:45.830549: step 131820, loss = 0.0970, acc = 0.9760 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 10:51:50.458965: step 131840, loss = 0.1076, acc = 0.9720 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 10:51:55.195316: step 131860, loss = 0.0964, acc = 0.9740 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 10:51:59.775455: step 131880, loss = 0.0996, acc = 0.9740 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 10:52:04.334437: step 131900, loss = 0.0845, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 10:52:09.085731: step 131920, loss = 0.0796, acc = 0.9800 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 10:52:13.710733: step 131940, loss = 0.1076, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 10:52:18.394322: step 131960, loss = 0.0960, acc = 0.9680 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 10:52:23.140098: step 131980, loss = 0.1206, acc = 0.9600 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 10:52:27.707354: step 132000, loss = 0.0975, acc = 0.9740 (252.4 examples/sec; 0.254 sec/batch)
[Eval] 2017-05-09 10:52:41.711807: step 132000, acc = 0.9625, f1 = 0.9614
[Test] 2017-05-09 10:52:51.439621: step 132000, acc = 0.9539, f1 = 0.9536
[Status] 2017-05-09 10:52:51.439720: step 132000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 10:52:56.087947: step 132020, loss = 0.1188, acc = 0.9680 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 10:53:00.603186: step 132040, loss = 0.1003, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 10:53:06.248914: step 132060, loss = 0.0908, acc = 0.9760 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 10:53:11.043715: step 132080, loss = 0.1018, acc = 0.9760 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 10:53:15.686746: step 132100, loss = 0.0974, acc = 0.9740 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 10:53:20.278458: step 132120, loss = 0.0892, acc = 0.9680 (251.7 examples/sec; 0.254 sec/batch)
2017-05-09 10:53:25.059471: step 132140, loss = 0.1151, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 10:53:29.718458: step 132160, loss = 0.1003, acc = 0.9760 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 10:53:34.394478: step 132180, loss = 0.0955, acc = 0.9740 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 10:53:38.970843: step 132200, loss = 0.1171, acc = 0.9540 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 10:53:43.998153: step 132220, loss = 0.0885, acc = 0.9780 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 10:53:48.518430: step 132240, loss = 0.0783, acc = 0.9820 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 10:53:53.308266: step 132260, loss = 0.0973, acc = 0.9780 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 10:53:57.929349: step 132280, loss = 0.1109, acc = 0.9720 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 10:54:02.548649: step 132300, loss = 0.0703, acc = 0.9880 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 10:54:07.203860: step 132320, loss = 0.0931, acc = 0.9620 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 10:54:12.016276: step 132340, loss = 0.0851, acc = 0.9800 (299.1 examples/sec; 0.214 sec/batch)
2017-05-09 10:54:16.644973: step 132360, loss = 0.1071, acc = 0.9700 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 10:54:21.331334: step 132380, loss = 0.0942, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 10:54:26.003368: step 132400, loss = 0.0938, acc = 0.9760 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 10:54:30.629161: step 132420, loss = 0.1146, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 10:54:35.147735: step 132440, loss = 0.0970, acc = 0.9680 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 10:54:39.814010: step 132460, loss = 0.1236, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 10:54:44.507130: step 132480, loss = 0.1174, acc = 0.9540 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 10:54:49.092799: step 132500, loss = 0.1174, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 10:54:53.774482: step 132520, loss = 0.0798, acc = 0.9840 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 10:54:58.538311: step 132540, loss = 0.0751, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 10:55:03.117074: step 132560, loss = 0.0975, acc = 0.9760 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 10:55:07.680295: step 132580, loss = 0.0787, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 10:55:12.431772: step 132600, loss = 0.1069, acc = 0.9740 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 10:55:17.056173: step 132620, loss = 0.1013, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 10:55:21.840406: step 132640, loss = 0.1025, acc = 0.9800 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 10:55:26.915663: step 132660, loss = 0.0887, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:55:31.756322: step 132680, loss = 0.0826, acc = 0.9740 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 10:55:36.337406: step 132700, loss = 0.1004, acc = 0.9680 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 10:55:41.079254: step 132720, loss = 0.0875, acc = 0.9780 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 10:55:45.638452: step 132740, loss = 0.1097, acc = 0.9580 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 10:55:50.279752: step 132760, loss = 0.1214, acc = 0.9700 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 10:55:54.953113: step 132780, loss = 0.0864, acc = 0.9860 (254.3 examples/sec; 0.252 sec/batch)
2017-05-09 10:55:59.727385: step 132800, loss = 0.1217, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 10:56:04.304431: step 132820, loss = 0.1059, acc = 0.9640 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 10:56:08.825061: step 132840, loss = 0.1376, acc = 0.9600 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 10:56:13.489191: step 132860, loss = 0.0750, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 10:56:18.088296: step 132880, loss = 0.1174, acc = 0.9680 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 10:56:22.634009: step 132900, loss = 0.0901, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 10:56:27.400894: step 132920, loss = 0.1052, acc = 0.9700 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 10:56:32.015496: step 132940, loss = 0.1054, acc = 0.9620 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 10:56:36.600654: step 132960, loss = 0.1034, acc = 0.9740 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 10:56:41.428697: step 132980, loss = 0.1118, acc = 0.9640 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 10:56:45.983053: step 133000, loss = 0.1104, acc = 0.9580 (280.8 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 10:57:00.305975: step 133000, acc = 0.9618, f1 = 0.9606
[Test] 2017-05-09 10:57:10.167680: step 133000, acc = 0.9529, f1 = 0.9525
[Status] 2017-05-09 10:57:10.167782: step 133000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 10:57:14.820361: step 133020, loss = 0.0820, acc = 0.9820 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 10:57:19.388922: step 133040, loss = 0.0992, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 10:57:24.923721: step 133060, loss = 0.0949, acc = 0.9740 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 10:57:29.557258: step 133080, loss = 0.1056, acc = 0.9720 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 10:57:34.188040: step 133100, loss = 0.1062, acc = 0.9660 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 10:57:38.846614: step 133120, loss = 0.1207, acc = 0.9660 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 10:57:43.392787: step 133140, loss = 0.1224, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 10:57:48.015874: step 133160, loss = 0.0993, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 10:57:52.543392: step 133180, loss = 0.1077, acc = 0.9660 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 10:57:57.212198: step 133200, loss = 0.0932, acc = 0.9680 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 10:58:01.859822: step 133220, loss = 0.1142, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 10:58:06.457118: step 133240, loss = 0.1201, acc = 0.9640 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 10:58:11.030185: step 133260, loss = 0.0881, acc = 0.9820 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 10:58:15.635365: step 133280, loss = 0.1040, acc = 0.9720 (299.0 examples/sec; 0.214 sec/batch)
2017-05-09 10:58:20.349227: step 133300, loss = 0.1111, acc = 0.9620 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 10:58:25.056929: step 133320, loss = 0.1204, acc = 0.9640 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 10:58:29.858964: step 133340, loss = 0.1120, acc = 0.9660 (256.9 examples/sec; 0.249 sec/batch)
2017-05-09 10:58:34.613357: step 133360, loss = 0.1095, acc = 0.9680 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 10:58:39.407555: step 133380, loss = 0.0880, acc = 0.9800 (258.2 examples/sec; 0.248 sec/batch)
2017-05-09 10:58:43.986291: step 133400, loss = 0.0980, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 10:58:48.658212: step 133420, loss = 0.1069, acc = 0.9640 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 10:58:53.245513: step 133440, loss = 0.1043, acc = 0.9720 (306.5 examples/sec; 0.209 sec/batch)
2017-05-09 10:58:57.826216: step 133460, loss = 0.1053, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 10:59:02.447627: step 133480, loss = 0.1195, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 10:59:07.334772: step 133500, loss = 0.1139, acc = 0.9680 (232.8 examples/sec; 0.275 sec/batch)
2017-05-09 10:59:11.875179: step 133520, loss = 0.0951, acc = 0.9640 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 10:59:16.409256: step 133540, loss = 0.0900, acc = 0.9700 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 10:59:20.957518: step 133560, loss = 0.0915, acc = 0.9760 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 10:59:25.738121: step 133580, loss = 0.0919, acc = 0.9700 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 10:59:30.380581: step 133600, loss = 0.1189, acc = 0.9600 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 10:59:34.977695: step 133620, loss = 0.1061, acc = 0.9660 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 10:59:39.765829: step 133640, loss = 0.0915, acc = 0.9800 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 10:59:44.258266: step 133660, loss = 0.1104, acc = 0.9580 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 10:59:48.930118: step 133680, loss = 0.0970, acc = 0.9720 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 10:59:53.602474: step 133700, loss = 0.0821, acc = 0.9760 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 10:59:58.218774: step 133720, loss = 0.0993, acc = 0.9680 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:00:02.728030: step 133740, loss = 0.0847, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 11:00:07.484104: step 133760, loss = 0.0828, acc = 0.9820 (297.5 examples/sec; 0.215 sec/batch)
2017-05-09 11:00:12.208957: step 133780, loss = 0.1276, acc = 0.9600 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 11:00:16.902976: step 133800, loss = 0.0972, acc = 0.9680 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 11:00:21.664542: step 133820, loss = 0.1023, acc = 0.9620 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 11:00:26.231071: step 133840, loss = 0.0830, acc = 0.9760 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 11:00:30.878036: step 133860, loss = 0.0876, acc = 0.9840 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 11:00:35.476951: step 133880, loss = 0.0940, acc = 0.9800 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 11:00:40.206276: step 133900, loss = 0.0929, acc = 0.9720 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 11:00:44.737025: step 133920, loss = 0.1018, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:00:49.389187: step 133940, loss = 0.1083, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 11:00:54.018916: step 133960, loss = 0.0826, acc = 0.9720 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:00:58.589857: step 133980, loss = 0.0893, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 11:01:03.356743: step 134000, loss = 0.0959, acc = 0.9740 (264.3 examples/sec; 0.242 sec/batch)
[Eval] 2017-05-09 11:01:17.464924: step 134000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 11:01:27.375896: step 134000, acc = 0.9565, f1 = 0.9562
[Status] 2017-05-09 11:01:27.375984: step 134000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 11:01:32.052495: step 134020, loss = 0.0996, acc = 0.9660 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:01:36.665775: step 134040, loss = 0.0853, acc = 0.9720 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 11:01:42.597905: step 134060, loss = 0.1351, acc = 0.9580 (130.9 examples/sec; 0.489 sec/batch)
2017-05-09 11:01:47.314902: step 134080, loss = 0.1115, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 11:01:52.007050: step 134100, loss = 0.0942, acc = 0.9760 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 11:01:56.899710: step 134120, loss = 0.0896, acc = 0.9720 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 11:02:01.543306: step 134140, loss = 0.0997, acc = 0.9880 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 11:02:06.142770: step 134160, loss = 0.0792, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 11:02:11.077329: step 134180, loss = 0.1209, acc = 0.9600 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 11:02:15.841349: step 134200, loss = 0.0943, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 11:02:20.404370: step 134220, loss = 0.1027, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 11:02:25.087328: step 134240, loss = 0.0838, acc = 0.9840 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:02:29.797588: step 134260, loss = 0.0947, acc = 0.9720 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 11:02:34.341626: step 134280, loss = 0.1014, acc = 0.9700 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 11:02:38.945988: step 134300, loss = 0.1182, acc = 0.9600 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 11:02:43.770527: step 134320, loss = 0.1124, acc = 0.9680 (258.4 examples/sec; 0.248 sec/batch)
2017-05-09 11:02:48.367362: step 134340, loss = 0.0980, acc = 0.9660 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 11:02:52.969618: step 134360, loss = 0.0991, acc = 0.9760 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:02:57.622906: step 134380, loss = 0.0910, acc = 0.9720 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 11:03:02.363829: step 134400, loss = 0.0868, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 11:03:06.951177: step 134420, loss = 0.0796, acc = 0.9840 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 11:03:11.609773: step 134440, loss = 0.1199, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 11:03:16.175675: step 134460, loss = 0.0908, acc = 0.9740 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 11:03:20.842791: step 134480, loss = 0.0876, acc = 0.9680 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 11:03:25.504116: step 134500, loss = 0.1142, acc = 0.9600 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 11:03:30.007004: step 134520, loss = 0.1032, acc = 0.9720 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 11:03:34.659871: step 134540, loss = 0.1148, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 11:03:39.457921: step 134560, loss = 0.1115, acc = 0.9660 (234.3 examples/sec; 0.273 sec/batch)
2017-05-09 11:03:43.977934: step 134580, loss = 0.1177, acc = 0.9620 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 11:03:48.799983: step 134600, loss = 0.0867, acc = 0.9740 (229.0 examples/sec; 0.280 sec/batch)
2017-05-09 11:03:53.475518: step 134620, loss = 0.1052, acc = 0.9580 (256.4 examples/sec; 0.250 sec/batch)
2017-05-09 11:03:58.147381: step 134640, loss = 0.0858, acc = 0.9820 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 11:04:02.780832: step 134660, loss = 0.0957, acc = 0.9740 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:04:07.405490: step 134680, loss = 0.0786, acc = 0.9780 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 11:04:12.172875: step 134700, loss = 0.1208, acc = 0.9660 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 11:04:16.838547: step 134720, loss = 0.1154, acc = 0.9700 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 11:04:21.448757: step 134740, loss = 0.1128, acc = 0.9620 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:04:26.147549: step 134760, loss = 0.0858, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 11:04:30.656990: step 134780, loss = 0.1199, acc = 0.9540 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 11:04:35.126606: step 134800, loss = 0.0918, acc = 0.9800 (301.2 examples/sec; 0.212 sec/batch)
2017-05-09 11:04:39.901353: step 134820, loss = 0.0931, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 11:04:44.561156: step 134840, loss = 0.0844, acc = 0.9760 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 11:04:49.062476: step 134860, loss = 0.0906, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:04:53.740748: step 134880, loss = 0.1011, acc = 0.9700 (252.4 examples/sec; 0.254 sec/batch)
2017-05-09 11:04:58.215558: step 134900, loss = 0.0930, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 11:05:02.871120: step 134920, loss = 0.0938, acc = 0.9740 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:05:07.580508: step 134940, loss = 0.0798, acc = 0.9820 (255.2 examples/sec; 0.251 sec/batch)
2017-05-09 11:05:12.341265: step 134960, loss = 0.0998, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 11:05:16.890966: step 134980, loss = 0.0762, acc = 0.9800 (298.4 examples/sec; 0.214 sec/batch)
2017-05-09 11:05:21.452627: step 135000, loss = 0.0842, acc = 0.9780 (280.1 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 11:05:35.487040: step 135000, acc = 0.9609, f1 = 0.9598
[Test] 2017-05-09 11:05:45.396366: step 135000, acc = 0.9524, f1 = 0.9520
[Status] 2017-05-09 11:05:45.396437: step 135000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 11:05:49.979016: step 135020, loss = 0.0945, acc = 0.9720 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 11:05:54.898767: step 135040, loss = 0.1359, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 11:05:59.579627: step 135060, loss = 0.1047, acc = 0.9620 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 11:06:05.561670: step 135080, loss = 0.1012, acc = 0.9680 (251.2 examples/sec; 0.255 sec/batch)
2017-05-09 11:06:10.128070: step 135100, loss = 0.0788, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 11:06:14.813981: step 135120, loss = 0.0986, acc = 0.9740 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 11:06:19.340574: step 135140, loss = 0.0858, acc = 0.9800 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:06:24.043484: step 135160, loss = 0.1087, acc = 0.9640 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 11:06:28.719351: step 135180, loss = 0.0966, acc = 0.9640 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:06:33.454062: step 135200, loss = 0.1264, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 11:06:38.140663: step 135220, loss = 0.1159, acc = 0.9660 (287.6 examples/sec; 0.222 sec/batch)
2017-05-09 11:06:42.655011: step 135240, loss = 0.0940, acc = 0.9820 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 11:06:47.280816: step 135260, loss = 0.0842, acc = 0.9840 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 11:06:51.979518: step 135280, loss = 0.1085, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:06:56.491277: step 135300, loss = 0.1036, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 11:07:00.992877: step 135320, loss = 0.1036, acc = 0.9680 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 11:07:05.689600: step 135340, loss = 0.0776, acc = 0.9780 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 11:07:10.384345: step 135360, loss = 0.0994, acc = 0.9700 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 11:07:14.931166: step 135380, loss = 0.0894, acc = 0.9780 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 11:07:19.526168: step 135400, loss = 0.1162, acc = 0.9700 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 11:07:24.255147: step 135420, loss = 0.0898, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 11:07:28.819701: step 135440, loss = 0.0936, acc = 0.9760 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 11:07:33.556508: step 135460, loss = 0.1272, acc = 0.9580 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 11:07:38.287919: step 135480, loss = 0.0965, acc = 0.9820 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 11:07:42.889405: step 135500, loss = 0.1018, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:07:47.529333: step 135520, loss = 0.0738, acc = 0.9860 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 11:07:52.341987: step 135540, loss = 0.1118, acc = 0.9680 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:07:56.938644: step 135560, loss = 0.0808, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 11:08:01.506049: step 135580, loss = 0.0873, acc = 0.9800 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 11:08:06.117362: step 135600, loss = 0.1052, acc = 0.9700 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 11:08:10.948835: step 135620, loss = 0.0982, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 11:08:15.484603: step 135640, loss = 0.0829, acc = 0.9800 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 11:08:20.041271: step 135660, loss = 0.1165, acc = 0.9600 (294.3 examples/sec; 0.218 sec/batch)
2017-05-09 11:08:24.731242: step 135680, loss = 0.1207, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 11:08:29.319977: step 135700, loss = 0.1152, acc = 0.9640 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 11:08:33.806023: step 135720, loss = 0.1010, acc = 0.9720 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 11:08:38.430811: step 135740, loss = 0.0994, acc = 0.9700 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 11:08:43.045355: step 135760, loss = 0.1262, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 11:08:47.606174: step 135780, loss = 0.0941, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:08:52.243527: step 135800, loss = 0.0870, acc = 0.9740 (233.0 examples/sec; 0.275 sec/batch)
2017-05-09 11:08:56.783512: step 135820, loss = 0.0821, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 11:09:01.392022: step 135840, loss = 0.1025, acc = 0.9660 (258.3 examples/sec; 0.248 sec/batch)
2017-05-09 11:09:05.987392: step 135860, loss = 0.0915, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:09:10.859096: step 135880, loss = 0.1326, acc = 0.9500 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 11:09:15.472182: step 135900, loss = 0.0885, acc = 0.9700 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 11:09:19.981769: step 135920, loss = 0.0813, acc = 0.9800 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:09:24.641466: step 135940, loss = 0.1033, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 11:09:29.185081: step 135960, loss = 0.0890, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:09:33.715376: step 135980, loss = 0.1090, acc = 0.9620 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 11:09:38.477364: step 136000, loss = 0.1020, acc = 0.9760 (229.3 examples/sec; 0.279 sec/batch)
[Eval] 2017-05-09 11:09:52.555602: step 136000, acc = 0.9620, f1 = 0.9606
[Test] 2017-05-09 11:10:02.082394: step 136000, acc = 0.9513, f1 = 0.9509
[Status] 2017-05-09 11:10:02.082494: step 136000, maxindex = 130000, maxdev = 0.9648, maxtst = 0.9563
2017-05-09 11:10:06.772667: step 136020, loss = 0.1094, acc = 0.9700 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:10:11.340418: step 136040, loss = 0.0997, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:10:15.913045: step 136060, loss = 0.0902, acc = 0.9860 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 11:10:21.662543: step 136080, loss = 0.1077, acc = 0.9740 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 11:10:26.197871: step 136100, loss = 0.1167, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 11:10:30.869251: step 136120, loss = 0.1018, acc = 0.9740 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:10:35.628047: step 136140, loss = 0.0991, acc = 0.9740 (251.3 examples/sec; 0.255 sec/batch)
2017-05-09 11:10:40.208672: step 136160, loss = 0.0733, acc = 0.9880 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 11:10:44.776096: step 136180, loss = 0.0865, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 11:10:49.462451: step 136200, loss = 0.0839, acc = 0.9800 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 11:10:54.176239: step 136220, loss = 0.1099, acc = 0.9660 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:10:58.897020: step 136240, loss = 0.1170, acc = 0.9680 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 11:11:03.396837: step 136260, loss = 0.0903, acc = 0.9720 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 11:11:08.134598: step 136280, loss = 0.1090, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 11:11:12.648344: step 136300, loss = 0.1136, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 11:11:17.493882: step 136320, loss = 0.0917, acc = 0.9760 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 11:11:22.593340: step 136340, loss = 0.0875, acc = 0.9800 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 11:11:27.177732: step 136360, loss = 0.0948, acc = 0.9700 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 11:11:31.758892: step 136380, loss = 0.1092, acc = 0.9700 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:11:36.580920: step 136400, loss = 0.0960, acc = 0.9660 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 11:11:41.122459: step 136420, loss = 0.0918, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 11:11:45.768586: step 136440, loss = 0.1107, acc = 0.9680 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 11:11:50.453900: step 136460, loss = 0.0840, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 11:11:55.297472: step 136480, loss = 0.1003, acc = 0.9660 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 11:11:59.845257: step 136500, loss = 0.0865, acc = 0.9680 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 11:12:04.520726: step 136520, loss = 0.0914, acc = 0.9760 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 11:12:09.259548: step 136540, loss = 0.1068, acc = 0.9780 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 11:12:13.792452: step 136560, loss = 0.0771, acc = 0.9920 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 11:12:18.404689: step 136580, loss = 0.0812, acc = 0.9740 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 11:12:23.045774: step 136600, loss = 0.0959, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 11:12:27.801348: step 136620, loss = 0.0931, acc = 0.9780 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 11:12:32.440289: step 136640, loss = 0.0866, acc = 0.9840 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 11:12:36.996894: step 136660, loss = 0.0873, acc = 0.9720 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 11:12:41.537704: step 136680, loss = 0.0966, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:12:46.122661: step 136700, loss = 0.0873, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 11:12:50.728439: step 136720, loss = 0.0678, acc = 0.9880 (301.0 examples/sec; 0.213 sec/batch)
2017-05-09 11:12:55.625486: step 136740, loss = 0.0878, acc = 0.9800 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 11:13:00.347351: step 136760, loss = 0.0888, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 11:13:04.989012: step 136780, loss = 0.1076, acc = 0.9620 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 11:13:09.938593: step 136800, loss = 0.0956, acc = 0.9720 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 11:13:14.607018: step 136820, loss = 0.0975, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 11:13:19.199481: step 136840, loss = 0.1094, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:13:23.849743: step 136860, loss = 0.0867, acc = 0.9780 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 11:13:28.351372: step 136880, loss = 0.0800, acc = 0.9780 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 11:13:33.044323: step 136900, loss = 0.1036, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 11:13:38.079327: step 136920, loss = 0.0889, acc = 0.9720 (220.3 examples/sec; 0.291 sec/batch)
2017-05-09 11:13:42.568793: step 136940, loss = 0.1002, acc = 0.9700 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 11:13:47.306103: step 136960, loss = 0.1222, acc = 0.9540 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 11:13:52.111382: step 136980, loss = 0.0750, acc = 0.9860 (247.6 examples/sec; 0.258 sec/batch)
2017-05-09 11:13:56.704867: step 137000, loss = 0.1172, acc = 0.9640 (269.3 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 11:14:10.771088: step 137000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 11:14:20.294292: step 137000, acc = 0.9566, f1 = 0.9562
[Status] 2017-05-09 11:14:20.294399: step 137000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:14:28.199167: step 137020, loss = 0.1002, acc = 0.9680 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 11:14:32.768479: step 137040, loss = 0.0748, acc = 0.9840 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 11:14:37.492607: step 137060, loss = 0.0857, acc = 0.9800 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 11:14:42.086478: step 137080, loss = 0.0844, acc = 0.9780 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 11:14:47.310943: step 137100, loss = 0.1104, acc = 0.9600 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 11:14:52.125379: step 137120, loss = 0.0927, acc = 0.9760 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:14:56.816257: step 137140, loss = 0.1051, acc = 0.9740 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 11:15:01.370113: step 137160, loss = 0.0768, acc = 0.9740 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 11:15:06.160016: step 137180, loss = 0.0995, acc = 0.9740 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 11:15:10.671853: step 137200, loss = 0.1040, acc = 0.9740 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 11:15:15.170556: step 137220, loss = 0.0962, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 11:15:19.847381: step 137240, loss = 0.0733, acc = 0.9820 (249.8 examples/sec; 0.256 sec/batch)
2017-05-09 11:15:24.346202: step 137260, loss = 0.0851, acc = 0.9760 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 11:15:28.988653: step 137280, loss = 0.0851, acc = 0.9780 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 11:15:33.727167: step 137300, loss = 0.0897, acc = 0.9800 (244.1 examples/sec; 0.262 sec/batch)
2017-05-09 11:15:38.329831: step 137320, loss = 0.0796, acc = 0.9840 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 11:15:42.949339: step 137340, loss = 0.1012, acc = 0.9820 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 11:15:47.550093: step 137360, loss = 0.1051, acc = 0.9720 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 11:15:52.181018: step 137380, loss = 0.1019, acc = 0.9720 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 11:15:56.673443: step 137400, loss = 0.1099, acc = 0.9600 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 11:16:01.226338: step 137420, loss = 0.0849, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 11:16:05.943028: step 137440, loss = 0.0814, acc = 0.9780 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 11:16:10.589000: step 137460, loss = 0.0870, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 11:16:15.254353: step 137480, loss = 0.0749, acc = 0.9840 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 11:16:19.938293: step 137500, loss = 0.0984, acc = 0.9700 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 11:16:24.556578: step 137520, loss = 0.0955, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:16:29.165638: step 137540, loss = 0.1008, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 11:16:34.028649: step 137560, loss = 0.1022, acc = 0.9720 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 11:16:38.649548: step 137580, loss = 0.0965, acc = 0.9760 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 11:16:43.377594: step 137600, loss = 0.1231, acc = 0.9640 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 11:16:47.947395: step 137620, loss = 0.1193, acc = 0.9700 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 11:16:52.650604: step 137640, loss = 0.0884, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 11:16:57.294671: step 137660, loss = 0.0849, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 11:17:01.941199: step 137680, loss = 0.1036, acc = 0.9700 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 11:17:06.769833: step 137700, loss = 0.1025, acc = 0.9640 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 11:17:11.491842: step 137720, loss = 0.1048, acc = 0.9720 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 11:17:16.051007: step 137740, loss = 0.1121, acc = 0.9640 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 11:17:20.981594: step 137760, loss = 0.0910, acc = 0.9680 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 11:17:25.614823: step 137780, loss = 0.0857, acc = 0.9780 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 11:17:30.249301: step 137800, loss = 0.1044, acc = 0.9680 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 11:17:35.104681: step 137820, loss = 0.0924, acc = 0.9720 (298.9 examples/sec; 0.214 sec/batch)
2017-05-09 11:17:39.783773: step 137840, loss = 0.0863, acc = 0.9780 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 11:17:44.406195: step 137860, loss = 0.0918, acc = 0.9780 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 11:17:49.138408: step 137880, loss = 0.1142, acc = 0.9720 (238.4 examples/sec; 0.268 sec/batch)
2017-05-09 11:17:53.619570: step 137900, loss = 0.1038, acc = 0.9680 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 11:17:58.167045: step 137920, loss = 0.1005, acc = 0.9680 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 11:18:02.668069: step 137940, loss = 0.1087, acc = 0.9640 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 11:18:07.456338: step 137960, loss = 0.1144, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:18:12.155694: step 137980, loss = 0.0877, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 11:18:16.762031: step 138000, loss = 0.0935, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 11:18:31.136771: step 138000, acc = 0.9638, f1 = 0.9625
[Test] 2017-05-09 11:18:40.819631: step 138000, acc = 0.9551, f1 = 0.9547
[Status] 2017-05-09 11:18:40.819720: step 138000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:18:45.448524: step 138020, loss = 0.0943, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 11:18:50.248544: step 138040, loss = 0.1037, acc = 0.9740 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 11:18:54.899509: step 138060, loss = 0.1042, acc = 0.9780 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 11:18:59.515917: step 138080, loss = 0.1010, acc = 0.9700 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 11:19:05.160739: step 138100, loss = 0.1040, acc = 0.9720 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 11:19:09.739750: step 138120, loss = 0.1066, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 11:19:14.398451: step 138140, loss = 0.0902, acc = 0.9780 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 11:19:19.240831: step 138160, loss = 0.1089, acc = 0.9600 (227.6 examples/sec; 0.281 sec/batch)
2017-05-09 11:19:23.887041: step 138180, loss = 0.0984, acc = 0.9720 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 11:19:28.522575: step 138200, loss = 0.0989, acc = 0.9660 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 11:19:33.094191: step 138220, loss = 0.0841, acc = 0.9840 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 11:19:37.802140: step 138240, loss = 0.1264, acc = 0.9580 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 11:19:42.270583: step 138260, loss = 0.0824, acc = 0.9760 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 11:19:46.880925: step 138280, loss = 0.0742, acc = 0.9780 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 11:19:51.690960: step 138300, loss = 0.0954, acc = 0.9640 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:19:56.408563: step 138320, loss = 0.0928, acc = 0.9740 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 11:20:00.987320: step 138340, loss = 0.0806, acc = 0.9780 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 11:20:05.784090: step 138360, loss = 0.0874, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 11:20:10.386062: step 138380, loss = 0.0900, acc = 0.9780 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 11:20:15.015625: step 138400, loss = 0.0927, acc = 0.9740 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 11:20:19.751770: step 138420, loss = 0.1251, acc = 0.9600 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 11:20:24.291776: step 138440, loss = 0.1089, acc = 0.9660 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 11:20:28.862036: step 138460, loss = 0.1185, acc = 0.9620 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:20:33.560094: step 138480, loss = 0.0804, acc = 0.9800 (247.9 examples/sec; 0.258 sec/batch)
2017-05-09 11:20:38.168530: step 138500, loss = 0.0906, acc = 0.9760 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 11:20:42.978699: step 138520, loss = 0.0964, acc = 0.9760 (254.0 examples/sec; 0.252 sec/batch)
2017-05-09 11:20:47.644653: step 138540, loss = 0.0913, acc = 0.9760 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 11:20:52.507487: step 138560, loss = 0.0911, acc = 0.9800 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 11:20:57.104155: step 138580, loss = 0.0923, acc = 0.9740 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 11:21:01.697596: step 138600, loss = 0.0763, acc = 0.9800 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 11:21:06.455849: step 138620, loss = 0.1054, acc = 0.9680 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:21:10.963472: step 138640, loss = 0.1039, acc = 0.9680 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 11:21:15.585309: step 138660, loss = 0.0935, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 11:21:20.361026: step 138680, loss = 0.0810, acc = 0.9780 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:21:24.878602: step 138700, loss = 0.1043, acc = 0.9620 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 11:21:29.524708: step 138720, loss = 0.0917, acc = 0.9780 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 11:21:34.220171: step 138740, loss = 0.0943, acc = 0.9760 (234.4 examples/sec; 0.273 sec/batch)
2017-05-09 11:21:38.658208: step 138760, loss = 0.0788, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:21:43.328983: step 138780, loss = 0.1081, acc = 0.9700 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 11:21:47.905794: step 138800, loss = 0.1092, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 11:21:52.606130: step 138820, loss = 0.0801, acc = 0.9800 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 11:21:57.259723: step 138840, loss = 0.0919, acc = 0.9740 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 11:22:01.923671: step 138860, loss = 0.1009, acc = 0.9720 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:22:06.590981: step 138880, loss = 0.1053, acc = 0.9680 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 11:22:11.138461: step 138900, loss = 0.0781, acc = 0.9820 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:22:15.707635: step 138920, loss = 0.0826, acc = 0.9800 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 11:22:20.538828: step 138940, loss = 0.1064, acc = 0.9580 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:22:25.195927: step 138960, loss = 0.1185, acc = 0.9580 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 11:22:29.750980: step 138980, loss = 0.0771, acc = 0.9840 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 11:22:34.995139: step 139000, loss = 0.1058, acc = 0.9680 (282.4 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 11:22:49.270261: step 139000, acc = 0.9634, f1 = 0.9623
[Test] 2017-05-09 11:22:58.521545: step 139000, acc = 0.9549, f1 = 0.9545
[Status] 2017-05-09 11:22:58.521666: step 139000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:23:03.175853: step 139020, loss = 0.1084, acc = 0.9700 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 11:23:07.921434: step 139040, loss = 0.1125, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 11:23:12.577694: step 139060, loss = 0.1033, acc = 0.9700 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 11:23:17.194059: step 139080, loss = 0.1210, acc = 0.9600 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 11:23:22.744561: step 139100, loss = 0.0940, acc = 0.9700 (157.3 examples/sec; 0.407 sec/batch)
2017-05-09 11:23:27.315220: step 139120, loss = 0.0702, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 11:23:31.739739: step 139140, loss = 0.1045, acc = 0.9640 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 11:23:36.473464: step 139160, loss = 0.0813, acc = 0.9800 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 11:23:41.123234: step 139180, loss = 0.1102, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 11:23:45.682926: step 139200, loss = 0.0976, acc = 0.9740 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 11:23:50.565195: step 139220, loss = 0.0898, acc = 0.9700 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 11:23:55.150522: step 139240, loss = 0.1055, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:23:59.664900: step 139260, loss = 0.0796, acc = 0.9840 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 11:24:04.446583: step 139280, loss = 0.1064, acc = 0.9640 (241.7 examples/sec; 0.265 sec/batch)
2017-05-09 11:24:08.989256: step 139300, loss = 0.0746, acc = 0.9820 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:24:13.706860: step 139320, loss = 0.0879, acc = 0.9760 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 11:24:18.436651: step 139340, loss = 0.1037, acc = 0.9720 (242.6 examples/sec; 0.264 sec/batch)
2017-05-09 11:24:22.923487: step 139360, loss = 0.1019, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 11:24:27.441858: step 139380, loss = 0.1156, acc = 0.9580 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 11:24:31.980958: step 139400, loss = 0.0905, acc = 0.9820 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 11:24:36.757961: step 139420, loss = 0.0897, acc = 0.9740 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 11:24:41.385326: step 139440, loss = 0.0890, acc = 0.9760 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 11:24:45.888098: step 139460, loss = 0.0911, acc = 0.9780 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 11:24:50.544304: step 139480, loss = 0.0861, acc = 0.9760 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 11:24:55.089736: step 139500, loss = 0.1003, acc = 0.9720 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 11:24:59.654970: step 139520, loss = 0.1095, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 11:25:04.393187: step 139540, loss = 0.1024, acc = 0.9740 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 11:25:08.871703: step 139560, loss = 0.1038, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 11:25:13.406507: step 139580, loss = 0.0962, acc = 0.9720 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 11:25:18.075128: step 139600, loss = 0.0883, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 11:25:22.522648: step 139620, loss = 0.1274, acc = 0.9660 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 11:25:27.048283: step 139640, loss = 0.1339, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 11:25:31.680591: step 139660, loss = 0.1201, acc = 0.9580 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 11:25:36.261944: step 139680, loss = 0.0833, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:25:40.811394: step 139700, loss = 0.0904, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:25:45.763509: step 139720, loss = 0.0903, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 11:25:50.233180: step 139740, loss = 0.1064, acc = 0.9680 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:25:54.849694: step 139760, loss = 0.1050, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 11:25:59.662108: step 139780, loss = 0.0909, acc = 0.9720 (224.8 examples/sec; 0.285 sec/batch)
2017-05-09 11:26:04.229978: step 139800, loss = 0.1022, acc = 0.9820 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:26:08.867552: step 139820, loss = 0.0820, acc = 0.9820 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 11:26:13.409743: step 139840, loss = 0.0991, acc = 0.9700 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 11:26:18.066538: step 139860, loss = 0.0860, acc = 0.9840 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 11:26:22.634181: step 139880, loss = 0.1024, acc = 0.9700 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 11:26:27.326370: step 139900, loss = 0.1035, acc = 0.9780 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 11:26:31.909512: step 139920, loss = 0.1137, acc = 0.9540 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 11:26:36.485453: step 139940, loss = 0.1051, acc = 0.9740 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 11:26:41.138133: step 139960, loss = 0.1245, acc = 0.9600 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 11:26:45.767658: step 139980, loss = 0.0979, acc = 0.9760 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 11:26:50.355539: step 140000, loss = 0.0921, acc = 0.9760 (274.3 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 11:27:04.468944: step 140000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 11:27:14.323266: step 140000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 11:27:14.323371: step 140000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:27:18.820177: step 140020, loss = 0.0945, acc = 0.9640 (298.3 examples/sec; 0.215 sec/batch)
2017-05-09 11:27:23.411108: step 140040, loss = 0.0749, acc = 0.9880 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 11:27:28.040831: step 140060, loss = 0.0878, acc = 0.9820 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 11:27:32.684150: step 140080, loss = 0.1075, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 11:27:37.258357: step 140100, loss = 0.0709, acc = 0.9900 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 11:27:43.136528: step 140120, loss = 0.0850, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 11:27:47.767527: step 140140, loss = 0.0894, acc = 0.9820 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 11:27:52.374562: step 140160, loss = 0.0876, acc = 0.9760 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 11:27:57.068791: step 140180, loss = 0.0840, acc = 0.9820 (255.4 examples/sec; 0.251 sec/batch)
2017-05-09 11:28:01.560535: step 140200, loss = 0.0948, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 11:28:06.157756: step 140220, loss = 0.0872, acc = 0.9780 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 11:28:11.041176: step 140240, loss = 0.0982, acc = 0.9700 (237.4 examples/sec; 0.270 sec/batch)
2017-05-09 11:28:15.679274: step 140260, loss = 0.0915, acc = 0.9800 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:28:20.332291: step 140280, loss = 0.0879, acc = 0.9740 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 11:28:24.817799: step 140300, loss = 0.0886, acc = 0.9740 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 11:28:29.587026: step 140320, loss = 0.0931, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 11:28:34.083640: step 140340, loss = 0.0844, acc = 0.9780 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:28:38.672531: step 140360, loss = 0.1059, acc = 0.9720 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 11:28:43.529973: step 140380, loss = 0.0830, acc = 0.9740 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 11:28:48.136277: step 140400, loss = 0.0973, acc = 0.9740 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 11:28:52.680351: step 140420, loss = 0.0833, acc = 0.9800 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 11:28:57.492293: step 140440, loss = 0.0947, acc = 0.9780 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 11:29:02.086017: step 140460, loss = 0.0812, acc = 0.9820 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 11:29:06.770191: step 140480, loss = 0.0970, acc = 0.9720 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 11:29:11.498175: step 140500, loss = 0.0796, acc = 0.9740 (236.4 examples/sec; 0.271 sec/batch)
2017-05-09 11:29:16.064241: step 140520, loss = 0.1060, acc = 0.9580 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 11:29:20.718928: step 140540, loss = 0.0688, acc = 0.9880 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 11:29:25.410764: step 140560, loss = 0.1001, acc = 0.9780 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 11:29:30.066439: step 140580, loss = 0.0984, acc = 0.9700 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 11:29:34.716585: step 140600, loss = 0.0912, acc = 0.9700 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 11:29:39.194423: step 140620, loss = 0.1148, acc = 0.9660 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 11:29:43.894910: step 140640, loss = 0.0900, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 11:29:48.429435: step 140660, loss = 0.0916, acc = 0.9660 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 11:29:53.041002: step 140680, loss = 0.1354, acc = 0.9460 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 11:29:57.761952: step 140700, loss = 0.1159, acc = 0.9620 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 11:30:02.255650: step 140720, loss = 0.0795, acc = 0.9840 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 11:30:06.846654: step 140740, loss = 0.0927, acc = 0.9720 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 11:30:11.742507: step 140760, loss = 0.1005, acc = 0.9700 (209.2 examples/sec; 0.306 sec/batch)
2017-05-09 11:30:16.319719: step 140780, loss = 0.1126, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 11:30:20.904477: step 140800, loss = 0.1166, acc = 0.9720 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 11:30:25.496322: step 140820, loss = 0.1054, acc = 0.9700 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 11:30:30.215608: step 140840, loss = 0.1098, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:30:34.779671: step 140860, loss = 0.1122, acc = 0.9560 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 11:30:39.469624: step 140880, loss = 0.0898, acc = 0.9800 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 11:30:44.214427: step 140900, loss = 0.1097, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 11:30:48.807449: step 140920, loss = 0.0971, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 11:30:53.299736: step 140940, loss = 0.0980, acc = 0.9700 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 11:30:57.963054: step 140960, loss = 0.1097, acc = 0.9620 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 11:31:02.571664: step 140980, loss = 0.1089, acc = 0.9600 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 11:31:07.178276: step 141000, loss = 0.1035, acc = 0.9780 (269.9 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 11:31:21.321853: step 141000, acc = 0.9637, f1 = 0.9624
[Test] 2017-05-09 11:31:31.155978: step 141000, acc = 0.9546, f1 = 0.9543
[Status] 2017-05-09 11:31:31.156056: step 141000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:31:35.677201: step 141020, loss = 0.0998, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 11:31:40.522047: step 141040, loss = 0.0912, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 11:31:45.181328: step 141060, loss = 0.0906, acc = 0.9780 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 11:31:49.920733: step 141080, loss = 0.1039, acc = 0.9780 (247.4 examples/sec; 0.259 sec/batch)
2017-05-09 11:31:54.475521: step 141100, loss = 0.0877, acc = 0.9860 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 11:31:59.875907: step 141120, loss = 0.1016, acc = 0.9760 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 11:32:04.390182: step 141140, loss = 0.0887, acc = 0.9780 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 11:32:08.987870: step 141160, loss = 0.0983, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 11:32:13.640371: step 141180, loss = 0.1305, acc = 0.9620 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 11:32:18.250691: step 141200, loss = 0.0882, acc = 0.9700 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 11:32:22.795642: step 141220, loss = 0.0779, acc = 0.9820 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 11:32:27.751995: step 141240, loss = 0.1163, acc = 0.9580 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 11:32:32.361669: step 141260, loss = 0.0967, acc = 0.9740 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:32:36.940584: step 141280, loss = 0.0986, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:32:41.726539: step 141300, loss = 0.1199, acc = 0.9660 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 11:32:46.300950: step 141320, loss = 0.0937, acc = 0.9780 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 11:32:50.904027: step 141340, loss = 0.1106, acc = 0.9660 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 11:32:55.561021: step 141360, loss = 0.0875, acc = 0.9760 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 11:33:00.385879: step 141380, loss = 0.1127, acc = 0.9640 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 11:33:04.960007: step 141400, loss = 0.0790, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 11:33:09.578851: step 141420, loss = 0.0930, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:33:14.262973: step 141440, loss = 0.1208, acc = 0.9560 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 11:33:18.826616: step 141460, loss = 0.1129, acc = 0.9720 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 11:33:23.363940: step 141480, loss = 0.0856, acc = 0.9800 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 11:33:28.269192: step 141500, loss = 0.1089, acc = 0.9620 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 11:33:32.895091: step 141520, loss = 0.1134, acc = 0.9660 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:33:37.517537: step 141540, loss = 0.1010, acc = 0.9700 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 11:33:42.249971: step 141560, loss = 0.0940, acc = 0.9700 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 11:33:46.779168: step 141580, loss = 0.1078, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:33:51.403667: step 141600, loss = 0.0906, acc = 0.9740 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 11:33:56.056949: step 141620, loss = 0.0880, acc = 0.9780 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:34:00.817847: step 141640, loss = 0.1049, acc = 0.9660 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 11:34:05.445414: step 141660, loss = 0.0819, acc = 0.9860 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 11:34:10.050449: step 141680, loss = 0.1071, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 11:34:14.747164: step 141700, loss = 0.1128, acc = 0.9580 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 11:34:19.354891: step 141720, loss = 0.0957, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:34:23.934550: step 141740, loss = 0.0985, acc = 0.9720 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 11:34:28.747978: step 141760, loss = 0.0885, acc = 0.9820 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:34:33.327168: step 141780, loss = 0.0866, acc = 0.9780 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:34:38.326782: step 141800, loss = 0.0935, acc = 0.9680 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 11:34:43.191505: step 141820, loss = 0.1123, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 11:34:47.882933: step 141840, loss = 0.0843, acc = 0.9760 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 11:34:52.562489: step 141860, loss = 0.1190, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 11:34:57.439923: step 141880, loss = 0.0911, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 11:35:01.981260: step 141900, loss = 0.0743, acc = 0.9840 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 11:35:06.477470: step 141920, loss = 0.0901, acc = 0.9780 (297.5 examples/sec; 0.215 sec/batch)
2017-05-09 11:35:11.298901: step 141940, loss = 0.0952, acc = 0.9760 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 11:35:15.932623: step 141960, loss = 0.1123, acc = 0.9640 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 11:35:20.553156: step 141980, loss = 0.0892, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 11:35:25.216508: step 142000, loss = 0.1021, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 11:35:39.195521: step 142000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 11:35:48.784680: step 142000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 11:35:48.784785: step 142000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:35:53.283885: step 142020, loss = 0.0916, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 11:35:57.930898: step 142040, loss = 0.0845, acc = 0.9800 (297.0 examples/sec; 0.215 sec/batch)
2017-05-09 11:36:02.517879: step 142060, loss = 0.0950, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 11:36:07.122171: step 142080, loss = 0.0929, acc = 0.9820 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 11:36:11.847471: step 142100, loss = 0.1047, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 11:36:16.461720: step 142120, loss = 0.0827, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:36:22.123898: step 142140, loss = 0.1020, acc = 0.9720 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 11:36:26.775360: step 142160, loss = 0.1129, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 11:36:31.423883: step 142180, loss = 0.0900, acc = 0.9820 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 11:36:36.070409: step 142200, loss = 0.1000, acc = 0.9760 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 11:36:40.842802: step 142220, loss = 0.0769, acc = 0.9840 (246.2 examples/sec; 0.260 sec/batch)
2017-05-09 11:36:45.595295: step 142240, loss = 0.0743, acc = 0.9800 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 11:36:50.221146: step 142260, loss = 0.0923, acc = 0.9720 (299.9 examples/sec; 0.213 sec/batch)
2017-05-09 11:36:54.876105: step 142280, loss = 0.1123, acc = 0.9640 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 11:36:59.656290: step 142300, loss = 0.0835, acc = 0.9840 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 11:37:04.378236: step 142320, loss = 0.0875, acc = 0.9820 (251.1 examples/sec; 0.255 sec/batch)
2017-05-09 11:37:09.060234: step 142340, loss = 0.0843, acc = 0.9800 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 11:37:13.846679: step 142360, loss = 0.1004, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:37:18.434725: step 142380, loss = 0.0873, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 11:37:23.097446: step 142400, loss = 0.0874, acc = 0.9740 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 11:37:27.893288: step 142420, loss = 0.1017, acc = 0.9700 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 11:37:32.483025: step 142440, loss = 0.0794, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 11:37:36.992640: step 142460, loss = 0.0985, acc = 0.9660 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 11:37:41.533367: step 142480, loss = 0.1089, acc = 0.9680 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:37:46.207504: step 142500, loss = 0.1385, acc = 0.9700 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 11:37:50.863247: step 142520, loss = 0.0802, acc = 0.9800 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 11:37:55.469925: step 142540, loss = 0.0834, acc = 0.9760 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 11:38:00.046768: step 142560, loss = 0.0748, acc = 0.9860 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 11:38:04.664995: step 142580, loss = 0.1165, acc = 0.9620 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:38:09.234527: step 142600, loss = 0.0811, acc = 0.9820 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 11:38:14.075496: step 142620, loss = 0.1075, acc = 0.9680 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 11:38:18.774925: step 142640, loss = 0.0919, acc = 0.9720 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 11:38:23.386727: step 142660, loss = 0.1013, acc = 0.9800 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 11:38:28.114744: step 142680, loss = 0.0844, acc = 0.9740 (235.6 examples/sec; 0.272 sec/batch)
2017-05-09 11:38:32.711899: step 142700, loss = 0.1220, acc = 0.9640 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 11:38:37.322113: step 142720, loss = 0.0828, acc = 0.9840 (297.9 examples/sec; 0.215 sec/batch)
2017-05-09 11:38:41.975735: step 142740, loss = 0.0807, acc = 0.9800 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 11:38:46.703780: step 142760, loss = 0.0983, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 11:38:51.251308: step 142780, loss = 0.0811, acc = 0.9800 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 11:38:55.919037: step 142800, loss = 0.1058, acc = 0.9720 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 11:39:00.630384: step 142820, loss = 0.1323, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 11:39:05.171841: step 142840, loss = 0.1002, acc = 0.9800 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 11:39:09.831164: step 142860, loss = 0.1000, acc = 0.9700 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 11:39:14.571892: step 142880, loss = 0.1140, acc = 0.9660 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 11:39:19.180150: step 142900, loss = 0.1114, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:39:23.864006: step 142920, loss = 0.0903, acc = 0.9840 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 11:39:28.708749: step 142940, loss = 0.0986, acc = 0.9740 (210.1 examples/sec; 0.305 sec/batch)
2017-05-09 11:39:33.268248: step 142960, loss = 0.1122, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:39:37.946098: step 142980, loss = 0.1013, acc = 0.9740 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 11:39:42.628286: step 143000, loss = 0.0986, acc = 0.9680 (268.5 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 11:39:57.375382: step 143000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 11:40:06.887455: step 143000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 11:40:06.887555: step 143000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:40:11.592299: step 143020, loss = 0.1120, acc = 0.9620 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 11:40:16.143423: step 143040, loss = 0.0943, acc = 0.9720 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 11:40:20.702389: step 143060, loss = 0.0832, acc = 0.9820 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 11:40:25.392367: step 143080, loss = 0.0935, acc = 0.9700 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 11:40:30.197101: step 143100, loss = 0.1343, acc = 0.9540 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 11:40:34.772872: step 143120, loss = 0.0927, acc = 0.9680 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 11:40:40.258009: step 143140, loss = 0.0842, acc = 0.9780 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 11:40:44.899016: step 143160, loss = 0.1078, acc = 0.9720 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 11:40:49.510680: step 143180, loss = 0.0954, acc = 0.9660 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 11:40:54.144496: step 143200, loss = 0.0913, acc = 0.9820 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 11:40:58.731013: step 143220, loss = 0.0934, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 11:41:03.357949: step 143240, loss = 0.0820, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:41:07.930767: step 143260, loss = 0.1113, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:41:12.519262: step 143280, loss = 0.1137, acc = 0.9720 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 11:41:17.158378: step 143300, loss = 0.1101, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:41:21.690043: step 143320, loss = 0.0956, acc = 0.9740 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 11:41:26.428865: step 143340, loss = 0.1075, acc = 0.9740 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 11:41:31.117959: step 143360, loss = 0.0801, acc = 0.9760 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 11:41:35.652033: step 143380, loss = 0.1092, acc = 0.9760 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 11:41:40.366874: step 143400, loss = 0.1020, acc = 0.9740 (232.6 examples/sec; 0.275 sec/batch)
2017-05-09 11:41:45.154828: step 143420, loss = 0.0778, acc = 0.9840 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 11:41:49.742824: step 143440, loss = 0.0815, acc = 0.9780 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 11:41:54.428929: step 143460, loss = 0.0954, acc = 0.9820 (249.1 examples/sec; 0.257 sec/batch)
2017-05-09 11:41:59.079739: step 143480, loss = 0.0944, acc = 0.9720 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 11:42:03.623489: step 143500, loss = 0.0907, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 11:42:08.225476: step 143520, loss = 0.0761, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 11:42:12.971246: step 143540, loss = 0.0852, acc = 0.9820 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 11:42:17.706455: step 143560, loss = 0.0982, acc = 0.9720 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 11:42:22.400794: step 143580, loss = 0.0767, acc = 0.9840 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 11:42:27.243900: step 143600, loss = 0.1254, acc = 0.9500 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 11:42:31.796511: step 143620, loss = 0.0940, acc = 0.9700 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 11:42:36.396760: step 143640, loss = 0.1245, acc = 0.9560 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 11:42:41.215299: step 143660, loss = 0.0982, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 11:42:45.746302: step 143680, loss = 0.1092, acc = 0.9600 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:42:50.283439: step 143700, loss = 0.0975, acc = 0.9680 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:42:55.038740: step 143720, loss = 0.1083, acc = 0.9660 (234.7 examples/sec; 0.273 sec/batch)
2017-05-09 11:42:59.664372: step 143740, loss = 0.0982, acc = 0.9720 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 11:43:04.133680: step 143760, loss = 0.0912, acc = 0.9840 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 11:43:08.934168: step 143780, loss = 0.1208, acc = 0.9640 (242.7 examples/sec; 0.264 sec/batch)
2017-05-09 11:43:13.537782: step 143800, loss = 0.1186, acc = 0.9560 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 11:43:18.182062: step 143820, loss = 0.0853, acc = 0.9900 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 11:43:22.742814: step 143840, loss = 0.0945, acc = 0.9760 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 11:43:27.828287: step 143860, loss = 0.0935, acc = 0.9760 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 11:43:32.512351: step 143880, loss = 0.1041, acc = 0.9700 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:43:37.123148: step 143900, loss = 0.0935, acc = 0.9700 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 11:43:41.867913: step 143920, loss = 0.1144, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 11:43:46.545213: step 143940, loss = 0.0878, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 11:43:51.180027: step 143960, loss = 0.1065, acc = 0.9680 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 11:43:55.804215: step 143980, loss = 0.1156, acc = 0.9640 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 11:44:00.399911: step 144000, loss = 0.1047, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 11:44:14.726891: step 144000, acc = 0.9634, f1 = 0.9622
[Test] 2017-05-09 11:44:24.698608: step 144000, acc = 0.9541, f1 = 0.9538
[Status] 2017-05-09 11:44:24.698709: step 144000, maxindex = 137000, maxdev = 0.9648, maxtst = 0.9566
2017-05-09 11:44:29.276672: step 144020, loss = 0.1085, acc = 0.9640 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 11:44:33.821826: step 144040, loss = 0.0908, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 11:44:38.540047: step 144060, loss = 0.0967, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 11:44:42.958955: step 144080, loss = 0.0987, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 11:44:47.421890: step 144100, loss = 0.0956, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 11:44:52.196238: step 144120, loss = 0.1095, acc = 0.9660 (233.6 examples/sec; 0.274 sec/batch)
2017-05-09 11:44:57.774092: step 144140, loss = 0.0870, acc = 0.9660 (134.1 examples/sec; 0.477 sec/batch)
2017-05-09 11:45:02.418711: step 144160, loss = 0.1072, acc = 0.9760 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 11:45:07.268932: step 144180, loss = 0.0979, acc = 0.9740 (223.5 examples/sec; 0.286 sec/batch)
2017-05-09 11:45:11.774118: step 144200, loss = 0.0821, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:45:16.282624: step 144220, loss = 0.0869, acc = 0.9760 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 11:45:20.914704: step 144240, loss = 0.0992, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 11:45:25.533085: step 144260, loss = 0.1070, acc = 0.9660 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 11:45:30.339400: step 144280, loss = 0.0959, acc = 0.9660 (246.8 examples/sec; 0.259 sec/batch)
2017-05-09 11:45:34.925793: step 144300, loss = 0.1031, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 11:45:39.618246: step 144320, loss = 0.0946, acc = 0.9760 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 11:45:44.304777: step 144340, loss = 0.0841, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 11:45:49.076867: step 144360, loss = 0.0957, acc = 0.9680 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 11:45:53.715273: step 144380, loss = 0.0893, acc = 0.9700 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 11:45:58.386000: step 144400, loss = 0.1229, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 11:46:03.163983: step 144420, loss = 0.0772, acc = 0.9840 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 11:46:08.095748: step 144440, loss = 0.1267, acc = 0.9580 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 11:46:12.760514: step 144460, loss = 0.1065, acc = 0.9640 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 11:46:17.378093: step 144480, loss = 0.1160, acc = 0.9660 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 11:46:21.974857: step 144500, loss = 0.1034, acc = 0.9720 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 11:46:26.606358: step 144520, loss = 0.0758, acc = 0.9860 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 11:46:31.568876: step 144540, loss = 0.1062, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 11:46:36.432780: step 144560, loss = 0.1233, acc = 0.9600 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 11:46:41.014633: step 144580, loss = 0.1088, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 11:46:45.631240: step 144600, loss = 0.1043, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 11:46:50.331243: step 144620, loss = 0.1030, acc = 0.9700 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 11:46:54.823183: step 144640, loss = 0.0864, acc = 0.9780 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 11:46:59.652143: step 144660, loss = 0.1050, acc = 0.9740 (236.2 examples/sec; 0.271 sec/batch)
2017-05-09 11:47:04.415356: step 144680, loss = 0.0887, acc = 0.9820 (237.5 examples/sec; 0.269 sec/batch)
2017-05-09 11:47:09.047906: step 144700, loss = 0.0970, acc = 0.9720 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 11:47:13.635819: step 144720, loss = 0.1044, acc = 0.9720 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 11:47:18.222251: step 144740, loss = 0.1061, acc = 0.9680 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 11:47:23.100848: step 144760, loss = 0.0761, acc = 0.9740 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:47:27.652718: step 144780, loss = 0.0984, acc = 0.9760 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 11:47:32.146250: step 144800, loss = 0.1009, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 11:47:36.917740: step 144820, loss = 0.0922, acc = 0.9720 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 11:47:41.497305: step 144840, loss = 0.0948, acc = 0.9820 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 11:47:46.132735: step 144860, loss = 0.0909, acc = 0.9780 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 11:47:50.800341: step 144880, loss = 0.1154, acc = 0.9740 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:47:55.325545: step 144900, loss = 0.0816, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 11:48:00.107205: step 144920, loss = 0.0844, acc = 0.9800 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 11:48:04.889168: step 144940, loss = 0.0871, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 11:48:09.505306: step 144960, loss = 0.1020, acc = 0.9700 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 11:48:14.065613: step 144980, loss = 0.1038, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 11:48:18.844167: step 145000, loss = 0.1016, acc = 0.9740 (244.9 examples/sec; 0.261 sec/batch)
[Eval] 2017-05-09 11:48:32.499932: step 145000, acc = 0.9648, f1 = 0.9637
[Test] 2017-05-09 11:48:42.220012: step 145000, acc = 0.9561, f1 = 0.9558
[Status] 2017-05-09 11:48:42.220111: step 145000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 11:48:50.282990: step 145020, loss = 0.1015, acc = 0.9720 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 11:48:54.887567: step 145040, loss = 0.1088, acc = 0.9640 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 11:48:59.356757: step 145060, loss = 0.0875, acc = 0.9780 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 11:49:03.927255: step 145080, loss = 0.0844, acc = 0.9780 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 11:49:08.624858: step 145100, loss = 0.1018, acc = 0.9720 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 11:49:13.319507: step 145120, loss = 0.0858, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 11:49:17.924345: step 145140, loss = 0.1035, acc = 0.9740 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 11:49:23.542294: step 145160, loss = 0.1153, acc = 0.9620 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 11:49:28.060103: step 145180, loss = 0.0992, acc = 0.9780 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 11:49:32.772348: step 145200, loss = 0.0773, acc = 0.9780 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 11:49:37.645724: step 145220, loss = 0.0924, acc = 0.9780 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 11:49:42.171102: step 145240, loss = 0.1047, acc = 0.9720 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 11:49:46.784753: step 145260, loss = 0.0782, acc = 0.9880 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 11:49:51.517589: step 145280, loss = 0.1053, acc = 0.9700 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 11:49:56.153871: step 145300, loss = 0.0962, acc = 0.9760 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 11:50:00.689081: step 145320, loss = 0.1086, acc = 0.9700 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 11:50:05.554163: step 145340, loss = 0.0940, acc = 0.9640 (232.5 examples/sec; 0.275 sec/batch)
2017-05-09 11:50:10.087302: step 145360, loss = 0.0897, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 11:50:14.720092: step 145380, loss = 0.1036, acc = 0.9620 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 11:50:19.227952: step 145400, loss = 0.1084, acc = 0.9660 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:50:24.002838: step 145420, loss = 0.1216, acc = 0.9580 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 11:50:28.507488: step 145440, loss = 0.0908, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 11:50:33.491007: step 145460, loss = 0.1003, acc = 0.9720 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 11:50:38.500598: step 145480, loss = 0.1100, acc = 0.9680 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 11:50:43.127971: step 145500, loss = 0.1134, acc = 0.9680 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 11:50:47.692760: step 145520, loss = 0.1097, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 11:50:52.478171: step 145540, loss = 0.1265, acc = 0.9520 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 11:50:57.078082: step 145560, loss = 0.1034, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 11:51:01.610475: step 145580, loss = 0.1049, acc = 0.9780 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 11:51:06.299790: step 145600, loss = 0.1175, acc = 0.9660 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:51:10.843996: step 145620, loss = 0.1025, acc = 0.9780 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 11:51:15.488712: step 145640, loss = 0.0771, acc = 0.9860 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 11:51:20.038377: step 145660, loss = 0.1023, acc = 0.9680 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 11:51:24.785831: step 145680, loss = 0.0933, acc = 0.9780 (296.2 examples/sec; 0.216 sec/batch)
2017-05-09 11:51:29.430269: step 145700, loss = 0.0859, acc = 0.9740 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 11:51:34.039145: step 145720, loss = 0.0729, acc = 0.9840 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 11:51:38.726695: step 145740, loss = 0.0631, acc = 0.9880 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 11:51:43.356826: step 145760, loss = 0.0929, acc = 0.9760 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 11:51:47.995759: step 145780, loss = 0.0700, acc = 0.9880 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 11:51:52.867651: step 145800, loss = 0.1089, acc = 0.9680 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:51:57.464684: step 145820, loss = 0.1077, acc = 0.9720 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 11:52:02.076233: step 145840, loss = 0.1156, acc = 0.9520 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 11:52:06.894984: step 145860, loss = 0.1080, acc = 0.9720 (220.6 examples/sec; 0.290 sec/batch)
2017-05-09 11:52:11.442602: step 145880, loss = 0.1092, acc = 0.9700 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 11:52:16.047777: step 145900, loss = 0.1054, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 11:52:20.871084: step 145920, loss = 0.0839, acc = 0.9780 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 11:52:25.738932: step 145940, loss = 0.0873, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 11:52:30.417266: step 145960, loss = 0.1095, acc = 0.9620 (249.0 examples/sec; 0.257 sec/batch)
2017-05-09 11:52:35.000702: step 145980, loss = 0.0917, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 11:52:39.607581: step 146000, loss = 0.0876, acc = 0.9800 (283.2 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 11:52:53.766198: step 146000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 11:53:03.325070: step 146000, acc = 0.9566, f1 = 0.9563
[Status] 2017-05-09 11:53:03.325158: step 146000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 11:53:08.171676: step 146020, loss = 0.1075, acc = 0.9640 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 11:53:12.711143: step 146040, loss = 0.0736, acc = 0.9860 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 11:53:17.213917: step 146060, loss = 0.1124, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 11:53:22.142529: step 146080, loss = 0.1012, acc = 0.9700 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 11:53:26.786054: step 146100, loss = 0.1112, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 11:53:31.380356: step 146120, loss = 0.1004, acc = 0.9720 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 11:53:36.249632: step 146140, loss = 0.1115, acc = 0.9680 (217.4 examples/sec; 0.294 sec/batch)
2017-05-09 11:53:41.862325: step 146160, loss = 0.1144, acc = 0.9760 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 11:53:46.456513: step 146180, loss = 0.0866, acc = 0.9720 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 11:53:51.252931: step 146200, loss = 0.0908, acc = 0.9780 (302.6 examples/sec; 0.211 sec/batch)
2017-05-09 11:53:55.844967: step 146220, loss = 0.1228, acc = 0.9620 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 11:54:00.529617: step 146240, loss = 0.1186, acc = 0.9620 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 11:54:05.285835: step 146260, loss = 0.0953, acc = 0.9760 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 11:54:09.868611: step 146280, loss = 0.0928, acc = 0.9720 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:54:14.381013: step 146300, loss = 0.1018, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 11:54:19.002748: step 146320, loss = 0.1160, acc = 0.9580 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 11:54:23.854746: step 146340, loss = 0.1067, acc = 0.9540 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 11:54:28.504679: step 146360, loss = 0.0802, acc = 0.9860 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 11:54:33.119057: step 146380, loss = 0.0856, acc = 0.9840 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 11:54:37.730104: step 146400, loss = 0.0910, acc = 0.9740 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 11:54:42.309278: step 146420, loss = 0.0986, acc = 0.9680 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 11:54:46.978549: step 146440, loss = 0.1062, acc = 0.9680 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 11:54:51.810819: step 146460, loss = 0.0876, acc = 0.9820 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 11:54:56.345873: step 146480, loss = 0.1061, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 11:55:00.889186: step 146500, loss = 0.0816, acc = 0.9760 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 11:55:05.602907: step 146520, loss = 0.0876, acc = 0.9820 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 11:55:10.068019: step 146540, loss = 0.0844, acc = 0.9760 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 11:55:14.655586: step 146560, loss = 0.1011, acc = 0.9660 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 11:55:19.419093: step 146580, loss = 0.0713, acc = 0.9860 (239.0 examples/sec; 0.268 sec/batch)
2017-05-09 11:55:23.944445: step 146600, loss = 0.1123, acc = 0.9700 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 11:55:28.546582: step 146620, loss = 0.0835, acc = 0.9780 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 11:55:33.123119: step 146640, loss = 0.1270, acc = 0.9560 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 11:55:37.823198: step 146660, loss = 0.1166, acc = 0.9680 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 11:55:42.476679: step 146680, loss = 0.0839, acc = 0.9800 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 11:55:47.014684: step 146700, loss = 0.1145, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 11:55:51.583573: step 146720, loss = 0.1029, acc = 0.9760 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 11:55:56.180085: step 146740, loss = 0.0831, acc = 0.9840 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 11:56:00.658546: step 146760, loss = 0.0984, acc = 0.9760 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 11:56:05.370329: step 146780, loss = 0.0830, acc = 0.9840 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 11:56:09.911759: step 146800, loss = 0.0825, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 11:56:14.460694: step 146820, loss = 0.1017, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 11:56:19.168839: step 146840, loss = 0.1035, acc = 0.9620 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 11:56:23.829927: step 146860, loss = 0.1025, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 11:56:28.443970: step 146880, loss = 0.0894, acc = 0.9820 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:56:33.304300: step 146900, loss = 0.0855, acc = 0.9700 (221.8 examples/sec; 0.289 sec/batch)
2017-05-09 11:56:37.992221: step 146920, loss = 0.0951, acc = 0.9740 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 11:56:42.479732: step 146940, loss = 0.1099, acc = 0.9660 (298.9 examples/sec; 0.214 sec/batch)
2017-05-09 11:56:47.206636: step 146960, loss = 0.0891, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 11:56:51.854359: step 146980, loss = 0.1050, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 11:56:56.514876: step 147000, loss = 0.1102, acc = 0.9600 (289.5 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 11:57:10.671643: step 147000, acc = 0.9647, f1 = 0.9636
[Test] 2017-05-09 11:57:20.496109: step 147000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 11:57:20.496212: step 147000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 11:57:25.053810: step 147020, loss = 0.0749, acc = 0.9840 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 11:57:29.751980: step 147040, loss = 0.0981, acc = 0.9760 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 11:57:34.441405: step 147060, loss = 0.0813, acc = 0.9840 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 11:57:39.123783: step 147080, loss = 0.1013, acc = 0.9700 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 11:57:43.812669: step 147100, loss = 0.1133, acc = 0.9720 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 11:57:48.539769: step 147120, loss = 0.0779, acc = 0.9820 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 11:57:53.115623: step 147140, loss = 0.1134, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 11:57:57.662008: step 147160, loss = 0.0862, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 11:58:03.155959: step 147180, loss = 0.0909, acc = 0.9720 (247.8 examples/sec; 0.258 sec/batch)
2017-05-09 11:58:07.778201: step 147200, loss = 0.1095, acc = 0.9700 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 11:58:12.352640: step 147220, loss = 0.0986, acc = 0.9640 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 11:58:17.158575: step 147240, loss = 0.0793, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 11:58:21.685103: step 147260, loss = 0.1103, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 11:58:26.256737: step 147280, loss = 0.0824, acc = 0.9800 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 11:58:30.920239: step 147300, loss = 0.1032, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 11:58:35.698530: step 147320, loss = 0.1176, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 11:58:40.347489: step 147340, loss = 0.0988, acc = 0.9700 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 11:58:44.965708: step 147360, loss = 0.1067, acc = 0.9660 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 11:58:49.848427: step 147380, loss = 0.0999, acc = 0.9700 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 11:58:54.412280: step 147400, loss = 0.0861, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 11:58:59.027095: step 147420, loss = 0.0913, acc = 0.9800 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 11:59:03.858898: step 147440, loss = 0.0886, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 11:59:08.467475: step 147460, loss = 0.0783, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 11:59:13.022597: step 147480, loss = 0.0884, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 11:59:17.893301: step 147500, loss = 0.1102, acc = 0.9600 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 11:59:22.364344: step 147520, loss = 0.1009, acc = 0.9700 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 11:59:26.965076: step 147540, loss = 0.0885, acc = 0.9800 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 11:59:31.702040: step 147560, loss = 0.0968, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 11:59:36.165738: step 147580, loss = 0.1156, acc = 0.9620 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 11:59:40.682417: step 147600, loss = 0.0993, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 11:59:45.494769: step 147620, loss = 0.1024, acc = 0.9700 (226.0 examples/sec; 0.283 sec/batch)
2017-05-09 11:59:50.145645: step 147640, loss = 0.0974, acc = 0.9780 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 11:59:54.875089: step 147660, loss = 0.0770, acc = 0.9800 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 11:59:59.435137: step 147680, loss = 0.0897, acc = 0.9780 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 12:00:04.206242: step 147700, loss = 0.1007, acc = 0.9740 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 12:00:08.799023: step 147720, loss = 0.1057, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 12:00:13.468310: step 147740, loss = 0.1112, acc = 0.9720 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 12:00:18.206778: step 147760, loss = 0.0953, acc = 0.9820 (297.5 examples/sec; 0.215 sec/batch)
2017-05-09 12:00:22.741720: step 147780, loss = 0.1106, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 12:00:27.338668: step 147800, loss = 0.0910, acc = 0.9700 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 12:00:31.942271: step 147820, loss = 0.1149, acc = 0.9680 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 12:00:36.466526: step 147840, loss = 0.0737, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 12:00:41.053372: step 147860, loss = 0.1039, acc = 0.9640 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 12:00:45.866406: step 147880, loss = 0.1105, acc = 0.9700 (234.5 examples/sec; 0.273 sec/batch)
2017-05-09 12:00:50.655463: step 147900, loss = 0.0815, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 12:00:55.249370: step 147920, loss = 0.0854, acc = 0.9680 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 12:00:59.836627: step 147940, loss = 0.0906, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 12:01:04.665253: step 147960, loss = 0.0703, acc = 0.9840 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 12:01:09.118716: step 147980, loss = 0.0765, acc = 0.9780 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 12:01:13.833361: step 148000, loss = 0.1006, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 12:01:27.870822: step 148000, acc = 0.9634, f1 = 0.9621
[Test] 2017-05-09 12:01:37.594800: step 148000, acc = 0.9536, f1 = 0.9532
[Status] 2017-05-09 12:01:37.594900: step 148000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 12:01:42.194811: step 148020, loss = 0.0808, acc = 0.9820 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 12:01:46.830563: step 148040, loss = 0.0834, acc = 0.9860 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 12:01:51.322981: step 148060, loss = 0.0897, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 12:01:55.951261: step 148080, loss = 0.0951, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 12:02:00.594467: step 148100, loss = 0.1219, acc = 0.9620 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 12:02:05.387315: step 148120, loss = 0.0927, acc = 0.9760 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 12:02:09.996682: step 148140, loss = 0.0896, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 12:02:14.593694: step 148160, loss = 0.0932, acc = 0.9780 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 12:02:20.310841: step 148180, loss = 0.0943, acc = 0.9620 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 12:02:24.836673: step 148200, loss = 0.1093, acc = 0.9560 (299.5 examples/sec; 0.214 sec/batch)
2017-05-09 12:02:29.715820: step 148220, loss = 0.0767, acc = 0.9840 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 12:02:34.462825: step 148240, loss = 0.1092, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 12:02:38.940820: step 148260, loss = 0.0942, acc = 0.9740 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 12:02:43.582336: step 148280, loss = 0.0959, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 12:02:48.366971: step 148300, loss = 0.1086, acc = 0.9540 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 12:02:53.004441: step 148320, loss = 0.0658, acc = 0.9920 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 12:02:57.698760: step 148340, loss = 0.0958, acc = 0.9760 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 12:03:02.364849: step 148360, loss = 0.1147, acc = 0.9600 (237.6 examples/sec; 0.269 sec/batch)
2017-05-09 12:03:06.834776: step 148380, loss = 0.0919, acc = 0.9700 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 12:03:11.436348: step 148400, loss = 0.0843, acc = 0.9800 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:03:16.135138: step 148420, loss = 0.0813, acc = 0.9820 (259.1 examples/sec; 0.247 sec/batch)
2017-05-09 12:03:20.825010: step 148440, loss = 0.1051, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 12:03:25.406855: step 148460, loss = 0.1010, acc = 0.9660 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 12:03:30.064682: step 148480, loss = 0.1045, acc = 0.9720 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 12:03:34.820521: step 148500, loss = 0.0807, acc = 0.9800 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 12:03:39.386532: step 148520, loss = 0.0910, acc = 0.9760 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 12:03:44.137541: step 148540, loss = 0.0971, acc = 0.9760 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 12:03:48.827187: step 148560, loss = 0.1078, acc = 0.9640 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:03:53.405142: step 148580, loss = 0.0973, acc = 0.9720 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 12:03:58.029434: step 148600, loss = 0.1025, acc = 0.9780 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 12:04:02.911399: step 148620, loss = 0.0773, acc = 0.9840 (234.9 examples/sec; 0.272 sec/batch)
2017-05-09 12:04:07.509244: step 148640, loss = 0.0860, acc = 0.9800 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 12:04:12.144433: step 148660, loss = 0.1019, acc = 0.9640 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 12:04:16.926002: step 148680, loss = 0.0820, acc = 0.9720 (223.0 examples/sec; 0.287 sec/batch)
2017-05-09 12:04:21.533289: step 148700, loss = 0.1385, acc = 0.9600 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 12:04:26.085773: step 148720, loss = 0.0890, acc = 0.9740 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 12:04:30.877452: step 148740, loss = 0.0887, acc = 0.9780 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 12:04:35.491463: step 148760, loss = 0.1085, acc = 0.9680 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 12:04:40.088197: step 148780, loss = 0.0948, acc = 0.9580 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 12:04:44.733421: step 148800, loss = 0.0827, acc = 0.9740 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 12:04:49.491965: step 148820, loss = 0.0948, acc = 0.9780 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 12:04:54.246482: step 148840, loss = 0.1158, acc = 0.9660 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 12:04:58.866362: step 148860, loss = 0.0879, acc = 0.9760 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 12:05:03.601164: step 148880, loss = 0.1090, acc = 0.9780 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 12:05:08.169325: step 148900, loss = 0.0945, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 12:05:12.804521: step 148920, loss = 0.0961, acc = 0.9720 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 12:05:17.578119: step 148940, loss = 0.0991, acc = 0.9740 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 12:05:22.066227: step 148960, loss = 0.1143, acc = 0.9600 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 12:05:26.684977: step 148980, loss = 0.0987, acc = 0.9780 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 12:05:31.154706: step 149000, loss = 0.1064, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 12:05:45.474428: step 149000, acc = 0.9647, f1 = 0.9636
[Test] 2017-05-09 12:05:54.807544: step 149000, acc = 0.9564, f1 = 0.9560
[Status] 2017-05-09 12:05:54.807635: step 149000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 12:05:59.370376: step 149020, loss = 0.1078, acc = 0.9700 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 12:06:04.067304: step 149040, loss = 0.0867, acc = 0.9780 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 12:06:08.857900: step 149060, loss = 0.0850, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 12:06:13.562865: step 149080, loss = 0.0988, acc = 0.9680 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 12:06:18.307415: step 149100, loss = 0.0905, acc = 0.9720 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 12:06:22.929725: step 149120, loss = 0.1287, acc = 0.9600 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 12:06:27.453728: step 149140, loss = 0.0860, acc = 0.9740 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 12:06:32.165195: step 149160, loss = 0.0740, acc = 0.9860 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 12:06:37.519785: step 149180, loss = 0.1047, acc = 0.9660 (163.4 examples/sec; 0.392 sec/batch)
2017-05-09 12:06:42.150169: step 149200, loss = 0.0709, acc = 0.9880 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 12:06:46.880943: step 149220, loss = 0.0778, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 12:06:51.487365: step 149240, loss = 0.1257, acc = 0.9560 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:06:56.041895: step 149260, loss = 0.0893, acc = 0.9840 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:07:00.860907: step 149280, loss = 0.0954, acc = 0.9840 (238.5 examples/sec; 0.268 sec/batch)
2017-05-09 12:07:05.391877: step 149300, loss = 0.0913, acc = 0.9760 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 12:07:09.994941: step 149320, loss = 0.0691, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 12:07:14.667831: step 149340, loss = 0.0774, acc = 0.9800 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 12:07:19.416801: step 149360, loss = 0.1073, acc = 0.9640 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 12:07:24.038416: step 149380, loss = 0.0914, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 12:07:28.582786: step 149400, loss = 0.0934, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 12:07:33.215634: step 149420, loss = 0.0906, acc = 0.9720 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 12:07:37.633289: step 149440, loss = 0.0849, acc = 0.9800 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 12:07:42.233685: step 149460, loss = 0.0913, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 12:07:47.053486: step 149480, loss = 0.0993, acc = 0.9720 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 12:07:51.684440: step 149500, loss = 0.1075, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 12:07:56.224939: step 149520, loss = 0.0817, acc = 0.9820 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 12:08:01.040539: step 149540, loss = 0.0908, acc = 0.9740 (227.4 examples/sec; 0.281 sec/batch)
2017-05-09 12:08:05.609186: step 149560, loss = 0.0866, acc = 0.9740 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 12:08:10.295492: step 149580, loss = 0.0833, acc = 0.9780 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 12:08:14.972879: step 149600, loss = 0.0876, acc = 0.9720 (242.9 examples/sec; 0.263 sec/batch)
2017-05-09 12:08:19.688327: step 149620, loss = 0.0885, acc = 0.9820 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 12:08:24.206841: step 149640, loss = 0.0921, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:08:28.726638: step 149660, loss = 0.1003, acc = 0.9680 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 12:08:33.435932: step 149680, loss = 0.0902, acc = 0.9700 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 12:08:37.881036: step 149700, loss = 0.0766, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 12:08:42.506266: step 149720, loss = 0.1007, acc = 0.9720 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 12:08:47.190188: step 149740, loss = 0.1027, acc = 0.9680 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 12:08:51.801609: step 149760, loss = 0.0862, acc = 0.9720 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 12:08:56.475316: step 149780, loss = 0.1047, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 12:09:01.504824: step 149800, loss = 0.0783, acc = 0.9760 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 12:09:06.077795: step 149820, loss = 0.0949, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 12:09:10.654492: step 149840, loss = 0.1371, acc = 0.9540 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 12:09:15.284736: step 149860, loss = 0.0866, acc = 0.9820 (251.1 examples/sec; 0.255 sec/batch)
2017-05-09 12:09:19.984902: step 149880, loss = 0.0868, acc = 0.9720 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 12:09:24.467760: step 149900, loss = 0.0853, acc = 0.9800 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 12:09:29.051065: step 149920, loss = 0.0917, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:09:33.755574: step 149940, loss = 0.1078, acc = 0.9740 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 12:09:38.392192: step 149960, loss = 0.0888, acc = 0.9780 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 12:09:43.001387: step 149980, loss = 0.1163, acc = 0.9680 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 12:09:47.911891: step 150000, loss = 0.0964, acc = 0.9780 (288.0 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 12:10:01.957490: step 150000, acc = 0.9632, f1 = 0.9620
[Test] 2017-05-09 12:10:11.270360: step 150000, acc = 0.9544, f1 = 0.9540
[Status] 2017-05-09 12:10:11.270447: step 150000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 12:10:15.950092: step 150020, loss = 0.0959, acc = 0.9780 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 12:10:20.571692: step 150040, loss = 0.1054, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 12:10:25.077364: step 150060, loss = 0.0941, acc = 0.9680 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 12:10:29.936168: step 150080, loss = 0.0976, acc = 0.9760 (252.8 examples/sec; 0.253 sec/batch)
2017-05-09 12:10:34.526074: step 150100, loss = 0.0992, acc = 0.9740 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 12:10:39.101403: step 150120, loss = 0.1044, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 12:10:44.112695: step 150140, loss = 0.1073, acc = 0.9660 (217.6 examples/sec; 0.294 sec/batch)
2017-05-09 12:10:48.890951: step 150160, loss = 0.1038, acc = 0.9640 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 12:10:53.676315: step 150180, loss = 0.0918, acc = 0.9660 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 12:10:59.473538: step 150200, loss = 0.0821, acc = 0.9840 (299.5 examples/sec; 0.214 sec/batch)
2017-05-09 12:11:04.171547: step 150220, loss = 0.0869, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:11:08.854099: step 150240, loss = 0.0954, acc = 0.9740 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 12:11:13.603236: step 150260, loss = 0.0945, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 12:11:18.240703: step 150280, loss = 0.0836, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 12:11:22.817080: step 150300, loss = 0.1046, acc = 0.9800 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 12:11:27.515748: step 150320, loss = 0.0791, acc = 0.9860 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 12:11:32.034258: step 150340, loss = 0.1015, acc = 0.9720 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 12:11:36.648281: step 150360, loss = 0.1006, acc = 0.9640 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 12:11:41.309318: step 150380, loss = 0.0985, acc = 0.9800 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 12:11:46.370512: step 150400, loss = 0.1081, acc = 0.9700 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 12:11:51.015313: step 150420, loss = 0.0931, acc = 0.9720 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 12:11:55.593129: step 150440, loss = 0.1142, acc = 0.9680 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 12:12:00.222109: step 150460, loss = 0.1118, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 12:12:04.804699: step 150480, loss = 0.1053, acc = 0.9680 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 12:12:09.431280: step 150500, loss = 0.0862, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 12:12:14.261369: step 150520, loss = 0.0869, acc = 0.9700 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 12:12:18.722602: step 150540, loss = 0.0979, acc = 0.9660 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 12:12:23.358460: step 150560, loss = 0.1186, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 12:12:28.254845: step 150580, loss = 0.0985, acc = 0.9740 (236.6 examples/sec; 0.271 sec/batch)
2017-05-09 12:12:32.800330: step 150600, loss = 0.0907, acc = 0.9800 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 12:12:37.384349: step 150620, loss = 0.0751, acc = 0.9820 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 12:12:42.223707: step 150640, loss = 0.0954, acc = 0.9780 (233.1 examples/sec; 0.275 sec/batch)
2017-05-09 12:12:46.710757: step 150660, loss = 0.0727, acc = 0.9880 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:12:51.334911: step 150680, loss = 0.0923, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 12:12:56.159587: step 150700, loss = 0.0936, acc = 0.9800 (240.2 examples/sec; 0.266 sec/batch)
2017-05-09 12:13:00.659770: step 150720, loss = 0.1015, acc = 0.9740 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 12:13:05.246434: step 150740, loss = 0.1035, acc = 0.9720 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 12:13:09.924680: step 150760, loss = 0.1036, acc = 0.9640 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 12:13:14.838510: step 150780, loss = 0.0820, acc = 0.9760 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 12:13:19.414588: step 150800, loss = 0.0879, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 12:13:23.901406: step 150820, loss = 0.0914, acc = 0.9720 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 12:13:28.557216: step 150840, loss = 0.0982, acc = 0.9660 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 12:13:32.981312: step 150860, loss = 0.0688, acc = 0.9900 (300.0 examples/sec; 0.213 sec/batch)
2017-05-09 12:13:37.436861: step 150880, loss = 0.1110, acc = 0.9640 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 12:13:41.993929: step 150900, loss = 0.0924, acc = 0.9700 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:13:46.592880: step 150920, loss = 0.0980, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 12:13:51.144452: step 150940, loss = 0.0967, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 12:13:55.800214: step 150960, loss = 0.1009, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:14:00.427128: step 150980, loss = 0.1215, acc = 0.9540 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 12:14:05.053287: step 151000, loss = 0.0863, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 12:14:19.210490: step 151000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 12:14:29.167099: step 151000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 12:14:29.167189: step 151000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 12:14:33.744081: step 151020, loss = 0.0824, acc = 0.9860 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 12:14:38.382024: step 151040, loss = 0.1029, acc = 0.9640 (254.2 examples/sec; 0.252 sec/batch)
2017-05-09 12:14:42.845881: step 151060, loss = 0.0977, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 12:14:47.409277: step 151080, loss = 0.0726, acc = 0.9820 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 12:14:52.043129: step 151100, loss = 0.1045, acc = 0.9740 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 12:14:56.805398: step 151120, loss = 0.1007, acc = 0.9700 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 12:15:01.382302: step 151140, loss = 0.0788, acc = 0.9800 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 12:15:05.963173: step 151160, loss = 0.1002, acc = 0.9660 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:15:10.830268: step 151180, loss = 0.0777, acc = 0.9960 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 12:15:16.097411: step 151200, loss = 0.0966, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 12:15:20.686757: step 151220, loss = 0.0661, acc = 0.9940 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 12:15:25.422186: step 151240, loss = 0.0989, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 12:15:29.972665: step 151260, loss = 0.0812, acc = 0.9700 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 12:15:34.646037: step 151280, loss = 0.0902, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 12:15:39.354908: step 151300, loss = 0.1178, acc = 0.9640 (299.7 examples/sec; 0.214 sec/batch)
2017-05-09 12:15:44.040574: step 151320, loss = 0.0838, acc = 0.9780 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:15:48.698654: step 151340, loss = 0.0991, acc = 0.9740 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 12:15:53.485137: step 151360, loss = 0.0813, acc = 0.9820 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 12:15:58.022619: step 151380, loss = 0.0756, acc = 0.9860 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 12:16:02.527610: step 151400, loss = 0.0855, acc = 0.9780 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 12:16:07.111007: step 151420, loss = 0.0823, acc = 0.9820 (267.2 examples/sec; 0.239 sec/batch)
2017-05-09 12:16:11.820219: step 151440, loss = 0.0977, acc = 0.9720 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:16:16.456143: step 151460, loss = 0.0818, acc = 0.9820 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 12:16:21.106801: step 151480, loss = 0.1088, acc = 0.9660 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 12:16:25.978245: step 151500, loss = 0.0949, acc = 0.9720 (257.3 examples/sec; 0.249 sec/batch)
2017-05-09 12:16:30.546628: step 151520, loss = 0.1083, acc = 0.9720 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 12:16:35.137515: step 151540, loss = 0.0848, acc = 0.9820 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:16:39.814959: step 151560, loss = 0.0663, acc = 0.9860 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 12:16:44.362132: step 151580, loss = 0.0841, acc = 0.9760 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 12:16:48.984896: step 151600, loss = 0.0969, acc = 0.9660 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 12:16:54.002046: step 151620, loss = 0.0786, acc = 0.9800 (188.3 examples/sec; 0.340 sec/batch)
2017-05-09 12:16:58.672001: step 151640, loss = 0.1182, acc = 0.9580 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 12:17:03.213694: step 151660, loss = 0.0987, acc = 0.9740 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:17:07.847247: step 151680, loss = 0.1000, acc = 0.9620 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 12:17:12.615754: step 151700, loss = 0.0832, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 12:17:17.184391: step 151720, loss = 0.0844, acc = 0.9800 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 12:17:21.838194: step 151740, loss = 0.0914, acc = 0.9800 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:17:26.412372: step 151760, loss = 0.0775, acc = 0.9800 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:17:31.017191: step 151780, loss = 0.0807, acc = 0.9840 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 12:17:35.681542: step 151800, loss = 0.0888, acc = 0.9780 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 12:17:40.270695: step 151820, loss = 0.0699, acc = 0.9900 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 12:17:45.114127: step 151840, loss = 0.0923, acc = 0.9720 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 12:17:49.796240: step 151860, loss = 0.0850, acc = 0.9800 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 12:17:54.506618: step 151880, loss = 0.1023, acc = 0.9720 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 12:17:59.148353: step 151900, loss = 0.0910, acc = 0.9720 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 12:18:03.736596: step 151920, loss = 0.0946, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 12:18:08.398855: step 151940, loss = 0.0787, acc = 0.9800 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 12:18:13.211860: step 151960, loss = 0.0981, acc = 0.9720 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 12:18:17.881172: step 151980, loss = 0.1063, acc = 0.9600 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 12:18:22.644748: step 152000, loss = 0.0919, acc = 0.9700 (266.1 examples/sec; 0.241 sec/batch)
[Eval] 2017-05-09 12:18:36.945268: step 152000, acc = 0.9636, f1 = 0.9623
[Test] 2017-05-09 12:18:46.690458: step 152000, acc = 0.9543, f1 = 0.9539
[Status] 2017-05-09 12:18:46.690562: step 152000, maxindex = 145000, maxdev = 0.9648, maxtst = 0.9561
2017-05-09 12:18:51.289093: step 152020, loss = 0.0903, acc = 0.9720 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 12:18:56.177972: step 152040, loss = 0.1093, acc = 0.9660 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 12:19:00.794442: step 152060, loss = 0.0789, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 12:19:05.389302: step 152080, loss = 0.0744, acc = 0.9880 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 12:19:10.197739: step 152100, loss = 0.0923, acc = 0.9700 (222.6 examples/sec; 0.288 sec/batch)
2017-05-09 12:19:14.783522: step 152120, loss = 0.0890, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 12:19:19.546781: step 152140, loss = 0.1024, acc = 0.9740 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 12:19:24.281838: step 152160, loss = 0.0906, acc = 0.9740 (246.0 examples/sec; 0.260 sec/batch)
2017-05-09 12:19:28.843312: step 152180, loss = 0.0908, acc = 0.9640 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 12:19:33.632642: step 152200, loss = 0.0990, acc = 0.9640 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 12:19:39.353177: step 152220, loss = 0.0702, acc = 0.9860 (247.0 examples/sec; 0.259 sec/batch)
2017-05-09 12:19:43.927647: step 152240, loss = 0.0819, acc = 0.9800 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 12:19:48.496494: step 152260, loss = 0.0832, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 12:19:53.029353: step 152280, loss = 0.0952, acc = 0.9740 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 12:19:57.651439: step 152300, loss = 0.0935, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 12:20:02.282630: step 152320, loss = 0.0903, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 12:20:06.974004: step 152340, loss = 0.1107, acc = 0.9640 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 12:20:11.707426: step 152360, loss = 0.0773, acc = 0.9820 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 12:20:16.329788: step 152380, loss = 0.0995, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 12:20:20.862096: step 152400, loss = 0.0669, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 12:20:25.570742: step 152420, loss = 0.0930, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 12:20:30.171443: step 152440, loss = 0.0933, acc = 0.9800 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 12:20:34.890828: step 152460, loss = 0.1156, acc = 0.9540 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 12:20:39.694128: step 152480, loss = 0.0814, acc = 0.9820 (236.7 examples/sec; 0.270 sec/batch)
2017-05-09 12:20:44.393470: step 152500, loss = 0.1109, acc = 0.9640 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 12:20:49.077292: step 152520, loss = 0.0991, acc = 0.9680 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 12:20:53.711501: step 152540, loss = 0.0934, acc = 0.9720 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 12:20:58.419957: step 152560, loss = 0.1078, acc = 0.9620 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 12:21:02.913091: step 152580, loss = 0.0981, acc = 0.9700 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 12:21:07.432319: step 152600, loss = 0.0805, acc = 0.9800 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 12:21:12.201332: step 152620, loss = 0.1014, acc = 0.9700 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 12:21:17.069399: step 152640, loss = 0.1083, acc = 0.9700 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 12:21:21.753918: step 152660, loss = 0.0852, acc = 0.9780 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 12:21:26.526206: step 152680, loss = 0.0864, acc = 0.9820 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 12:21:31.162233: step 152700, loss = 0.0748, acc = 0.9800 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 12:21:36.001949: step 152720, loss = 0.0819, acc = 0.9800 (253.5 examples/sec; 0.252 sec/batch)
2017-05-09 12:21:40.756931: step 152740, loss = 0.0820, acc = 0.9820 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 12:21:45.494499: step 152760, loss = 0.1033, acc = 0.9780 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 12:21:50.078406: step 152780, loss = 0.1147, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 12:21:54.645919: step 152800, loss = 0.1034, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 12:21:59.463714: step 152820, loss = 0.0885, acc = 0.9740 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 12:22:04.324676: step 152840, loss = 0.0646, acc = 0.9840 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 12:22:09.165017: step 152860, loss = 0.0848, acc = 0.9780 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 12:22:13.779044: step 152880, loss = 0.1106, acc = 0.9640 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:22:18.502361: step 152900, loss = 0.0673, acc = 0.9900 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 12:22:23.135567: step 152920, loss = 0.0751, acc = 0.9860 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 12:22:27.895290: step 152940, loss = 0.0828, acc = 0.9880 (245.2 examples/sec; 0.261 sec/batch)
2017-05-09 12:22:32.552423: step 152960, loss = 0.0843, acc = 0.9820 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 12:22:37.061888: step 152980, loss = 0.0692, acc = 0.9860 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 12:22:41.670428: step 153000, loss = 0.0939, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 12:22:55.942155: step 153000, acc = 0.9649, f1 = 0.9637
[Test] 2017-05-09 12:23:05.672699: step 153000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 12:23:05.672790: step 153000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:23:13.717636: step 153020, loss = 0.0943, acc = 0.9760 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 12:23:18.297718: step 153040, loss = 0.1090, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 12:23:22.798887: step 153060, loss = 0.0994, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:23:27.560693: step 153080, loss = 0.0879, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 12:23:32.059067: step 153100, loss = 0.0818, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:23:36.729491: step 153120, loss = 0.0791, acc = 0.9880 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 12:23:41.500554: step 153140, loss = 0.1202, acc = 0.9700 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 12:23:46.028188: step 153160, loss = 0.0790, acc = 0.9760 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 12:23:50.698660: step 153180, loss = 0.1017, acc = 0.9740 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 12:23:55.428731: step 153200, loss = 0.0940, acc = 0.9720 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 12:24:01.189220: step 153220, loss = 0.0739, acc = 0.9820 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 12:24:05.788346: step 153240, loss = 0.1167, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 12:24:10.374661: step 153260, loss = 0.0853, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 12:24:15.119307: step 153280, loss = 0.1201, acc = 0.9640 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 12:24:19.686098: step 153300, loss = 0.1081, acc = 0.9600 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 12:24:24.367849: step 153320, loss = 0.0847, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 12:24:29.186629: step 153340, loss = 0.0832, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 12:24:33.906589: step 153360, loss = 0.0790, acc = 0.9760 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 12:24:38.546639: step 153380, loss = 0.1128, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 12:24:43.286199: step 153400, loss = 0.0897, acc = 0.9720 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 12:24:47.923153: step 153420, loss = 0.0955, acc = 0.9720 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 12:24:52.680875: step 153440, loss = 0.0730, acc = 0.9860 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 12:24:57.433238: step 153460, loss = 0.1058, acc = 0.9780 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 12:25:02.183346: step 153480, loss = 0.0791, acc = 0.9800 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 12:25:06.826836: step 153500, loss = 0.1183, acc = 0.9640 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 12:25:11.483744: step 153520, loss = 0.1071, acc = 0.9680 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 12:25:16.145524: step 153540, loss = 0.1121, acc = 0.9700 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:25:20.737948: step 153560, loss = 0.0990, acc = 0.9680 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:25:25.373981: step 153580, loss = 0.0955, acc = 0.9760 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 12:25:29.999612: step 153600, loss = 0.0757, acc = 0.9840 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 12:25:34.443422: step 153620, loss = 0.0800, acc = 0.9860 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 12:25:39.000281: step 153640, loss = 0.0951, acc = 0.9740 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 12:25:43.511900: step 153660, loss = 0.1073, acc = 0.9720 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 12:25:48.050944: step 153680, loss = 0.1108, acc = 0.9580 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 12:25:52.522838: step 153700, loss = 0.1022, acc = 0.9720 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 12:25:57.269293: step 153720, loss = 0.0911, acc = 0.9720 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 12:26:01.906094: step 153740, loss = 0.1112, acc = 0.9700 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 12:26:06.459165: step 153760, loss = 0.1049, acc = 0.9740 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 12:26:11.163481: step 153780, loss = 0.0918, acc = 0.9740 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 12:26:15.870369: step 153800, loss = 0.0937, acc = 0.9680 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 12:26:20.498074: step 153820, loss = 0.1026, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 12:26:25.172727: step 153840, loss = 0.0962, acc = 0.9680 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 12:26:29.809111: step 153860, loss = 0.0804, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 12:26:34.426409: step 153880, loss = 0.0966, acc = 0.9800 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 12:26:39.152602: step 153900, loss = 0.0945, acc = 0.9800 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 12:26:43.631459: step 153920, loss = 0.0838, acc = 0.9820 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 12:26:48.201314: step 153940, loss = 0.0875, acc = 0.9820 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 12:26:52.743731: step 153960, loss = 0.0754, acc = 0.9780 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 12:26:57.449263: step 153980, loss = 0.0930, acc = 0.9800 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 12:27:02.056103: step 154000, loss = 0.1054, acc = 0.9660 (276.0 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 12:27:16.330110: step 154000, acc = 0.9628, f1 = 0.9615
[Test] 2017-05-09 12:27:26.074292: step 154000, acc = 0.9534, f1 = 0.9530
[Status] 2017-05-09 12:27:26.074392: step 154000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:27:30.656408: step 154020, loss = 0.0841, acc = 0.9840 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 12:27:35.369781: step 154040, loss = 0.0772, acc = 0.9840 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 12:27:40.124567: step 154060, loss = 0.0879, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:27:44.812289: step 154080, loss = 0.0810, acc = 0.9820 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 12:27:49.437682: step 154100, loss = 0.0959, acc = 0.9680 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 12:27:54.201531: step 154120, loss = 0.1057, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:27:58.739784: step 154140, loss = 0.0798, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 12:28:03.477810: step 154160, loss = 0.0781, acc = 0.9880 (251.0 examples/sec; 0.255 sec/batch)
2017-05-09 12:28:08.384742: step 154180, loss = 0.0946, acc = 0.9700 (210.0 examples/sec; 0.305 sec/batch)
2017-05-09 12:28:13.031377: step 154200, loss = 0.1028, acc = 0.9720 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 12:28:18.668045: step 154220, loss = 0.0741, acc = 0.9820 (136.7 examples/sec; 0.468 sec/batch)
2017-05-09 12:28:23.413504: step 154240, loss = 0.0837, acc = 0.9840 (247.7 examples/sec; 0.258 sec/batch)
2017-05-09 12:28:27.878776: step 154260, loss = 0.1322, acc = 0.9680 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 12:28:32.391751: step 154280, loss = 0.0915, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 12:28:37.128048: step 154300, loss = 0.0906, acc = 0.9740 (222.8 examples/sec; 0.287 sec/batch)
2017-05-09 12:28:41.631043: step 154320, loss = 0.0802, acc = 0.9800 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 12:28:46.279466: step 154340, loss = 0.0875, acc = 0.9840 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 12:28:50.866416: step 154360, loss = 0.1094, acc = 0.9700 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 12:28:55.659758: step 154380, loss = 0.1181, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:29:00.337647: step 154400, loss = 0.0832, acc = 0.9860 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 12:29:05.034083: step 154420, loss = 0.0941, acc = 0.9680 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 12:29:09.908261: step 154440, loss = 0.0899, acc = 0.9740 (228.5 examples/sec; 0.280 sec/batch)
2017-05-09 12:29:14.532751: step 154460, loss = 0.0875, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 12:29:19.165494: step 154480, loss = 0.0844, acc = 0.9800 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 12:29:23.901210: step 154500, loss = 0.1048, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 12:29:28.350927: step 154520, loss = 0.0855, acc = 0.9760 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 12:29:32.906128: step 154540, loss = 0.1090, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:29:37.641201: step 154560, loss = 0.0899, acc = 0.9760 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 12:29:42.269660: step 154580, loss = 0.0847, acc = 0.9720 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 12:29:47.152653: step 154600, loss = 0.0962, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:29:51.920952: step 154620, loss = 0.0936, acc = 0.9780 (246.6 examples/sec; 0.259 sec/batch)
2017-05-09 12:29:56.606406: step 154640, loss = 0.0995, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 12:30:01.225304: step 154660, loss = 0.0936, acc = 0.9800 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 12:30:05.937509: step 154680, loss = 0.0963, acc = 0.9740 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 12:30:10.870702: step 154700, loss = 0.0927, acc = 0.9720 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 12:30:15.547550: step 154720, loss = 0.0724, acc = 0.9820 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 12:30:20.120929: step 154740, loss = 0.0760, acc = 0.9800 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 12:30:24.910672: step 154760, loss = 0.0797, acc = 0.9820 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 12:30:29.523328: step 154780, loss = 0.1066, acc = 0.9680 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 12:30:34.233912: step 154800, loss = 0.0821, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 12:30:39.039318: step 154820, loss = 0.0954, acc = 0.9680 (234.7 examples/sec; 0.273 sec/batch)
2017-05-09 12:30:43.508832: step 154840, loss = 0.0857, acc = 0.9800 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:30:48.201693: step 154860, loss = 0.1277, acc = 0.9600 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 12:30:53.116868: step 154880, loss = 0.0704, acc = 0.9840 (248.8 examples/sec; 0.257 sec/batch)
2017-05-09 12:30:57.683161: step 154900, loss = 0.0864, acc = 0.9840 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 12:31:02.274491: step 154920, loss = 0.0930, acc = 0.9700 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 12:31:06.926309: step 154940, loss = 0.0752, acc = 0.9780 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 12:31:11.543199: step 154960, loss = 0.1225, acc = 0.9540 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 12:31:16.485428: step 154980, loss = 0.1012, acc = 0.9720 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 12:31:21.027754: step 155000, loss = 0.0875, acc = 0.9780 (286.3 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 12:31:35.444343: step 155000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 12:31:45.196468: step 155000, acc = 0.9560, f1 = 0.9557
[Status] 2017-05-09 12:31:45.196564: step 155000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:31:49.820791: step 155020, loss = 0.0833, acc = 0.9780 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 12:31:54.705919: step 155040, loss = 0.0678, acc = 0.9860 (254.3 examples/sec; 0.252 sec/batch)
2017-05-09 12:31:59.539218: step 155060, loss = 0.0974, acc = 0.9780 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 12:32:04.415479: step 155080, loss = 0.0744, acc = 0.9800 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:32:09.296959: step 155100, loss = 0.1057, acc = 0.9700 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 12:32:13.820980: step 155120, loss = 0.0992, acc = 0.9780 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 12:32:18.425799: step 155140, loss = 0.1190, acc = 0.9560 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 12:32:22.977135: step 155160, loss = 0.1151, acc = 0.9620 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 12:32:27.731618: step 155180, loss = 0.1139, acc = 0.9720 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 12:32:32.359410: step 155200, loss = 0.0755, acc = 0.9780 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 12:32:36.979815: step 155220, loss = 0.0825, acc = 0.9760 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 12:32:42.511379: step 155240, loss = 0.0796, acc = 0.9800 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 12:32:47.079805: step 155260, loss = 0.0824, acc = 0.9740 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:32:51.577967: step 155280, loss = 0.0871, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 12:32:56.519264: step 155300, loss = 0.0867, acc = 0.9780 (207.6 examples/sec; 0.308 sec/batch)
2017-05-09 12:33:01.048143: step 155320, loss = 0.0920, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 12:33:05.563696: step 155340, loss = 0.0784, acc = 0.9760 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 12:33:10.177773: step 155360, loss = 0.1046, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 12:33:14.648368: step 155380, loss = 0.0861, acc = 0.9760 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 12:33:19.123379: step 155400, loss = 0.1220, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 12:33:23.884687: step 155420, loss = 0.0954, acc = 0.9760 (222.5 examples/sec; 0.288 sec/batch)
2017-05-09 12:33:28.523470: step 155440, loss = 0.0953, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 12:33:33.139750: step 155460, loss = 0.0921, acc = 0.9740 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 12:33:37.797220: step 155480, loss = 0.0929, acc = 0.9820 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 12:33:42.604184: step 155500, loss = 0.0883, acc = 0.9740 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 12:33:47.322532: step 155520, loss = 0.1048, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:33:51.803013: step 155540, loss = 0.1155, acc = 0.9720 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 12:33:56.579933: step 155560, loss = 0.0975, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 12:34:01.165337: step 155580, loss = 0.0930, acc = 0.9720 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 12:34:06.097233: step 155600, loss = 0.1018, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:34:10.982715: step 155620, loss = 0.0806, acc = 0.9720 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 12:34:15.587831: step 155640, loss = 0.0803, acc = 0.9840 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 12:34:20.273763: step 155660, loss = 0.0789, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 12:34:24.997743: step 155680, loss = 0.0912, acc = 0.9680 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 12:34:29.529112: step 155700, loss = 0.1066, acc = 0.9700 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 12:34:34.210314: step 155720, loss = 0.1054, acc = 0.9680 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 12:34:38.930818: step 155740, loss = 0.0894, acc = 0.9820 (234.2 examples/sec; 0.273 sec/batch)
2017-05-09 12:34:43.479104: step 155760, loss = 0.1042, acc = 0.9720 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 12:34:48.062690: step 155780, loss = 0.1267, acc = 0.9620 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:34:52.673049: step 155800, loss = 0.0968, acc = 0.9820 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 12:34:57.503994: step 155820, loss = 0.0991, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 12:35:02.132691: step 155840, loss = 0.1032, acc = 0.9680 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 12:35:06.721445: step 155860, loss = 0.0917, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 12:35:11.451039: step 155880, loss = 0.1058, acc = 0.9720 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 12:35:16.130795: step 155900, loss = 0.0930, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 12:35:20.674111: step 155920, loss = 0.0921, acc = 0.9780 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:35:25.342791: step 155940, loss = 0.0974, acc = 0.9700 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 12:35:29.944004: step 155960, loss = 0.0793, acc = 0.9840 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 12:35:34.535602: step 155980, loss = 0.0940, acc = 0.9640 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 12:35:39.548543: step 156000, loss = 0.1030, acc = 0.9620 (290.6 examples/sec; 0.220 sec/batch)
[Eval] 2017-05-09 12:35:53.796449: step 156000, acc = 0.9643, f1 = 0.9630
[Test] 2017-05-09 12:36:03.101933: step 156000, acc = 0.9548, f1 = 0.9545
[Status] 2017-05-09 12:36:03.102033: step 156000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:36:07.804438: step 156020, loss = 0.0989, acc = 0.9740 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 12:36:12.648238: step 156040, loss = 0.1158, acc = 0.9640 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 12:36:17.290048: step 156060, loss = 0.1387, acc = 0.9540 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 12:36:21.997189: step 156080, loss = 0.1073, acc = 0.9700 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 12:36:26.832569: step 156100, loss = 0.1010, acc = 0.9640 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 12:36:31.371881: step 156120, loss = 0.0957, acc = 0.9780 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 12:36:36.062219: step 156140, loss = 0.0832, acc = 0.9820 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 12:36:40.875914: step 156160, loss = 0.1106, acc = 0.9660 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 12:36:45.428913: step 156180, loss = 0.1017, acc = 0.9760 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 12:36:50.074132: step 156200, loss = 0.0864, acc = 0.9780 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 12:36:54.927732: step 156220, loss = 0.0959, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 12:37:00.642638: step 156240, loss = 0.0829, acc = 0.9800 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 12:37:05.177485: step 156260, loss = 0.0711, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 12:37:09.887074: step 156280, loss = 0.0901, acc = 0.9800 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 12:37:14.449379: step 156300, loss = 0.1014, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 12:37:18.989049: step 156320, loss = 0.0882, acc = 0.9780 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 12:37:23.656490: step 156340, loss = 0.0871, acc = 0.9740 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 12:37:28.278404: step 156360, loss = 0.0966, acc = 0.9820 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 12:37:32.924509: step 156380, loss = 0.1065, acc = 0.9720 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 12:37:37.679305: step 156400, loss = 0.0957, acc = 0.9700 (247.9 examples/sec; 0.258 sec/batch)
2017-05-09 12:37:42.421627: step 156420, loss = 0.1133, acc = 0.9620 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 12:37:47.096666: step 156440, loss = 0.0829, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 12:37:51.928851: step 156460, loss = 0.0989, acc = 0.9800 (235.1 examples/sec; 0.272 sec/batch)
2017-05-09 12:37:56.461951: step 156480, loss = 0.0949, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 12:38:01.046332: step 156500, loss = 0.0899, acc = 0.9760 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 12:38:05.563337: step 156520, loss = 0.1161, acc = 0.9740 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 12:38:10.271060: step 156540, loss = 0.0748, acc = 0.9840 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 12:38:14.888154: step 156560, loss = 0.0876, acc = 0.9780 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 12:38:19.444600: step 156580, loss = 0.1158, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 12:38:24.016034: step 156600, loss = 0.0957, acc = 0.9800 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 12:38:28.576387: step 156620, loss = 0.0890, acc = 0.9740 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:38:33.134992: step 156640, loss = 0.0864, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 12:38:37.910710: step 156660, loss = 0.1008, acc = 0.9760 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 12:38:42.477517: step 156680, loss = 0.0983, acc = 0.9780 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 12:38:47.059639: step 156700, loss = 0.0901, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 12:38:51.954966: step 156720, loss = 0.0861, acc = 0.9840 (223.2 examples/sec; 0.287 sec/batch)
2017-05-09 12:38:56.624332: step 156740, loss = 0.0695, acc = 0.9880 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 12:39:01.284136: step 156760, loss = 0.1007, acc = 0.9780 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 12:39:05.837089: step 156780, loss = 0.1042, acc = 0.9680 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 12:39:10.633430: step 156800, loss = 0.1166, acc = 0.9600 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:39:15.213604: step 156820, loss = 0.0781, acc = 0.9860 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 12:39:20.008124: step 156840, loss = 0.0977, acc = 0.9840 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 12:39:24.710791: step 156860, loss = 0.1020, acc = 0.9680 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 12:39:29.385334: step 156880, loss = 0.0917, acc = 0.9680 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 12:39:33.987356: step 156900, loss = 0.0913, acc = 0.9740 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 12:39:38.766047: step 156920, loss = 0.0832, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 12:39:43.436583: step 156940, loss = 0.1012, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 12:39:47.981952: step 156960, loss = 0.0789, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 12:39:52.748752: step 156980, loss = 0.1197, acc = 0.9680 (221.9 examples/sec; 0.288 sec/batch)
2017-05-09 12:39:57.408246: step 157000, loss = 0.0879, acc = 0.9820 (280.8 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 12:40:11.600995: step 157000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 12:40:20.830033: step 157000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 12:40:20.830137: step 157000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:40:25.679319: step 157020, loss = 0.1041, acc = 0.9580 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 12:40:30.275151: step 157040, loss = 0.0943, acc = 0.9780 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 12:40:34.844684: step 157060, loss = 0.1020, acc = 0.9740 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:40:39.657779: step 157080, loss = 0.1006, acc = 0.9620 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 12:40:44.181163: step 157100, loss = 0.1049, acc = 0.9620 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 12:40:48.735872: step 157120, loss = 0.0767, acc = 0.9800 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 12:40:53.478219: step 157140, loss = 0.0965, acc = 0.9720 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 12:40:58.026180: step 157160, loss = 0.0837, acc = 0.9840 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 12:41:02.706237: step 157180, loss = 0.0846, acc = 0.9840 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 12:41:07.346851: step 157200, loss = 0.0832, acc = 0.9740 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 12:41:11.898609: step 157220, loss = 0.1192, acc = 0.9780 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 12:41:16.388668: step 157240, loss = 0.0758, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 12:41:22.065647: step 157260, loss = 0.0791, acc = 0.9840 (231.8 examples/sec; 0.276 sec/batch)
2017-05-09 12:41:26.738233: step 157280, loss = 0.0885, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:41:31.374572: step 157300, loss = 0.0827, acc = 0.9800 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 12:41:36.081830: step 157320, loss = 0.0990, acc = 0.9720 (235.0 examples/sec; 0.272 sec/batch)
2017-05-09 12:41:40.764945: step 157340, loss = 0.0808, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 12:41:45.387383: step 157360, loss = 0.1147, acc = 0.9640 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 12:41:49.959434: step 157380, loss = 0.1068, acc = 0.9640 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 12:41:54.567000: step 157400, loss = 0.0937, acc = 0.9700 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 12:41:59.111251: step 157420, loss = 0.0963, acc = 0.9720 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:42:03.654891: step 157440, loss = 0.1120, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 12:42:08.316665: step 157460, loss = 0.0957, acc = 0.9720 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 12:42:12.870998: step 157480, loss = 0.1174, acc = 0.9600 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 12:42:17.380116: step 157500, loss = 0.0885, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 12:42:22.239875: step 157520, loss = 0.0968, acc = 0.9760 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 12:42:26.910113: step 157540, loss = 0.0983, acc = 0.9680 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 12:42:31.822424: step 157560, loss = 0.0940, acc = 0.9720 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 12:42:36.568623: step 157580, loss = 0.0981, acc = 0.9700 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 12:42:41.155768: step 157600, loss = 0.1007, acc = 0.9740 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 12:42:45.649044: step 157620, loss = 0.1004, acc = 0.9640 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 12:42:50.372823: step 157640, loss = 0.0851, acc = 0.9740 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 12:42:54.840462: step 157660, loss = 0.0810, acc = 0.9820 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 12:42:59.688903: step 157680, loss = 0.1063, acc = 0.9720 (215.4 examples/sec; 0.297 sec/batch)
2017-05-09 12:43:04.302221: step 157700, loss = 0.1066, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 12:43:09.068238: step 157720, loss = 0.0918, acc = 0.9680 (259.0 examples/sec; 0.247 sec/batch)
2017-05-09 12:43:13.755685: step 157740, loss = 0.1046, acc = 0.9700 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 12:43:18.298170: step 157760, loss = 0.1392, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:43:23.066017: step 157780, loss = 0.1033, acc = 0.9720 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 12:43:27.579435: step 157800, loss = 0.0880, acc = 0.9800 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 12:43:32.212888: step 157820, loss = 0.1091, acc = 0.9740 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 12:43:36.945791: step 157840, loss = 0.1044, acc = 0.9660 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 12:43:41.540598: step 157860, loss = 0.0817, acc = 0.9800 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 12:43:46.078955: step 157880, loss = 0.0969, acc = 0.9700 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 12:43:50.785836: step 157900, loss = 0.1149, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 12:43:55.412147: step 157920, loss = 0.0748, acc = 0.9840 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 12:43:59.988658: step 157940, loss = 0.0794, acc = 0.9860 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 12:44:04.441431: step 157960, loss = 0.1144, acc = 0.9640 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 12:44:09.121230: step 157980, loss = 0.1001, acc = 0.9720 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 12:44:13.756974: step 158000, loss = 0.1036, acc = 0.9700 (264.6 examples/sec; 0.242 sec/batch)
[Eval] 2017-05-09 12:44:28.080746: step 158000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 12:44:37.934530: step 158000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 12:44:37.934629: step 158000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:44:42.643544: step 158020, loss = 0.0808, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 12:44:47.217676: step 158040, loss = 0.0876, acc = 0.9760 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 12:44:51.774276: step 158060, loss = 0.1274, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 12:44:56.531098: step 158080, loss = 0.1041, acc = 0.9740 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 12:45:01.185500: step 158100, loss = 0.0755, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 12:45:05.907265: step 158120, loss = 0.1075, acc = 0.9660 (226.2 examples/sec; 0.283 sec/batch)
2017-05-09 12:45:10.563996: step 158140, loss = 0.0868, acc = 0.9820 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 12:45:15.170643: step 158160, loss = 0.1117, acc = 0.9720 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 12:45:20.092050: step 158180, loss = 0.0671, acc = 0.9900 (236.4 examples/sec; 0.271 sec/batch)
2017-05-09 12:45:24.843356: step 158200, loss = 0.0971, acc = 0.9760 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 12:45:29.488018: step 158220, loss = 0.1112, acc = 0.9680 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 12:45:34.127524: step 158240, loss = 0.0923, acc = 0.9680 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 12:45:39.884114: step 158260, loss = 0.1185, acc = 0.9700 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 12:45:44.500053: step 158280, loss = 0.0985, acc = 0.9660 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 12:45:49.032628: step 158300, loss = 0.1183, acc = 0.9640 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 12:45:53.854270: step 158320, loss = 0.0763, acc = 0.9820 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 12:45:58.369252: step 158340, loss = 0.1060, acc = 0.9700 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 12:46:02.941533: step 158360, loss = 0.1226, acc = 0.9660 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 12:46:08.002587: step 158380, loss = 0.0989, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 12:46:12.642883: step 158400, loss = 0.1069, acc = 0.9620 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 12:46:17.302555: step 158420, loss = 0.1196, acc = 0.9560 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 12:46:22.275893: step 158440, loss = 0.1066, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 12:46:26.841436: step 158460, loss = 0.0860, acc = 0.9800 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 12:46:31.548881: step 158480, loss = 0.1120, acc = 0.9620 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 12:46:36.185317: step 158500, loss = 0.1171, acc = 0.9560 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 12:46:40.746306: step 158520, loss = 0.0978, acc = 0.9820 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:46:45.448894: step 158540, loss = 0.1017, acc = 0.9640 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 12:46:50.345243: step 158560, loss = 0.0639, acc = 0.9920 (228.7 examples/sec; 0.280 sec/batch)
2017-05-09 12:46:54.923481: step 158580, loss = 0.0792, acc = 0.9820 (257.1 examples/sec; 0.249 sec/batch)
2017-05-09 12:46:59.500648: step 158600, loss = 0.0695, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 12:47:04.067784: step 158620, loss = 0.1027, acc = 0.9740 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 12:47:08.857300: step 158640, loss = 0.0867, acc = 0.9780 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 12:47:13.494266: step 158660, loss = 0.0799, acc = 0.9840 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 12:47:18.120155: step 158680, loss = 0.0838, acc = 0.9820 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 12:47:22.906477: step 158700, loss = 0.0749, acc = 0.9840 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 12:47:27.430286: step 158720, loss = 0.0834, acc = 0.9800 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 12:47:32.298093: step 158740, loss = 0.1029, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 12:47:37.042732: step 158760, loss = 0.0943, acc = 0.9700 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 12:47:41.876195: step 158780, loss = 0.0974, acc = 0.9640 (239.1 examples/sec; 0.268 sec/batch)
2017-05-09 12:47:46.609808: step 158800, loss = 0.0843, acc = 0.9760 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 12:47:51.415592: step 158820, loss = 0.0909, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:47:56.064924: step 158840, loss = 0.1066, acc = 0.9680 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 12:48:00.769881: step 158860, loss = 0.0856, acc = 0.9720 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 12:48:05.395458: step 158880, loss = 0.0778, acc = 0.9820 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 12:48:09.954233: step 158900, loss = 0.0835, acc = 0.9760 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 12:48:14.498179: step 158920, loss = 0.0916, acc = 0.9680 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 12:48:19.074690: step 158940, loss = 0.0997, acc = 0.9740 (288.9 examples/sec; 0.221 sec/batch)
2017-05-09 12:48:23.492406: step 158960, loss = 0.0819, acc = 0.9840 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 12:48:28.108373: step 158980, loss = 0.1059, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 12:48:32.862096: step 159000, loss = 0.0898, acc = 0.9760 (292.5 examples/sec; 0.219 sec/batch)
[Eval] 2017-05-09 12:48:46.821882: step 159000, acc = 0.9646, f1 = 0.9633
[Test] 2017-05-09 12:48:56.343593: step 159000, acc = 0.9551, f1 = 0.9547
[Status] 2017-05-09 12:48:56.343686: step 159000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:49:01.024910: step 159020, loss = 0.1244, acc = 0.9600 (258.6 examples/sec; 0.248 sec/batch)
2017-05-09 12:49:05.525911: step 159040, loss = 0.0998, acc = 0.9740 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 12:49:10.203153: step 159060, loss = 0.1004, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 12:49:14.930708: step 159080, loss = 0.1209, acc = 0.9660 (229.0 examples/sec; 0.280 sec/batch)
2017-05-09 12:49:19.621309: step 159100, loss = 0.0992, acc = 0.9740 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 12:49:24.209372: step 159120, loss = 0.1051, acc = 0.9740 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 12:49:28.818409: step 159140, loss = 0.0984, acc = 0.9820 (258.4 examples/sec; 0.248 sec/batch)
2017-05-09 12:49:33.669686: step 159160, loss = 0.0959, acc = 0.9720 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 12:49:38.322471: step 159180, loss = 0.1029, acc = 0.9720 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 12:49:42.995182: step 159200, loss = 0.0834, acc = 0.9780 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 12:49:47.605306: step 159220, loss = 0.1016, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 12:49:52.175177: step 159240, loss = 0.0951, acc = 0.9700 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 12:49:57.519394: step 159260, loss = 0.0769, acc = 0.9800 (156.4 examples/sec; 0.409 sec/batch)
2017-05-09 12:50:02.268108: step 159280, loss = 0.0912, acc = 0.9800 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 12:50:06.868106: step 159300, loss = 0.0803, acc = 0.9780 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:50:11.556275: step 159320, loss = 0.0829, acc = 0.9780 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 12:50:16.230761: step 159340, loss = 0.0826, acc = 0.9780 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 12:50:20.770199: step 159360, loss = 0.0898, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 12:50:25.377706: step 159380, loss = 0.0888, acc = 0.9760 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 12:50:30.128619: step 159400, loss = 0.1155, acc = 0.9660 (245.7 examples/sec; 0.260 sec/batch)
2017-05-09 12:50:34.662219: step 159420, loss = 0.1065, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 12:50:39.257044: step 159440, loss = 0.0922, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 12:50:43.926400: step 159460, loss = 0.0973, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 12:50:48.610982: step 159480, loss = 0.0899, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 12:50:53.176156: step 159500, loss = 0.0858, acc = 0.9840 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 12:50:57.762084: step 159520, loss = 0.0930, acc = 0.9720 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 12:51:02.562990: step 159540, loss = 0.0882, acc = 0.9780 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 12:51:07.142892: step 159560, loss = 0.0929, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 12:51:11.669299: step 159580, loss = 0.0812, acc = 0.9720 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 12:51:16.462235: step 159600, loss = 0.0759, acc = 0.9860 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 12:51:21.060498: step 159620, loss = 0.0800, acc = 0.9840 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 12:51:25.700327: step 159640, loss = 0.0884, acc = 0.9780 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 12:51:30.441085: step 159660, loss = 0.1117, acc = 0.9740 (242.0 examples/sec; 0.264 sec/batch)
2017-05-09 12:51:35.080474: step 159680, loss = 0.0949, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 12:51:39.650865: step 159700, loss = 0.0887, acc = 0.9820 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 12:51:44.260764: step 159720, loss = 0.0866, acc = 0.9780 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:51:48.848623: step 159740, loss = 0.0980, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 12:51:53.443261: step 159760, loss = 0.0811, acc = 0.9840 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 12:51:58.043676: step 159780, loss = 0.0965, acc = 0.9760 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 12:52:02.845505: step 159800, loss = 0.0992, acc = 0.9780 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:52:07.559974: step 159820, loss = 0.0688, acc = 0.9860 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 12:52:12.171151: step 159840, loss = 0.0875, acc = 0.9800 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 12:52:16.975907: step 159860, loss = 0.0962, acc = 0.9740 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 12:52:21.563967: step 159880, loss = 0.0895, acc = 0.9760 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 12:52:26.333123: step 159900, loss = 0.0830, acc = 0.9820 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 12:52:31.244594: step 159920, loss = 0.0924, acc = 0.9800 (204.1 examples/sec; 0.313 sec/batch)
2017-05-09 12:52:35.866473: step 159940, loss = 0.0928, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 12:52:40.477531: step 159960, loss = 0.1123, acc = 0.9560 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 12:52:45.162721: step 159980, loss = 0.0870, acc = 0.9800 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 12:52:49.807992: step 160000, loss = 0.0949, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 12:53:03.951648: step 160000, acc = 0.9649, f1 = 0.9637
[Test] 2017-05-09 12:53:13.574031: step 160000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 12:53:13.574134: step 160000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:53:18.419130: step 160020, loss = 0.0860, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 12:53:22.992060: step 160040, loss = 0.0973, acc = 0.9720 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 12:53:27.586078: step 160060, loss = 0.0949, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 12:53:32.192322: step 160080, loss = 0.0964, acc = 0.9700 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 12:53:36.853809: step 160100, loss = 0.0920, acc = 0.9800 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 12:53:41.447365: step 160120, loss = 0.0984, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 12:53:45.995431: step 160140, loss = 0.0866, acc = 0.9800 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 12:53:50.626806: step 160160, loss = 0.0845, acc = 0.9820 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 12:53:55.146237: step 160180, loss = 0.0961, acc = 0.9780 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 12:53:59.744459: step 160200, loss = 0.0972, acc = 0.9700 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 12:54:04.419901: step 160220, loss = 0.0956, acc = 0.9700 (299.5 examples/sec; 0.214 sec/batch)
2017-05-09 12:54:08.889325: step 160240, loss = 0.0678, acc = 0.9880 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 12:54:13.490181: step 160260, loss = 0.1031, acc = 0.9720 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 12:54:19.109420: step 160280, loss = 0.0815, acc = 0.9820 (298.7 examples/sec; 0.214 sec/batch)
2017-05-09 12:54:23.679791: step 160300, loss = 0.1062, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:54:28.320226: step 160320, loss = 0.1100, acc = 0.9600 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 12:54:33.254611: step 160340, loss = 0.0938, acc = 0.9760 (216.8 examples/sec; 0.295 sec/batch)
2017-05-09 12:54:37.870559: step 160360, loss = 0.0926, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 12:54:42.412357: step 160380, loss = 0.1059, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 12:54:47.062467: step 160400, loss = 0.0922, acc = 0.9740 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 12:54:51.712315: step 160420, loss = 0.1507, acc = 0.9500 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 12:54:56.452286: step 160440, loss = 0.0912, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 12:55:00.987210: step 160460, loss = 0.1125, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 12:55:05.685056: step 160480, loss = 0.0826, acc = 0.9840 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 12:55:10.217725: step 160500, loss = 0.0936, acc = 0.9740 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 12:55:14.791426: step 160520, loss = 0.1134, acc = 0.9660 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 12:55:19.509054: step 160540, loss = 0.0787, acc = 0.9820 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 12:55:24.057488: step 160560, loss = 0.0953, acc = 0.9800 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 12:55:28.640909: step 160580, loss = 0.1158, acc = 0.9660 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 12:55:33.416418: step 160600, loss = 0.0924, acc = 0.9780 (229.7 examples/sec; 0.279 sec/batch)
2017-05-09 12:55:38.015895: step 160620, loss = 0.1044, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 12:55:42.605950: step 160640, loss = 0.0710, acc = 0.9880 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 12:55:47.071436: step 160660, loss = 0.0791, acc = 0.9820 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 12:55:51.749404: step 160680, loss = 0.0843, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 12:55:56.420760: step 160700, loss = 0.0655, acc = 0.9900 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 12:56:01.049689: step 160720, loss = 0.0797, acc = 0.9800 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:56:05.862732: step 160740, loss = 0.1109, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 12:56:10.781551: step 160760, loss = 0.0677, acc = 0.9860 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 12:56:15.331021: step 160780, loss = 0.1118, acc = 0.9720 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 12:56:20.043385: step 160800, loss = 0.0859, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:56:24.592964: step 160820, loss = 0.0884, acc = 0.9720 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:56:29.098683: step 160840, loss = 0.0760, acc = 0.9840 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 12:56:33.693728: step 160860, loss = 0.0821, acc = 0.9840 (305.2 examples/sec; 0.210 sec/batch)
2017-05-09 12:56:38.253697: step 160880, loss = 0.0809, acc = 0.9820 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 12:56:42.724045: step 160900, loss = 0.1204, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 12:56:47.487115: step 160920, loss = 0.1233, acc = 0.9560 (241.5 examples/sec; 0.265 sec/batch)
2017-05-09 12:56:52.175068: step 160940, loss = 0.1083, acc = 0.9740 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 12:56:56.820847: step 160960, loss = 0.0980, acc = 0.9760 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 12:57:01.443123: step 160980, loss = 0.1063, acc = 0.9760 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 12:57:06.314523: step 161000, loss = 0.1157, acc = 0.9580 (280.9 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 12:57:20.349087: step 161000, acc = 0.9641, f1 = 0.9629
[Test] 2017-05-09 12:57:29.912645: step 161000, acc = 0.9551, f1 = 0.9547
[Status] 2017-05-09 12:57:29.912786: step 161000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 12:57:34.589540: step 161020, loss = 0.0826, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 12:57:39.257110: step 161040, loss = 0.0891, acc = 0.9780 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 12:57:44.016201: step 161060, loss = 0.0894, acc = 0.9760 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 12:57:48.851847: step 161080, loss = 0.0944, acc = 0.9740 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 12:57:53.426066: step 161100, loss = 0.0828, acc = 0.9760 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 12:57:58.021198: step 161120, loss = 0.0777, acc = 0.9800 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 12:58:02.824588: step 161140, loss = 0.0762, acc = 0.9840 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 12:58:07.583537: step 161160, loss = 0.0746, acc = 0.9880 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 12:58:12.144654: step 161180, loss = 0.0706, acc = 0.9840 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 12:58:16.803317: step 161200, loss = 0.0945, acc = 0.9740 (299.1 examples/sec; 0.214 sec/batch)
2017-05-09 12:58:21.464181: step 161220, loss = 0.0751, acc = 0.9840 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 12:58:26.054596: step 161240, loss = 0.0856, acc = 0.9760 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 12:58:31.157334: step 161260, loss = 0.0859, acc = 0.9740 (225.0 examples/sec; 0.284 sec/batch)
2017-05-09 12:58:36.669337: step 161280, loss = 0.0869, acc = 0.9820 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 12:58:41.358235: step 161300, loss = 0.1040, acc = 0.9660 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 12:58:46.084362: step 161320, loss = 0.0869, acc = 0.9780 (235.9 examples/sec; 0.271 sec/batch)
2017-05-09 12:58:50.834872: step 161340, loss = 0.0737, acc = 0.9800 (248.7 examples/sec; 0.257 sec/batch)
2017-05-09 12:58:55.541687: step 161360, loss = 0.0753, acc = 0.9800 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 12:59:00.054586: step 161380, loss = 0.1040, acc = 0.9640 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 12:59:04.648748: step 161400, loss = 0.0831, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 12:59:09.234072: step 161420, loss = 0.0910, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 12:59:13.839242: step 161440, loss = 0.0887, acc = 0.9840 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 12:59:18.688980: step 161460, loss = 0.0999, acc = 0.9740 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 12:59:23.385035: step 161480, loss = 0.0857, acc = 0.9780 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 12:59:27.922585: step 161500, loss = 0.0991, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 12:59:32.637567: step 161520, loss = 0.0926, acc = 0.9820 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 12:59:37.283850: step 161540, loss = 0.1034, acc = 0.9720 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 12:59:41.968101: step 161560, loss = 0.1053, acc = 0.9680 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 12:59:46.742794: step 161580, loss = 0.1205, acc = 0.9640 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 12:59:51.491937: step 161600, loss = 0.0878, acc = 0.9800 (255.9 examples/sec; 0.250 sec/batch)
2017-05-09 12:59:56.119929: step 161620, loss = 0.0962, acc = 0.9740 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 13:00:00.849201: step 161640, loss = 0.1068, acc = 0.9720 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 13:00:05.589526: step 161660, loss = 0.1155, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 13:00:10.165210: step 161680, loss = 0.0985, acc = 0.9820 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 13:00:14.771165: step 161700, loss = 0.0954, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 13:00:19.459306: step 161720, loss = 0.1121, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 13:00:24.024625: step 161740, loss = 0.0709, acc = 0.9860 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 13:00:28.585330: step 161760, loss = 0.0858, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 13:00:33.313712: step 161780, loss = 0.0968, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 13:00:37.896499: step 161800, loss = 0.0907, acc = 0.9720 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 13:00:42.572404: step 161820, loss = 0.0872, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 13:00:47.318700: step 161840, loss = 0.0932, acc = 0.9780 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 13:00:51.818936: step 161860, loss = 0.0748, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:00:56.414716: step 161880, loss = 0.0949, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 13:01:01.009329: step 161900, loss = 0.0873, acc = 0.9820 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 13:01:05.868158: step 161920, loss = 0.0885, acc = 0.9740 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 13:01:10.555430: step 161940, loss = 0.0910, acc = 0.9780 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 13:01:15.139395: step 161960, loss = 0.0758, acc = 0.9820 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:01:20.038976: step 161980, loss = 0.0875, acc = 0.9680 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 13:01:24.546156: step 162000, loss = 0.0823, acc = 0.9760 (289.0 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 13:01:38.517130: step 162000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 13:01:48.362260: step 162000, acc = 0.9560, f1 = 0.9556
[Status] 2017-05-09 13:01:48.362356: step 162000, maxindex = 153000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:01:53.012451: step 162020, loss = 0.0991, acc = 0.9680 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 13:01:57.640360: step 162040, loss = 0.0926, acc = 0.9700 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 13:02:02.249645: step 162060, loss = 0.1287, acc = 0.9620 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 13:02:06.779865: step 162080, loss = 0.0684, acc = 0.9860 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 13:02:11.353144: step 162100, loss = 0.0914, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:02:16.059433: step 162120, loss = 0.0785, acc = 0.9800 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 13:02:20.652585: step 162140, loss = 0.1149, acc = 0.9680 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:02:25.231186: step 162160, loss = 0.0760, acc = 0.9760 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 13:02:29.799030: step 162180, loss = 0.0814, acc = 0.9820 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 13:02:34.473676: step 162200, loss = 0.0851, acc = 0.9780 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 13:02:39.028372: step 162220, loss = 0.1003, acc = 0.9740 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 13:02:43.852602: step 162240, loss = 0.0903, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 13:02:48.549490: step 162260, loss = 0.0864, acc = 0.9800 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 13:02:53.195010: step 162280, loss = 0.0880, acc = 0.9800 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 13:02:58.732653: step 162300, loss = 0.0749, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:03:03.432167: step 162320, loss = 0.0851, acc = 0.9780 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 13:03:07.932272: step 162340, loss = 0.0763, acc = 0.9800 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 13:03:12.621222: step 162360, loss = 0.1008, acc = 0.9640 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 13:03:17.442275: step 162380, loss = 0.0875, acc = 0.9800 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 13:03:22.192865: step 162400, loss = 0.0837, acc = 0.9820 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 13:03:26.711410: step 162420, loss = 0.0901, acc = 0.9740 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:03:31.455849: step 162440, loss = 0.0909, acc = 0.9720 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 13:03:36.150480: step 162460, loss = 0.0973, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 13:03:40.694143: step 162480, loss = 0.0822, acc = 0.9840 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:03:45.347504: step 162500, loss = 0.0916, acc = 0.9720 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 13:03:50.071767: step 162520, loss = 0.0998, acc = 0.9660 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 13:03:54.615704: step 162540, loss = 0.0976, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 13:03:59.377063: step 162560, loss = 0.0897, acc = 0.9740 (232.9 examples/sec; 0.275 sec/batch)
2017-05-09 13:04:04.108038: step 162580, loss = 0.0666, acc = 0.9860 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 13:04:08.804974: step 162600, loss = 0.0915, acc = 0.9740 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 13:04:13.495357: step 162620, loss = 0.0942, acc = 0.9820 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 13:04:18.350166: step 162640, loss = 0.0908, acc = 0.9700 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 13:04:22.884804: step 162660, loss = 0.0963, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 13:04:27.460656: step 162680, loss = 0.0911, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 13:04:32.047427: step 162700, loss = 0.0822, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 13:04:36.720871: step 162720, loss = 0.0953, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 13:04:41.261237: step 162740, loss = 0.0826, acc = 0.9760 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 13:04:46.177639: step 162760, loss = 0.1056, acc = 0.9680 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 13:04:50.732353: step 162780, loss = 0.1380, acc = 0.9520 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 13:04:55.216169: step 162800, loss = 0.0960, acc = 0.9720 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 13:04:59.808111: step 162820, loss = 0.1012, acc = 0.9640 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 13:05:04.477582: step 162840, loss = 0.0954, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 13:05:09.154214: step 162860, loss = 0.0835, acc = 0.9820 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 13:05:13.991557: step 162880, loss = 0.0908, acc = 0.9700 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 13:05:18.490955: step 162900, loss = 0.0814, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:05:23.039186: step 162920, loss = 0.0820, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 13:05:27.739674: step 162940, loss = 0.1190, acc = 0.9580 (235.0 examples/sec; 0.272 sec/batch)
2017-05-09 13:05:32.534669: step 162960, loss = 0.0753, acc = 0.9820 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 13:05:37.218876: step 162980, loss = 0.0986, acc = 0.9760 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 13:05:41.849861: step 163000, loss = 0.1118, acc = 0.9720 (261.6 examples/sec; 0.245 sec/batch)
[Eval] 2017-05-09 13:05:55.829242: step 163000, acc = 0.9649, f1 = 0.9638
[Test] 2017-05-09 13:06:05.792438: step 163000, acc = 0.9564, f1 = 0.9560
[Status] 2017-05-09 13:06:05.792547: step 163000, maxindex = 163000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 13:06:13.759638: step 163020, loss = 0.0916, acc = 0.9720 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 13:06:18.291046: step 163040, loss = 0.0886, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 13:06:22.919690: step 163060, loss = 0.0764, acc = 0.9860 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 13:06:27.737035: step 163080, loss = 0.0914, acc = 0.9780 (221.7 examples/sec; 0.289 sec/batch)
2017-05-09 13:06:32.342421: step 163100, loss = 0.0971, acc = 0.9780 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:06:36.951017: step 163120, loss = 0.1257, acc = 0.9600 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 13:06:41.613211: step 163140, loss = 0.0777, acc = 0.9860 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 13:06:46.284281: step 163160, loss = 0.0808, acc = 0.9860 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 13:06:51.073045: step 163180, loss = 0.0889, acc = 0.9800 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 13:06:55.711675: step 163200, loss = 0.0799, acc = 0.9800 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 13:07:00.582410: step 163220, loss = 0.0820, acc = 0.9840 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 13:07:05.189571: step 163240, loss = 0.1111, acc = 0.9600 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:07:09.776818: step 163260, loss = 0.0957, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 13:07:14.552089: step 163280, loss = 0.0828, acc = 0.9700 (222.9 examples/sec; 0.287 sec/batch)
2017-05-09 13:07:19.944648: step 163300, loss = 0.1078, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:07:24.466823: step 163320, loss = 0.0985, acc = 0.9740 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 13:07:29.194815: step 163340, loss = 0.1164, acc = 0.9620 (237.5 examples/sec; 0.270 sec/batch)
2017-05-09 13:07:33.780116: step 163360, loss = 0.1008, acc = 0.9680 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 13:07:38.468896: step 163380, loss = 0.0947, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 13:07:43.079061: step 163400, loss = 0.0767, acc = 0.9840 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 13:07:47.890648: step 163420, loss = 0.0866, acc = 0.9820 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 13:07:52.355580: step 163440, loss = 0.1029, acc = 0.9760 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 13:07:56.912071: step 163460, loss = 0.1043, acc = 0.9660 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 13:08:01.687929: step 163480, loss = 0.0946, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:08:06.185898: step 163500, loss = 0.0929, acc = 0.9760 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 13:08:10.741961: step 163520, loss = 0.1029, acc = 0.9680 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 13:08:15.489607: step 163540, loss = 0.1162, acc = 0.9720 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 13:08:19.964013: step 163560, loss = 0.1120, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 13:08:24.622105: step 163580, loss = 0.0622, acc = 0.9920 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 13:08:29.429534: step 163600, loss = 0.0971, acc = 0.9720 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:08:34.030839: step 163620, loss = 0.0866, acc = 0.9740 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 13:08:38.727989: step 163640, loss = 0.1005, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 13:08:43.441255: step 163660, loss = 0.0963, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 13:08:47.953229: step 163680, loss = 0.0989, acc = 0.9720 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 13:08:52.570150: step 163700, loss = 0.0856, acc = 0.9820 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 13:08:57.397296: step 163720, loss = 0.0996, acc = 0.9780 (240.1 examples/sec; 0.267 sec/batch)
2017-05-09 13:09:01.955455: step 163740, loss = 0.0767, acc = 0.9860 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 13:09:06.589871: step 163760, loss = 0.0824, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:09:11.439413: step 163780, loss = 0.1141, acc = 0.9620 (236.5 examples/sec; 0.271 sec/batch)
2017-05-09 13:09:16.084031: step 163800, loss = 0.1108, acc = 0.9640 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 13:09:20.659664: step 163820, loss = 0.0918, acc = 0.9760 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 13:09:25.280646: step 163840, loss = 0.0741, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:09:29.995113: step 163860, loss = 0.0811, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 13:09:34.529089: step 163880, loss = 0.1318, acc = 0.9700 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 13:09:39.158952: step 163900, loss = 0.0761, acc = 0.9820 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 13:09:43.866619: step 163920, loss = 0.0840, acc = 0.9780 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 13:09:48.432780: step 163940, loss = 0.0854, acc = 0.9800 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 13:09:53.154573: step 163960, loss = 0.0783, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 13:09:57.886314: step 163980, loss = 0.1186, acc = 0.9620 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 13:10:02.535118: step 164000, loss = 0.0892, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 13:10:16.636407: step 164000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 13:10:26.464904: step 164000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 13:10:26.464991: step 164000, maxindex = 163000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 13:10:30.946322: step 164020, loss = 0.0845, acc = 0.9740 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 13:10:35.514618: step 164040, loss = 0.0943, acc = 0.9800 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 13:10:40.124306: step 164060, loss = 0.0821, acc = 0.9800 (249.0 examples/sec; 0.257 sec/batch)
2017-05-09 13:10:44.518355: step 164080, loss = 0.0861, acc = 0.9760 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 13:10:49.017109: step 164100, loss = 0.0762, acc = 0.9800 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 13:10:53.618390: step 164120, loss = 0.0820, acc = 0.9740 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 13:10:58.530370: step 164140, loss = 0.0882, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 13:11:03.221330: step 164160, loss = 0.0843, acc = 0.9720 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 13:11:07.807163: step 164180, loss = 0.1038, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 13:11:12.534673: step 164200, loss = 0.0793, acc = 0.9820 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 13:11:17.094754: step 164220, loss = 0.0773, acc = 0.9840 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:11:21.767314: step 164240, loss = 0.0943, acc = 0.9740 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 13:11:26.577385: step 164260, loss = 0.0838, acc = 0.9780 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 13:11:31.288561: step 164280, loss = 0.0949, acc = 0.9780 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 13:11:36.902141: step 164300, loss = 0.0911, acc = 0.9840 (136.5 examples/sec; 0.469 sec/batch)
2017-05-09 13:11:41.777977: step 164320, loss = 0.0924, acc = 0.9700 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 13:11:46.453336: step 164340, loss = 0.0903, acc = 0.9780 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 13:11:51.019863: step 164360, loss = 0.0945, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:11:55.799455: step 164380, loss = 0.0894, acc = 0.9760 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 13:12:00.429324: step 164400, loss = 0.0915, acc = 0.9720 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 13:12:04.987978: step 164420, loss = 0.1039, acc = 0.9720 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 13:12:09.665547: step 164440, loss = 0.1055, acc = 0.9660 (305.0 examples/sec; 0.210 sec/batch)
2017-05-09 13:12:14.287563: step 164460, loss = 0.1025, acc = 0.9700 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 13:12:18.899442: step 164480, loss = 0.0804, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:12:23.865248: step 164500, loss = 0.0899, acc = 0.9780 (226.4 examples/sec; 0.283 sec/batch)
2017-05-09 13:12:28.472383: step 164520, loss = 0.0790, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:12:33.004972: step 164540, loss = 0.1024, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 13:12:37.588982: step 164560, loss = 0.0937, acc = 0.9760 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 13:12:42.567654: step 164580, loss = 0.0796, acc = 0.9800 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 13:12:47.144540: step 164600, loss = 0.0963, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:12:51.916849: step 164620, loss = 0.0815, acc = 0.9720 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 13:12:56.589806: step 164640, loss = 0.0972, acc = 0.9740 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 13:13:01.103170: step 164660, loss = 0.1091, acc = 0.9700 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 13:13:05.835980: step 164680, loss = 0.0943, acc = 0.9760 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 13:13:10.480903: step 164700, loss = 0.0737, acc = 0.9860 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 13:13:15.064027: step 164720, loss = 0.0868, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 13:13:19.692879: step 164740, loss = 0.0881, acc = 0.9760 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 13:13:24.596803: step 164760, loss = 0.1019, acc = 0.9780 (237.9 examples/sec; 0.269 sec/batch)
2017-05-09 13:13:29.107965: step 164780, loss = 0.1121, acc = 0.9600 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 13:13:33.721277: step 164800, loss = 0.0783, acc = 0.9860 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 13:13:38.254668: step 164820, loss = 0.0851, acc = 0.9760 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 13:13:43.387849: step 164840, loss = 0.0740, acc = 0.9840 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 13:13:48.057579: step 164860, loss = 0.0785, acc = 0.9800 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 13:13:52.820784: step 164880, loss = 0.0834, acc = 0.9780 (257.7 examples/sec; 0.248 sec/batch)
2017-05-09 13:13:57.584176: step 164900, loss = 0.0703, acc = 0.9840 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 13:14:02.164414: step 164920, loss = 0.1028, acc = 0.9620 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 13:14:06.740909: step 164940, loss = 0.0781, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:14:11.468172: step 164960, loss = 0.0808, acc = 0.9700 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 13:14:15.922034: step 164980, loss = 0.0981, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:14:20.613467: step 165000, loss = 0.0890, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 13:14:34.585738: step 165000, acc = 0.9649, f1 = 0.9638
[Test] 2017-05-09 13:14:44.330355: step 165000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 13:14:44.330426: step 165000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:14:52.189181: step 165020, loss = 0.0792, acc = 0.9740 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 13:14:56.925554: step 165040, loss = 0.0693, acc = 0.9840 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 13:15:01.517978: step 165060, loss = 0.1232, acc = 0.9760 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 13:15:06.081346: step 165080, loss = 0.1155, acc = 0.9640 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 13:15:10.724544: step 165100, loss = 0.0947, acc = 0.9760 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 13:15:15.550419: step 165120, loss = 0.0835, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 13:15:20.151459: step 165140, loss = 0.0827, acc = 0.9820 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 13:15:25.011796: step 165160, loss = 0.0984, acc = 0.9700 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 13:15:29.562673: step 165180, loss = 0.0778, acc = 0.9860 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 13:15:34.160816: step 165200, loss = 0.1001, acc = 0.9760 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 13:15:38.988010: step 165220, loss = 0.0857, acc = 0.9800 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:15:43.623776: step 165240, loss = 0.0956, acc = 0.9700 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 13:15:48.265151: step 165260, loss = 0.0748, acc = 0.9800 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 13:15:53.093195: step 165280, loss = 0.0860, acc = 0.9760 (233.2 examples/sec; 0.274 sec/batch)
2017-05-09 13:15:57.822954: step 165300, loss = 0.0998, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 13:16:03.189439: step 165320, loss = 0.0793, acc = 0.9820 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 13:16:07.899697: step 165340, loss = 0.0798, acc = 0.9800 (256.0 examples/sec; 0.250 sec/batch)
2017-05-09 13:16:12.510720: step 165360, loss = 0.0869, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 13:16:17.134190: step 165380, loss = 0.1205, acc = 0.9620 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 13:16:21.883827: step 165400, loss = 0.0743, acc = 0.9780 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 13:16:26.651218: step 165420, loss = 0.0842, acc = 0.9800 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 13:16:31.275138: step 165440, loss = 0.0881, acc = 0.9760 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 13:16:35.919639: step 165460, loss = 0.0845, acc = 0.9780 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 13:16:40.575639: step 165480, loss = 0.1191, acc = 0.9680 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 13:16:45.156951: step 165500, loss = 0.0877, acc = 0.9660 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 13:16:49.724513: step 165520, loss = 0.0858, acc = 0.9720 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 13:16:54.420823: step 165540, loss = 0.0851, acc = 0.9780 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 13:16:58.944665: step 165560, loss = 0.0882, acc = 0.9760 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 13:17:03.567696: step 165580, loss = 0.0848, acc = 0.9860 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 13:17:08.346929: step 165600, loss = 0.1026, acc = 0.9760 (240.2 examples/sec; 0.266 sec/batch)
2017-05-09 13:17:12.792819: step 165620, loss = 0.1127, acc = 0.9700 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 13:17:17.444798: step 165640, loss = 0.1252, acc = 0.9560 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 13:17:22.094970: step 165660, loss = 0.0986, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:17:26.801134: step 165680, loss = 0.0893, acc = 0.9780 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 13:17:31.553451: step 165700, loss = 0.0808, acc = 0.9820 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 13:17:36.106317: step 165720, loss = 0.0925, acc = 0.9780 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 13:17:40.859105: step 165740, loss = 0.1120, acc = 0.9800 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 13:17:45.332996: step 165760, loss = 0.0733, acc = 0.9800 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 13:17:49.980222: step 165780, loss = 0.1229, acc = 0.9560 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 13:17:54.722691: step 165800, loss = 0.1047, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 13:17:59.214653: step 165820, loss = 0.0992, acc = 0.9720 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 13:18:03.872409: step 165840, loss = 0.0833, acc = 0.9700 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 13:18:08.572602: step 165860, loss = 0.1338, acc = 0.9580 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 13:18:13.098974: step 165880, loss = 0.1009, acc = 0.9740 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:18:17.766887: step 165900, loss = 0.0800, acc = 0.9840 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 13:18:22.741017: step 165920, loss = 0.1060, acc = 0.9660 (219.6 examples/sec; 0.291 sec/batch)
2017-05-09 13:18:27.275006: step 165940, loss = 0.0971, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 13:18:31.755187: step 165960, loss = 0.1034, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 13:18:36.592313: step 165980, loss = 0.0999, acc = 0.9740 (217.0 examples/sec; 0.295 sec/batch)
2017-05-09 13:18:41.229889: step 166000, loss = 0.0977, acc = 0.9740 (262.1 examples/sec; 0.244 sec/batch)
[Eval] 2017-05-09 13:18:55.233388: step 166000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 13:19:04.654737: step 166000, acc = 0.9556, f1 = 0.9553
[Status] 2017-05-09 13:19:04.655126: step 166000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:19:09.214135: step 166020, loss = 0.0936, acc = 0.9780 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:19:13.795232: step 166040, loss = 0.0946, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:19:18.404653: step 166060, loss = 0.0804, acc = 0.9760 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 13:19:23.038177: step 166080, loss = 0.0960, acc = 0.9760 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 13:19:27.611498: step 166100, loss = 0.0956, acc = 0.9700 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 13:19:32.238528: step 166120, loss = 0.0859, acc = 0.9780 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 13:19:36.923985: step 166140, loss = 0.0766, acc = 0.9840 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 13:19:41.524033: step 166160, loss = 0.0855, acc = 0.9760 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 13:19:46.165245: step 166180, loss = 0.0877, acc = 0.9760 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 13:19:50.992497: step 166200, loss = 0.1133, acc = 0.9680 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 13:19:55.541075: step 166220, loss = 0.1188, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:20:00.160147: step 166240, loss = 0.0872, acc = 0.9780 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 13:20:04.889982: step 166260, loss = 0.0936, acc = 0.9740 (253.3 examples/sec; 0.253 sec/batch)
2017-05-09 13:20:09.375950: step 166280, loss = 0.0832, acc = 0.9720 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 13:20:13.972092: step 166300, loss = 0.0835, acc = 0.9760 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 13:20:19.658448: step 166320, loss = 0.0852, acc = 0.9780 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 13:20:24.395100: step 166340, loss = 0.0839, acc = 0.9700 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 13:20:28.995863: step 166360, loss = 0.0635, acc = 0.9900 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 13:20:33.529588: step 166380, loss = 0.1022, acc = 0.9680 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 13:20:38.220383: step 166400, loss = 0.0947, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:20:42.750076: step 166420, loss = 0.0708, acc = 0.9820 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 13:20:47.297456: step 166440, loss = 0.0726, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 13:20:51.948307: step 166460, loss = 0.0740, acc = 0.9900 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 13:20:56.585425: step 166480, loss = 0.0788, acc = 0.9820 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 13:21:01.177878: step 166500, loss = 0.0868, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 13:21:06.036638: step 166520, loss = 0.0922, acc = 0.9720 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:21:10.778690: step 166540, loss = 0.0909, acc = 0.9760 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 13:21:15.355915: step 166560, loss = 0.0888, acc = 0.9740 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 13:21:20.180232: step 166580, loss = 0.0923, acc = 0.9760 (298.7 examples/sec; 0.214 sec/batch)
2017-05-09 13:21:24.802541: step 166600, loss = 0.1130, acc = 0.9600 (257.0 examples/sec; 0.249 sec/batch)
2017-05-09 13:21:29.432320: step 166620, loss = 0.0779, acc = 0.9760 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 13:21:34.061681: step 166640, loss = 0.0806, acc = 0.9860 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 13:21:38.699102: step 166660, loss = 0.0972, acc = 0.9720 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 13:21:43.232793: step 166680, loss = 0.1047, acc = 0.9680 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 13:21:47.702713: step 166700, loss = 0.1015, acc = 0.9700 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 13:21:52.324691: step 166720, loss = 0.0672, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:21:56.835993: step 166740, loss = 0.1016, acc = 0.9680 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 13:22:01.392318: step 166760, loss = 0.1059, acc = 0.9660 (259.6 examples/sec; 0.246 sec/batch)
2017-05-09 13:22:06.194745: step 166780, loss = 0.1141, acc = 0.9680 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:22:10.853524: step 166800, loss = 0.1140, acc = 0.9720 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 13:22:15.384939: step 166820, loss = 0.0968, acc = 0.9740 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 13:22:20.333104: step 166840, loss = 0.1151, acc = 0.9720 (225.3 examples/sec; 0.284 sec/batch)
2017-05-09 13:22:24.949582: step 166860, loss = 0.0967, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 13:22:29.538915: step 166880, loss = 0.0905, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:22:34.175403: step 166900, loss = 0.1000, acc = 0.9780 (238.6 examples/sec; 0.268 sec/batch)
2017-05-09 13:22:38.750851: step 166920, loss = 0.0741, acc = 0.9860 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:22:43.331936: step 166940, loss = 0.1011, acc = 0.9740 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 13:22:48.006464: step 166960, loss = 0.0829, acc = 0.9820 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:22:52.962617: step 166980, loss = 0.1338, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 13:22:57.606105: step 167000, loss = 0.1082, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 13:23:11.587539: step 167000, acc = 0.9643, f1 = 0.9630
[Test] 2017-05-09 13:23:21.260608: step 167000, acc = 0.9550, f1 = 0.9547
[Status] 2017-05-09 13:23:21.260667: step 167000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:23:25.926355: step 167020, loss = 0.0774, acc = 0.9860 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 13:23:30.486695: step 167040, loss = 0.0901, acc = 0.9740 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 13:23:35.135652: step 167060, loss = 0.1138, acc = 0.9680 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 13:23:39.684395: step 167080, loss = 0.0883, acc = 0.9760 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 13:23:44.265036: step 167100, loss = 0.0976, acc = 0.9720 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 13:23:49.231915: step 167120, loss = 0.0972, acc = 0.9680 (214.4 examples/sec; 0.298 sec/batch)
2017-05-09 13:23:53.678671: step 167140, loss = 0.1235, acc = 0.9640 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 13:23:58.346374: step 167160, loss = 0.1011, acc = 0.9700 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 13:24:02.995137: step 167180, loss = 0.0970, acc = 0.9760 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:24:07.885482: step 167200, loss = 0.0807, acc = 0.9920 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 13:24:12.443975: step 167220, loss = 0.0879, acc = 0.9720 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 13:24:17.086770: step 167240, loss = 0.0777, acc = 0.9840 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 13:24:21.909328: step 167260, loss = 0.0894, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 13:24:26.423846: step 167280, loss = 0.0995, acc = 0.9700 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 13:24:31.163778: step 167300, loss = 0.1082, acc = 0.9620 (256.0 examples/sec; 0.250 sec/batch)
2017-05-09 13:24:35.974262: step 167320, loss = 0.1087, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 13:24:41.215247: step 167340, loss = 0.1089, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 13:24:45.813748: step 167360, loss = 0.1112, acc = 0.9700 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 13:24:50.599027: step 167380, loss = 0.0783, acc = 0.9820 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 13:24:55.288212: step 167400, loss = 0.0912, acc = 0.9680 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:24:59.906210: step 167420, loss = 0.0958, acc = 0.9680 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 13:25:04.545079: step 167440, loss = 0.0779, acc = 0.9760 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 13:25:09.352029: step 167460, loss = 0.0956, acc = 0.9740 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 13:25:13.984252: step 167480, loss = 0.0822, acc = 0.9800 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 13:25:18.569645: step 167500, loss = 0.0977, acc = 0.9640 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 13:25:23.171155: step 167520, loss = 0.1225, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 13:25:27.688182: step 167540, loss = 0.1069, acc = 0.9600 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:25:32.358719: step 167560, loss = 0.0808, acc = 0.9780 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 13:25:36.992862: step 167580, loss = 0.1268, acc = 0.9660 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 13:25:41.712305: step 167600, loss = 0.1009, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 13:25:46.323805: step 167620, loss = 0.0982, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:25:50.972109: step 167640, loss = 0.1119, acc = 0.9700 (244.0 examples/sec; 0.262 sec/batch)
2017-05-09 13:25:55.617700: step 167660, loss = 0.1112, acc = 0.9640 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 13:26:00.375466: step 167680, loss = 0.0765, acc = 0.9860 (260.7 examples/sec; 0.245 sec/batch)
2017-05-09 13:26:05.046324: step 167700, loss = 0.0745, acc = 0.9900 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 13:26:09.847581: step 167720, loss = 0.0854, acc = 0.9800 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 13:26:14.475725: step 167740, loss = 0.0840, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 13:26:19.159927: step 167760, loss = 0.0744, acc = 0.9740 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 13:26:24.024116: step 167780, loss = 0.0827, acc = 0.9860 (247.6 examples/sec; 0.259 sec/batch)
2017-05-09 13:26:29.021478: step 167800, loss = 0.0803, acc = 0.9740 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 13:26:33.661413: step 167820, loss = 0.1042, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 13:26:38.503413: step 167840, loss = 0.0875, acc = 0.9760 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 13:26:43.044630: step 167860, loss = 0.1036, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 13:26:47.603145: step 167880, loss = 0.0987, acc = 0.9720 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:26:52.209703: step 167900, loss = 0.0944, acc = 0.9740 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 13:26:56.930420: step 167920, loss = 0.0945, acc = 0.9760 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 13:27:01.461805: step 167940, loss = 0.0718, acc = 0.9840 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 13:27:06.121469: step 167960, loss = 0.0828, acc = 0.9780 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 13:27:10.956241: step 167980, loss = 0.0754, acc = 0.9780 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 13:27:15.468252: step 168000, loss = 0.0770, acc = 0.9780 (283.0 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 13:27:29.399443: step 168000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 13:27:38.990333: step 168000, acc = 0.9562, f1 = 0.9559
[Status] 2017-05-09 13:27:38.990414: step 168000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:27:43.567222: step 168020, loss = 0.0789, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:27:48.446484: step 168040, loss = 0.0926, acc = 0.9680 (250.3 examples/sec; 0.256 sec/batch)
2017-05-09 13:27:53.200506: step 168060, loss = 0.0902, acc = 0.9780 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 13:27:57.834307: step 168080, loss = 0.0976, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 13:28:02.370168: step 168100, loss = 0.1068, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 13:28:07.174183: step 168120, loss = 0.0965, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 13:28:11.614769: step 168140, loss = 0.1057, acc = 0.9720 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 13:28:16.052436: step 168160, loss = 0.0821, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 13:28:20.838700: step 168180, loss = 0.0779, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 13:28:25.321541: step 168200, loss = 0.0795, acc = 0.9820 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:28:29.932540: step 168220, loss = 0.0997, acc = 0.9640 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:28:34.696129: step 168240, loss = 0.1057, acc = 0.9700 (300.0 examples/sec; 0.213 sec/batch)
2017-05-09 13:28:39.269518: step 168260, loss = 0.0932, acc = 0.9820 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 13:28:43.881138: step 168280, loss = 0.0919, acc = 0.9820 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 13:28:48.646635: step 168300, loss = 0.0714, acc = 0.9900 (243.3 examples/sec; 0.263 sec/batch)
2017-05-09 13:28:53.340173: step 168320, loss = 0.0868, acc = 0.9760 (236.2 examples/sec; 0.271 sec/batch)
2017-05-09 13:28:58.936818: step 168340, loss = 0.0964, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 13:29:03.638966: step 168360, loss = 0.0688, acc = 0.9840 (243.2 examples/sec; 0.263 sec/batch)
2017-05-09 13:29:08.168726: step 168380, loss = 0.1213, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 13:29:12.827728: step 168400, loss = 0.1224, acc = 0.9580 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 13:29:17.297366: step 168420, loss = 0.1215, acc = 0.9700 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 13:29:21.922419: step 168440, loss = 0.0833, acc = 0.9760 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 13:29:26.408423: step 168460, loss = 0.1114, acc = 0.9680 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 13:29:31.122228: step 168480, loss = 0.1073, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 13:29:35.787932: step 168500, loss = 0.0793, acc = 0.9840 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 13:29:40.323075: step 168520, loss = 0.0998, acc = 0.9720 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 13:29:44.994135: step 168540, loss = 0.0718, acc = 0.9840 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 13:29:49.754871: step 168560, loss = 0.0964, acc = 0.9700 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 13:29:54.310402: step 168580, loss = 0.0866, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 13:29:58.794197: step 168600, loss = 0.0999, acc = 0.9740 (300.1 examples/sec; 0.213 sec/batch)
2017-05-09 13:30:03.746605: step 168620, loss = 0.0894, acc = 0.9760 (225.8 examples/sec; 0.283 sec/batch)
2017-05-09 13:30:08.458094: step 168640, loss = 0.0927, acc = 0.9700 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 13:30:13.208346: step 168660, loss = 0.0818, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 13:30:17.902713: step 168680, loss = 0.0869, acc = 0.9780 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 13:30:22.748256: step 168700, loss = 0.1083, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 13:30:27.356139: step 168720, loss = 0.0799, acc = 0.9780 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 13:30:32.031335: step 168740, loss = 0.0871, acc = 0.9740 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 13:30:36.738631: step 168760, loss = 0.0911, acc = 0.9740 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 13:30:41.358240: step 168780, loss = 0.1153, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 13:30:46.085625: step 168800, loss = 0.0931, acc = 0.9820 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 13:30:50.961219: step 168820, loss = 0.0965, acc = 0.9680 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 13:30:55.659175: step 168840, loss = 0.1120, acc = 0.9620 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 13:31:00.300634: step 168860, loss = 0.0821, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:31:05.171817: step 168880, loss = 0.0962, acc = 0.9700 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 13:31:09.745338: step 168900, loss = 0.0882, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:31:14.462083: step 168920, loss = 0.1058, acc = 0.9700 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 13:31:19.205219: step 168940, loss = 0.0740, acc = 0.9720 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 13:31:23.819826: step 168960, loss = 0.0926, acc = 0.9760 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 13:31:28.397372: step 168980, loss = 0.1108, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 13:31:33.077815: step 169000, loss = 0.0871, acc = 0.9780 (264.5 examples/sec; 0.242 sec/batch)
[Eval] 2017-05-09 13:31:47.265708: step 169000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 13:31:56.952728: step 169000, acc = 0.9560, f1 = 0.9557
[Status] 2017-05-09 13:31:56.952814: step 169000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:32:01.442565: step 169020, loss = 0.0770, acc = 0.9800 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:32:06.263136: step 169040, loss = 0.0878, acc = 0.9780 (299.7 examples/sec; 0.214 sec/batch)
2017-05-09 13:32:10.856268: step 169060, loss = 0.0806, acc = 0.9820 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 13:32:15.416880: step 169080, loss = 0.0856, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 13:32:20.130758: step 169100, loss = 0.0661, acc = 0.9940 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:32:24.778627: step 169120, loss = 0.0842, acc = 0.9840 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 13:32:29.373640: step 169140, loss = 0.0726, acc = 0.9860 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 13:32:34.215713: step 169160, loss = 0.0878, acc = 0.9800 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 13:32:38.760657: step 169180, loss = 0.0939, acc = 0.9760 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 13:32:43.476999: step 169200, loss = 0.0769, acc = 0.9860 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 13:32:48.145071: step 169220, loss = 0.1020, acc = 0.9700 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 13:32:52.939393: step 169240, loss = 0.0786, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 13:32:57.622891: step 169260, loss = 0.0893, acc = 0.9720 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 13:33:02.222376: step 169280, loss = 0.0777, acc = 0.9780 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 13:33:07.172487: step 169300, loss = 0.0924, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 13:33:11.752224: step 169320, loss = 0.1015, acc = 0.9760 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:33:17.042711: step 169340, loss = 0.0847, acc = 0.9760 (160.3 examples/sec; 0.399 sec/batch)
2017-05-09 13:33:21.747581: step 169360, loss = 0.0982, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:33:26.339187: step 169380, loss = 0.0848, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 13:33:30.948815: step 169400, loss = 0.1046, acc = 0.9640 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 13:33:35.438615: step 169420, loss = 0.1149, acc = 0.9600 (297.2 examples/sec; 0.215 sec/batch)
2017-05-09 13:33:40.546145: step 169440, loss = 0.0960, acc = 0.9760 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 13:33:45.566961: step 169460, loss = 0.0938, acc = 0.9700 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 13:33:50.150762: step 169480, loss = 0.0792, acc = 0.9840 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 13:33:54.900698: step 169500, loss = 0.0775, acc = 0.9840 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 13:33:59.442856: step 169520, loss = 0.0964, acc = 0.9800 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 13:34:04.025296: step 169540, loss = 0.0871, acc = 0.9780 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 13:34:08.886636: step 169560, loss = 0.1081, acc = 0.9680 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 13:34:13.402711: step 169580, loss = 0.0783, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:34:17.938872: step 169600, loss = 0.0933, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:34:22.647542: step 169620, loss = 0.0983, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 13:34:27.206454: step 169640, loss = 0.0913, acc = 0.9680 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 13:34:31.789449: step 169660, loss = 0.0896, acc = 0.9760 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 13:34:36.515436: step 169680, loss = 0.1021, acc = 0.9720 (230.5 examples/sec; 0.278 sec/batch)
2017-05-09 13:34:41.032900: step 169700, loss = 0.1202, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 13:34:45.541753: step 169720, loss = 0.0893, acc = 0.9800 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 13:34:50.353338: step 169740, loss = 0.0877, acc = 0.9780 (218.7 examples/sec; 0.293 sec/batch)
2017-05-09 13:34:54.901523: step 169760, loss = 0.0841, acc = 0.9820 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 13:34:59.483043: step 169780, loss = 0.0938, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 13:35:04.024654: step 169800, loss = 0.0821, acc = 0.9780 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 13:35:08.698727: step 169820, loss = 0.0751, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:35:13.160721: step 169840, loss = 0.0974, acc = 0.9700 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 13:35:17.748117: step 169860, loss = 0.0848, acc = 0.9780 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 13:35:22.299600: step 169880, loss = 0.0827, acc = 0.9780 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 13:35:26.795857: step 169900, loss = 0.0917, acc = 0.9740 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 13:35:31.471219: step 169920, loss = 0.0863, acc = 0.9760 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 13:35:36.263499: step 169940, loss = 0.0969, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 13:35:40.788863: step 169960, loss = 0.1020, acc = 0.9680 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 13:35:45.328509: step 169980, loss = 0.0848, acc = 0.9780 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 13:35:50.240568: step 170000, loss = 0.0911, acc = 0.9780 (284.8 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 13:36:04.228112: step 170000, acc = 0.9631, f1 = 0.9619
[Test] 2017-05-09 13:36:13.613742: step 170000, acc = 0.9542, f1 = 0.9538
[Status] 2017-05-09 13:36:13.613823: step 170000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:36:18.353496: step 170020, loss = 0.0933, acc = 0.9800 (237.6 examples/sec; 0.269 sec/batch)
2017-05-09 13:36:22.756790: step 170040, loss = 0.1166, acc = 0.9660 (302.2 examples/sec; 0.212 sec/batch)
2017-05-09 13:36:27.376270: step 170060, loss = 0.0963, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 13:36:32.068505: step 170080, loss = 0.0884, acc = 0.9760 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 13:36:36.545610: step 170100, loss = 0.1094, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 13:36:40.970898: step 170120, loss = 0.0890, acc = 0.9780 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 13:36:45.456449: step 170140, loss = 0.0994, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 13:36:50.138036: step 170160, loss = 0.1046, acc = 0.9800 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 13:36:54.701977: step 170180, loss = 0.0919, acc = 0.9680 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 13:36:59.332884: step 170200, loss = 0.0757, acc = 0.9840 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 13:37:04.164957: step 170220, loss = 0.0961, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 13:37:08.840493: step 170240, loss = 0.1164, acc = 0.9560 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 13:37:13.488684: step 170260, loss = 0.0967, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:37:18.336914: step 170280, loss = 0.0666, acc = 0.9920 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 13:37:22.860360: step 170300, loss = 0.0786, acc = 0.9800 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 13:37:27.392406: step 170320, loss = 0.1007, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:37:32.219960: step 170340, loss = 0.1104, acc = 0.9700 (237.7 examples/sec; 0.269 sec/batch)
2017-05-09 13:37:37.770063: step 170360, loss = 0.0982, acc = 0.9700 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 13:37:42.532429: step 170380, loss = 0.1091, acc = 0.9620 (255.3 examples/sec; 0.251 sec/batch)
2017-05-09 13:37:47.094369: step 170400, loss = 0.1107, acc = 0.9740 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 13:37:51.627859: step 170420, loss = 0.0848, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 13:37:56.275029: step 170440, loss = 0.0841, acc = 0.9760 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 13:38:01.107599: step 170460, loss = 0.1019, acc = 0.9700 (215.2 examples/sec; 0.297 sec/batch)
2017-05-09 13:38:05.847605: step 170480, loss = 0.1339, acc = 0.9520 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 13:38:10.409685: step 170500, loss = 0.0763, acc = 0.9820 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 13:38:15.087533: step 170520, loss = 0.0990, acc = 0.9660 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 13:38:19.788568: step 170540, loss = 0.0859, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 13:38:24.342658: step 170560, loss = 0.1015, acc = 0.9680 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:38:28.938150: step 170580, loss = 0.0815, acc = 0.9780 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 13:38:33.625841: step 170600, loss = 0.0882, acc = 0.9740 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 13:38:38.155855: step 170620, loss = 0.0854, acc = 0.9860 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 13:38:42.804189: step 170640, loss = 0.0975, acc = 0.9840 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 13:38:47.456943: step 170660, loss = 0.0957, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 13:38:52.035877: step 170680, loss = 0.0934, acc = 0.9760 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 13:38:56.461589: step 170700, loss = 0.1168, acc = 0.9700 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 13:39:01.511249: step 170720, loss = 0.0787, acc = 0.9860 (213.2 examples/sec; 0.300 sec/batch)
2017-05-09 13:39:06.113607: step 170740, loss = 0.1028, acc = 0.9740 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 13:39:10.754034: step 170760, loss = 0.0863, acc = 0.9680 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 13:39:15.564200: step 170780, loss = 0.0922, acc = 0.9720 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 13:39:20.371029: step 170800, loss = 0.0989, acc = 0.9720 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:39:25.062371: step 170820, loss = 0.0739, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:39:29.715124: step 170840, loss = 0.0858, acc = 0.9740 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 13:39:34.575903: step 170860, loss = 0.0958, acc = 0.9700 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:39:39.214805: step 170880, loss = 0.1070, acc = 0.9680 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 13:39:43.694849: step 170900, loss = 0.1042, acc = 0.9760 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 13:39:48.434669: step 170920, loss = 0.0971, acc = 0.9760 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 13:39:53.334295: step 170940, loss = 0.1005, acc = 0.9800 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 13:39:57.830467: step 170960, loss = 0.0954, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 13:40:02.541918: step 170980, loss = 0.0911, acc = 0.9640 (241.3 examples/sec; 0.265 sec/batch)
2017-05-09 13:40:07.118632: step 171000, loss = 0.0742, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 13:40:21.211453: step 171000, acc = 0.9638, f1 = 0.9627
[Test] 2017-05-09 13:40:30.801003: step 171000, acc = 0.9551, f1 = 0.9547
[Status] 2017-05-09 13:40:30.801091: step 171000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:40:35.408391: step 171020, loss = 0.0863, acc = 0.9760 (258.2 examples/sec; 0.248 sec/batch)
2017-05-09 13:40:40.031976: step 171040, loss = 0.1191, acc = 0.9580 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 13:40:44.742308: step 171060, loss = 0.1169, acc = 0.9640 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 13:40:49.445293: step 171080, loss = 0.0929, acc = 0.9760 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 13:40:54.019105: step 171100, loss = 0.0714, acc = 0.9800 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 13:40:58.582034: step 171120, loss = 0.0895, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 13:41:03.294172: step 171140, loss = 0.0899, acc = 0.9680 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 13:41:07.896596: step 171160, loss = 0.1045, acc = 0.9660 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 13:41:12.511134: step 171180, loss = 0.0775, acc = 0.9780 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 13:41:17.196114: step 171200, loss = 0.1108, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 13:41:21.835251: step 171220, loss = 0.1005, acc = 0.9620 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 13:41:26.419636: step 171240, loss = 0.0857, acc = 0.9820 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 13:41:31.221546: step 171260, loss = 0.0725, acc = 0.9780 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 13:41:35.942513: step 171280, loss = 0.0875, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 13:41:40.595401: step 171300, loss = 0.0843, acc = 0.9800 (252.9 examples/sec; 0.253 sec/batch)
2017-05-09 13:41:45.459048: step 171320, loss = 0.0764, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 13:41:50.110404: step 171340, loss = 0.0806, acc = 0.9800 (243.4 examples/sec; 0.263 sec/batch)
2017-05-09 13:41:55.380229: step 171360, loss = 0.0775, acc = 0.9800 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 13:42:00.101675: step 171380, loss = 0.0988, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 13:42:04.796461: step 171400, loss = 0.0925, acc = 0.9780 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 13:42:09.426727: step 171420, loss = 0.1247, acc = 0.9460 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 13:42:14.737180: step 171440, loss = 0.0761, acc = 0.9780 (243.8 examples/sec; 0.263 sec/batch)
2017-05-09 13:42:19.337780: step 171460, loss = 0.0902, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:42:23.978593: step 171480, loss = 0.1043, acc = 0.9740 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 13:42:28.554358: step 171500, loss = 0.0785, acc = 0.9780 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 13:42:33.408536: step 171520, loss = 0.0845, acc = 0.9700 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 13:42:38.062934: step 171540, loss = 0.0749, acc = 0.9800 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 13:42:42.739881: step 171560, loss = 0.0934, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 13:42:47.549922: step 171580, loss = 0.1012, acc = 0.9760 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 13:42:52.242046: step 171600, loss = 0.0852, acc = 0.9820 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 13:42:56.871351: step 171620, loss = 0.0948, acc = 0.9720 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 13:43:01.758130: step 171640, loss = 0.0920, acc = 0.9720 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 13:43:06.325903: step 171660, loss = 0.0831, acc = 0.9840 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 13:43:10.961670: step 171680, loss = 0.1052, acc = 0.9780 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 13:43:15.655903: step 171700, loss = 0.0894, acc = 0.9820 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 13:43:20.211604: step 171720, loss = 0.1163, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 13:43:24.846366: step 171740, loss = 0.1060, acc = 0.9640 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 13:43:29.390288: step 171760, loss = 0.0774, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 13:43:34.207705: step 171780, loss = 0.0968, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 13:43:38.764953: step 171800, loss = 0.0739, acc = 0.9820 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:43:43.466085: step 171820, loss = 0.0978, acc = 0.9680 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 13:43:48.229912: step 171840, loss = 0.1310, acc = 0.9480 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 13:43:52.894638: step 171860, loss = 0.0937, acc = 0.9760 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 13:43:57.651522: step 171880, loss = 0.1251, acc = 0.9660 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 13:44:02.586739: step 171900, loss = 0.0964, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 13:44:07.176362: step 171920, loss = 0.1003, acc = 0.9700 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 13:44:11.912716: step 171940, loss = 0.0898, acc = 0.9760 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 13:44:16.713225: step 171960, loss = 0.0762, acc = 0.9820 (222.0 examples/sec; 0.288 sec/batch)
2017-05-09 13:44:21.145075: step 171980, loss = 0.0633, acc = 0.9920 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 13:44:25.794214: step 172000, loss = 0.1245, acc = 0.9700 (271.6 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 13:44:39.904574: step 172000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 13:44:49.722315: step 172000, acc = 0.9556, f1 = 0.9552
[Status] 2017-05-09 13:44:49.722411: step 172000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:44:54.274396: step 172020, loss = 0.1016, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 13:44:58.992941: step 172040, loss = 0.0801, acc = 0.9780 (249.8 examples/sec; 0.256 sec/batch)
2017-05-09 13:45:03.686838: step 172060, loss = 0.0862, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 13:45:08.231958: step 172080, loss = 0.0894, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 13:45:12.884355: step 172100, loss = 0.0773, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 13:45:18.048434: step 172120, loss = 0.1274, acc = 0.9640 (218.5 examples/sec; 0.293 sec/batch)
2017-05-09 13:45:22.767579: step 172140, loss = 0.0991, acc = 0.9800 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 13:45:27.331672: step 172160, loss = 0.0937, acc = 0.9760 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 13:45:32.089674: step 172180, loss = 0.0984, acc = 0.9760 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 13:45:36.739311: step 172200, loss = 0.1138, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 13:45:41.311229: step 172220, loss = 0.0928, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 13:45:46.029936: step 172240, loss = 0.1087, acc = 0.9760 (247.8 examples/sec; 0.258 sec/batch)
2017-05-09 13:45:50.521078: step 172260, loss = 0.0866, acc = 0.9800 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 13:45:55.090434: step 172280, loss = 0.1106, acc = 0.9760 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 13:45:59.679761: step 172300, loss = 0.1018, acc = 0.9720 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 13:46:04.273341: step 172320, loss = 0.0949, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:46:08.748687: step 172340, loss = 0.0821, acc = 0.9840 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 13:46:13.282381: step 172360, loss = 0.0966, acc = 0.9740 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 13:46:18.982917: step 172380, loss = 0.0872, acc = 0.9740 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 13:46:23.513615: step 172400, loss = 0.1100, acc = 0.9620 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 13:46:28.155521: step 172420, loss = 0.0886, acc = 0.9740 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 13:46:32.950552: step 172440, loss = 0.0885, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 13:46:37.464965: step 172460, loss = 0.1061, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 13:46:42.026673: step 172480, loss = 0.0932, acc = 0.9800 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 13:46:46.715941: step 172500, loss = 0.0679, acc = 0.9840 (269.5 examples/sec; 0.238 sec/batch)
2017-05-09 13:46:51.421464: step 172520, loss = 0.0949, acc = 0.9820 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 13:46:56.095951: step 172540, loss = 0.0824, acc = 0.9720 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 13:47:00.957582: step 172560, loss = 0.0836, acc = 0.9760 (216.6 examples/sec; 0.296 sec/batch)
2017-05-09 13:47:05.421546: step 172580, loss = 0.0914, acc = 0.9800 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 13:47:10.066951: step 172600, loss = 0.0930, acc = 0.9760 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 13:47:14.905769: step 172620, loss = 0.0779, acc = 0.9740 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 13:47:19.532687: step 172640, loss = 0.0855, acc = 0.9800 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 13:47:24.117793: step 172660, loss = 0.0912, acc = 0.9780 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 13:47:28.720634: step 172680, loss = 0.0760, acc = 0.9820 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 13:47:33.614552: step 172700, loss = 0.0987, acc = 0.9720 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 13:47:38.134805: step 172720, loss = 0.0956, acc = 0.9660 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 13:47:42.638655: step 172740, loss = 0.1139, acc = 0.9640 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 13:47:47.303842: step 172760, loss = 0.0758, acc = 0.9800 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 13:47:51.955701: step 172780, loss = 0.1034, acc = 0.9760 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 13:47:56.527124: step 172800, loss = 0.0832, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 13:48:01.080543: step 172820, loss = 0.1122, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 13:48:05.742146: step 172840, loss = 0.0744, acc = 0.9860 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 13:48:10.396307: step 172860, loss = 0.1288, acc = 0.9600 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 13:48:15.184057: step 172880, loss = 0.1009, acc = 0.9720 (241.1 examples/sec; 0.265 sec/batch)
2017-05-09 13:48:19.783035: step 172900, loss = 0.0832, acc = 0.9760 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 13:48:24.393642: step 172920, loss = 0.1113, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:48:28.999966: step 172940, loss = 0.0816, acc = 0.9780 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 13:48:33.771073: step 172960, loss = 0.0760, acc = 0.9760 (248.5 examples/sec; 0.258 sec/batch)
2017-05-09 13:48:38.284320: step 172980, loss = 0.0918, acc = 0.9760 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 13:48:42.935567: step 173000, loss = 0.0864, acc = 0.9780 (270.7 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 13:48:57.172042: step 173000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 13:49:07.012322: step 173000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 13:49:07.012425: step 173000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:49:11.554855: step 173020, loss = 0.1064, acc = 0.9660 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 13:49:16.435447: step 173040, loss = 0.0851, acc = 0.9760 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 13:49:20.959318: step 173060, loss = 0.0991, acc = 0.9720 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 13:49:25.512328: step 173080, loss = 0.0956, acc = 0.9720 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 13:49:30.156857: step 173100, loss = 0.0849, acc = 0.9700 (242.4 examples/sec; 0.264 sec/batch)
2017-05-09 13:49:34.845807: step 173120, loss = 0.0849, acc = 0.9740 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 13:49:39.562326: step 173140, loss = 0.0902, acc = 0.9700 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 13:49:44.507763: step 173160, loss = 0.0958, acc = 0.9740 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 13:49:49.256749: step 173180, loss = 0.0952, acc = 0.9780 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 13:49:53.769951: step 173200, loss = 0.0951, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 13:49:58.268813: step 173220, loss = 0.0845, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:50:03.039418: step 173240, loss = 0.0942, acc = 0.9700 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 13:50:08.107085: step 173260, loss = 0.1006, acc = 0.9660 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 13:50:12.623298: step 173280, loss = 0.0876, acc = 0.9820 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 13:50:17.509324: step 173300, loss = 0.1110, acc = 0.9740 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 13:50:22.470344: step 173320, loss = 0.0943, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 13:50:27.003170: step 173340, loss = 0.0911, acc = 0.9840 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 13:50:31.926375: step 173360, loss = 0.1004, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 13:50:37.303368: step 173380, loss = 0.0790, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 13:50:41.847128: step 173400, loss = 0.1055, acc = 0.9680 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 13:50:46.373676: step 173420, loss = 0.0833, acc = 0.9740 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 13:50:50.908466: step 173440, loss = 0.0964, acc = 0.9720 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 13:50:55.469812: step 173460, loss = 0.0989, acc = 0.9700 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 13:51:00.039358: step 173480, loss = 0.0808, acc = 0.9720 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 13:51:04.575562: step 173500, loss = 0.0814, acc = 0.9820 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 13:51:09.139611: step 173520, loss = 0.0981, acc = 0.9760 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 13:51:13.827421: step 173540, loss = 0.0837, acc = 0.9720 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 13:51:18.428713: step 173560, loss = 0.0952, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 13:51:23.161532: step 173580, loss = 0.1055, acc = 0.9720 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 13:51:27.814965: step 173600, loss = 0.0758, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 13:51:32.380935: step 173620, loss = 0.0890, acc = 0.9800 (265.0 examples/sec; 0.241 sec/batch)
2017-05-09 13:51:36.999992: step 173640, loss = 0.0987, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 13:51:41.921168: step 173660, loss = 0.0954, acc = 0.9740 (225.6 examples/sec; 0.284 sec/batch)
2017-05-09 13:51:46.616114: step 173680, loss = 0.0898, acc = 0.9780 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 13:51:51.098726: step 173700, loss = 0.0876, acc = 0.9840 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 13:51:55.773955: step 173720, loss = 0.0830, acc = 0.9820 (247.5 examples/sec; 0.259 sec/batch)
2017-05-09 13:52:00.524336: step 173740, loss = 0.1012, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 13:52:05.034268: step 173760, loss = 0.1060, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 13:52:09.671079: step 173780, loss = 0.0934, acc = 0.9760 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 13:52:14.332475: step 173800, loss = 0.0702, acc = 0.9840 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 13:52:18.988419: step 173820, loss = 0.0817, acc = 0.9780 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 13:52:23.689551: step 173840, loss = 0.0976, acc = 0.9700 (255.7 examples/sec; 0.250 sec/batch)
2017-05-09 13:52:28.364888: step 173860, loss = 0.0912, acc = 0.9720 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 13:52:32.912255: step 173880, loss = 0.0760, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 13:52:37.478847: step 173900, loss = 0.1267, acc = 0.9660 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 13:52:42.135428: step 173920, loss = 0.0955, acc = 0.9800 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 13:52:46.752489: step 173940, loss = 0.0998, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 13:52:51.345787: step 173960, loss = 0.0826, acc = 0.9820 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 13:52:56.308480: step 173980, loss = 0.0709, acc = 0.9840 (225.2 examples/sec; 0.284 sec/batch)
2017-05-09 13:53:00.838392: step 174000, loss = 0.1081, acc = 0.9760 (290.6 examples/sec; 0.220 sec/batch)
[Eval] 2017-05-09 13:53:14.931623: step 174000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 13:53:24.877133: step 174000, acc = 0.9553, f1 = 0.9550
[Status] 2017-05-09 13:53:24.877229: step 174000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:53:29.426568: step 174020, loss = 0.1219, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 13:53:34.119389: step 174040, loss = 0.0762, acc = 0.9840 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 13:53:38.953137: step 174060, loss = 0.0986, acc = 0.9720 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 13:53:43.577182: step 174080, loss = 0.0958, acc = 0.9740 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:53:48.110317: step 174100, loss = 0.1278, acc = 0.9740 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 13:53:52.635277: step 174120, loss = 0.1078, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 13:53:57.444812: step 174140, loss = 0.0959, acc = 0.9760 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 13:54:02.006306: step 174160, loss = 0.1084, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 13:54:06.638162: step 174180, loss = 0.1040, acc = 0.9760 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 13:54:11.530267: step 174200, loss = 0.0805, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 13:54:16.130865: step 174220, loss = 0.0826, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 13:54:20.720349: step 174240, loss = 0.0930, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 13:54:25.310711: step 174260, loss = 0.0785, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:54:29.914846: step 174280, loss = 0.0828, acc = 0.9840 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 13:54:34.513345: step 174300, loss = 0.0788, acc = 0.9780 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 13:54:39.169315: step 174320, loss = 0.0978, acc = 0.9720 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 13:54:43.979617: step 174340, loss = 0.0912, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 13:54:48.606326: step 174360, loss = 0.1118, acc = 0.9620 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 13:54:54.583639: step 174380, loss = 0.0799, acc = 0.9780 (109.4 examples/sec; 0.585 sec/batch)
2017-05-09 13:54:59.163890: step 174400, loss = 0.0681, acc = 0.9860 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 13:55:03.755737: step 174420, loss = 0.0792, acc = 0.9840 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 13:55:08.444788: step 174440, loss = 0.1000, acc = 0.9860 (235.6 examples/sec; 0.272 sec/batch)
2017-05-09 13:55:13.045368: step 174460, loss = 0.0758, acc = 0.9860 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 13:55:17.696292: step 174480, loss = 0.0729, acc = 0.9840 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 13:55:22.470015: step 174500, loss = 0.0944, acc = 0.9740 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 13:55:27.201262: step 174520, loss = 0.0908, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 13:55:31.724936: step 174540, loss = 0.0836, acc = 0.9760 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 13:55:36.268909: step 174560, loss = 0.0903, acc = 0.9800 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 13:55:40.886868: step 174580, loss = 0.1123, acc = 0.9640 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 13:55:45.489827: step 174600, loss = 0.0818, acc = 0.9820 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 13:55:50.092212: step 174620, loss = 0.0875, acc = 0.9780 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 13:55:54.875708: step 174640, loss = 0.1092, acc = 0.9640 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 13:55:59.307246: step 174660, loss = 0.0872, acc = 0.9760 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 13:56:04.168254: step 174680, loss = 0.0922, acc = 0.9760 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 13:56:08.884756: step 174700, loss = 0.0809, acc = 0.9860 (243.5 examples/sec; 0.263 sec/batch)
2017-05-09 13:56:13.422233: step 174720, loss = 0.1202, acc = 0.9660 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:56:18.032958: step 174740, loss = 0.0859, acc = 0.9780 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 13:56:22.504919: step 174760, loss = 0.1022, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 13:56:27.236827: step 174780, loss = 0.0918, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 13:56:31.812263: step 174800, loss = 0.0895, acc = 0.9740 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 13:56:36.501800: step 174820, loss = 0.0771, acc = 0.9820 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 13:56:41.235507: step 174840, loss = 0.0807, acc = 0.9760 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 13:56:45.748143: step 174860, loss = 0.0845, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 13:56:50.187923: step 174880, loss = 0.0906, acc = 0.9740 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 13:56:54.868161: step 174900, loss = 0.0882, acc = 0.9740 (250.0 examples/sec; 0.256 sec/batch)
2017-05-09 13:56:59.475623: step 174920, loss = 0.0857, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 13:57:04.109514: step 174940, loss = 0.0964, acc = 0.9660 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 13:57:08.717578: step 174960, loss = 0.0804, acc = 0.9760 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 13:57:13.504544: step 174980, loss = 0.0808, acc = 0.9740 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 13:57:18.075055: step 175000, loss = 0.0726, acc = 0.9800 (288.2 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 13:57:32.065859: step 175000, acc = 0.9639, f1 = 0.9627
[Test] 2017-05-09 13:57:41.719854: step 175000, acc = 0.9543, f1 = 0.9539
[Status] 2017-05-09 13:57:41.719949: step 175000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 13:57:46.311758: step 175020, loss = 0.1211, acc = 0.9680 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 13:57:50.934731: step 175040, loss = 0.0739, acc = 0.9860 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 13:57:55.711161: step 175060, loss = 0.0889, acc = 0.9800 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 13:58:00.257294: step 175080, loss = 0.0855, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 13:58:04.877732: step 175100, loss = 0.0825, acc = 0.9700 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 13:58:09.562270: step 175120, loss = 0.0836, acc = 0.9820 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 13:58:14.173604: step 175140, loss = 0.1022, acc = 0.9740 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 13:58:18.801591: step 175160, loss = 0.0827, acc = 0.9780 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 13:58:23.306108: step 175180, loss = 0.0589, acc = 0.9900 (295.8 examples/sec; 0.216 sec/batch)
2017-05-09 13:58:28.046034: step 175200, loss = 0.0724, acc = 0.9800 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 13:58:32.693541: step 175220, loss = 0.0922, acc = 0.9780 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 13:58:37.289423: step 175240, loss = 0.1491, acc = 0.9700 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 13:58:42.102123: step 175260, loss = 0.0825, acc = 0.9800 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:58:46.694175: step 175280, loss = 0.0900, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 13:58:51.572219: step 175300, loss = 0.0777, acc = 0.9800 (226.4 examples/sec; 0.283 sec/batch)
2017-05-09 13:58:56.267342: step 175320, loss = 0.1045, acc = 0.9680 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 13:59:00.841287: step 175340, loss = 0.0974, acc = 0.9740 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 13:59:05.371877: step 175360, loss = 0.1184, acc = 0.9620 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 13:59:10.101134: step 175380, loss = 0.0935, acc = 0.9740 (234.5 examples/sec; 0.273 sec/batch)
2017-05-09 13:59:15.339079: step 175400, loss = 0.0841, acc = 0.9780 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 13:59:19.998323: step 175420, loss = 0.1017, acc = 0.9780 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 13:59:24.835026: step 175440, loss = 0.1188, acc = 0.9680 (228.1 examples/sec; 0.281 sec/batch)
2017-05-09 13:59:29.467028: step 175460, loss = 0.0928, acc = 0.9720 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 13:59:34.206935: step 175480, loss = 0.0961, acc = 0.9680 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 13:59:38.841108: step 175500, loss = 0.0887, acc = 0.9720 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 13:59:43.505338: step 175520, loss = 0.1148, acc = 0.9580 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 13:59:47.966394: step 175540, loss = 0.0933, acc = 0.9720 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 13:59:52.492012: step 175560, loss = 0.0986, acc = 0.9720 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 13:59:57.047642: step 175580, loss = 0.0801, acc = 0.9780 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 14:00:01.640549: step 175600, loss = 0.1018, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 14:00:06.275992: step 175620, loss = 0.0917, acc = 0.9780 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 14:00:11.006378: step 175640, loss = 0.0871, acc = 0.9780 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 14:00:15.590471: step 175660, loss = 0.1000, acc = 0.9620 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 14:00:20.309904: step 175680, loss = 0.0873, acc = 0.9780 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 14:00:25.279442: step 175700, loss = 0.1189, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:00:29.821215: step 175720, loss = 0.1025, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 14:00:34.448784: step 175740, loss = 0.0821, acc = 0.9780 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 14:00:39.473697: step 175760, loss = 0.0796, acc = 0.9860 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 14:00:44.032275: step 175780, loss = 0.0937, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 14:00:48.588154: step 175800, loss = 0.0921, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 14:00:53.247016: step 175820, loss = 0.0976, acc = 0.9800 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 14:00:57.943885: step 175840, loss = 0.0890, acc = 0.9760 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 14:01:02.470126: step 175860, loss = 0.1143, acc = 0.9620 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:01:06.886911: step 175880, loss = 0.0882, acc = 0.9780 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 14:01:11.664967: step 175900, loss = 0.1160, acc = 0.9700 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 14:01:16.361398: step 175920, loss = 0.1013, acc = 0.9760 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 14:01:20.938075: step 175940, loss = 0.0738, acc = 0.9900 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 14:01:25.695574: step 175960, loss = 0.0992, acc = 0.9600 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 14:01:30.331203: step 175980, loss = 0.0956, acc = 0.9780 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 14:01:34.939687: step 176000, loss = 0.1154, acc = 0.9620 (269.4 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 14:01:48.862497: step 176000, acc = 0.9636, f1 = 0.9625
[Test] 2017-05-09 14:01:58.827711: step 176000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 14:01:58.827813: step 176000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:02:03.523502: step 176020, loss = 0.1132, acc = 0.9640 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 14:02:08.515548: step 176040, loss = 0.0706, acc = 0.9840 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 14:02:13.214860: step 176060, loss = 0.0853, acc = 0.9840 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 14:02:17.805168: step 176080, loss = 0.0841, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 14:02:22.602683: step 176100, loss = 0.0880, acc = 0.9780 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 14:02:27.388861: step 176120, loss = 0.0762, acc = 0.9860 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:02:31.868350: step 176140, loss = 0.0795, acc = 0.9840 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 14:02:36.541261: step 176160, loss = 0.0984, acc = 0.9740 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:02:41.376678: step 176180, loss = 0.0799, acc = 0.9760 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 14:02:45.938773: step 176200, loss = 0.0929, acc = 0.9740 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 14:02:50.543315: step 176220, loss = 0.0848, acc = 0.9820 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 14:02:55.130956: step 176240, loss = 0.1258, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:02:59.786908: step 176260, loss = 0.0743, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 14:03:04.326573: step 176280, loss = 0.0985, acc = 0.9680 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 14:03:09.123277: step 176300, loss = 0.0816, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:03:13.728285: step 176320, loss = 0.0693, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:03:18.306901: step 176340, loss = 0.0896, acc = 0.9720 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 14:03:23.044953: step 176360, loss = 0.0885, acc = 0.9780 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 14:03:27.766920: step 176380, loss = 0.1031, acc = 0.9760 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:03:33.440156: step 176400, loss = 0.0806, acc = 0.9740 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 14:03:38.332614: step 176420, loss = 0.0895, acc = 0.9720 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 14:03:42.955826: step 176440, loss = 0.0792, acc = 0.9820 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 14:03:47.474340: step 176460, loss = 0.0692, acc = 0.9820 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 14:03:52.202982: step 176480, loss = 0.0785, acc = 0.9840 (245.9 examples/sec; 0.260 sec/batch)
2017-05-09 14:03:56.827645: step 176500, loss = 0.0906, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 14:04:01.378483: step 176520, loss = 0.1011, acc = 0.9800 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 14:04:05.939519: step 176540, loss = 0.0671, acc = 0.9820 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:04:10.666778: step 176560, loss = 0.0566, acc = 0.9920 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 14:04:15.314681: step 176580, loss = 0.0869, acc = 0.9760 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 14:04:19.858521: step 176600, loss = 0.1055, acc = 0.9720 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 14:04:24.746138: step 176620, loss = 0.0896, acc = 0.9680 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 14:04:29.421625: step 176640, loss = 0.0957, acc = 0.9620 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 14:04:34.029177: step 176660, loss = 0.0978, acc = 0.9760 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:04:38.922858: step 176680, loss = 0.0880, acc = 0.9760 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 14:04:43.617005: step 176700, loss = 0.0697, acc = 0.9760 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 14:04:48.257845: step 176720, loss = 0.0961, acc = 0.9780 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 14:04:53.030364: step 176740, loss = 0.1034, acc = 0.9700 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 14:04:57.552415: step 176760, loss = 0.0710, acc = 0.9860 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 14:05:02.122612: step 176780, loss = 0.0888, acc = 0.9760 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 14:05:06.646131: step 176800, loss = 0.0871, acc = 0.9740 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 14:05:11.370543: step 176820, loss = 0.0750, acc = 0.9860 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 14:05:15.915614: step 176840, loss = 0.0960, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 14:05:20.683138: step 176860, loss = 0.1015, acc = 0.9720 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 14:05:25.586953: step 176880, loss = 0.0962, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 14:05:30.183053: step 176900, loss = 0.0774, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 14:05:34.826449: step 176920, loss = 0.0829, acc = 0.9820 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 14:05:39.569046: step 176940, loss = 0.1042, acc = 0.9760 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 14:05:44.082431: step 176960, loss = 0.0940, acc = 0.9720 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 14:05:48.786851: step 176980, loss = 0.0726, acc = 0.9820 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 14:05:53.404451: step 177000, loss = 0.0967, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 14:06:07.441502: step 177000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 14:06:16.889751: step 177000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 14:06:16.889825: step 177000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:06:21.577003: step 177020, loss = 0.0850, acc = 0.9800 (249.5 examples/sec; 0.257 sec/batch)
2017-05-09 14:06:26.170929: step 177040, loss = 0.0907, acc = 0.9700 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:06:30.919453: step 177060, loss = 0.0943, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:06:35.630613: step 177080, loss = 0.0673, acc = 0.9860 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:06:40.381746: step 177100, loss = 0.0777, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:06:45.034045: step 177120, loss = 0.0982, acc = 0.9700 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 14:06:49.598696: step 177140, loss = 0.0622, acc = 0.9860 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:06:54.104916: step 177160, loss = 0.0847, acc = 0.9780 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 14:06:58.782963: step 177180, loss = 0.0725, acc = 0.9880 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 14:07:03.348216: step 177200, loss = 0.0942, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 14:07:08.034770: step 177220, loss = 0.1029, acc = 0.9600 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 14:07:12.725238: step 177240, loss = 0.0767, acc = 0.9860 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 14:07:17.230867: step 177260, loss = 0.0880, acc = 0.9760 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:07:22.060113: step 177280, loss = 0.0927, acc = 0.9740 (227.2 examples/sec; 0.282 sec/batch)
2017-05-09 14:07:26.848097: step 177300, loss = 0.0869, acc = 0.9780 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 14:07:31.855215: step 177320, loss = 0.1079, acc = 0.9600 (201.2 examples/sec; 0.318 sec/batch)
2017-05-09 14:07:36.382926: step 177340, loss = 0.1087, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 14:07:41.222361: step 177360, loss = 0.0976, acc = 0.9620 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:07:45.955383: step 177380, loss = 0.1001, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 14:07:50.584867: step 177400, loss = 0.0873, acc = 0.9680 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 14:07:56.062259: step 177420, loss = 0.0708, acc = 0.9820 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 14:08:00.505353: step 177440, loss = 0.0871, acc = 0.9800 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 14:08:05.097930: step 177460, loss = 0.0978, acc = 0.9740 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 14:08:09.693280: step 177480, loss = 0.1128, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:08:14.264295: step 177500, loss = 0.1045, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:08:18.842836: step 177520, loss = 0.0956, acc = 0.9720 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 14:08:23.446994: step 177540, loss = 0.0826, acc = 0.9820 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 14:08:28.160185: step 177560, loss = 0.0996, acc = 0.9660 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 14:08:32.659060: step 177580, loss = 0.0794, acc = 0.9880 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 14:08:37.341386: step 177600, loss = 0.0728, acc = 0.9880 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 14:08:41.878845: step 177620, loss = 0.0872, acc = 0.9800 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:08:46.401164: step 177640, loss = 0.0827, acc = 0.9880 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:08:51.179832: step 177660, loss = 0.0761, acc = 0.9860 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 14:08:55.725740: step 177680, loss = 0.0899, acc = 0.9740 (295.6 examples/sec; 0.216 sec/batch)
2017-05-09 14:09:00.280050: step 177700, loss = 0.0753, acc = 0.9760 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 14:09:04.962391: step 177720, loss = 0.0796, acc = 0.9760 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 14:09:09.456657: step 177740, loss = 0.0826, acc = 0.9760 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 14:09:14.132387: step 177760, loss = 0.0948, acc = 0.9640 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:09:18.829450: step 177780, loss = 0.1026, acc = 0.9720 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 14:09:23.305551: step 177800, loss = 0.0735, acc = 0.9820 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 14:09:27.874658: step 177820, loss = 0.0984, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 14:09:32.452033: step 177840, loss = 0.0890, acc = 0.9660 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 14:09:37.223726: step 177860, loss = 0.1128, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:09:41.820099: step 177880, loss = 0.1004, acc = 0.9680 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 14:09:46.898963: step 177900, loss = 0.1020, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:09:51.592114: step 177920, loss = 0.0810, acc = 0.9740 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 14:09:56.047795: step 177940, loss = 0.0940, acc = 0.9740 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 14:10:00.684541: step 177960, loss = 0.0704, acc = 0.9840 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 14:10:05.463502: step 177980, loss = 0.0674, acc = 0.9900 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:10:10.308818: step 178000, loss = 0.0947, acc = 0.9680 (276.2 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 14:10:24.376908: step 178000, acc = 0.9638, f1 = 0.9626
[Test] 2017-05-09 14:10:34.272903: step 178000, acc = 0.9548, f1 = 0.9544
[Status] 2017-05-09 14:10:34.272985: step 178000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:10:38.923796: step 178020, loss = 0.0964, acc = 0.9660 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 14:10:43.453644: step 178040, loss = 0.0715, acc = 0.9880 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 14:10:48.116512: step 178060, loss = 0.1074, acc = 0.9700 (249.8 examples/sec; 0.256 sec/batch)
2017-05-09 14:10:52.698694: step 178080, loss = 0.1101, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 14:10:57.308617: step 178100, loss = 0.0900, acc = 0.9760 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 14:11:01.886756: step 178120, loss = 0.0939, acc = 0.9720 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 14:11:06.645600: step 178140, loss = 0.0768, acc = 0.9840 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 14:11:11.209141: step 178160, loss = 0.0963, acc = 0.9720 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 14:11:15.736332: step 178180, loss = 0.0807, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:11:20.437507: step 178200, loss = 0.0728, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:11:25.030295: step 178220, loss = 0.0772, acc = 0.9800 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 14:11:29.683637: step 178240, loss = 0.0969, acc = 0.9760 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 14:11:34.427766: step 178260, loss = 0.0944, acc = 0.9780 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 14:11:39.134590: step 178280, loss = 0.0904, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:11:43.914242: step 178300, loss = 0.0944, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 14:11:48.457061: step 178320, loss = 0.0927, acc = 0.9720 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 14:11:53.158594: step 178340, loss = 0.0771, acc = 0.9800 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 14:11:57.728969: step 178360, loss = 0.0823, acc = 0.9840 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:12:02.257202: step 178380, loss = 0.0752, acc = 0.9860 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:12:07.288417: step 178400, loss = 0.0832, acc = 0.9820 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 14:12:12.861604: step 178420, loss = 0.0943, acc = 0.9700 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 14:12:17.486609: step 178440, loss = 0.0816, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 14:12:22.247999: step 178460, loss = 0.1081, acc = 0.9660 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 14:12:26.855781: step 178480, loss = 0.1063, acc = 0.9620 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:12:31.502424: step 178500, loss = 0.1059, acc = 0.9740 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 14:12:36.210550: step 178520, loss = 0.0793, acc = 0.9860 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 14:12:40.877827: step 178540, loss = 0.0770, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 14:12:45.410541: step 178560, loss = 0.0893, acc = 0.9740 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 14:12:50.022235: step 178580, loss = 0.0964, acc = 0.9760 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 14:12:54.921969: step 178600, loss = 0.1044, acc = 0.9700 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 14:12:59.515390: step 178620, loss = 0.1118, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 14:13:03.972073: step 178640, loss = 0.0953, acc = 0.9740 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 14:13:08.701235: step 178660, loss = 0.0870, acc = 0.9740 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 14:13:13.240844: step 178680, loss = 0.1059, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 14:13:17.754141: step 178700, loss = 0.0917, acc = 0.9660 (291.6 examples/sec; 0.220 sec/batch)
2017-05-09 14:13:22.618580: step 178720, loss = 0.0813, acc = 0.9760 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 14:13:27.244752: step 178740, loss = 0.0643, acc = 0.9920 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 14:13:31.809116: step 178760, loss = 0.0982, acc = 0.9680 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 14:13:36.512646: step 178780, loss = 0.1178, acc = 0.9680 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 14:13:41.549370: step 178800, loss = 0.1010, acc = 0.9740 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 14:13:46.097134: step 178820, loss = 0.1007, acc = 0.9760 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 14:13:50.689245: step 178840, loss = 0.0858, acc = 0.9780 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 14:13:55.367244: step 178860, loss = 0.0827, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 14:13:59.868006: step 178880, loss = 0.0755, acc = 0.9800 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 14:14:04.358389: step 178900, loss = 0.0997, acc = 0.9680 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 14:14:09.215718: step 178920, loss = 0.1003, acc = 0.9740 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 14:14:13.731798: step 178940, loss = 0.0902, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 14:14:18.202357: step 178960, loss = 0.0812, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 14:14:23.001439: step 178980, loss = 0.0787, acc = 0.9740 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 14:14:27.583299: step 179000, loss = 0.0970, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 14:14:41.603527: step 179000, acc = 0.9638, f1 = 0.9627
[Test] 2017-05-09 14:14:51.207832: step 179000, acc = 0.9554, f1 = 0.9550
[Status] 2017-05-09 14:14:51.207922: step 179000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:14:55.686706: step 179020, loss = 0.1144, acc = 0.9620 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 14:15:00.187712: step 179040, loss = 0.0736, acc = 0.9860 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 14:15:04.948435: step 179060, loss = 0.0989, acc = 0.9760 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 14:15:09.712442: step 179080, loss = 0.0965, acc = 0.9700 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 14:15:14.289630: step 179100, loss = 0.0775, acc = 0.9800 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 14:15:19.074276: step 179120, loss = 0.0948, acc = 0.9780 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 14:15:23.887421: step 179140, loss = 0.1005, acc = 0.9700 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 14:15:28.548850: step 179160, loss = 0.0829, acc = 0.9820 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:15:33.049156: step 179180, loss = 0.0838, acc = 0.9820 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 14:15:37.835574: step 179200, loss = 0.0897, acc = 0.9760 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 14:15:42.362537: step 179220, loss = 0.1032, acc = 0.9680 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 14:15:46.970914: step 179240, loss = 0.0997, acc = 0.9740 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 14:15:51.769649: step 179260, loss = 0.1190, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 14:15:56.348483: step 179280, loss = 0.1297, acc = 0.9600 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 14:16:00.952721: step 179300, loss = 0.0874, acc = 0.9800 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 14:16:05.876366: step 179320, loss = 0.1080, acc = 0.9680 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 14:16:10.601890: step 179340, loss = 0.0885, acc = 0.9800 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 14:16:15.079619: step 179360, loss = 0.0946, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 14:16:19.703636: step 179380, loss = 0.0969, acc = 0.9640 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 14:16:24.428927: step 179400, loss = 0.0868, acc = 0.9760 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 14:16:29.608817: step 179420, loss = 0.0981, acc = 0.9760 (162.3 examples/sec; 0.394 sec/batch)
2017-05-09 14:16:34.416623: step 179440, loss = 0.0935, acc = 0.9800 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 14:16:39.107439: step 179460, loss = 0.0774, acc = 0.9780 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 14:16:43.716686: step 179480, loss = 0.1021, acc = 0.9700 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 14:16:48.472058: step 179500, loss = 0.0899, acc = 0.9680 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 14:16:53.265310: step 179520, loss = 0.0875, acc = 0.9740 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:16:57.866375: step 179540, loss = 0.0889, acc = 0.9800 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 14:17:02.515287: step 179560, loss = 0.0848, acc = 0.9760 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 14:17:07.298955: step 179580, loss = 0.0879, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 14:17:11.981524: step 179600, loss = 0.1075, acc = 0.9700 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 14:17:16.560669: step 179620, loss = 0.0837, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 14:17:21.206968: step 179640, loss = 0.0723, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:17:25.744492: step 179660, loss = 0.0872, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 14:17:30.388554: step 179680, loss = 0.1196, acc = 0.9660 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 14:17:35.178540: step 179700, loss = 0.0852, acc = 0.9760 (292.9 examples/sec; 0.219 sec/batch)
2017-05-09 14:17:39.790288: step 179720, loss = 0.0891, acc = 0.9740 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 14:17:44.328087: step 179740, loss = 0.0766, acc = 0.9800 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 14:17:49.031272: step 179760, loss = 0.0831, acc = 0.9740 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 14:17:53.702766: step 179780, loss = 0.0980, acc = 0.9660 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 14:17:58.346414: step 179800, loss = 0.0738, acc = 0.9860 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 14:18:02.951563: step 179820, loss = 0.0755, acc = 0.9880 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 14:18:07.620493: step 179840, loss = 0.0799, acc = 0.9800 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 14:18:12.144940: step 179860, loss = 0.1049, acc = 0.9620 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 14:18:16.713338: step 179880, loss = 0.0770, acc = 0.9820 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 14:18:21.633279: step 179900, loss = 0.0896, acc = 0.9760 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 14:18:26.282494: step 179920, loss = 0.0980, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 14:18:30.870646: step 179940, loss = 0.1045, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:18:35.667007: step 179960, loss = 0.0857, acc = 0.9840 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 14:18:40.329407: step 179980, loss = 0.1030, acc = 0.9800 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 14:18:45.027519: step 180000, loss = 0.0625, acc = 0.9880 (288.0 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 14:18:59.114582: step 180000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 14:19:08.964129: step 180000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 14:19:08.964228: step 180000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:19:13.573481: step 180020, loss = 0.0981, acc = 0.9740 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 14:19:18.057678: step 180040, loss = 0.0681, acc = 0.9840 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 14:19:22.716323: step 180060, loss = 0.1051, acc = 0.9740 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 14:19:27.261637: step 180080, loss = 0.0996, acc = 0.9740 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 14:19:31.898987: step 180100, loss = 0.0980, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:19:36.632100: step 180120, loss = 0.0983, acc = 0.9760 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 14:19:41.261577: step 180140, loss = 0.0980, acc = 0.9740 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 14:19:45.777034: step 180160, loss = 0.1164, acc = 0.9740 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 14:19:50.471849: step 180180, loss = 0.0869, acc = 0.9800 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 14:19:55.129135: step 180200, loss = 0.0766, acc = 0.9820 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 14:19:59.706325: step 180220, loss = 0.1284, acc = 0.9680 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 14:20:04.451190: step 180240, loss = 0.0841, acc = 0.9780 (242.6 examples/sec; 0.264 sec/batch)
2017-05-09 14:20:08.910315: step 180260, loss = 0.1108, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 14:20:13.449586: step 180280, loss = 0.0809, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 14:20:18.052214: step 180300, loss = 0.0971, acc = 0.9740 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 14:20:22.706705: step 180320, loss = 0.0884, acc = 0.9800 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 14:20:27.294146: step 180340, loss = 0.0798, acc = 0.9800 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 14:20:31.903511: step 180360, loss = 0.0838, acc = 0.9800 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 14:20:36.946533: step 180380, loss = 0.0889, acc = 0.9780 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 14:20:41.548575: step 180400, loss = 0.0801, acc = 0.9720 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 14:20:46.158551: step 180420, loss = 0.0923, acc = 0.9740 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 14:20:52.067254: step 180440, loss = 0.0857, acc = 0.9820 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 14:20:56.576016: step 180460, loss = 0.0901, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:21:01.146574: step 180480, loss = 0.0838, acc = 0.9780 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 14:21:05.930148: step 180500, loss = 0.1059, acc = 0.9620 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 14:21:10.874495: step 180520, loss = 0.0826, acc = 0.9780 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 14:21:15.422706: step 180540, loss = 0.1109, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 14:21:20.273122: step 180560, loss = 0.0838, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 14:21:24.807223: step 180580, loss = 0.0946, acc = 0.9720 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 14:21:29.428613: step 180600, loss = 0.0948, acc = 0.9740 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 14:21:34.261730: step 180620, loss = 0.0699, acc = 0.9860 (232.8 examples/sec; 0.275 sec/batch)
2017-05-09 14:21:38.869069: step 180640, loss = 0.1035, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 14:21:43.537062: step 180660, loss = 0.1070, acc = 0.9640 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 14:21:48.230771: step 180680, loss = 0.0736, acc = 0.9840 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 14:21:52.912476: step 180700, loss = 0.0982, acc = 0.9700 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 14:21:57.555297: step 180720, loss = 0.0848, acc = 0.9740 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 14:22:02.300411: step 180740, loss = 0.0800, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 14:22:07.230217: step 180760, loss = 0.0866, acc = 0.9780 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:22:11.713073: step 180780, loss = 0.1056, acc = 0.9660 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 14:22:16.277973: step 180800, loss = 0.0874, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 14:22:20.993185: step 180820, loss = 0.0991, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 14:22:25.560465: step 180840, loss = 0.0851, acc = 0.9760 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 14:22:30.103709: step 180860, loss = 0.0992, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 14:22:34.906883: step 180880, loss = 0.0947, acc = 0.9740 (231.4 examples/sec; 0.277 sec/batch)
2017-05-09 14:22:39.578975: step 180900, loss = 0.0654, acc = 0.9900 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 14:22:44.253980: step 180920, loss = 0.0708, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 14:22:48.956956: step 180940, loss = 0.0966, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 14:22:53.751415: step 180960, loss = 0.0793, acc = 0.9860 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 14:22:58.337174: step 180980, loss = 0.0991, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 14:23:02.896038: step 181000, loss = 0.0881, acc = 0.9800 (285.8 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 14:23:16.987892: step 181000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 14:23:26.882754: step 181000, acc = 0.9562, f1 = 0.9559
[Status] 2017-05-09 14:23:26.882901: step 181000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:23:31.455426: step 181020, loss = 0.0914, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:23:36.103441: step 181040, loss = 0.0834, acc = 0.9820 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 14:23:40.707329: step 181060, loss = 0.0953, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 14:23:45.367944: step 181080, loss = 0.0783, acc = 0.9820 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 14:23:50.164746: step 181100, loss = 0.0921, acc = 0.9840 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 14:23:54.876798: step 181120, loss = 0.0830, acc = 0.9820 (256.2 examples/sec; 0.250 sec/batch)
2017-05-09 14:23:59.370840: step 181140, loss = 0.0935, acc = 0.9700 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 14:24:04.134629: step 181160, loss = 0.1083, acc = 0.9720 (238.4 examples/sec; 0.268 sec/batch)
2017-05-09 14:24:08.730987: step 181180, loss = 0.0862, acc = 0.9800 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 14:24:13.270620: step 181200, loss = 0.0804, acc = 0.9840 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 14:24:17.818442: step 181220, loss = 0.1102, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 14:24:22.520205: step 181240, loss = 0.0839, acc = 0.9760 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 14:24:26.981864: step 181260, loss = 0.1058, acc = 0.9680 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 14:24:31.543028: step 181280, loss = 0.0868, acc = 0.9780 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 14:24:36.323926: step 181300, loss = 0.1047, acc = 0.9660 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 14:24:40.906332: step 181320, loss = 0.0993, acc = 0.9680 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 14:24:45.541094: step 181340, loss = 0.0923, acc = 0.9780 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 14:24:50.322236: step 181360, loss = 0.0878, acc = 0.9780 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 14:24:54.931890: step 181380, loss = 0.0819, acc = 0.9780 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 14:24:59.421497: step 181400, loss = 0.0967, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 14:25:04.217076: step 181420, loss = 0.1007, acc = 0.9760 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 14:25:09.671610: step 181440, loss = 0.1288, acc = 0.9620 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 14:25:14.340174: step 181460, loss = 0.0971, acc = 0.9660 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 14:25:18.891688: step 181480, loss = 0.0746, acc = 0.9840 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 14:25:23.785927: step 181500, loss = 0.0833, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 14:25:28.352245: step 181520, loss = 0.0690, acc = 0.9860 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 14:25:33.033278: step 181540, loss = 0.0876, acc = 0.9700 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 14:25:37.790784: step 181560, loss = 0.0915, acc = 0.9700 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 14:25:42.503571: step 181580, loss = 0.0832, acc = 0.9820 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 14:25:47.171189: step 181600, loss = 0.0929, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:25:51.948770: step 181620, loss = 0.0943, acc = 0.9780 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 14:25:56.541283: step 181640, loss = 0.0781, acc = 0.9840 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 14:26:01.143211: step 181660, loss = 0.0802, acc = 0.9760 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 14:26:05.967040: step 181680, loss = 0.0819, acc = 0.9820 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 14:26:10.575203: step 181700, loss = 0.1029, acc = 0.9680 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:26:15.239575: step 181720, loss = 0.0999, acc = 0.9700 (256.2 examples/sec; 0.250 sec/batch)
2017-05-09 14:26:19.810662: step 181740, loss = 0.1048, acc = 0.9700 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 14:26:24.344891: step 181760, loss = 0.0960, acc = 0.9780 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 14:26:28.957014: step 181780, loss = 0.0856, acc = 0.9800 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 14:26:33.522296: step 181800, loss = 0.0796, acc = 0.9800 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 14:26:38.464700: step 181820, loss = 0.0958, acc = 0.9680 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 14:26:43.014705: step 181840, loss = 0.1016, acc = 0.9680 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 14:26:47.503372: step 181860, loss = 0.0741, acc = 0.9780 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:26:52.460167: step 181880, loss = 0.0881, acc = 0.9840 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:26:57.013233: step 181900, loss = 0.0901, acc = 0.9820 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 14:27:01.555922: step 181920, loss = 0.0903, acc = 0.9720 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 14:27:06.243535: step 181940, loss = 0.0798, acc = 0.9760 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 14:27:10.779459: step 181960, loss = 0.0814, acc = 0.9860 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 14:27:15.411923: step 181980, loss = 0.1228, acc = 0.9520 (256.5 examples/sec; 0.250 sec/batch)
2017-05-09 14:27:20.234088: step 182000, loss = 0.1010, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 14:27:34.205676: step 182000, acc = 0.9644, f1 = 0.9631
[Test] 2017-05-09 14:27:43.736657: step 182000, acc = 0.9549, f1 = 0.9545
[Status] 2017-05-09 14:27:43.736759: step 182000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:27:48.546333: step 182020, loss = 0.1028, acc = 0.9740 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 14:27:52.971798: step 182040, loss = 0.0909, acc = 0.9700 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 14:27:57.532848: step 182060, loss = 0.0767, acc = 0.9840 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 14:28:02.338026: step 182080, loss = 0.0941, acc = 0.9740 (225.7 examples/sec; 0.284 sec/batch)
2017-05-09 14:28:06.962749: step 182100, loss = 0.0912, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:28:11.570518: step 182120, loss = 0.0728, acc = 0.9860 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 14:28:16.235840: step 182140, loss = 0.0882, acc = 0.9780 (241.3 examples/sec; 0.265 sec/batch)
2017-05-09 14:28:20.756144: step 182160, loss = 0.0945, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 14:28:25.297081: step 182180, loss = 0.0881, acc = 0.9780 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 14:28:29.852205: step 182200, loss = 0.0702, acc = 0.9840 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 14:28:34.438211: step 182220, loss = 0.0941, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 14:28:39.029008: step 182240, loss = 0.1395, acc = 0.9440 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 14:28:43.670933: step 182260, loss = 0.0959, acc = 0.9700 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 14:28:48.515418: step 182280, loss = 0.1025, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 14:28:53.114192: step 182300, loss = 0.0966, acc = 0.9760 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 14:28:57.753135: step 182320, loss = 0.0983, acc = 0.9700 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 14:29:02.426698: step 182340, loss = 0.0868, acc = 0.9800 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 14:29:06.938785: step 182360, loss = 0.0996, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 14:29:11.521745: step 182380, loss = 0.0746, acc = 0.9780 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:29:16.236967: step 182400, loss = 0.0671, acc = 0.9880 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 14:29:20.978739: step 182420, loss = 0.1117, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 14:29:25.499857: step 182440, loss = 0.0920, acc = 0.9780 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 14:29:31.191979: step 182460, loss = 0.0707, acc = 0.9900 (230.8 examples/sec; 0.277 sec/batch)
2017-05-09 14:29:35.836491: step 182480, loss = 0.1119, acc = 0.9660 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:29:40.375213: step 182500, loss = 0.0729, acc = 0.9880 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 14:29:44.970983: step 182520, loss = 0.1008, acc = 0.9780 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 14:29:49.756025: step 182540, loss = 0.0750, acc = 0.9820 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 14:29:54.425867: step 182560, loss = 0.0668, acc = 0.9880 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 14:29:59.010452: step 182580, loss = 0.1172, acc = 0.9680 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 14:30:03.604888: step 182600, loss = 0.0693, acc = 0.9880 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 14:30:08.125643: step 182620, loss = 0.0778, acc = 0.9860 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 14:30:12.738461: step 182640, loss = 0.0953, acc = 0.9740 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 14:30:17.404718: step 182660, loss = 0.0854, acc = 0.9820 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 14:30:21.870273: step 182680, loss = 0.0933, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 14:30:26.532361: step 182700, loss = 0.0785, acc = 0.9840 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 14:30:31.301129: step 182720, loss = 0.0814, acc = 0.9800 (233.8 examples/sec; 0.274 sec/batch)
2017-05-09 14:30:35.836932: step 182740, loss = 0.0853, acc = 0.9780 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 14:30:40.492645: step 182760, loss = 0.0904, acc = 0.9760 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:30:45.145659: step 182780, loss = 0.0980, acc = 0.9740 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 14:30:49.987254: step 182800, loss = 0.0747, acc = 0.9860 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 14:30:54.538207: step 182820, loss = 0.1022, acc = 0.9720 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 14:30:59.266281: step 182840, loss = 0.1007, acc = 0.9700 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 14:31:03.985872: step 182860, loss = 0.0947, acc = 0.9800 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 14:31:08.523549: step 182880, loss = 0.0928, acc = 0.9700 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 14:31:13.156577: step 182900, loss = 0.0780, acc = 0.9780 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:31:17.808509: step 182920, loss = 0.0803, acc = 0.9820 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 14:31:22.603251: step 182940, loss = 0.0971, acc = 0.9700 (260.7 examples/sec; 0.245 sec/batch)
2017-05-09 14:31:27.180983: step 182960, loss = 0.0795, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 14:31:31.926053: step 182980, loss = 0.0854, acc = 0.9880 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 14:31:36.441463: step 183000, loss = 0.0961, acc = 0.9680 (295.7 examples/sec; 0.216 sec/batch)
[Eval] 2017-05-09 14:31:50.505718: step 183000, acc = 0.9634, f1 = 0.9622
[Test] 2017-05-09 14:32:00.463720: step 183000, acc = 0.9543, f1 = 0.9540
[Status] 2017-05-09 14:32:00.463806: step 183000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:32:05.157685: step 183020, loss = 0.0791, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 14:32:09.949881: step 183040, loss = 0.0707, acc = 0.9880 (260.9 examples/sec; 0.245 sec/batch)
2017-05-09 14:32:14.861749: step 183060, loss = 0.1124, acc = 0.9640 (253.0 examples/sec; 0.253 sec/batch)
2017-05-09 14:32:19.703965: step 183080, loss = 0.0884, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 14:32:24.288676: step 183100, loss = 0.0799, acc = 0.9820 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 14:32:28.843031: step 183120, loss = 0.1130, acc = 0.9640 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 14:32:33.573122: step 183140, loss = 0.0965, acc = 0.9740 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 14:32:38.072254: step 183160, loss = 0.1056, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 14:32:42.765453: step 183180, loss = 0.0896, acc = 0.9820 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 14:32:47.480658: step 183200, loss = 0.0919, acc = 0.9720 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 14:32:52.268860: step 183220, loss = 0.1067, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 14:32:56.885380: step 183240, loss = 0.0906, acc = 0.9720 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 14:33:01.678858: step 183260, loss = 0.0986, acc = 0.9720 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 14:33:06.366625: step 183280, loss = 0.0801, acc = 0.9780 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 14:33:11.247868: step 183300, loss = 0.0951, acc = 0.9680 (242.8 examples/sec; 0.264 sec/batch)
2017-05-09 14:33:15.979723: step 183320, loss = 0.1024, acc = 0.9680 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 14:33:20.558996: step 183340, loss = 0.0825, acc = 0.9740 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 14:33:25.248701: step 183360, loss = 0.0817, acc = 0.9760 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 14:33:29.902823: step 183380, loss = 0.1085, acc = 0.9720 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 14:33:34.455910: step 183400, loss = 0.0785, acc = 0.9840 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 14:33:39.118305: step 183420, loss = 0.1476, acc = 0.9500 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 14:33:44.111755: step 183440, loss = 0.0917, acc = 0.9740 (213.3 examples/sec; 0.300 sec/batch)
2017-05-09 14:33:49.457000: step 183460, loss = 0.0894, acc = 0.9780 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 14:33:54.032175: step 183480, loss = 0.0950, acc = 0.9700 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 14:33:58.811949: step 183500, loss = 0.0817, acc = 0.9820 (234.3 examples/sec; 0.273 sec/batch)
2017-05-09 14:34:03.531177: step 183520, loss = 0.0873, acc = 0.9820 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 14:34:08.149995: step 183540, loss = 0.0772, acc = 0.9800 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 14:34:12.777911: step 183560, loss = 0.0912, acc = 0.9780 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 14:34:17.542798: step 183580, loss = 0.0829, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 14:34:22.280416: step 183600, loss = 0.0814, acc = 0.9840 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 14:34:26.871958: step 183620, loss = 0.0748, acc = 0.9820 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 14:34:31.727965: step 183640, loss = 0.0823, acc = 0.9720 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:34:36.247689: step 183660, loss = 0.0819, acc = 0.9760 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:34:40.754244: step 183680, loss = 0.0695, acc = 0.9840 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 14:34:45.470236: step 183700, loss = 0.0947, acc = 0.9740 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 14:34:50.029159: step 183720, loss = 0.0867, acc = 0.9780 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 14:34:54.554973: step 183740, loss = 0.0993, acc = 0.9680 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 14:34:59.308123: step 183760, loss = 0.0931, acc = 0.9780 (226.8 examples/sec; 0.282 sec/batch)
2017-05-09 14:35:04.333734: step 183780, loss = 0.0969, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 14:35:08.938722: step 183800, loss = 0.0753, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 14:35:13.565718: step 183820, loss = 0.0861, acc = 0.9820 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 14:35:18.376990: step 183840, loss = 0.0889, acc = 0.9800 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 14:35:22.941945: step 183860, loss = 0.0995, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:35:27.506918: step 183880, loss = 0.1008, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 14:35:32.139152: step 183900, loss = 0.0949, acc = 0.9720 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 14:35:36.756051: step 183920, loss = 0.0898, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:35:41.394568: step 183940, loss = 0.0768, acc = 0.9820 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:35:46.113894: step 183960, loss = 0.0835, acc = 0.9820 (299.1 examples/sec; 0.214 sec/batch)
2017-05-09 14:35:50.631347: step 183980, loss = 0.0859, acc = 0.9680 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 14:35:55.355603: step 184000, loss = 0.0966, acc = 0.9640 (247.2 examples/sec; 0.259 sec/batch)
[Eval] 2017-05-09 14:36:09.426409: step 184000, acc = 0.9630, f1 = 0.9618
[Test] 2017-05-09 14:36:19.270921: step 184000, acc = 0.9541, f1 = 0.9537
[Status] 2017-05-09 14:36:19.271028: step 184000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:36:23.892803: step 184020, loss = 0.1067, acc = 0.9580 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 14:36:28.564932: step 184040, loss = 0.1007, acc = 0.9800 (248.2 examples/sec; 0.258 sec/batch)
2017-05-09 14:36:33.307636: step 184060, loss = 0.0714, acc = 0.9820 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:36:37.852202: step 184080, loss = 0.0936, acc = 0.9720 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 14:36:42.351105: step 184100, loss = 0.0969, acc = 0.9680 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 14:36:47.000761: step 184120, loss = 0.0822, acc = 0.9760 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 14:36:51.524990: step 184140, loss = 0.1027, acc = 0.9720 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 14:36:56.352314: step 184160, loss = 0.0981, acc = 0.9720 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 14:37:01.027763: step 184180, loss = 0.0717, acc = 0.9820 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 14:37:05.819341: step 184200, loss = 0.0844, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 14:37:10.507774: step 184220, loss = 0.0758, acc = 0.9820 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 14:37:15.250020: step 184240, loss = 0.0873, acc = 0.9760 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 14:37:19.765434: step 184260, loss = 0.0751, acc = 0.9840 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 14:37:24.368921: step 184280, loss = 0.1021, acc = 0.9700 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 14:37:29.015795: step 184300, loss = 0.0977, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:37:33.703503: step 184320, loss = 0.0708, acc = 0.9820 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 14:37:38.315318: step 184340, loss = 0.0944, acc = 0.9760 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:37:42.955499: step 184360, loss = 0.0954, acc = 0.9740 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 14:37:47.612598: step 184380, loss = 0.1017, acc = 0.9680 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 14:37:52.280283: step 184400, loss = 0.1089, acc = 0.9720 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 14:37:56.780150: step 184420, loss = 0.0832, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 14:38:01.447868: step 184440, loss = 0.1215, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 14:38:07.385850: step 184460, loss = 0.0885, acc = 0.9820 (135.3 examples/sec; 0.473 sec/batch)
2017-05-09 14:38:11.949083: step 184480, loss = 0.0906, acc = 0.9700 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 14:38:16.798925: step 184500, loss = 0.0888, acc = 0.9820 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 14:38:21.421379: step 184520, loss = 0.1018, acc = 0.9780 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 14:38:26.061314: step 184540, loss = 0.0770, acc = 0.9860 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 14:38:31.063277: step 184560, loss = 0.1179, acc = 0.9660 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 14:38:35.654485: step 184580, loss = 0.1111, acc = 0.9660 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 14:38:40.311198: step 184600, loss = 0.0812, acc = 0.9700 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 14:38:44.974539: step 184620, loss = 0.0933, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 14:38:49.585571: step 184640, loss = 0.0906, acc = 0.9780 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 14:38:54.074030: step 184660, loss = 0.1218, acc = 0.9700 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 14:38:58.772309: step 184680, loss = 0.1041, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 14:39:03.456029: step 184700, loss = 0.0864, acc = 0.9800 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 14:39:07.944917: step 184720, loss = 0.1013, acc = 0.9700 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 14:39:12.531280: step 184740, loss = 0.0867, acc = 0.9800 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 14:39:17.233650: step 184760, loss = 0.1228, acc = 0.9680 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 14:39:21.937909: step 184780, loss = 0.1271, acc = 0.9540 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 14:39:26.535598: step 184800, loss = 0.0824, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 14:39:31.277470: step 184820, loss = 0.0787, acc = 0.9800 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 14:39:35.830097: step 184840, loss = 0.0794, acc = 0.9820 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 14:39:40.492456: step 184860, loss = 0.0843, acc = 0.9800 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 14:39:45.122840: step 184880, loss = 0.1098, acc = 0.9660 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 14:39:49.700734: step 184900, loss = 0.0894, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 14:39:54.273414: step 184920, loss = 0.0870, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 14:39:59.156564: step 184940, loss = 0.1020, acc = 0.9700 (227.1 examples/sec; 0.282 sec/batch)
2017-05-09 14:40:03.622235: step 184960, loss = 0.0866, acc = 0.9720 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 14:40:08.272871: step 184980, loss = 0.0640, acc = 0.9900 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 14:40:12.917966: step 185000, loss = 0.0802, acc = 0.9720 (288.4 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 14:40:26.964792: step 185000, acc = 0.9647, f1 = 0.9636
[Test] 2017-05-09 14:40:36.599713: step 185000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 14:40:36.599793: step 185000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:40:41.113365: step 185020, loss = 0.0865, acc = 0.9820 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 14:40:45.796763: step 185040, loss = 0.0804, acc = 0.9780 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 14:40:50.519013: step 185060, loss = 0.1081, acc = 0.9640 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 14:40:54.971652: step 185080, loss = 0.0776, acc = 0.9780 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 14:40:59.833641: step 185100, loss = 0.0753, acc = 0.9840 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:41:04.413756: step 185120, loss = 0.0597, acc = 0.9920 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 14:41:09.165155: step 185140, loss = 0.0981, acc = 0.9760 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 14:41:13.926558: step 185160, loss = 0.0851, acc = 0.9820 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 14:41:18.512653: step 185180, loss = 0.0904, acc = 0.9720 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 14:41:23.026345: step 185200, loss = 0.0833, acc = 0.9820 (302.2 examples/sec; 0.212 sec/batch)
2017-05-09 14:41:27.624170: step 185220, loss = 0.0989, acc = 0.9680 (269.5 examples/sec; 0.238 sec/batch)
2017-05-09 14:41:32.366508: step 185240, loss = 0.0912, acc = 0.9840 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 14:41:37.005273: step 185260, loss = 0.0723, acc = 0.9860 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 14:41:41.684099: step 185280, loss = 0.0788, acc = 0.9820 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 14:41:46.348702: step 185300, loss = 0.1122, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:41:50.899441: step 185320, loss = 0.0785, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 14:41:55.345873: step 185340, loss = 0.1046, acc = 0.9680 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 14:42:00.149880: step 185360, loss = 0.0821, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 14:42:04.629051: step 185380, loss = 0.0931, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 14:42:09.237469: step 185400, loss = 0.1094, acc = 0.9620 (251.6 examples/sec; 0.254 sec/batch)
2017-05-09 14:42:13.813220: step 185420, loss = 0.0687, acc = 0.9840 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 14:42:18.384463: step 185440, loss = 0.1042, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 14:42:22.974417: step 185460, loss = 0.0953, acc = 0.9720 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 14:42:28.649672: step 185480, loss = 0.1110, acc = 0.9660 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 14:42:33.302890: step 185500, loss = 0.0826, acc = 0.9800 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 14:42:37.831369: step 185520, loss = 0.1002, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 14:42:42.546924: step 185540, loss = 0.0792, acc = 0.9760 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 14:42:47.273180: step 185560, loss = 0.0789, acc = 0.9740 (295.8 examples/sec; 0.216 sec/batch)
2017-05-09 14:42:51.840469: step 185580, loss = 0.0783, acc = 0.9840 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 14:42:56.487999: step 185600, loss = 0.0981, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 14:43:01.076906: step 185620, loss = 0.0814, acc = 0.9780 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 14:43:05.686659: step 185640, loss = 0.0821, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:43:10.283947: step 185660, loss = 0.0899, acc = 0.9780 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 14:43:14.958608: step 185680, loss = 0.1009, acc = 0.9800 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 14:43:19.526057: step 185700, loss = 0.0886, acc = 0.9700 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 14:43:24.039569: step 185720, loss = 0.0829, acc = 0.9800 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 14:43:28.775369: step 185740, loss = 0.0758, acc = 0.9800 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 14:43:33.443079: step 185760, loss = 0.0996, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 14:43:38.014615: step 185780, loss = 0.0910, acc = 0.9780 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 14:43:42.665277: step 185800, loss = 0.0827, acc = 0.9780 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 14:43:47.509153: step 185820, loss = 0.0728, acc = 0.9880 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 14:43:52.160777: step 185840, loss = 0.0877, acc = 0.9780 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 14:43:56.790089: step 185860, loss = 0.0769, acc = 0.9860 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 14:44:01.695691: step 185880, loss = 0.1044, acc = 0.9700 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 14:44:06.308638: step 185900, loss = 0.0689, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 14:44:10.895544: step 185920, loss = 0.0947, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 14:44:15.563895: step 185940, loss = 0.1149, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 14:44:20.029453: step 185960, loss = 0.0751, acc = 0.9880 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 14:44:24.550443: step 185980, loss = 0.0847, acc = 0.9820 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 14:44:29.348233: step 186000, loss = 0.0674, acc = 0.9840 (294.7 examples/sec; 0.217 sec/batch)
[Eval] 2017-05-09 14:44:43.234400: step 186000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 14:44:52.840535: step 186000, acc = 0.9547, f1 = 0.9544
[Status] 2017-05-09 14:44:52.840633: step 186000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:44:57.415500: step 186020, loss = 0.1022, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:45:02.153883: step 186040, loss = 0.1096, acc = 0.9660 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 14:45:06.690053: step 186060, loss = 0.0820, acc = 0.9820 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 14:45:11.353972: step 186080, loss = 0.1016, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 14:45:16.049418: step 186100, loss = 0.1045, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 14:45:20.616187: step 186120, loss = 0.1006, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 14:45:25.087108: step 186140, loss = 0.0942, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 14:45:29.729598: step 186160, loss = 0.0870, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:45:34.279672: step 186180, loss = 0.0976, acc = 0.9720 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 14:45:39.082829: step 186200, loss = 0.0950, acc = 0.9680 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 14:45:43.870444: step 186220, loss = 0.0812, acc = 0.9820 (246.3 examples/sec; 0.260 sec/batch)
2017-05-09 14:45:48.416920: step 186240, loss = 0.0723, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:45:53.015891: step 186260, loss = 0.1107, acc = 0.9620 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 14:45:57.655916: step 186280, loss = 0.1245, acc = 0.9580 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 14:46:02.320164: step 186300, loss = 0.0754, acc = 0.9860 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 14:46:06.872766: step 186320, loss = 0.1196, acc = 0.9640 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 14:46:11.491148: step 186340, loss = 0.0741, acc = 0.9780 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 14:46:16.125843: step 186360, loss = 0.1151, acc = 0.9540 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 14:46:20.712298: step 186380, loss = 0.1092, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 14:46:25.294026: step 186400, loss = 0.0914, acc = 0.9720 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 14:46:30.069829: step 186420, loss = 0.0965, acc = 0.9780 (219.9 examples/sec; 0.291 sec/batch)
2017-05-09 14:46:34.560488: step 186440, loss = 0.1103, acc = 0.9700 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 14:46:39.164244: step 186460, loss = 0.0974, acc = 0.9720 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 14:46:44.880694: step 186480, loss = 0.0820, acc = 0.9840 (240.6 examples/sec; 0.266 sec/batch)
2017-05-09 14:46:49.448421: step 186500, loss = 0.0741, acc = 0.9860 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 14:46:54.207723: step 186520, loss = 0.0767, acc = 0.9760 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 14:46:59.520628: step 186540, loss = 0.1078, acc = 0.9720 (240.1 examples/sec; 0.267 sec/batch)
2017-05-09 14:47:04.063333: step 186560, loss = 0.1042, acc = 0.9640 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 14:47:08.615380: step 186580, loss = 0.0799, acc = 0.9820 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 14:47:13.263979: step 186600, loss = 0.0860, acc = 0.9800 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 14:47:18.038721: step 186620, loss = 0.0920, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 14:47:22.647669: step 186640, loss = 0.0839, acc = 0.9720 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 14:47:27.324431: step 186660, loss = 0.1079, acc = 0.9700 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 14:47:32.002036: step 186680, loss = 0.0963, acc = 0.9760 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 14:47:36.607302: step 186700, loss = 0.0776, acc = 0.9780 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 14:47:41.153298: step 186720, loss = 0.0781, acc = 0.9840 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 14:47:45.902705: step 186740, loss = 0.0790, acc = 0.9800 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 14:47:50.499823: step 186760, loss = 0.0734, acc = 0.9920 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 14:47:55.050413: step 186780, loss = 0.0872, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 14:47:59.795997: step 186800, loss = 0.0932, acc = 0.9720 (226.7 examples/sec; 0.282 sec/batch)
2017-05-09 14:48:04.365420: step 186820, loss = 0.1127, acc = 0.9640 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 14:48:08.909460: step 186840, loss = 0.1057, acc = 0.9700 (297.6 examples/sec; 0.215 sec/batch)
2017-05-09 14:48:13.476171: step 186860, loss = 0.0848, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 14:48:18.118354: step 186880, loss = 0.0998, acc = 0.9740 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 14:48:22.670618: step 186900, loss = 0.0669, acc = 0.9900 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 14:48:27.254185: step 186920, loss = 0.0863, acc = 0.9700 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 14:48:32.078969: step 186940, loss = 0.0911, acc = 0.9900 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 14:48:36.707254: step 186960, loss = 0.0879, acc = 0.9740 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 14:48:41.338946: step 186980, loss = 0.0818, acc = 0.9760 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 14:48:46.172426: step 187000, loss = 0.0880, acc = 0.9780 (271.1 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 14:49:00.263517: step 187000, acc = 0.9634, f1 = 0.9621
[Test] 2017-05-09 14:49:09.614954: step 187000, acc = 0.9538, f1 = 0.9534
[Status] 2017-05-09 14:49:09.615039: step 187000, maxindex = 165000, maxdev = 0.9649, maxtst = 0.9562
2017-05-09 14:49:14.336013: step 187020, loss = 0.0995, acc = 0.9680 (235.3 examples/sec; 0.272 sec/batch)
2017-05-09 14:49:18.933951: step 187040, loss = 0.0717, acc = 0.9840 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 14:49:23.595177: step 187060, loss = 0.0980, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 14:49:28.185796: step 187080, loss = 0.0790, acc = 0.9780 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:49:32.793500: step 187100, loss = 0.0802, acc = 0.9800 (299.0 examples/sec; 0.214 sec/batch)
2017-05-09 14:49:37.445709: step 187120, loss = 0.0887, acc = 0.9760 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 14:49:41.943362: step 187140, loss = 0.0959, acc = 0.9720 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 14:49:46.691357: step 187160, loss = 0.0945, acc = 0.9740 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 14:49:51.362510: step 187180, loss = 0.0996, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:49:56.030751: step 187200, loss = 0.1106, acc = 0.9700 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 14:50:00.737505: step 187220, loss = 0.0873, acc = 0.9820 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 14:50:05.294547: step 187240, loss = 0.1133, acc = 0.9680 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 14:50:10.261688: step 187260, loss = 0.0888, acc = 0.9760 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 14:50:14.920196: step 187280, loss = 0.0856, acc = 0.9740 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 14:50:19.824609: step 187300, loss = 0.0836, acc = 0.9780 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 14:50:24.447161: step 187320, loss = 0.1029, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 14:50:29.076383: step 187340, loss = 0.0850, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 14:50:33.778657: step 187360, loss = 0.0829, acc = 0.9760 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 14:50:38.364490: step 187380, loss = 0.0906, acc = 0.9800 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 14:50:43.056397: step 187400, loss = 0.0940, acc = 0.9760 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 14:50:47.881739: step 187420, loss = 0.0962, acc = 0.9760 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 14:50:52.316539: step 187440, loss = 0.0621, acc = 0.9880 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 14:50:56.968763: step 187460, loss = 0.0826, acc = 0.9820 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 14:51:01.731129: step 187480, loss = 0.0728, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 14:51:07.115427: step 187500, loss = 0.0783, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:51:11.675954: step 187520, loss = 0.0871, acc = 0.9800 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 14:51:16.357686: step 187540, loss = 0.0866, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 14:51:20.880039: step 187560, loss = 0.0821, acc = 0.9780 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 14:51:25.457154: step 187580, loss = 0.0879, acc = 0.9720 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 14:51:30.266307: step 187600, loss = 0.1049, acc = 0.9700 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 14:51:34.873836: step 187620, loss = 0.1003, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 14:51:39.442427: step 187640, loss = 0.0975, acc = 0.9740 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 14:51:44.297298: step 187660, loss = 0.1007, acc = 0.9700 (237.2 examples/sec; 0.270 sec/batch)
2017-05-09 14:51:48.882176: step 187680, loss = 0.0861, acc = 0.9780 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 14:51:53.569906: step 187700, loss = 0.0794, acc = 0.9860 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 14:51:58.221251: step 187720, loss = 0.0844, acc = 0.9860 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 14:52:03.014028: step 187740, loss = 0.0862, acc = 0.9800 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 14:52:07.707820: step 187760, loss = 0.0748, acc = 0.9900 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 14:52:12.310230: step 187780, loss = 0.1060, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 14:52:16.983558: step 187800, loss = 0.0835, acc = 0.9760 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 14:52:21.486563: step 187820, loss = 0.1078, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 14:52:26.065299: step 187840, loss = 0.1061, acc = 0.9720 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 14:52:30.785334: step 187860, loss = 0.0870, acc = 0.9820 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 14:52:35.473518: step 187880, loss = 0.1176, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 14:52:39.950732: step 187900, loss = 0.0839, acc = 0.9740 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 14:52:44.750565: step 187920, loss = 0.0922, acc = 0.9700 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 14:52:49.475854: step 187940, loss = 0.1008, acc = 0.9660 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 14:52:54.082441: step 187960, loss = 0.0983, acc = 0.9700 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 14:52:58.696966: step 187980, loss = 0.0906, acc = 0.9800 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 14:53:03.538683: step 188000, loss = 0.0849, acc = 0.9780 (274.5 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 14:53:17.564048: step 188000, acc = 0.9649, f1 = 0.9638
[Test] 2017-05-09 14:53:26.824825: step 188000, acc = 0.9564, f1 = 0.9560
[Status] 2017-05-09 14:53:26.824894: step 188000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 14:53:34.812174: step 188020, loss = 0.0824, acc = 0.9760 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 14:53:39.480649: step 188040, loss = 0.1137, acc = 0.9700 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 14:53:44.022131: step 188060, loss = 0.0983, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 14:53:48.544708: step 188080, loss = 0.0867, acc = 0.9740 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 14:53:53.256506: step 188100, loss = 0.0743, acc = 0.9840 (247.7 examples/sec; 0.258 sec/batch)
2017-05-09 14:53:57.959969: step 188120, loss = 0.0685, acc = 0.9880 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 14:54:02.623313: step 188140, loss = 0.0775, acc = 0.9820 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 14:54:07.312722: step 188160, loss = 0.0938, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 14:54:12.261324: step 188180, loss = 0.0857, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 14:54:16.881144: step 188200, loss = 0.1161, acc = 0.9640 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 14:54:21.495809: step 188220, loss = 0.0869, acc = 0.9820 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 14:54:26.272346: step 188240, loss = 0.0796, acc = 0.9780 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 14:54:30.887869: step 188260, loss = 0.0674, acc = 0.9840 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:54:35.529084: step 188280, loss = 0.0815, acc = 0.9840 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 14:54:40.167208: step 188300, loss = 0.1009, acc = 0.9700 (251.0 examples/sec; 0.255 sec/batch)
2017-05-09 14:54:44.758192: step 188320, loss = 0.0833, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 14:54:49.328860: step 188340, loss = 0.0965, acc = 0.9800 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 14:54:53.858797: step 188360, loss = 0.0836, acc = 0.9780 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 14:54:58.769992: step 188380, loss = 0.0819, acc = 0.9780 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 14:55:03.374276: step 188400, loss = 0.0896, acc = 0.9760 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 14:55:07.943849: step 188420, loss = 0.0964, acc = 0.9780 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 14:55:12.613965: step 188440, loss = 0.0793, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 14:55:17.257361: step 188460, loss = 0.0867, acc = 0.9820 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:55:21.947506: step 188480, loss = 0.1234, acc = 0.9620 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 14:55:27.930050: step 188500, loss = 0.0793, acc = 0.9800 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 14:55:32.478667: step 188520, loss = 0.0763, acc = 0.9840 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 14:55:37.155262: step 188540, loss = 0.1066, acc = 0.9600 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 14:55:42.187417: step 188560, loss = 0.0758, acc = 0.9820 (251.5 examples/sec; 0.254 sec/batch)
2017-05-09 14:55:46.881743: step 188580, loss = 0.0931, acc = 0.9780 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 14:55:51.556685: step 188600, loss = 0.0805, acc = 0.9780 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:55:56.318817: step 188620, loss = 0.1072, acc = 0.9680 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 14:56:00.889843: step 188640, loss = 0.0844, acc = 0.9780 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 14:56:05.506358: step 188660, loss = 0.1013, acc = 0.9680 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 14:56:10.171293: step 188680, loss = 0.0894, acc = 0.9760 (243.2 examples/sec; 0.263 sec/batch)
2017-05-09 14:56:14.688905: step 188700, loss = 0.0824, acc = 0.9720 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 14:56:19.342749: step 188720, loss = 0.0758, acc = 0.9880 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 14:56:23.954886: step 188740, loss = 0.0636, acc = 0.9880 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 14:56:28.695453: step 188760, loss = 0.0781, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 14:56:33.238781: step 188780, loss = 0.0862, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 14:56:37.882185: step 188800, loss = 0.0628, acc = 0.9920 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 14:56:42.660917: step 188820, loss = 0.0735, acc = 0.9800 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:56:47.336619: step 188840, loss = 0.0908, acc = 0.9820 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 14:56:51.990549: step 188860, loss = 0.0924, acc = 0.9800 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 14:56:56.758902: step 188880, loss = 0.0779, acc = 0.9820 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 14:57:01.372016: step 188900, loss = 0.0889, acc = 0.9780 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 14:57:06.003110: step 188920, loss = 0.0907, acc = 0.9720 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 14:57:10.707960: step 188940, loss = 0.0893, acc = 0.9800 (308.4 examples/sec; 0.207 sec/batch)
2017-05-09 14:57:15.244719: step 188960, loss = 0.0742, acc = 0.9880 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 14:57:19.769446: step 188980, loss = 0.1005, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 14:57:24.792602: step 189000, loss = 0.1100, acc = 0.9720 (235.7 examples/sec; 0.272 sec/batch)
[Eval] 2017-05-09 14:57:38.511118: step 189000, acc = 0.9631, f1 = 0.9618
[Test] 2017-05-09 14:57:48.175396: step 189000, acc = 0.9534, f1 = 0.9530
[Status] 2017-05-09 14:57:48.175476: step 189000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 14:57:52.786402: step 189020, loss = 0.1035, acc = 0.9700 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 14:57:57.483570: step 189040, loss = 0.0761, acc = 0.9820 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 14:58:02.051957: step 189060, loss = 0.0850, acc = 0.9740 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 14:58:06.642298: step 189080, loss = 0.0655, acc = 0.9920 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 14:58:11.450862: step 189100, loss = 0.1119, acc = 0.9700 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 14:58:15.935115: step 189120, loss = 0.0933, acc = 0.9680 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 14:58:20.600280: step 189140, loss = 0.0889, acc = 0.9720 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 14:58:25.343651: step 189160, loss = 0.1033, acc = 0.9720 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 14:58:29.919870: step 189180, loss = 0.1061, acc = 0.9680 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 14:58:34.596584: step 189200, loss = 0.0735, acc = 0.9880 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 14:58:39.356293: step 189220, loss = 0.0968, acc = 0.9680 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 14:58:43.841071: step 189240, loss = 0.0910, acc = 0.9740 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 14:58:48.506874: step 189260, loss = 0.0963, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 14:58:53.098577: step 189280, loss = 0.0675, acc = 0.9860 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 14:58:57.896310: step 189300, loss = 0.1017, acc = 0.9720 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 14:59:02.520845: step 189320, loss = 0.1085, acc = 0.9640 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 14:59:07.066122: step 189340, loss = 0.0986, acc = 0.9680 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 14:59:11.733244: step 189360, loss = 0.1276, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 14:59:16.489542: step 189380, loss = 0.0775, acc = 0.9880 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 14:59:21.052366: step 189400, loss = 0.0796, acc = 0.9800 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 14:59:25.720103: step 189420, loss = 0.0847, acc = 0.9780 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 14:59:30.340452: step 189440, loss = 0.0881, acc = 0.9860 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 14:59:35.022129: step 189460, loss = 0.0811, acc = 0.9760 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 14:59:39.726928: step 189480, loss = 0.0883, acc = 0.9780 (257.3 examples/sec; 0.249 sec/batch)
2017-05-09 14:59:45.323053: step 189500, loss = 0.0855, acc = 0.9780 (157.4 examples/sec; 0.407 sec/batch)
2017-05-09 14:59:49.868697: step 189520, loss = 0.0822, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 14:59:54.386827: step 189540, loss = 0.0769, acc = 0.9740 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 14:59:59.037159: step 189560, loss = 0.0756, acc = 0.9780 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:00:03.597923: step 189580, loss = 0.1028, acc = 0.9620 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 15:00:08.218476: step 189600, loss = 0.0825, acc = 0.9840 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 15:00:13.002792: step 189620, loss = 0.0957, acc = 0.9700 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 15:00:17.614713: step 189640, loss = 0.0760, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 15:00:22.314434: step 189660, loss = 0.0800, acc = 0.9820 (258.6 examples/sec; 0.248 sec/batch)
2017-05-09 15:00:27.390807: step 189680, loss = 0.0805, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 15:00:31.858427: step 189700, loss = 0.0663, acc = 0.9920 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 15:00:36.455363: step 189720, loss = 0.0852, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 15:00:41.309936: step 189740, loss = 0.0867, acc = 0.9780 (229.6 examples/sec; 0.279 sec/batch)
2017-05-09 15:00:45.912130: step 189760, loss = 0.0881, acc = 0.9780 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 15:00:50.467215: step 189780, loss = 0.0904, acc = 0.9660 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:00:55.060409: step 189800, loss = 0.0847, acc = 0.9760 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 15:00:59.818002: step 189820, loss = 0.0719, acc = 0.9840 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 15:01:04.423045: step 189840, loss = 0.0888, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 15:01:09.072611: step 189860, loss = 0.1199, acc = 0.9740 (258.4 examples/sec; 0.248 sec/batch)
2017-05-09 15:01:13.855015: step 189880, loss = 0.0653, acc = 0.9900 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 15:01:18.475185: step 189900, loss = 0.0927, acc = 0.9760 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 15:01:23.102397: step 189920, loss = 0.0775, acc = 0.9800 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 15:01:27.802798: step 189940, loss = 0.1054, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:01:32.393797: step 189960, loss = 0.0970, acc = 0.9780 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 15:01:36.978970: step 189980, loss = 0.0735, acc = 0.9860 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 15:01:41.849106: step 190000, loss = 0.0868, acc = 0.9880 (239.3 examples/sec; 0.267 sec/batch)
[Eval] 2017-05-09 15:01:55.523357: step 190000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 15:02:05.336557: step 190000, acc = 0.9558, f1 = 0.9555
[Status] 2017-05-09 15:02:05.336642: step 190000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 15:02:10.009649: step 190020, loss = 0.1004, acc = 0.9620 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 15:02:14.880576: step 190040, loss = 0.0849, acc = 0.9800 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 15:02:19.535371: step 190060, loss = 0.0972, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 15:02:24.079881: step 190080, loss = 0.0782, acc = 0.9860 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 15:02:28.818410: step 190100, loss = 0.0974, acc = 0.9720 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 15:02:33.394050: step 190120, loss = 0.0972, acc = 0.9660 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 15:02:38.072005: step 190140, loss = 0.0780, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 15:02:42.763077: step 190160, loss = 0.0864, acc = 0.9720 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 15:02:47.510366: step 190180, loss = 0.1095, acc = 0.9620 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 15:02:52.168996: step 190200, loss = 0.0746, acc = 0.9780 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 15:02:56.741246: step 190220, loss = 0.0868, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 15:03:01.510800: step 190240, loss = 0.0870, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 15:03:06.059768: step 190260, loss = 0.0817, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 15:03:10.659509: step 190280, loss = 0.1188, acc = 0.9620 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 15:03:15.538873: step 190300, loss = 0.0728, acc = 0.9860 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 15:03:20.219050: step 190320, loss = 0.0772, acc = 0.9840 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 15:03:24.752658: step 190340, loss = 0.0803, acc = 0.9800 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 15:03:29.441667: step 190360, loss = 0.0995, acc = 0.9660 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 15:03:34.065030: step 190380, loss = 0.1050, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 15:03:38.643421: step 190400, loss = 0.0873, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 15:03:43.646253: step 190420, loss = 0.0911, acc = 0.9740 (219.1 examples/sec; 0.292 sec/batch)
2017-05-09 15:03:48.263413: step 190440, loss = 0.1065, acc = 0.9680 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 15:03:52.807098: step 190460, loss = 0.0939, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 15:03:57.612862: step 190480, loss = 0.0876, acc = 0.9780 (236.1 examples/sec; 0.271 sec/batch)
2017-05-09 15:04:02.146260: step 190500, loss = 0.0750, acc = 0.9840 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 15:04:07.691646: step 190520, loss = 0.0911, acc = 0.9740 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 15:04:12.664706: step 190540, loss = 0.0817, acc = 0.9780 (193.7 examples/sec; 0.330 sec/batch)
2017-05-09 15:04:17.257061: step 190560, loss = 0.1022, acc = 0.9640 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 15:04:21.863570: step 190580, loss = 0.1046, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 15:04:26.495003: step 190600, loss = 0.0930, acc = 0.9680 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:04:31.119874: step 190620, loss = 0.0927, acc = 0.9740 (297.6 examples/sec; 0.215 sec/batch)
2017-05-09 15:04:35.633944: step 190640, loss = 0.0727, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 15:04:40.196502: step 190660, loss = 0.0736, acc = 0.9820 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 15:04:44.987612: step 190680, loss = 0.0787, acc = 0.9800 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 15:04:49.453150: step 190700, loss = 0.0761, acc = 0.9820 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 15:04:54.157411: step 190720, loss = 0.0813, acc = 0.9800 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 15:04:58.919038: step 190740, loss = 0.1021, acc = 0.9720 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 15:05:03.444879: step 190760, loss = 0.0909, acc = 0.9740 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 15:05:07.943928: step 190780, loss = 0.0918, acc = 0.9800 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 15:05:12.663017: step 190800, loss = 0.0870, acc = 0.9840 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 15:05:17.266171: step 190820, loss = 0.0766, acc = 0.9800 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 15:05:21.974318: step 190840, loss = 0.0949, acc = 0.9760 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 15:05:26.638016: step 190860, loss = 0.0715, acc = 0.9840 (253.3 examples/sec; 0.253 sec/batch)
2017-05-09 15:05:31.189577: step 190880, loss = 0.1122, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 15:05:35.764889: step 190900, loss = 0.0910, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 15:05:40.342181: step 190920, loss = 0.0780, acc = 0.9780 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 15:05:45.157430: step 190940, loss = 0.0722, acc = 0.9860 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 15:05:49.678276: step 190960, loss = 0.0823, acc = 0.9840 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 15:05:54.206723: step 190980, loss = 0.0841, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 15:05:58.961656: step 191000, loss = 0.1125, acc = 0.9640 (284.2 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 15:06:12.963286: step 191000, acc = 0.9639, f1 = 0.9627
[Test] 2017-05-09 15:06:22.431757: step 191000, acc = 0.9543, f1 = 0.9539
[Status] 2017-05-09 15:06:22.431876: step 191000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 15:06:27.239812: step 191020, loss = 0.0809, acc = 0.9740 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 15:06:31.840653: step 191040, loss = 0.0802, acc = 0.9780 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:06:36.365263: step 191060, loss = 0.0950, acc = 0.9700 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 15:06:41.136239: step 191080, loss = 0.0958, acc = 0.9740 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 15:06:45.731394: step 191100, loss = 0.0705, acc = 0.9860 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 15:06:50.381845: step 191120, loss = 0.0902, acc = 0.9720 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 15:06:54.966168: step 191140, loss = 0.0981, acc = 0.9740 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 15:06:59.932909: step 191160, loss = 0.0886, acc = 0.9800 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 15:07:04.559731: step 191180, loss = 0.0918, acc = 0.9700 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 15:07:09.062500: step 191200, loss = 0.0996, acc = 0.9700 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 15:07:14.030113: step 191220, loss = 0.0853, acc = 0.9740 (237.3 examples/sec; 0.270 sec/batch)
2017-05-09 15:07:18.460630: step 191240, loss = 0.0851, acc = 0.9780 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 15:07:23.063106: step 191260, loss = 0.0920, acc = 0.9800 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 15:07:27.778588: step 191280, loss = 0.0880, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 15:07:32.224976: step 191300, loss = 0.0925, acc = 0.9780 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 15:07:36.815587: step 191320, loss = 0.0859, acc = 0.9800 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 15:07:41.649145: step 191340, loss = 0.1228, acc = 0.9620 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 15:07:46.275539: step 191360, loss = 0.0765, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 15:07:50.881047: step 191380, loss = 0.0662, acc = 0.9900 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 15:07:55.520407: step 191400, loss = 0.0998, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 15:08:00.094404: step 191420, loss = 0.0737, acc = 0.9800 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 15:08:04.578480: step 191440, loss = 0.0786, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 15:08:09.324061: step 191460, loss = 0.1377, acc = 0.9600 (234.2 examples/sec; 0.273 sec/batch)
2017-05-09 15:08:13.882475: step 191480, loss = 0.1055, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 15:08:18.553982: step 191500, loss = 0.0913, acc = 0.9660 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 15:08:23.763720: step 191520, loss = 0.0956, acc = 0.9680 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 15:08:28.562540: step 191540, loss = 0.0685, acc = 0.9820 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 15:08:33.077499: step 191560, loss = 0.0769, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 15:08:37.581919: step 191580, loss = 0.0816, acc = 0.9820 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:08:42.196641: step 191600, loss = 0.0997, acc = 0.9720 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 15:08:46.711420: step 191620, loss = 0.0654, acc = 0.9900 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:08:51.321486: step 191640, loss = 0.0774, acc = 0.9840 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:08:56.454426: step 191660, loss = 0.0930, acc = 0.9660 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 15:09:01.028322: step 191680, loss = 0.0824, acc = 0.9800 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 15:09:05.599706: step 191700, loss = 0.0905, acc = 0.9740 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 15:09:10.315012: step 191720, loss = 0.0749, acc = 0.9880 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 15:09:14.875544: step 191740, loss = 0.1020, acc = 0.9600 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 15:09:19.435193: step 191760, loss = 0.1083, acc = 0.9740 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 15:09:24.096800: step 191780, loss = 0.0985, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:09:28.911575: step 191800, loss = 0.0900, acc = 0.9840 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 15:09:33.424423: step 191820, loss = 0.0941, acc = 0.9740 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 15:09:37.886406: step 191840, loss = 0.0755, acc = 0.9820 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 15:09:42.593414: step 191860, loss = 0.0785, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 15:09:47.289988: step 191880, loss = 0.0929, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 15:09:51.904285: step 191900, loss = 0.0958, acc = 0.9760 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 15:09:56.726921: step 191920, loss = 0.0762, acc = 0.9840 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 15:10:01.248225: step 191940, loss = 0.0848, acc = 0.9780 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 15:10:05.909981: step 191960, loss = 0.0916, acc = 0.9740 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 15:10:10.784157: step 191980, loss = 0.0908, acc = 0.9740 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:10:15.252318: step 192000, loss = 0.1113, acc = 0.9640 (288.6 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 15:10:29.307610: step 192000, acc = 0.9623, f1 = 0.9610
[Test] 2017-05-09 15:10:38.934251: step 192000, acc = 0.9525, f1 = 0.9521
[Status] 2017-05-09 15:10:38.934334: step 192000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 15:10:43.424234: step 192020, loss = 0.0840, acc = 0.9820 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:10:47.861480: step 192040, loss = 0.1075, acc = 0.9620 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 15:10:52.611957: step 192060, loss = 0.0751, acc = 0.9780 (238.2 examples/sec; 0.269 sec/batch)
2017-05-09 15:10:57.033116: step 192080, loss = 0.0894, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:11:01.669674: step 192100, loss = 0.0934, acc = 0.9720 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:11:06.219602: step 192120, loss = 0.0715, acc = 0.9760 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 15:11:10.939435: step 192140, loss = 0.1083, acc = 0.9700 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 15:11:15.469890: step 192160, loss = 0.0687, acc = 0.9880 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 15:11:20.044976: step 192180, loss = 0.1046, acc = 0.9680 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 15:11:24.929004: step 192200, loss = 0.0989, acc = 0.9840 (254.8 examples/sec; 0.251 sec/batch)
2017-05-09 15:11:29.571905: step 192220, loss = 0.0896, acc = 0.9760 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 15:11:34.258273: step 192240, loss = 0.1162, acc = 0.9640 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 15:11:38.947494: step 192260, loss = 0.0823, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 15:11:43.473527: step 192280, loss = 0.0879, acc = 0.9780 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 15:11:48.128940: step 192300, loss = 0.1110, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 15:11:52.920343: step 192320, loss = 0.0902, acc = 0.9740 (233.2 examples/sec; 0.274 sec/batch)
2017-05-09 15:11:57.455288: step 192340, loss = 0.0673, acc = 0.9820 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 15:12:01.959639: step 192360, loss = 0.0804, acc = 0.9860 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 15:12:06.604341: step 192380, loss = 0.0882, acc = 0.9820 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 15:12:11.119075: step 192400, loss = 0.0763, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:12:15.640625: step 192420, loss = 0.0907, acc = 0.9740 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 15:12:20.457767: step 192440, loss = 0.0907, acc = 0.9820 (218.5 examples/sec; 0.293 sec/batch)
2017-05-09 15:12:24.992567: step 192460, loss = 0.0811, acc = 0.9820 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 15:12:29.645149: step 192480, loss = 0.0676, acc = 0.9820 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 15:12:34.199543: step 192500, loss = 0.1081, acc = 0.9680 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 15:12:38.816710: step 192520, loss = 0.0737, acc = 0.9880 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 15:12:44.433540: step 192540, loss = 0.1020, acc = 0.9700 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 15:12:49.402955: step 192560, loss = 0.0869, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 15:12:54.193399: step 192580, loss = 0.1004, acc = 0.9680 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 15:12:58.898907: step 192600, loss = 0.0739, acc = 0.9860 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 15:13:03.484620: step 192620, loss = 0.0894, acc = 0.9680 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 15:13:08.184739: step 192640, loss = 0.0875, acc = 0.9720 (301.4 examples/sec; 0.212 sec/batch)
2017-05-09 15:13:12.695618: step 192660, loss = 0.1122, acc = 0.9700 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 15:13:17.325547: step 192680, loss = 0.0908, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 15:13:22.072031: step 192700, loss = 0.0988, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 15:13:26.661630: step 192720, loss = 0.0806, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 15:13:31.394785: step 192740, loss = 0.0728, acc = 0.9820 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 15:13:36.108200: step 192760, loss = 0.0989, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 15:13:40.629128: step 192780, loss = 0.0835, acc = 0.9820 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 15:13:45.189937: step 192800, loss = 0.0804, acc = 0.9860 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 15:13:49.960158: step 192820, loss = 0.0974, acc = 0.9680 (237.2 examples/sec; 0.270 sec/batch)
2017-05-09 15:13:54.452749: step 192840, loss = 0.1007, acc = 0.9720 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 15:13:59.028701: step 192860, loss = 0.0849, acc = 0.9780 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 15:14:03.718989: step 192880, loss = 0.0921, acc = 0.9780 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 15:14:08.533838: step 192900, loss = 0.1010, acc = 0.9640 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 15:14:13.104971: step 192920, loss = 0.0807, acc = 0.9800 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:14:17.623629: step 192940, loss = 0.0822, acc = 0.9840 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 15:14:22.433750: step 192960, loss = 0.0810, acc = 0.9760 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 15:14:26.866352: step 192980, loss = 0.1200, acc = 0.9600 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:14:31.495354: step 193000, loss = 0.0717, acc = 0.9860 (275.2 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 15:14:45.538246: step 193000, acc = 0.9628, f1 = 0.9614
[Test] 2017-05-09 15:14:55.294890: step 193000, acc = 0.9529, f1 = 0.9525
[Status] 2017-05-09 15:14:55.294990: step 193000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 15:14:59.861111: step 193020, loss = 0.0914, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:15:04.487727: step 193040, loss = 0.1025, acc = 0.9680 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 15:15:09.267212: step 193060, loss = 0.0906, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 15:15:13.990271: step 193080, loss = 0.0872, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:15:18.558125: step 193100, loss = 0.0843, acc = 0.9780 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 15:15:23.213332: step 193120, loss = 0.0987, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 15:15:27.779677: step 193140, loss = 0.0821, acc = 0.9740 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 15:15:32.415549: step 193160, loss = 0.0981, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:15:37.220034: step 193180, loss = 0.0808, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 15:15:41.819313: step 193200, loss = 0.0812, acc = 0.9780 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 15:15:46.420922: step 193220, loss = 0.0923, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 15:15:51.308678: step 193240, loss = 0.0839, acc = 0.9720 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 15:15:55.936478: step 193260, loss = 0.0930, acc = 0.9820 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 15:16:00.540573: step 193280, loss = 0.1018, acc = 0.9780 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 15:16:05.512759: step 193300, loss = 0.1093, acc = 0.9700 (221.5 examples/sec; 0.289 sec/batch)
2017-05-09 15:16:10.065146: step 193320, loss = 0.0978, acc = 0.9680 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 15:16:14.650142: step 193340, loss = 0.0978, acc = 0.9800 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 15:16:19.304727: step 193360, loss = 0.0708, acc = 0.9900 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 15:16:24.183040: step 193380, loss = 0.0717, acc = 0.9820 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:16:28.795053: step 193400, loss = 0.0945, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 15:16:33.367992: step 193420, loss = 0.1068, acc = 0.9660 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 15:16:38.100090: step 193440, loss = 0.0743, acc = 0.9860 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 15:16:42.656166: step 193460, loss = 0.0670, acc = 0.9920 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 15:16:47.192012: step 193480, loss = 0.0946, acc = 0.9760 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 15:16:52.030655: step 193500, loss = 0.0897, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 15:16:56.592446: step 193520, loss = 0.0735, acc = 0.9860 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 15:17:02.159871: step 193540, loss = 0.0931, acc = 0.9780 (229.8 examples/sec; 0.279 sec/batch)
2017-05-09 15:17:06.997727: step 193560, loss = 0.0661, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:17:11.528467: step 193580, loss = 0.0804, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 15:17:16.048300: step 193600, loss = 0.0901, acc = 0.9740 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 15:17:20.782979: step 193620, loss = 0.0802, acc = 0.9900 (251.6 examples/sec; 0.254 sec/batch)
2017-05-09 15:17:25.257279: step 193640, loss = 0.0872, acc = 0.9760 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 15:17:29.892912: step 193660, loss = 0.1004, acc = 0.9660 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 15:17:34.506117: step 193680, loss = 0.1040, acc = 0.9640 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 15:17:39.561124: step 193700, loss = 0.0745, acc = 0.9820 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 15:17:44.210103: step 193720, loss = 0.1003, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 15:17:48.735625: step 193740, loss = 0.0880, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 15:17:53.613200: step 193760, loss = 0.0780, acc = 0.9720 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 15:17:58.196054: step 193780, loss = 0.1073, acc = 0.9640 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 15:18:02.687536: step 193800, loss = 0.0854, acc = 0.9780 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 15:18:07.644305: step 193820, loss = 0.0990, acc = 0.9680 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 15:18:12.283588: step 193840, loss = 0.0879, acc = 0.9740 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 15:18:16.840064: step 193860, loss = 0.0855, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:18:21.499474: step 193880, loss = 0.0971, acc = 0.9660 (241.2 examples/sec; 0.265 sec/batch)
2017-05-09 15:18:26.020252: step 193900, loss = 0.0860, acc = 0.9800 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 15:18:30.645079: step 193920, loss = 0.1208, acc = 0.9640 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 15:18:35.213338: step 193940, loss = 0.0937, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 15:18:39.909922: step 193960, loss = 0.0714, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 15:18:44.461455: step 193980, loss = 0.1034, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 15:18:48.988288: step 194000, loss = 0.0799, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 15:19:03.028528: step 194000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 15:19:13.031558: step 194000, acc = 0.9553, f1 = 0.9550
[Status] 2017-05-09 15:19:13.031669: step 194000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 15:19:17.589532: step 194020, loss = 0.0897, acc = 0.9700 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 15:19:22.286578: step 194040, loss = 0.0772, acc = 0.9820 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 15:19:26.837688: step 194060, loss = 0.0823, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 15:19:31.387339: step 194080, loss = 0.0891, acc = 0.9740 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 15:19:36.179621: step 194100, loss = 0.0993, acc = 0.9680 (233.5 examples/sec; 0.274 sec/batch)
2017-05-09 15:19:40.687642: step 194120, loss = 0.0913, acc = 0.9700 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 15:19:45.193751: step 194140, loss = 0.0981, acc = 0.9700 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 15:19:49.754550: step 194160, loss = 0.0894, acc = 0.9740 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 15:19:54.493328: step 194180, loss = 0.0823, acc = 0.9740 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 15:19:59.104723: step 194200, loss = 0.0724, acc = 0.9860 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 15:20:04.033486: step 194220, loss = 0.0889, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 15:20:08.756154: step 194240, loss = 0.0953, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 15:20:13.247149: step 194260, loss = 0.0795, acc = 0.9800 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 15:20:17.895282: step 194280, loss = 0.0719, acc = 0.9860 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 15:20:22.524698: step 194300, loss = 0.0729, acc = 0.9860 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 15:20:27.094457: step 194320, loss = 0.0849, acc = 0.9820 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 15:20:31.656287: step 194340, loss = 0.0624, acc = 0.9940 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 15:20:36.615416: step 194360, loss = 0.0763, acc = 0.9840 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 15:20:41.124710: step 194380, loss = 0.0963, acc = 0.9700 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:20:45.684988: step 194400, loss = 0.0917, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 15:20:50.472973: step 194420, loss = 0.0722, acc = 0.9800 (229.7 examples/sec; 0.279 sec/batch)
2017-05-09 15:20:54.969236: step 194440, loss = 0.1037, acc = 0.9700 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 15:20:59.608319: step 194460, loss = 0.0983, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:21:04.244878: step 194480, loss = 0.0928, acc = 0.9780 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 15:21:08.988922: step 194500, loss = 0.1030, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 15:21:13.565426: step 194520, loss = 0.0943, acc = 0.9700 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:21:19.165391: step 194540, loss = 0.1025, acc = 0.9660 (133.7 examples/sec; 0.479 sec/batch)
2017-05-09 15:21:23.908661: step 194560, loss = 0.0800, acc = 0.9780 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 15:21:28.591342: step 194580, loss = 0.0799, acc = 0.9780 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 15:21:33.235147: step 194600, loss = 0.0843, acc = 0.9740 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 15:21:37.871218: step 194620, loss = 0.1150, acc = 0.9640 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 15:21:42.461468: step 194640, loss = 0.0834, acc = 0.9760 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 15:21:47.059936: step 194660, loss = 0.0869, acc = 0.9680 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 15:21:51.771960: step 194680, loss = 0.0952, acc = 0.9780 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:21:56.325805: step 194700, loss = 0.1053, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 15:22:00.969642: step 194720, loss = 0.0623, acc = 0.9880 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 15:22:05.870252: step 194740, loss = 0.0834, acc = 0.9820 (235.5 examples/sec; 0.272 sec/batch)
2017-05-09 15:22:10.478245: step 194760, loss = 0.0843, acc = 0.9740 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 15:22:15.057610: step 194780, loss = 0.0721, acc = 0.9860 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 15:22:19.620776: step 194800, loss = 0.0786, acc = 0.9880 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 15:22:24.461645: step 194820, loss = 0.0768, acc = 0.9820 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 15:22:28.999108: step 194840, loss = 0.1055, acc = 0.9660 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 15:22:33.580064: step 194860, loss = 0.0898, acc = 0.9720 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:22:38.140396: step 194880, loss = 0.0917, acc = 0.9800 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 15:22:42.778445: step 194900, loss = 0.0831, acc = 0.9720 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 15:22:47.431052: step 194920, loss = 0.0949, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 15:22:52.166611: step 194940, loss = 0.0840, acc = 0.9760 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 15:22:56.879579: step 194960, loss = 0.1002, acc = 0.9700 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 15:23:01.601043: step 194980, loss = 0.0859, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 15:23:06.247744: step 195000, loss = 0.0733, acc = 0.9840 (255.4 examples/sec; 0.251 sec/batch)
[Eval] 2017-05-09 15:23:20.003469: step 195000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 15:23:29.784634: step 195000, acc = 0.9556, f1 = 0.9553
[Status] 2017-05-09 15:23:29.784719: step 195000, maxindex = 188000, maxdev = 0.9649, maxtst = 0.9564
2017-05-09 15:23:34.353439: step 195020, loss = 0.0939, acc = 0.9740 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 15:23:39.150881: step 195040, loss = 0.0789, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 15:23:43.699381: step 195060, loss = 0.0906, acc = 0.9840 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 15:23:48.297635: step 195080, loss = 0.1133, acc = 0.9660 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 15:23:52.993220: step 195100, loss = 0.0909, acc = 0.9700 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 15:23:57.515965: step 195120, loss = 0.0931, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:24:02.017280: step 195140, loss = 0.0777, acc = 0.9780 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 15:24:06.722715: step 195160, loss = 0.0701, acc = 0.9800 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 15:24:11.238330: step 195180, loss = 0.0821, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 15:24:15.914061: step 195200, loss = 0.0776, acc = 0.9860 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 15:24:20.460518: step 195220, loss = 0.0939, acc = 0.9840 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 15:24:25.328009: step 195240, loss = 0.0819, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 15:24:30.093382: step 195260, loss = 0.1060, acc = 0.9700 (243.3 examples/sec; 0.263 sec/batch)
2017-05-09 15:24:34.694418: step 195280, loss = 0.0953, acc = 0.9720 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 15:24:39.326339: step 195300, loss = 0.1054, acc = 0.9760 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 15:24:43.975667: step 195320, loss = 0.1163, acc = 0.9700 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 15:24:48.465617: step 195340, loss = 0.0879, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 15:24:53.277501: step 195360, loss = 0.0990, acc = 0.9680 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 15:24:58.253848: step 195380, loss = 0.0722, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:25:02.828522: step 195400, loss = 0.0789, acc = 0.9820 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 15:25:07.338167: step 195420, loss = 0.0980, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 15:25:12.038394: step 195440, loss = 0.0724, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 15:25:16.543192: step 195460, loss = 0.0868, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 15:25:21.509826: step 195480, loss = 0.0779, acc = 0.9780 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 15:25:26.285750: step 195500, loss = 0.0900, acc = 0.9800 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 15:25:30.869427: step 195520, loss = 0.0846, acc = 0.9840 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 15:25:35.545247: step 195540, loss = 0.0908, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:25:41.443026: step 195560, loss = 0.0845, acc = 0.9780 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 15:25:45.927651: step 195580, loss = 0.1011, acc = 0.9780 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 15:25:50.477619: step 195600, loss = 0.1000, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 15:25:55.081175: step 195620, loss = 0.1254, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:25:59.690316: step 195640, loss = 0.0739, acc = 0.9840 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:26:04.519301: step 195660, loss = 0.1035, acc = 0.9640 (219.6 examples/sec; 0.291 sec/batch)
2017-05-09 15:26:09.314686: step 195680, loss = 0.0712, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 15:26:13.851613: step 195700, loss = 0.0647, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 15:26:18.375647: step 195720, loss = 0.0873, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 15:26:23.201454: step 195740, loss = 0.0834, acc = 0.9840 (220.4 examples/sec; 0.290 sec/batch)
2017-05-09 15:26:27.741814: step 195760, loss = 0.0806, acc = 0.9840 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:26:32.376553: step 195780, loss = 0.0668, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:26:37.197531: step 195800, loss = 0.0967, acc = 0.9720 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 15:26:41.674899: step 195820, loss = 0.0893, acc = 0.9800 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 15:26:46.251743: step 195840, loss = 0.0997, acc = 0.9700 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 15:26:50.798817: step 195860, loss = 0.0964, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:26:55.630776: step 195880, loss = 0.1331, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 15:27:00.226976: step 195900, loss = 0.0777, acc = 0.9820 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 15:27:04.890158: step 195920, loss = 0.0761, acc = 0.9880 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 15:27:09.643904: step 195940, loss = 0.0850, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 15:27:14.268166: step 195960, loss = 0.0840, acc = 0.9820 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 15:27:19.062288: step 195980, loss = 0.0813, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 15:27:23.756277: step 196000, loss = 0.0815, acc = 0.9760 (284.6 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 15:27:37.959269: step 196000, acc = 0.9651, f1 = 0.9640
[Test] 2017-05-09 15:27:47.186122: step 196000, acc = 0.9558, f1 = 0.9555
[Status] 2017-05-09 15:27:47.186206: step 196000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:27:55.112527: step 196020, loss = 0.0712, acc = 0.9820 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 15:27:59.649864: step 196040, loss = 0.0873, acc = 0.9820 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 15:28:04.203257: step 196060, loss = 0.0971, acc = 0.9680 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:28:08.838448: step 196080, loss = 0.0803, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 15:28:13.435422: step 196100, loss = 0.0758, acc = 0.9760 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 15:28:17.962068: step 196120, loss = 0.0854, acc = 0.9740 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 15:28:22.746985: step 196140, loss = 0.0819, acc = 0.9800 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 15:28:27.216738: step 196160, loss = 0.1096, acc = 0.9660 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 15:28:31.926848: step 196180, loss = 0.0932, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 15:28:36.428104: step 196200, loss = 0.1010, acc = 0.9680 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:28:41.211123: step 196220, loss = 0.0757, acc = 0.9860 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 15:28:45.827484: step 196240, loss = 0.0867, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 15:28:50.558544: step 196260, loss = 0.0744, acc = 0.9860 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 15:28:55.504394: step 196280, loss = 0.1008, acc = 0.9640 (220.6 examples/sec; 0.290 sec/batch)
2017-05-09 15:29:00.181987: step 196300, loss = 0.0798, acc = 0.9860 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 15:29:04.732082: step 196320, loss = 0.0788, acc = 0.9820 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 15:29:09.444979: step 196340, loss = 0.0908, acc = 0.9700 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 15:29:14.072094: step 196360, loss = 0.0751, acc = 0.9840 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 15:29:18.596339: step 196380, loss = 0.0929, acc = 0.9820 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 15:29:23.194109: step 196400, loss = 0.1080, acc = 0.9780 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 15:29:27.722426: step 196420, loss = 0.0816, acc = 0.9840 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 15:29:32.216981: step 196440, loss = 0.0681, acc = 0.9840 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 15:29:36.906916: step 196460, loss = 0.0838, acc = 0.9820 (243.5 examples/sec; 0.263 sec/batch)
2017-05-09 15:29:41.575347: step 196480, loss = 0.0921, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 15:29:46.179470: step 196500, loss = 0.0749, acc = 0.9840 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 15:29:50.735711: step 196520, loss = 0.0703, acc = 0.9880 (298.4 examples/sec; 0.214 sec/batch)
2017-05-09 15:29:55.409211: step 196540, loss = 0.0865, acc = 0.9700 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 15:30:00.942974: step 196560, loss = 0.0958, acc = 0.9740 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 15:30:05.542502: step 196580, loss = 0.0945, acc = 0.9740 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 15:30:10.322933: step 196600, loss = 0.0869, acc = 0.9780 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:30:14.872992: step 196620, loss = 0.0843, acc = 0.9860 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 15:30:19.387482: step 196640, loss = 0.0905, acc = 0.9780 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 15:30:24.268790: step 196660, loss = 0.0837, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 15:30:29.066098: step 196680, loss = 0.0783, acc = 0.9820 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 15:30:33.715318: step 196700, loss = 0.0805, acc = 0.9800 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 15:30:38.619334: step 196720, loss = 0.0840, acc = 0.9740 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 15:30:43.288521: step 196740, loss = 0.0664, acc = 0.9900 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 15:30:47.856882: step 196760, loss = 0.1029, acc = 0.9740 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 15:30:52.695344: step 196780, loss = 0.1109, acc = 0.9760 (205.9 examples/sec; 0.311 sec/batch)
2017-05-09 15:30:57.167476: step 196800, loss = 0.0800, acc = 0.9760 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 15:31:01.683743: step 196820, loss = 0.1093, acc = 0.9680 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 15:31:06.334849: step 196840, loss = 0.0797, acc = 0.9760 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 15:31:10.874892: step 196860, loss = 0.0598, acc = 0.9900 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 15:31:15.406947: step 196880, loss = 0.1023, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 15:31:20.160437: step 196900, loss = 0.0730, acc = 0.9840 (258.2 examples/sec; 0.248 sec/batch)
2017-05-09 15:31:24.869003: step 196920, loss = 0.0775, acc = 0.9800 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 15:31:29.527412: step 196940, loss = 0.0777, acc = 0.9840 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 15:31:34.136451: step 196960, loss = 0.0793, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 15:31:39.001504: step 196980, loss = 0.0939, acc = 0.9740 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 15:31:43.552331: step 197000, loss = 0.0875, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 15:31:57.634929: step 197000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 15:32:07.300413: step 197000, acc = 0.9561, f1 = 0.9558
[Status] 2017-05-09 15:32:07.300495: step 197000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:32:11.881925: step 197020, loss = 0.0806, acc = 0.9800 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 15:32:16.353611: step 197040, loss = 0.0744, acc = 0.9800 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:32:21.138724: step 197060, loss = 0.0849, acc = 0.9800 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 15:32:25.646985: step 197080, loss = 0.0779, acc = 0.9800 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 15:32:30.281636: step 197100, loss = 0.0994, acc = 0.9720 (256.9 examples/sec; 0.249 sec/batch)
2017-05-09 15:32:34.899377: step 197120, loss = 0.1126, acc = 0.9640 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 15:32:39.388079: step 197140, loss = 0.0821, acc = 0.9720 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 15:32:44.040525: step 197160, loss = 0.0892, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 15:32:48.917313: step 197180, loss = 0.0818, acc = 0.9800 (218.7 examples/sec; 0.293 sec/batch)
2017-05-09 15:32:53.557235: step 197200, loss = 0.0749, acc = 0.9800 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 15:32:58.090093: step 197220, loss = 0.0955, acc = 0.9760 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 15:33:02.568388: step 197240, loss = 0.1112, acc = 0.9740 (296.9 examples/sec; 0.216 sec/batch)
2017-05-09 15:33:07.238631: step 197260, loss = 0.0984, acc = 0.9700 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 15:33:11.777070: step 197280, loss = 0.0825, acc = 0.9840 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 15:33:16.428664: step 197300, loss = 0.1203, acc = 0.9680 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 15:33:21.149961: step 197320, loss = 0.0774, acc = 0.9800 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 15:33:25.738511: step 197340, loss = 0.0749, acc = 0.9840 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 15:33:30.424242: step 197360, loss = 0.1362, acc = 0.9500 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 15:33:35.220205: step 197380, loss = 0.0787, acc = 0.9780 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 15:33:39.716609: step 197400, loss = 0.0868, acc = 0.9780 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 15:33:44.346439: step 197420, loss = 0.0823, acc = 0.9780 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 15:33:49.061332: step 197440, loss = 0.0786, acc = 0.9780 (250.5 examples/sec; 0.256 sec/batch)
2017-05-09 15:33:53.735560: step 197460, loss = 0.1117, acc = 0.9720 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 15:33:58.328355: step 197480, loss = 0.0928, acc = 0.9740 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 15:34:02.934782: step 197500, loss = 0.0774, acc = 0.9800 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 15:34:07.797796: step 197520, loss = 0.0866, acc = 0.9760 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 15:34:12.531610: step 197540, loss = 0.0819, acc = 0.9840 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 15:34:17.076624: step 197560, loss = 0.0974, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 15:34:22.544493: step 197580, loss = 0.0773, acc = 0.9800 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 15:34:27.135405: step 197600, loss = 0.0651, acc = 0.9860 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 15:34:31.682665: step 197620, loss = 0.0774, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 15:34:36.304565: step 197640, loss = 0.0733, acc = 0.9900 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 15:34:40.960121: step 197660, loss = 0.0859, acc = 0.9800 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 15:34:45.479797: step 197680, loss = 0.0673, acc = 0.9880 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 15:34:50.199078: step 197700, loss = 0.1132, acc = 0.9700 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 15:34:54.735968: step 197720, loss = 0.0742, acc = 0.9800 (299.0 examples/sec; 0.214 sec/batch)
2017-05-09 15:34:59.222029: step 197740, loss = 0.0738, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 15:35:04.008339: step 197760, loss = 0.0675, acc = 0.9860 (230.7 examples/sec; 0.277 sec/batch)
2017-05-09 15:35:08.571333: step 197780, loss = 0.1011, acc = 0.9680 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 15:35:13.158229: step 197800, loss = 0.0830, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 15:35:17.950494: step 197820, loss = 0.0854, acc = 0.9760 (221.3 examples/sec; 0.289 sec/batch)
2017-05-09 15:35:22.582812: step 197840, loss = 0.1038, acc = 0.9680 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 15:35:27.124616: step 197860, loss = 0.0817, acc = 0.9820 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 15:35:31.775102: step 197880, loss = 0.1066, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 15:35:36.640648: step 197900, loss = 0.0749, acc = 0.9780 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 15:35:41.265065: step 197920, loss = 0.0741, acc = 0.9800 (297.7 examples/sec; 0.215 sec/batch)
2017-05-09 15:35:45.939343: step 197940, loss = 0.0707, acc = 0.9840 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 15:35:50.623631: step 197960, loss = 0.0794, acc = 0.9800 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 15:35:55.045795: step 197980, loss = 0.1000, acc = 0.9740 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 15:35:59.593363: step 198000, loss = 0.0905, acc = 0.9780 (266.0 examples/sec; 0.241 sec/batch)
[Eval] 2017-05-09 15:36:13.695644: step 198000, acc = 0.9639, f1 = 0.9628
[Test] 2017-05-09 15:36:23.359944: step 198000, acc = 0.9557, f1 = 0.9553
[Status] 2017-05-09 15:36:23.360035: step 198000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:36:28.018155: step 198020, loss = 0.0812, acc = 0.9800 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 15:36:32.704819: step 198040, loss = 0.0923, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 15:36:37.234294: step 198060, loss = 0.0898, acc = 0.9740 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 15:36:41.895978: step 198080, loss = 0.1003, acc = 0.9640 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 15:36:46.643220: step 198100, loss = 0.1070, acc = 0.9600 (247.4 examples/sec; 0.259 sec/batch)
2017-05-09 15:36:51.153362: step 198120, loss = 0.0918, acc = 0.9760 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:36:55.738911: step 198140, loss = 0.0876, acc = 0.9800 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 15:37:00.210058: step 198160, loss = 0.0863, acc = 0.9760 (309.2 examples/sec; 0.207 sec/batch)
2017-05-09 15:37:04.970109: step 198180, loss = 0.1012, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:37:09.516262: step 198200, loss = 0.0986, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 15:37:14.080486: step 198220, loss = 0.0877, acc = 0.9820 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 15:37:18.717221: step 198240, loss = 0.0893, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 15:37:23.275667: step 198260, loss = 0.0956, acc = 0.9740 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 15:37:27.846202: step 198280, loss = 0.0875, acc = 0.9760 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 15:37:32.637710: step 198300, loss = 0.0718, acc = 0.9780 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 15:37:37.217758: step 198320, loss = 0.0777, acc = 0.9800 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 15:37:41.726626: step 198340, loss = 0.0939, acc = 0.9780 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 15:37:46.315447: step 198360, loss = 0.0843, acc = 0.9800 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 15:37:51.004128: step 198380, loss = 0.0786, acc = 0.9840 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 15:37:55.618780: step 198400, loss = 0.0897, acc = 0.9820 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 15:38:00.244329: step 198420, loss = 0.0862, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:38:04.842370: step 198440, loss = 0.0763, acc = 0.9900 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 15:38:09.488559: step 198460, loss = 0.0912, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 15:38:13.967425: step 198480, loss = 0.0963, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:38:18.688788: step 198500, loss = 0.0866, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:38:23.259870: step 198520, loss = 0.0737, acc = 0.9860 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 15:38:27.832048: step 198540, loss = 0.0790, acc = 0.9820 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 15:38:32.536441: step 198560, loss = 0.0799, acc = 0.9880 (235.9 examples/sec; 0.271 sec/batch)
2017-05-09 15:38:38.104991: step 198580, loss = 0.0766, acc = 0.9800 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 15:38:42.750859: step 198600, loss = 0.0848, acc = 0.9760 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 15:38:47.441834: step 198620, loss = 0.0882, acc = 0.9760 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 15:38:52.153984: step 198640, loss = 0.1061, acc = 0.9680 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 15:38:56.718906: step 198660, loss = 0.0710, acc = 0.9820 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 15:39:01.302907: step 198680, loss = 0.0885, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 15:39:06.120665: step 198700, loss = 0.0935, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 15:39:10.832898: step 198720, loss = 0.0739, acc = 0.9800 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 15:39:15.395271: step 198740, loss = 0.0874, acc = 0.9740 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 15:39:20.236321: step 198760, loss = 0.1000, acc = 0.9720 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 15:39:24.762189: step 198780, loss = 0.0730, acc = 0.9860 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 15:39:29.372537: step 198800, loss = 0.0937, acc = 0.9680 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 15:39:34.205625: step 198820, loss = 0.0976, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 15:39:38.731143: step 198840, loss = 0.0869, acc = 0.9800 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 15:39:43.588996: step 198860, loss = 0.0859, acc = 0.9800 (212.1 examples/sec; 0.302 sec/batch)
2017-05-09 15:39:48.285124: step 198880, loss = 0.0600, acc = 0.9840 (251.6 examples/sec; 0.254 sec/batch)
2017-05-09 15:39:52.941302: step 198900, loss = 0.1029, acc = 0.9680 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 15:39:57.477836: step 198920, loss = 0.0663, acc = 0.9840 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 15:40:02.120090: step 198940, loss = 0.0864, acc = 0.9780 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 15:40:06.803809: step 198960, loss = 0.1164, acc = 0.9660 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 15:40:11.373827: step 198980, loss = 0.1090, acc = 0.9720 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 15:40:15.938748: step 199000, loss = 0.0847, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 15:40:30.054177: step 199000, acc = 0.9650, f1 = 0.9639
[Test] 2017-05-09 15:40:39.795774: step 199000, acc = 0.9558, f1 = 0.9555
[Status] 2017-05-09 15:40:39.795862: step 199000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:40:44.507907: step 199020, loss = 0.0818, acc = 0.9800 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 15:40:49.446490: step 199040, loss = 0.0804, acc = 0.9820 (215.2 examples/sec; 0.297 sec/batch)
2017-05-09 15:40:54.125327: step 199060, loss = 0.1012, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 15:40:58.707326: step 199080, loss = 0.0952, acc = 0.9740 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 15:41:03.352473: step 199100, loss = 0.0781, acc = 0.9820 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 15:41:08.086158: step 199120, loss = 0.0897, acc = 0.9720 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 15:41:12.784992: step 199140, loss = 0.0794, acc = 0.9880 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 15:41:17.342282: step 199160, loss = 0.0782, acc = 0.9840 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 15:41:21.970577: step 199180, loss = 0.0742, acc = 0.9880 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 15:41:26.566603: step 199200, loss = 0.0839, acc = 0.9780 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 15:41:31.227114: step 199220, loss = 0.0902, acc = 0.9780 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 15:41:35.932034: step 199240, loss = 0.1005, acc = 0.9680 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 15:41:40.578204: step 199260, loss = 0.0903, acc = 0.9680 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 15:41:45.148324: step 199280, loss = 0.0923, acc = 0.9820 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 15:41:50.152973: step 199300, loss = 0.0988, acc = 0.9720 (221.8 examples/sec; 0.289 sec/batch)
2017-05-09 15:41:54.667847: step 199320, loss = 0.0857, acc = 0.9820 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 15:41:59.192060: step 199340, loss = 0.0755, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 15:42:04.015686: step 199360, loss = 0.0906, acc = 0.9780 (229.5 examples/sec; 0.279 sec/batch)
2017-05-09 15:42:08.744331: step 199380, loss = 0.0838, acc = 0.9800 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 15:42:13.426581: step 199400, loss = 0.0866, acc = 0.9780 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:42:18.075570: step 199420, loss = 0.0812, acc = 0.9740 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 15:42:22.831077: step 199440, loss = 0.1138, acc = 0.9700 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 15:42:27.252912: step 199460, loss = 0.0893, acc = 0.9780 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 15:42:32.110376: step 199480, loss = 0.0977, acc = 0.9720 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 15:42:36.954996: step 199500, loss = 0.0972, acc = 0.9740 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 15:42:41.519448: step 199520, loss = 0.1115, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 15:42:46.106957: step 199540, loss = 0.0795, acc = 0.9780 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 15:42:50.863571: step 199560, loss = 0.0906, acc = 0.9840 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 15:42:56.170869: step 199580, loss = 0.0921, acc = 0.9720 (166.1 examples/sec; 0.385 sec/batch)
2017-05-09 15:43:00.821099: step 199600, loss = 0.0872, acc = 0.9740 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 15:43:05.459118: step 199620, loss = 0.0973, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 15:43:10.147542: step 199640, loss = 0.0937, acc = 0.9740 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 15:43:14.829520: step 199660, loss = 0.0851, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 15:43:19.752602: step 199680, loss = 0.0711, acc = 0.9840 (235.3 examples/sec; 0.272 sec/batch)
2017-05-09 15:43:24.308012: step 199700, loss = 0.0976, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 15:43:28.939200: step 199720, loss = 0.0832, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 15:43:33.592806: step 199740, loss = 0.0721, acc = 0.9900 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 15:43:38.298378: step 199760, loss = 0.0773, acc = 0.9820 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 15:43:42.877342: step 199780, loss = 0.0712, acc = 0.9740 (262.0 examples/sec; 0.244 sec/batch)
2017-05-09 15:43:47.477760: step 199800, loss = 0.0732, acc = 0.9880 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 15:43:52.425231: step 199820, loss = 0.0948, acc = 0.9800 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 15:43:56.963229: step 199840, loss = 0.0762, acc = 0.9840 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:44:01.528672: step 199860, loss = 0.0770, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 15:44:06.180636: step 199880, loss = 0.0847, acc = 0.9880 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 15:44:10.791325: step 199900, loss = 0.0984, acc = 0.9740 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 15:44:15.440482: step 199920, loss = 0.0701, acc = 0.9860 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 15:44:20.206072: step 199940, loss = 0.0889, acc = 0.9780 (237.1 examples/sec; 0.270 sec/batch)
2017-05-09 15:44:24.680071: step 199960, loss = 0.0978, acc = 0.9740 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 15:44:29.827228: step 199980, loss = 0.0895, acc = 0.9740 (206.0 examples/sec; 0.311 sec/batch)
2017-05-09 15:44:34.476837: step 200000, loss = 0.0943, acc = 0.9780 (294.2 examples/sec; 0.218 sec/batch)
[Eval] 2017-05-09 15:44:48.512525: step 200000, acc = 0.9648, f1 = 0.9637
[Test] 2017-05-09 15:44:58.202672: step 200000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 15:44:58.202756: step 200000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:45:02.778541: step 200020, loss = 0.0976, acc = 0.9720 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 15:45:07.571826: step 200040, loss = 0.0827, acc = 0.9760 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 15:45:12.163683: step 200060, loss = 0.0898, acc = 0.9740 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:45:16.864450: step 200080, loss = 0.0665, acc = 0.9860 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 15:45:21.890351: step 200100, loss = 0.1091, acc = 0.9660 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 15:45:26.539922: step 200120, loss = 0.0901, acc = 0.9740 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 15:45:31.152433: step 200140, loss = 0.0882, acc = 0.9780 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:45:35.974388: step 200160, loss = 0.0935, acc = 0.9720 (219.9 examples/sec; 0.291 sec/batch)
2017-05-09 15:45:40.546900: step 200180, loss = 0.0941, acc = 0.9760 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 15:45:45.186043: step 200200, loss = 0.0774, acc = 0.9780 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 15:45:49.718950: step 200220, loss = 0.0697, acc = 0.9900 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:45:54.464948: step 200240, loss = 0.0865, acc = 0.9780 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 15:45:59.043959: step 200260, loss = 0.1019, acc = 0.9860 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 15:46:03.887742: step 200280, loss = 0.0811, acc = 0.9880 (253.3 examples/sec; 0.253 sec/batch)
2017-05-09 15:46:08.866225: step 200300, loss = 0.0731, acc = 0.9820 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 15:46:13.396875: step 200320, loss = 0.0840, acc = 0.9800 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 15:46:17.986710: step 200340, loss = 0.1019, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 15:46:22.577500: step 200360, loss = 0.0851, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 15:46:27.113850: step 200380, loss = 0.0674, acc = 0.9880 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 15:46:31.581853: step 200400, loss = 0.1031, acc = 0.9660 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 15:46:36.207408: step 200420, loss = 0.0833, acc = 0.9820 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 15:46:40.809825: step 200440, loss = 0.0918, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:46:45.524520: step 200460, loss = 0.1417, acc = 0.9480 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 15:46:50.392887: step 200480, loss = 0.0985, acc = 0.9680 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 15:46:55.250117: step 200500, loss = 0.0855, acc = 0.9720 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 15:46:59.792957: step 200520, loss = 0.0950, acc = 0.9760 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 15:47:04.498395: step 200540, loss = 0.0823, acc = 0.9720 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 15:47:09.005119: step 200560, loss = 0.1049, acc = 0.9720 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 15:47:13.494239: step 200580, loss = 0.0865, acc = 0.9760 (297.6 examples/sec; 0.215 sec/batch)
2017-05-09 15:47:19.000110: step 200600, loss = 0.0926, acc = 0.9740 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 15:47:23.620691: step 200620, loss = 0.0687, acc = 0.9900 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 15:47:28.072433: step 200640, loss = 0.0811, acc = 0.9840 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 15:47:32.635004: step 200660, loss = 0.0812, acc = 0.9800 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 15:47:37.312307: step 200680, loss = 0.1010, acc = 0.9740 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 15:47:41.894074: step 200700, loss = 0.1092, acc = 0.9620 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 15:47:46.503066: step 200720, loss = 0.0830, acc = 0.9800 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 15:47:51.282577: step 200740, loss = 0.1009, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 15:47:55.824036: step 200760, loss = 0.0973, acc = 0.9740 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 15:48:00.294600: step 200780, loss = 0.0815, acc = 0.9740 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 15:48:05.281125: step 200800, loss = 0.0656, acc = 0.9820 (230.5 examples/sec; 0.278 sec/batch)
2017-05-09 15:48:09.860869: step 200820, loss = 0.1104, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 15:48:14.429324: step 200840, loss = 0.0820, acc = 0.9780 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 15:48:19.032370: step 200860, loss = 0.0714, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 15:48:23.802133: step 200880, loss = 0.0732, acc = 0.9880 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 15:48:28.482776: step 200900, loss = 0.0961, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 15:48:33.091548: step 200920, loss = 0.0790, acc = 0.9820 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 15:48:37.870058: step 200940, loss = 0.0776, acc = 0.9920 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 15:48:42.623783: step 200960, loss = 0.0823, acc = 0.9760 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 15:48:47.431564: step 200980, loss = 0.1228, acc = 0.9600 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 15:48:52.299215: step 201000, loss = 0.0666, acc = 0.9860 (281.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 15:49:06.429022: step 201000, acc = 0.9619, f1 = 0.9606
[Test] 2017-05-09 15:49:15.701938: step 201000, acc = 0.9522, f1 = 0.9518
[Status] 2017-05-09 15:49:15.702027: step 201000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:49:20.363725: step 201020, loss = 0.1087, acc = 0.9740 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 15:49:24.927906: step 201040, loss = 0.1100, acc = 0.9660 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 15:49:29.849660: step 201060, loss = 0.1014, acc = 0.9660 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 15:49:34.412409: step 201080, loss = 0.0925, acc = 0.9760 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 15:49:39.060204: step 201100, loss = 0.1352, acc = 0.9520 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 15:49:43.666649: step 201120, loss = 0.0877, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 15:49:48.296940: step 201140, loss = 0.0793, acc = 0.9800 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 15:49:52.940202: step 201160, loss = 0.0744, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 15:49:57.555487: step 201180, loss = 0.0850, acc = 0.9720 (265.0 examples/sec; 0.241 sec/batch)
2017-05-09 15:50:02.009509: step 201200, loss = 0.0773, acc = 0.9780 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:50:07.013029: step 201220, loss = 0.0891, acc = 0.9740 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 15:50:11.443815: step 201240, loss = 0.0656, acc = 0.9920 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 15:50:16.032576: step 201260, loss = 0.0831, acc = 0.9860 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 15:50:20.774733: step 201280, loss = 0.0857, acc = 0.9800 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 15:50:25.290624: step 201300, loss = 0.0772, acc = 0.9820 (297.1 examples/sec; 0.215 sec/batch)
2017-05-09 15:50:29.913457: step 201320, loss = 0.0866, acc = 0.9800 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 15:50:34.627465: step 201340, loss = 0.0799, acc = 0.9820 (244.2 examples/sec; 0.262 sec/batch)
2017-05-09 15:50:39.269387: step 201360, loss = 0.0760, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 15:50:43.858248: step 201380, loss = 0.0686, acc = 0.9880 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 15:50:48.381933: step 201400, loss = 0.0644, acc = 0.9920 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 15:50:53.836117: step 201420, loss = 0.0926, acc = 0.9760 (191.2 examples/sec; 0.335 sec/batch)
2017-05-09 15:50:58.402920: step 201440, loss = 0.0681, acc = 0.9860 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:51:02.953816: step 201460, loss = 0.0687, acc = 0.9880 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 15:51:07.718525: step 201480, loss = 0.0729, acc = 0.9860 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 15:51:12.294428: step 201500, loss = 0.0804, acc = 0.9820 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 15:51:16.828040: step 201520, loss = 0.0712, acc = 0.9880 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 15:51:21.590669: step 201540, loss = 0.0887, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:51:26.113239: step 201560, loss = 0.0842, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 15:51:30.782586: step 201580, loss = 0.0865, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 15:51:36.392913: step 201600, loss = 0.0951, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 15:51:41.063562: step 201620, loss = 0.0683, acc = 0.9860 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 15:51:45.658995: step 201640, loss = 0.0883, acc = 0.9840 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 15:51:50.285934: step 201660, loss = 0.1047, acc = 0.9640 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 15:51:54.907290: step 201680, loss = 0.0888, acc = 0.9740 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 15:51:59.555643: step 201700, loss = 0.0780, acc = 0.9740 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 15:52:04.250646: step 201720, loss = 0.0704, acc = 0.9840 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 15:52:09.042376: step 201740, loss = 0.0805, acc = 0.9740 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 15:52:13.661328: step 201760, loss = 0.0766, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:52:18.418773: step 201780, loss = 0.0868, acc = 0.9760 (234.1 examples/sec; 0.273 sec/batch)
2017-05-09 15:52:23.042530: step 201800, loss = 0.0814, acc = 0.9800 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 15:52:27.600230: step 201820, loss = 0.0871, acc = 0.9760 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 15:52:32.489766: step 201840, loss = 0.1111, acc = 0.9660 (226.1 examples/sec; 0.283 sec/batch)
2017-05-09 15:52:36.917267: step 201860, loss = 0.0703, acc = 0.9860 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 15:52:41.578823: step 201880, loss = 0.0790, acc = 0.9780 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 15:52:46.131608: step 201900, loss = 0.0797, acc = 0.9840 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 15:52:51.012003: step 201920, loss = 0.0852, acc = 0.9800 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 15:52:55.651474: step 201940, loss = 0.0961, acc = 0.9760 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:53:00.482096: step 201960, loss = 0.0713, acc = 0.9880 (250.1 examples/sec; 0.256 sec/batch)
2017-05-09 15:53:05.298328: step 201980, loss = 0.0906, acc = 0.9700 (247.9 examples/sec; 0.258 sec/batch)
2017-05-09 15:53:09.867905: step 202000, loss = 0.1066, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 15:53:24.065069: step 202000, acc = 0.9644, f1 = 0.9633
[Test] 2017-05-09 15:53:33.858591: step 202000, acc = 0.9565, f1 = 0.9561
[Status] 2017-05-09 15:53:33.858705: step 202000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:53:38.433174: step 202020, loss = 0.0785, acc = 0.9800 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 15:53:43.307388: step 202040, loss = 0.0776, acc = 0.9860 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 15:53:48.119290: step 202060, loss = 0.0897, acc = 0.9840 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 15:53:52.673044: step 202080, loss = 0.1043, acc = 0.9620 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:53:57.201199: step 202100, loss = 0.0900, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 15:54:01.809531: step 202120, loss = 0.0675, acc = 0.9800 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 15:54:06.391302: step 202140, loss = 0.0870, acc = 0.9820 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 15:54:11.039906: step 202160, loss = 0.0858, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 15:54:15.617835: step 202180, loss = 0.0859, acc = 0.9820 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 15:54:20.478158: step 202200, loss = 0.0733, acc = 0.9840 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 15:54:25.134621: step 202220, loss = 0.0751, acc = 0.9780 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 15:54:29.715912: step 202240, loss = 0.0905, acc = 0.9660 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 15:54:34.710135: step 202260, loss = 0.1131, acc = 0.9580 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 15:54:39.281208: step 202280, loss = 0.0857, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 15:54:43.943041: step 202300, loss = 0.0693, acc = 0.9900 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 15:54:48.749231: step 202320, loss = 0.0798, acc = 0.9840 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 15:54:53.149096: step 202340, loss = 0.0715, acc = 0.9880 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 15:54:57.672425: step 202360, loss = 0.0711, acc = 0.9840 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 15:55:02.419574: step 202380, loss = 0.0940, acc = 0.9760 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 15:55:06.993932: step 202400, loss = 0.0739, acc = 0.9780 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 15:55:11.674093: step 202420, loss = 0.0762, acc = 0.9840 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 15:55:16.520464: step 202440, loss = 0.0804, acc = 0.9780 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 15:55:21.185085: step 202460, loss = 0.0995, acc = 0.9800 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 15:55:25.883554: step 202480, loss = 0.0853, acc = 0.9800 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 15:55:30.545585: step 202500, loss = 0.0754, acc = 0.9940 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 15:55:35.324440: step 202520, loss = 0.0682, acc = 0.9780 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 15:55:39.962355: step 202540, loss = 0.0689, acc = 0.9860 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 15:55:44.651567: step 202560, loss = 0.0852, acc = 0.9700 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 15:55:49.451102: step 202580, loss = 0.0910, acc = 0.9740 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 15:55:53.963163: step 202600, loss = 0.0808, acc = 0.9820 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 15:55:59.614559: step 202620, loss = 0.0926, acc = 0.9700 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 15:56:04.464375: step 202640, loss = 0.0849, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 15:56:09.144362: step 202660, loss = 0.0730, acc = 0.9800 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 15:56:13.781234: step 202680, loss = 0.0826, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 15:56:18.432718: step 202700, loss = 0.0922, acc = 0.9700 (257.3 examples/sec; 0.249 sec/batch)
2017-05-09 15:56:22.987762: step 202720, loss = 0.0882, acc = 0.9780 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 15:56:27.551924: step 202740, loss = 0.0674, acc = 0.9880 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 15:56:32.115928: step 202760, loss = 0.0951, acc = 0.9880 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 15:56:36.580067: step 202780, loss = 0.0946, acc = 0.9780 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 15:56:41.252795: step 202800, loss = 0.0817, acc = 0.9800 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 15:56:46.229678: step 202820, loss = 0.0748, acc = 0.9820 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 15:56:50.964557: step 202840, loss = 0.0853, acc = 0.9720 (258.1 examples/sec; 0.248 sec/batch)
2017-05-09 15:56:55.561072: step 202860, loss = 0.0936, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 15:57:00.224551: step 202880, loss = 0.0734, acc = 0.9860 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 15:57:04.831049: step 202900, loss = 0.0752, acc = 0.9840 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 15:57:09.585679: step 202920, loss = 0.0826, acc = 0.9780 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 15:57:14.190603: step 202940, loss = 0.0759, acc = 0.9840 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 15:57:18.715010: step 202960, loss = 0.0698, acc = 0.9880 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 15:57:23.338800: step 202980, loss = 0.0694, acc = 0.9840 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 15:57:28.025598: step 203000, loss = 0.0903, acc = 0.9780 (236.0 examples/sec; 0.271 sec/batch)
[Eval] 2017-05-09 15:57:41.815701: step 203000, acc = 0.9634, f1 = 0.9623
[Test] 2017-05-09 15:57:51.451202: step 203000, acc = 0.9550, f1 = 0.9546
[Status] 2017-05-09 15:57:51.451270: step 203000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 15:57:55.917403: step 203020, loss = 0.0777, acc = 0.9780 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 15:58:00.661077: step 203040, loss = 0.0659, acc = 0.9840 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 15:58:05.244393: step 203060, loss = 0.0865, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:58:09.899518: step 203080, loss = 0.0870, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 15:58:14.534941: step 203100, loss = 0.1212, acc = 0.9560 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 15:58:19.142944: step 203120, loss = 0.0948, acc = 0.9680 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 15:58:23.840089: step 203140, loss = 0.0997, acc = 0.9640 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 15:58:28.708022: step 203160, loss = 0.0880, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 15:58:33.253200: step 203180, loss = 0.0815, acc = 0.9800 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 15:58:37.976762: step 203200, loss = 0.0742, acc = 0.9820 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 15:58:42.689933: step 203220, loss = 0.0751, acc = 0.9800 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 15:58:47.300971: step 203240, loss = 0.0777, acc = 0.9760 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 15:58:51.838219: step 203260, loss = 0.0986, acc = 0.9640 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 15:58:56.402806: step 203280, loss = 0.0715, acc = 0.9880 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 15:59:01.452931: step 203300, loss = 0.0629, acc = 0.9920 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 15:59:05.995420: step 203320, loss = 0.1028, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 15:59:10.539383: step 203340, loss = 0.1025, acc = 0.9700 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 15:59:15.156857: step 203360, loss = 0.1011, acc = 0.9800 (297.8 examples/sec; 0.215 sec/batch)
2017-05-09 15:59:19.679081: step 203380, loss = 0.0740, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 15:59:24.195779: step 203400, loss = 0.0796, acc = 0.9780 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 15:59:28.810427: step 203420, loss = 0.0835, acc = 0.9740 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 15:59:33.385411: step 203440, loss = 0.0909, acc = 0.9780 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 15:59:38.036986: step 203460, loss = 0.0651, acc = 0.9840 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 15:59:42.815366: step 203480, loss = 0.0924, acc = 0.9760 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 15:59:47.374955: step 203500, loss = 0.0909, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 15:59:51.923616: step 203520, loss = 0.0942, acc = 0.9760 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 15:59:56.628791: step 203540, loss = 0.0707, acc = 0.9820 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 16:00:01.531636: step 203560, loss = 0.0703, acc = 0.9840 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 16:00:06.068758: step 203580, loss = 0.0715, acc = 0.9840 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 16:00:10.713643: step 203600, loss = 0.0684, acc = 0.9860 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 16:00:16.156609: step 203620, loss = 0.0591, acc = 0.9880 (298.6 examples/sec; 0.214 sec/batch)
2017-05-09 16:00:20.646614: step 203640, loss = 0.0896, acc = 0.9780 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 16:00:25.177001: step 203660, loss = 0.0767, acc = 0.9860 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 16:00:29.929076: step 203680, loss = 0.0735, acc = 0.9860 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 16:00:34.516765: step 203700, loss = 0.0973, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 16:00:39.106823: step 203720, loss = 0.0756, acc = 0.9820 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:00:43.675498: step 203740, loss = 0.0930, acc = 0.9640 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 16:00:48.329470: step 203760, loss = 0.0755, acc = 0.9800 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 16:00:53.062909: step 203780, loss = 0.0771, acc = 0.9840 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:00:57.801897: step 203800, loss = 0.0803, acc = 0.9800 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 16:01:02.856433: step 203820, loss = 0.0931, acc = 0.9680 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 16:01:07.484003: step 203840, loss = 0.0656, acc = 0.9940 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 16:01:12.045618: step 203860, loss = 0.1189, acc = 0.9520 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 16:01:17.008587: step 203880, loss = 0.0917, acc = 0.9760 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 16:01:21.801849: step 203900, loss = 0.0816, acc = 0.9800 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 16:01:26.485228: step 203920, loss = 0.1028, acc = 0.9740 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 16:01:31.215450: step 203940, loss = 0.0959, acc = 0.9720 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:01:35.834424: step 203960, loss = 0.0909, acc = 0.9720 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 16:01:40.535606: step 203980, loss = 0.0872, acc = 0.9720 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:01:45.212720: step 204000, loss = 0.0944, acc = 0.9760 (281.8 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 16:01:59.190941: step 204000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 16:02:08.817718: step 204000, acc = 0.9565, f1 = 0.9562
[Status] 2017-05-09 16:02:08.817785: step 204000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:02:13.463090: step 204020, loss = 0.0950, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 16:02:18.057474: step 204040, loss = 0.0787, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 16:02:22.600907: step 204060, loss = 0.0907, acc = 0.9820 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 16:02:27.312588: step 204080, loss = 0.1048, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 16:02:31.896451: step 204100, loss = 0.0990, acc = 0.9760 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 16:02:36.652900: step 204120, loss = 0.0604, acc = 0.9840 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 16:02:41.502318: step 204140, loss = 0.0843, acc = 0.9760 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 16:02:45.921120: step 204160, loss = 0.0780, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:02:50.423539: step 204180, loss = 0.1155, acc = 0.9660 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 16:02:55.065819: step 204200, loss = 0.0600, acc = 0.9880 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 16:02:59.954268: step 204220, loss = 0.0710, acc = 0.9840 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 16:03:04.683752: step 204240, loss = 0.0753, acc = 0.9820 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 16:03:09.309205: step 204260, loss = 0.0895, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 16:03:14.211192: step 204280, loss = 0.0864, acc = 0.9720 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 16:03:18.770801: step 204300, loss = 0.0893, acc = 0.9800 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 16:03:23.348103: step 204320, loss = 0.0979, acc = 0.9660 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:03:28.318430: step 204340, loss = 0.0822, acc = 0.9840 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 16:03:32.953793: step 204360, loss = 0.1004, acc = 0.9700 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 16:03:37.565938: step 204380, loss = 0.0723, acc = 0.9820 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 16:03:42.355431: step 204400, loss = 0.0745, acc = 0.9820 (227.2 examples/sec; 0.282 sec/batch)
2017-05-09 16:03:46.964487: step 204420, loss = 0.0874, acc = 0.9760 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 16:03:51.574172: step 204440, loss = 0.0998, acc = 0.9820 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 16:03:56.285938: step 204460, loss = 0.1098, acc = 0.9720 (243.5 examples/sec; 0.263 sec/batch)
2017-05-09 16:04:00.910209: step 204480, loss = 0.0966, acc = 0.9660 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 16:04:05.526569: step 204500, loss = 0.0775, acc = 0.9740 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 16:04:10.041178: step 204520, loss = 0.0781, acc = 0.9800 (297.0 examples/sec; 0.216 sec/batch)
2017-05-09 16:04:14.789469: step 204540, loss = 0.0879, acc = 0.9800 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 16:04:19.433785: step 204560, loss = 0.0819, acc = 0.9800 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 16:04:23.981000: step 204580, loss = 0.0769, acc = 0.9860 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:04:28.533676: step 204600, loss = 0.0667, acc = 0.9880 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 16:04:33.947310: step 204620, loss = 0.0985, acc = 0.9740 (136.4 examples/sec; 0.469 sec/batch)
2017-05-09 16:04:38.599232: step 204640, loss = 0.0739, acc = 0.9840 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 16:04:43.296979: step 204660, loss = 0.0836, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 16:04:47.823719: step 204680, loss = 0.0798, acc = 0.9820 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 16:04:52.400232: step 204700, loss = 0.0954, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 16:04:57.309478: step 204720, loss = 0.0837, acc = 0.9820 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:05:01.868552: step 204740, loss = 0.0806, acc = 0.9860 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 16:05:06.481331: step 204760, loss = 0.0894, acc = 0.9740 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:05:11.120785: step 204780, loss = 0.0962, acc = 0.9740 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 16:05:15.780129: step 204800, loss = 0.0681, acc = 0.9800 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 16:05:20.384748: step 204820, loss = 0.0839, acc = 0.9780 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 16:05:25.116848: step 204840, loss = 0.0868, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 16:05:29.683054: step 204860, loss = 0.0730, acc = 0.9820 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 16:05:34.224024: step 204880, loss = 0.0811, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 16:05:38.787232: step 204900, loss = 0.0883, acc = 0.9780 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 16:05:43.611868: step 204920, loss = 0.0901, acc = 0.9740 (212.0 examples/sec; 0.302 sec/batch)
2017-05-09 16:05:48.188847: step 204940, loss = 0.0705, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 16:05:52.872986: step 204960, loss = 0.0872, acc = 0.9800 (260.1 examples/sec; 0.246 sec/batch)
2017-05-09 16:05:57.627755: step 204980, loss = 0.0765, acc = 0.9800 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 16:06:02.192790: step 205000, loss = 0.0910, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 16:06:16.173874: step 205000, acc = 0.9645, f1 = 0.9634
[Test] 2017-05-09 16:06:26.280499: step 205000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 16:06:26.280593: step 205000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:06:30.768457: step 205020, loss = 0.0953, acc = 0.9680 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 16:06:35.388835: step 205040, loss = 0.0822, acc = 0.9840 (239.6 examples/sec; 0.267 sec/batch)
2017-05-09 16:06:40.056111: step 205060, loss = 0.0894, acc = 0.9760 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 16:06:44.612420: step 205080, loss = 0.1025, acc = 0.9680 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 16:06:49.160153: step 205100, loss = 0.0873, acc = 0.9840 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:06:53.739236: step 205120, loss = 0.0636, acc = 0.9940 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 16:06:58.351323: step 205140, loss = 0.1105, acc = 0.9640 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:07:03.104232: step 205160, loss = 0.0761, acc = 0.9760 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 16:07:08.014284: step 205180, loss = 0.0866, acc = 0.9720 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 16:07:12.731226: step 205200, loss = 0.0883, acc = 0.9760 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 16:07:17.246174: step 205220, loss = 0.0765, acc = 0.9840 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:07:21.968487: step 205240, loss = 0.1061, acc = 0.9600 (246.9 examples/sec; 0.259 sec/batch)
2017-05-09 16:07:26.636553: step 205260, loss = 0.0878, acc = 0.9780 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 16:07:31.243735: step 205280, loss = 0.0756, acc = 0.9800 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 16:07:35.868871: step 205300, loss = 0.0832, acc = 0.9780 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 16:07:40.527139: step 205320, loss = 0.0875, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 16:07:45.032552: step 205340, loss = 0.0898, acc = 0.9760 (298.3 examples/sec; 0.215 sec/batch)
2017-05-09 16:07:49.673418: step 205360, loss = 0.0732, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 16:07:54.254228: step 205380, loss = 0.0907, acc = 0.9660 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 16:07:58.829285: step 205400, loss = 0.0696, acc = 0.9840 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 16:08:03.311171: step 205420, loss = 0.0601, acc = 0.9860 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 16:08:08.116583: step 205440, loss = 0.0795, acc = 0.9880 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 16:08:12.769022: step 205460, loss = 0.0811, acc = 0.9760 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 16:08:17.496965: step 205480, loss = 0.0854, acc = 0.9780 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 16:08:22.294287: step 205500, loss = 0.0931, acc = 0.9800 (219.1 examples/sec; 0.292 sec/batch)
2017-05-09 16:08:26.854494: step 205520, loss = 0.0864, acc = 0.9760 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 16:08:31.521533: step 205540, loss = 0.0813, acc = 0.9900 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 16:08:36.067679: step 205560, loss = 0.0828, acc = 0.9800 (300.8 examples/sec; 0.213 sec/batch)
2017-05-09 16:08:40.791846: step 205580, loss = 0.0820, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 16:08:45.374859: step 205600, loss = 0.0796, acc = 0.9740 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:08:49.894405: step 205620, loss = 0.0712, acc = 0.9900 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 16:08:55.461793: step 205640, loss = 0.0785, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 16:09:00.041659: step 205660, loss = 0.0804, acc = 0.9840 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 16:09:04.597133: step 205680, loss = 0.0885, acc = 0.9780 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 16:09:09.326700: step 205700, loss = 0.0864, acc = 0.9780 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 16:09:13.904778: step 205720, loss = 0.0732, acc = 0.9820 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 16:09:18.408506: step 205740, loss = 0.0669, acc = 0.9900 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 16:09:23.096460: step 205760, loss = 0.0814, acc = 0.9800 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 16:09:27.628398: step 205780, loss = 0.0808, acc = 0.9800 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 16:09:32.187173: step 205800, loss = 0.0805, acc = 0.9820 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 16:09:36.906934: step 205820, loss = 0.0939, acc = 0.9820 (240.0 examples/sec; 0.267 sec/batch)
2017-05-09 16:09:41.365900: step 205840, loss = 0.0881, acc = 0.9720 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 16:09:45.938369: step 205860, loss = 0.0886, acc = 0.9680 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:09:50.618736: step 205880, loss = 0.0892, acc = 0.9680 (232.7 examples/sec; 0.275 sec/batch)
2017-05-09 16:09:55.270809: step 205900, loss = 0.0768, acc = 0.9800 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 16:09:59.834448: step 205920, loss = 0.0983, acc = 0.9740 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 16:10:04.398933: step 205940, loss = 0.0839, acc = 0.9820 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:10:09.174470: step 205960, loss = 0.1011, acc = 0.9760 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 16:10:13.749528: step 205980, loss = 0.0857, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:10:18.286003: step 206000, loss = 0.1013, acc = 0.9660 (274.2 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 16:10:32.342318: step 206000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 16:10:42.089533: step 206000, acc = 0.9565, f1 = 0.9562
[Status] 2017-05-09 16:10:42.089649: step 206000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:10:46.583055: step 206020, loss = 0.0849, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:10:51.275493: step 206040, loss = 0.1016, acc = 0.9680 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 16:10:55.876587: step 206060, loss = 0.0704, acc = 0.9820 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 16:11:00.466214: step 206080, loss = 0.0781, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 16:11:05.244275: step 206100, loss = 0.0683, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 16:11:09.813021: step 206120, loss = 0.0894, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:11:14.418087: step 206140, loss = 0.1079, acc = 0.9760 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 16:11:19.094632: step 206160, loss = 0.0839, acc = 0.9800 (246.1 examples/sec; 0.260 sec/batch)
2017-05-09 16:11:23.605620: step 206180, loss = 0.1133, acc = 0.9620 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:11:28.331513: step 206200, loss = 0.1029, acc = 0.9680 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 16:11:32.886180: step 206220, loss = 0.0870, acc = 0.9680 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 16:11:37.573512: step 206240, loss = 0.0837, acc = 0.9780 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 16:11:42.178630: step 206260, loss = 0.0949, acc = 0.9680 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:11:46.718677: step 206280, loss = 0.0769, acc = 0.9860 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:11:51.374853: step 206300, loss = 0.0907, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 16:11:55.881664: step 206320, loss = 0.0889, acc = 0.9740 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 16:12:00.589436: step 206340, loss = 0.0922, acc = 0.9820 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 16:12:05.390094: step 206360, loss = 0.0904, acc = 0.9800 (230.3 examples/sec; 0.278 sec/batch)
2017-05-09 16:12:10.297101: step 206380, loss = 0.0886, acc = 0.9780 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 16:12:14.843207: step 206400, loss = 0.0855, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 16:12:19.507613: step 206420, loss = 0.0839, acc = 0.9760 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 16:12:24.062875: step 206440, loss = 0.0717, acc = 0.9900 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 16:12:28.716853: step 206460, loss = 0.0878, acc = 0.9780 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 16:12:33.734229: step 206480, loss = 0.0569, acc = 0.9920 (255.1 examples/sec; 0.251 sec/batch)
2017-05-09 16:12:38.327820: step 206500, loss = 0.0648, acc = 0.9900 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:12:42.899816: step 206520, loss = 0.0846, acc = 0.9820 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 16:12:47.325380: step 206540, loss = 0.0825, acc = 0.9800 (300.0 examples/sec; 0.213 sec/batch)
2017-05-09 16:12:52.077675: step 206560, loss = 0.0793, acc = 0.9800 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 16:12:56.721919: step 206580, loss = 0.1153, acc = 0.9660 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 16:13:01.316421: step 206600, loss = 0.0850, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 16:13:06.054347: step 206620, loss = 0.0896, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 16:13:11.673115: step 206640, loss = 0.1180, acc = 0.9700 (292.9 examples/sec; 0.219 sec/batch)
2017-05-09 16:13:16.166654: step 206660, loss = 0.0757, acc = 0.9840 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 16:13:20.972870: step 206680, loss = 0.0758, acc = 0.9820 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 16:13:25.719532: step 206700, loss = 0.0692, acc = 0.9900 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 16:13:30.551521: step 206720, loss = 0.0908, acc = 0.9720 (254.6 examples/sec; 0.251 sec/batch)
2017-05-09 16:13:35.374369: step 206740, loss = 0.0863, acc = 0.9740 (224.7 examples/sec; 0.285 sec/batch)
2017-05-09 16:13:39.920045: step 206760, loss = 0.0916, acc = 0.9800 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 16:13:44.506210: step 206780, loss = 0.0770, acc = 0.9780 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 16:13:49.053962: step 206800, loss = 0.0983, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 16:13:53.659603: step 206820, loss = 0.1145, acc = 0.9700 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 16:13:58.169097: step 206840, loss = 0.0838, acc = 0.9760 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 16:14:02.756993: step 206860, loss = 0.0790, acc = 0.9740 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 16:14:07.702260: step 206880, loss = 0.0823, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 16:14:12.216962: step 206900, loss = 0.0607, acc = 0.9920 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 16:14:16.806552: step 206920, loss = 0.0960, acc = 0.9740 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 16:14:21.663926: step 206940, loss = 0.0932, acc = 0.9720 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 16:14:26.281128: step 206960, loss = 0.0844, acc = 0.9760 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 16:14:30.920345: step 206980, loss = 0.0753, acc = 0.9800 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 16:14:35.781882: step 207000, loss = 0.0765, acc = 0.9840 (246.7 examples/sec; 0.259 sec/batch)
[Eval] 2017-05-09 16:14:49.810031: step 207000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 16:14:59.424216: step 207000, acc = 0.9562, f1 = 0.9559
[Status] 2017-05-09 16:14:59.424303: step 207000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:15:04.079872: step 207020, loss = 0.0895, acc = 0.9780 (301.2 examples/sec; 0.212 sec/batch)
2017-05-09 16:15:08.680364: step 207040, loss = 0.0983, acc = 0.9660 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 16:15:13.275060: step 207060, loss = 0.1035, acc = 0.9600 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 16:15:17.979365: step 207080, loss = 0.0905, acc = 0.9700 (247.3 examples/sec; 0.259 sec/batch)
2017-05-09 16:15:22.732615: step 207100, loss = 0.1092, acc = 0.9640 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 16:15:27.314727: step 207120, loss = 0.0815, acc = 0.9840 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 16:15:31.905149: step 207140, loss = 0.1048, acc = 0.9700 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 16:15:36.511579: step 207160, loss = 0.0754, acc = 0.9760 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 16:15:41.160343: step 207180, loss = 0.1051, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 16:15:45.716518: step 207200, loss = 0.0933, acc = 0.9720 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 16:15:50.704644: step 207220, loss = 0.0943, acc = 0.9700 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 16:15:55.325514: step 207240, loss = 0.0767, acc = 0.9840 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 16:15:59.991907: step 207260, loss = 0.0848, acc = 0.9760 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 16:16:04.974801: step 207280, loss = 0.0945, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 16:16:09.504264: step 207300, loss = 0.0990, acc = 0.9740 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 16:16:14.056642: step 207320, loss = 0.0852, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 16:16:18.956675: step 207340, loss = 0.0768, acc = 0.9840 (235.7 examples/sec; 0.271 sec/batch)
2017-05-09 16:16:23.615984: step 207360, loss = 0.0949, acc = 0.9760 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 16:16:28.225939: step 207380, loss = 0.0943, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 16:16:32.779280: step 207400, loss = 0.0673, acc = 0.9800 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 16:16:37.638347: step 207420, loss = 0.1051, acc = 0.9640 (254.3 examples/sec; 0.252 sec/batch)
2017-05-09 16:16:42.107047: step 207440, loss = 0.0790, acc = 0.9800 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 16:16:46.720301: step 207460, loss = 0.0688, acc = 0.9840 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 16:16:51.704577: step 207480, loss = 0.1060, acc = 0.9740 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 16:16:56.271331: step 207500, loss = 0.0783, acc = 0.9820 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 16:17:01.026214: step 207520, loss = 0.0891, acc = 0.9740 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 16:17:05.818320: step 207540, loss = 0.0732, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 16:17:10.487217: step 207560, loss = 0.0753, acc = 0.9840 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 16:17:14.990478: step 207580, loss = 0.0570, acc = 0.9920 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 16:17:19.710641: step 207600, loss = 0.1207, acc = 0.9620 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:17:24.293460: step 207620, loss = 0.0891, acc = 0.9780 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:17:28.918427: step 207640, loss = 0.0989, acc = 0.9720 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 16:17:34.367483: step 207660, loss = 0.0827, acc = 0.9780 (247.8 examples/sec; 0.258 sec/batch)
2017-05-09 16:17:38.906875: step 207680, loss = 0.0708, acc = 0.9780 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 16:17:43.558551: step 207700, loss = 0.0706, acc = 0.9880 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 16:17:48.071139: step 207720, loss = 0.0873, acc = 0.9860 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 16:17:52.701419: step 207740, loss = 0.0827, acc = 0.9820 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 16:17:57.242650: step 207760, loss = 0.0909, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:18:01.874222: step 207780, loss = 0.1087, acc = 0.9600 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 16:18:06.632766: step 207800, loss = 0.0726, acc = 0.9840 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 16:18:11.237878: step 207820, loss = 0.0892, acc = 0.9760 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 16:18:15.970164: step 207840, loss = 0.0854, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 16:18:20.685371: step 207860, loss = 0.0976, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 16:18:25.316001: step 207880, loss = 0.0662, acc = 0.9920 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 16:18:29.802663: step 207900, loss = 0.1013, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:18:34.641487: step 207920, loss = 0.0710, acc = 0.9860 (216.0 examples/sec; 0.296 sec/batch)
2017-05-09 16:18:39.123114: step 207940, loss = 0.0816, acc = 0.9800 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 16:18:43.750109: step 207960, loss = 0.0853, acc = 0.9840 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 16:18:48.367372: step 207980, loss = 0.0948, acc = 0.9780 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 16:18:53.175274: step 208000, loss = 0.0921, acc = 0.9720 (281.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 16:19:07.296589: step 208000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 16:19:17.204703: step 208000, acc = 0.9550, f1 = 0.9546
[Status] 2017-05-09 16:19:17.204790: step 208000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:19:21.801085: step 208020, loss = 0.0943, acc = 0.9740 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 16:19:26.348149: step 208040, loss = 0.0826, acc = 0.9860 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 16:19:31.145115: step 208060, loss = 0.0838, acc = 0.9680 (235.6 examples/sec; 0.272 sec/batch)
2017-05-09 16:19:35.839677: step 208080, loss = 0.0901, acc = 0.9720 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 16:19:40.478205: step 208100, loss = 0.0793, acc = 0.9740 (256.7 examples/sec; 0.249 sec/batch)
2017-05-09 16:19:45.122858: step 208120, loss = 0.0778, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 16:19:49.837665: step 208140, loss = 0.0886, acc = 0.9760 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 16:19:54.492567: step 208160, loss = 0.0890, acc = 0.9740 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 16:19:59.177446: step 208180, loss = 0.0994, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 16:20:03.838088: step 208200, loss = 0.0926, acc = 0.9780 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 16:20:08.319875: step 208220, loss = 0.0886, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 16:20:12.794233: step 208240, loss = 0.0834, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 16:20:17.568080: step 208260, loss = 0.0629, acc = 0.9860 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:20:22.119301: step 208280, loss = 0.0913, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 16:20:26.606989: step 208300, loss = 0.0879, acc = 0.9760 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 16:20:31.286169: step 208320, loss = 0.0850, acc = 0.9800 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 16:20:35.872573: step 208340, loss = 0.0922, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 16:20:40.431686: step 208360, loss = 0.0935, acc = 0.9740 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 16:20:45.164610: step 208380, loss = 0.0786, acc = 0.9800 (229.0 examples/sec; 0.279 sec/batch)
2017-05-09 16:20:49.736634: step 208400, loss = 0.0818, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 16:20:54.440605: step 208420, loss = 0.0616, acc = 0.9980 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 16:20:59.026658: step 208440, loss = 0.0948, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 16:21:03.835790: step 208460, loss = 0.0793, acc = 0.9740 (247.6 examples/sec; 0.259 sec/batch)
2017-05-09 16:21:08.381811: step 208480, loss = 0.1055, acc = 0.9760 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 16:21:12.992167: step 208500, loss = 0.0895, acc = 0.9740 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 16:21:17.888652: step 208520, loss = 0.1032, acc = 0.9720 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 16:21:22.543994: step 208540, loss = 0.0756, acc = 0.9800 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 16:21:27.426787: step 208560, loss = 0.0854, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 16:21:32.120190: step 208580, loss = 0.0728, acc = 0.9860 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 16:21:36.710597: step 208600, loss = 0.0934, acc = 0.9780 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 16:21:41.346181: step 208620, loss = 0.0901, acc = 0.9780 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 16:21:46.025871: step 208640, loss = 0.0950, acc = 0.9740 (242.9 examples/sec; 0.263 sec/batch)
2017-05-09 16:21:51.569323: step 208660, loss = 0.0778, acc = 0.9740 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:21:56.147832: step 208680, loss = 0.0845, acc = 0.9820 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 16:22:00.875687: step 208700, loss = 0.0768, acc = 0.9840 (239.5 examples/sec; 0.267 sec/batch)
2017-05-09 16:22:05.524531: step 208720, loss = 0.0778, acc = 0.9780 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 16:22:10.056492: step 208740, loss = 0.0829, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 16:22:14.817764: step 208760, loss = 0.0871, acc = 0.9780 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 16:22:19.643716: step 208780, loss = 0.1014, acc = 0.9740 (262.0 examples/sec; 0.244 sec/batch)
2017-05-09 16:22:24.229174: step 208800, loss = 0.0747, acc = 0.9880 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 16:22:28.762459: step 208820, loss = 0.0862, acc = 0.9820 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 16:22:33.468971: step 208840, loss = 0.1007, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 16:22:38.039947: step 208860, loss = 0.0617, acc = 0.9900 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 16:22:42.711775: step 208880, loss = 0.0729, acc = 0.9920 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 16:22:47.502976: step 208900, loss = 0.0993, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 16:22:52.131978: step 208920, loss = 0.0825, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 16:22:56.758051: step 208940, loss = 0.0877, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 16:23:01.515904: step 208960, loss = 0.0677, acc = 0.9880 (248.7 examples/sec; 0.257 sec/batch)
2017-05-09 16:23:06.059747: step 208980, loss = 0.0722, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:23:10.654773: step 209000, loss = 0.0807, acc = 0.9800 (277.9 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 16:23:24.755911: step 209000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 16:23:34.535682: step 209000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 16:23:34.535770: step 209000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:23:39.027010: step 209020, loss = 0.1016, acc = 0.9700 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:23:43.582906: step 209040, loss = 0.0790, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 16:23:48.332675: step 209060, loss = 0.0868, acc = 0.9760 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 16:23:52.929053: step 209080, loss = 0.0947, acc = 0.9740 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 16:23:57.481663: step 209100, loss = 0.0840, acc = 0.9800 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 16:24:02.294698: step 209120, loss = 0.0886, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 16:24:06.917211: step 209140, loss = 0.1132, acc = 0.9720 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 16:24:11.531280: step 209160, loss = 0.0632, acc = 0.9840 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 16:24:16.412039: step 209180, loss = 0.1245, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 16:24:20.904548: step 209200, loss = 0.0690, acc = 0.9800 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 16:24:25.407294: step 209220, loss = 0.0935, acc = 0.9760 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 16:24:30.078817: step 209240, loss = 0.0714, acc = 0.9880 (239.5 examples/sec; 0.267 sec/batch)
2017-05-09 16:24:34.600292: step 209260, loss = 0.0633, acc = 0.9860 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 16:24:39.196955: step 209280, loss = 0.0904, acc = 0.9800 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 16:24:43.868852: step 209300, loss = 0.0883, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 16:24:48.786152: step 209320, loss = 0.0758, acc = 0.9820 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 16:24:53.434671: step 209340, loss = 0.0944, acc = 0.9740 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 16:24:58.033087: step 209360, loss = 0.0835, acc = 0.9800 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 16:25:02.853600: step 209380, loss = 0.0865, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:25:07.551188: step 209400, loss = 0.0802, acc = 0.9840 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 16:25:12.184165: step 209420, loss = 0.0665, acc = 0.9880 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:25:16.834309: step 209440, loss = 0.0896, acc = 0.9800 (301.7 examples/sec; 0.212 sec/batch)
2017-05-09 16:25:21.453598: step 209460, loss = 0.1186, acc = 0.9640 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 16:25:26.014462: step 209480, loss = 0.0630, acc = 0.9900 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:25:30.685509: step 209500, loss = 0.0997, acc = 0.9660 (249.7 examples/sec; 0.256 sec/batch)
2017-05-09 16:25:35.271575: step 209520, loss = 0.0759, acc = 0.9780 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 16:25:39.908033: step 209540, loss = 0.0717, acc = 0.9800 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 16:25:44.486327: step 209560, loss = 0.1170, acc = 0.9720 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 16:25:49.076317: step 209580, loss = 0.0774, acc = 0.9760 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 16:25:53.814883: step 209600, loss = 0.1107, acc = 0.9640 (251.5 examples/sec; 0.255 sec/batch)
2017-05-09 16:25:58.467172: step 209620, loss = 0.0752, acc = 0.9900 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 16:26:03.224174: step 209640, loss = 0.0744, acc = 0.9880 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 16:26:08.551732: step 209660, loss = 0.0855, acc = 0.9820 (158.3 examples/sec; 0.404 sec/batch)
2017-05-09 16:26:13.118861: step 209680, loss = 0.0952, acc = 0.9720 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 16:26:17.800607: step 209700, loss = 0.0886, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 16:26:22.291468: step 209720, loss = 0.1074, acc = 0.9760 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 16:26:26.921473: step 209740, loss = 0.1121, acc = 0.9760 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 16:26:31.611213: step 209760, loss = 0.0814, acc = 0.9740 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 16:26:36.179563: step 209780, loss = 0.0869, acc = 0.9800 (259.5 examples/sec; 0.247 sec/batch)
2017-05-09 16:26:40.746488: step 209800, loss = 0.0794, acc = 0.9840 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:26:45.585099: step 209820, loss = 0.0731, acc = 0.9820 (235.8 examples/sec; 0.271 sec/batch)
2017-05-09 16:26:50.145180: step 209840, loss = 0.0936, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:26:54.744234: step 209860, loss = 0.0670, acc = 0.9860 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 16:26:59.264002: step 209880, loss = 0.1014, acc = 0.9760 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:27:03.987890: step 209900, loss = 0.0849, acc = 0.9760 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 16:27:08.593169: step 209920, loss = 0.0896, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 16:27:13.171378: step 209940, loss = 0.1024, acc = 0.9620 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 16:27:17.901949: step 209960, loss = 0.0901, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 16:27:22.422255: step 209980, loss = 0.0936, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 16:27:26.959180: step 210000, loss = 0.0742, acc = 0.9840 (289.6 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-09 16:27:41.134152: step 210000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 16:27:50.763911: step 210000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 16:27:50.764021: step 210000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:27:55.289933: step 210020, loss = 0.0943, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:27:59.938626: step 210040, loss = 0.0955, acc = 0.9680 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 16:28:04.440253: step 210060, loss = 0.0890, acc = 0.9800 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:28:09.240024: step 210080, loss = 0.0803, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 16:28:13.824819: step 210100, loss = 0.0851, acc = 0.9720 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 16:28:18.655379: step 210120, loss = 0.0777, acc = 0.9780 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 16:28:23.284128: step 210140, loss = 0.1110, acc = 0.9780 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 16:28:27.887606: step 210160, loss = 0.0964, acc = 0.9720 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 16:28:32.705656: step 210180, loss = 0.0974, acc = 0.9720 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 16:28:37.377032: step 210200, loss = 0.0842, acc = 0.9820 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 16:28:42.025347: step 210220, loss = 0.0783, acc = 0.9820 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:28:46.812056: step 210240, loss = 0.0903, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 16:28:51.474179: step 210260, loss = 0.0798, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 16:28:56.100346: step 210280, loss = 0.0649, acc = 0.9960 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:29:00.848220: step 210300, loss = 0.0737, acc = 0.9860 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 16:29:05.569941: step 210320, loss = 0.0799, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 16:29:10.054006: step 210340, loss = 0.0857, acc = 0.9780 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 16:29:14.581005: step 210360, loss = 0.1049, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 16:29:19.329274: step 210380, loss = 0.0856, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 16:29:23.913066: step 210400, loss = 0.0883, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:29:28.538428: step 210420, loss = 0.0772, acc = 0.9760 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 16:29:33.463068: step 210440, loss = 0.0703, acc = 0.9820 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 16:29:37.962473: step 210460, loss = 0.0831, acc = 0.9760 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:29:42.730710: step 210480, loss = 0.0813, acc = 0.9800 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 16:29:47.638022: step 210500, loss = 0.0623, acc = 0.9900 (229.1 examples/sec; 0.279 sec/batch)
2017-05-09 16:29:52.124022: step 210520, loss = 0.0707, acc = 0.9820 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:29:56.677230: step 210540, loss = 0.0851, acc = 0.9720 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 16:30:01.602764: step 210560, loss = 0.0988, acc = 0.9700 (223.6 examples/sec; 0.286 sec/batch)
2017-05-09 16:30:06.110227: step 210580, loss = 0.0676, acc = 0.9880 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 16:30:10.561599: step 210600, loss = 0.0815, acc = 0.9780 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 16:30:15.274626: step 210620, loss = 0.0731, acc = 0.9780 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 16:30:19.827545: step 210640, loss = 0.0861, acc = 0.9760 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 16:30:24.434018: step 210660, loss = 0.0797, acc = 0.9800 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 16:30:30.124549: step 210680, loss = 0.0852, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 16:30:34.750563: step 210700, loss = 0.0775, acc = 0.9820 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 16:30:39.394634: step 210720, loss = 0.0991, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 16:30:44.075057: step 210740, loss = 0.0809, acc = 0.9800 (257.7 examples/sec; 0.248 sec/batch)
2017-05-09 16:30:48.828512: step 210760, loss = 0.0783, acc = 0.9840 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 16:30:53.562566: step 210780, loss = 0.0823, acc = 0.9800 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 16:30:58.152697: step 210800, loss = 0.0896, acc = 0.9760 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 16:31:02.960533: step 210820, loss = 0.0704, acc = 0.9900 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 16:31:07.506326: step 210840, loss = 0.0978, acc = 0.9760 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 16:31:12.077883: step 210860, loss = 0.0688, acc = 0.9900 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 16:31:16.873280: step 210880, loss = 0.0892, acc = 0.9760 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 16:31:21.492630: step 210900, loss = 0.1035, acc = 0.9700 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 16:31:26.057597: step 210920, loss = 0.0643, acc = 0.9900 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 16:31:30.892719: step 210940, loss = 0.0975, acc = 0.9680 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 16:31:35.599478: step 210960, loss = 0.0823, acc = 0.9760 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 16:31:40.122656: step 210980, loss = 0.0929, acc = 0.9740 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 16:31:44.870067: step 211000, loss = 0.0780, acc = 0.9800 (253.9 examples/sec; 0.252 sec/batch)
[Eval] 2017-05-09 16:31:58.958342: step 211000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 16:32:08.288927: step 211000, acc = 0.9564, f1 = 0.9561
[Status] 2017-05-09 16:32:08.288996: step 211000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:32:13.016009: step 211020, loss = 0.0769, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 16:32:17.549413: step 211040, loss = 0.1049, acc = 0.9600 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 16:32:22.131755: step 211060, loss = 0.0761, acc = 0.9840 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 16:32:26.630850: step 211080, loss = 0.0839, acc = 0.9760 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 16:32:31.470078: step 211100, loss = 0.0804, acc = 0.9760 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 16:32:35.956121: step 211120, loss = 0.0901, acc = 0.9780 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 16:32:40.546443: step 211140, loss = 0.0831, acc = 0.9760 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 16:32:45.201464: step 211160, loss = 0.0767, acc = 0.9820 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 16:32:49.796182: step 211180, loss = 0.0809, acc = 0.9820 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 16:32:54.436206: step 211200, loss = 0.1093, acc = 0.9720 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 16:32:59.271970: step 211220, loss = 0.0717, acc = 0.9860 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 16:33:03.748126: step 211240, loss = 0.0728, acc = 0.9840 (299.0 examples/sec; 0.214 sec/batch)
2017-05-09 16:33:08.181002: step 211260, loss = 0.0769, acc = 0.9860 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 16:33:12.978901: step 211280, loss = 0.0697, acc = 0.9820 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 16:33:17.628939: step 211300, loss = 0.0645, acc = 0.9880 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 16:33:22.340243: step 211320, loss = 0.0988, acc = 0.9700 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:33:27.017810: step 211340, loss = 0.1046, acc = 0.9620 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 16:33:31.745444: step 211360, loss = 0.0844, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 16:33:36.338234: step 211380, loss = 0.0710, acc = 0.9860 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 16:33:41.019206: step 211400, loss = 0.0737, acc = 0.9840 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 16:33:45.765206: step 211420, loss = 0.0679, acc = 0.9860 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 16:33:50.308239: step 211440, loss = 0.0748, acc = 0.9820 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 16:33:54.928226: step 211460, loss = 0.1085, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:33:59.552295: step 211480, loss = 0.0944, acc = 0.9720 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 16:34:04.131714: step 211500, loss = 0.0806, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:34:08.764522: step 211520, loss = 0.0760, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 16:34:13.388318: step 211540, loss = 0.0874, acc = 0.9820 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 16:34:18.193289: step 211560, loss = 0.1096, acc = 0.9620 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 16:34:22.786691: step 211580, loss = 0.0921, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 16:34:27.380322: step 211600, loss = 0.0682, acc = 0.9840 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:34:32.284602: step 211620, loss = 0.0779, acc = 0.9840 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 16:34:36.898715: step 211640, loss = 0.0844, acc = 0.9760 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:34:41.578391: step 211660, loss = 0.0712, acc = 0.9840 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 16:34:46.982793: step 211680, loss = 0.0873, acc = 0.9740 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 16:34:51.637993: step 211700, loss = 0.0713, acc = 0.9840 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:34:56.287448: step 211720, loss = 0.0872, acc = 0.9820 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 16:35:01.067801: step 211740, loss = 0.0759, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:35:05.744005: step 211760, loss = 0.0835, acc = 0.9800 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:35:10.265709: step 211780, loss = 0.0761, acc = 0.9800 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 16:35:14.841846: step 211800, loss = 0.1066, acc = 0.9720 (306.3 examples/sec; 0.209 sec/batch)
2017-05-09 16:35:19.337033: step 211820, loss = 0.0822, acc = 0.9840 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 16:35:24.119222: step 211840, loss = 0.0650, acc = 0.9900 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 16:35:28.926995: step 211860, loss = 0.0768, acc = 0.9800 (238.4 examples/sec; 0.269 sec/batch)
2017-05-09 16:35:33.500621: step 211880, loss = 0.0864, acc = 0.9820 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:35:38.076960: step 211900, loss = 0.0682, acc = 0.9780 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 16:35:42.679999: step 211920, loss = 0.0815, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 16:35:47.423768: step 211940, loss = 0.0770, acc = 0.9840 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 16:35:52.012362: step 211960, loss = 0.0693, acc = 0.9900 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:35:56.542953: step 211980, loss = 0.0785, acc = 0.9820 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 16:36:01.320764: step 212000, loss = 0.0926, acc = 0.9760 (276.9 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 16:36:15.320397: step 212000, acc = 0.9641, f1 = 0.9628
[Test] 2017-05-09 16:36:24.703584: step 212000, acc = 0.9551, f1 = 0.9548
[Status] 2017-05-09 16:36:24.703669: step 212000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:36:29.500372: step 212020, loss = 0.1134, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 16:36:34.152659: step 212040, loss = 0.0848, acc = 0.9840 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 16:36:38.644197: step 212060, loss = 0.0901, acc = 0.9700 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 16:36:43.529456: step 212080, loss = 0.1033, acc = 0.9680 (234.1 examples/sec; 0.273 sec/batch)
2017-05-09 16:36:48.146651: step 212100, loss = 0.0768, acc = 0.9760 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 16:36:52.733303: step 212120, loss = 0.0807, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:36:57.468664: step 212140, loss = 0.0940, acc = 0.9760 (248.2 examples/sec; 0.258 sec/batch)
2017-05-09 16:37:02.014081: step 212160, loss = 0.0891, acc = 0.9820 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:37:06.496568: step 212180, loss = 0.0807, acc = 0.9860 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:37:11.076976: step 212200, loss = 0.0843, acc = 0.9720 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 16:37:15.717414: step 212220, loss = 0.0755, acc = 0.9820 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 16:37:20.178755: step 212240, loss = 0.0740, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 16:37:24.674034: step 212260, loss = 0.0943, acc = 0.9720 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 16:37:29.504907: step 212280, loss = 0.0912, acc = 0.9720 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 16:37:33.984027: step 212300, loss = 0.0892, acc = 0.9760 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 16:37:38.501805: step 212320, loss = 0.0741, acc = 0.9840 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 16:37:43.031391: step 212340, loss = 0.0789, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 16:37:47.707190: step 212360, loss = 0.0796, acc = 0.9800 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 16:37:52.268862: step 212380, loss = 0.0833, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 16:37:56.963733: step 212400, loss = 0.0850, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 16:38:01.467139: step 212420, loss = 0.1205, acc = 0.9580 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 16:38:06.074875: step 212440, loss = 0.0718, acc = 0.9860 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 16:38:10.693597: step 212460, loss = 0.0834, acc = 0.9840 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 16:38:15.527633: step 212480, loss = 0.0774, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 16:38:20.082876: step 212500, loss = 0.0826, acc = 0.9820 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 16:38:24.756748: step 212520, loss = 0.1026, acc = 0.9800 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 16:38:29.398218: step 212540, loss = 0.0640, acc = 0.9880 (295.6 examples/sec; 0.217 sec/batch)
2017-05-09 16:38:33.972891: step 212560, loss = 0.0890, acc = 0.9760 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 16:38:38.721010: step 212580, loss = 0.0755, acc = 0.9860 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:38:43.507012: step 212600, loss = 0.0786, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:38:48.149636: step 212620, loss = 0.0800, acc = 0.9860 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 16:38:52.782928: step 212640, loss = 0.1134, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 16:38:57.498476: step 212660, loss = 0.0808, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 16:39:02.014816: step 212680, loss = 0.0822, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 16:39:07.583411: step 212700, loss = 0.0937, acc = 0.9800 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 16:39:12.327328: step 212720, loss = 0.0776, acc = 0.9880 (255.8 examples/sec; 0.250 sec/batch)
2017-05-09 16:39:16.954328: step 212740, loss = 0.1123, acc = 0.9680 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 16:39:21.484766: step 212760, loss = 0.0874, acc = 0.9780 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:39:26.009015: step 212780, loss = 0.0698, acc = 0.9840 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 16:39:30.842933: step 212800, loss = 0.0739, acc = 0.9820 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 16:39:35.319178: step 212820, loss = 0.0829, acc = 0.9780 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 16:39:39.906797: step 212840, loss = 0.0765, acc = 0.9780 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 16:39:44.866832: step 212860, loss = 0.0781, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 16:39:49.444301: step 212880, loss = 0.0987, acc = 0.9640 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 16:39:54.120448: step 212900, loss = 0.0808, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 16:39:59.080082: step 212920, loss = 0.0747, acc = 0.9840 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:40:03.772356: step 212940, loss = 0.1272, acc = 0.9640 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:40:08.382260: step 212960, loss = 0.0972, acc = 0.9740 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 16:40:13.177098: step 212980, loss = 0.0803, acc = 0.9820 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 16:40:17.772166: step 213000, loss = 0.1048, acc = 0.9600 (291.3 examples/sec; 0.220 sec/batch)
[Eval] 2017-05-09 16:40:31.796609: step 213000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 16:40:41.761198: step 213000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 16:40:41.761272: step 213000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:40:46.337397: step 213020, loss = 0.0809, acc = 0.9860 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 16:40:51.115544: step 213040, loss = 0.0644, acc = 0.9840 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 16:40:55.743579: step 213060, loss = 0.0931, acc = 0.9720 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 16:41:00.546906: step 213080, loss = 0.1075, acc = 0.9700 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 16:41:05.158866: step 213100, loss = 0.1025, acc = 0.9680 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 16:41:09.799045: step 213120, loss = 0.0751, acc = 0.9820 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:41:14.626364: step 213140, loss = 0.0868, acc = 0.9780 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 16:41:19.201479: step 213160, loss = 0.0674, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 16:41:23.820124: step 213180, loss = 0.0988, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 16:41:28.723499: step 213200, loss = 0.0961, acc = 0.9720 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 16:41:33.240244: step 213220, loss = 0.0739, acc = 0.9960 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 16:41:37.909321: step 213240, loss = 0.0917, acc = 0.9720 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 16:41:42.668357: step 213260, loss = 0.1068, acc = 0.9680 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 16:41:47.217834: step 213280, loss = 0.1048, acc = 0.9600 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 16:41:51.778093: step 213300, loss = 0.0863, acc = 0.9780 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 16:41:56.381894: step 213320, loss = 0.0805, acc = 0.9800 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 16:42:01.033653: step 213340, loss = 0.0726, acc = 0.9840 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 16:42:05.938165: step 213360, loss = 0.0798, acc = 0.9740 (215.0 examples/sec; 0.298 sec/batch)
2017-05-09 16:42:10.540113: step 213380, loss = 0.0857, acc = 0.9780 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 16:42:15.205436: step 213400, loss = 0.0898, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 16:42:19.797495: step 213420, loss = 0.0909, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 16:42:24.402471: step 213440, loss = 0.0895, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 16:42:29.141978: step 213460, loss = 0.0764, acc = 0.9860 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 16:42:33.819493: step 213480, loss = 0.0943, acc = 0.9760 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 16:42:38.467790: step 213500, loss = 0.0864, acc = 0.9740 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 16:42:43.191989: step 213520, loss = 0.0765, acc = 0.9840 (228.8 examples/sec; 0.280 sec/batch)
2017-05-09 16:42:47.737865: step 213540, loss = 0.0766, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 16:42:52.259912: step 213560, loss = 0.0800, acc = 0.9840 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 16:42:56.903464: step 213580, loss = 0.1042, acc = 0.9700 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 16:43:01.654996: step 213600, loss = 0.0940, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 16:43:06.178813: step 213620, loss = 0.1040, acc = 0.9680 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 16:43:10.751659: step 213640, loss = 0.0804, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:43:15.613230: step 213660, loss = 0.1074, acc = 0.9720 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 16:43:20.226795: step 213680, loss = 0.0548, acc = 0.9900 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:43:25.507305: step 213700, loss = 0.1094, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 16:43:30.106718: step 213720, loss = 0.0871, acc = 0.9840 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 16:43:34.696036: step 213740, loss = 0.0846, acc = 0.9840 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 16:43:39.251660: step 213760, loss = 0.0919, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:43:43.904803: step 213780, loss = 0.0751, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 16:43:48.520793: step 213800, loss = 0.0929, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:43:53.082432: step 213820, loss = 0.0913, acc = 0.9780 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:43:57.929398: step 213840, loss = 0.0768, acc = 0.9820 (242.2 examples/sec; 0.264 sec/batch)
2017-05-09 16:44:02.606270: step 213860, loss = 0.0997, acc = 0.9700 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:44:07.180087: step 213880, loss = 0.0889, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 16:44:11.884826: step 213900, loss = 0.0631, acc = 0.9900 (245.1 examples/sec; 0.261 sec/batch)
2017-05-09 16:44:16.464067: step 213920, loss = 0.0783, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 16:44:20.987980: step 213940, loss = 0.0820, acc = 0.9820 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 16:44:25.535886: step 213960, loss = 0.1053, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 16:44:30.684769: step 213980, loss = 0.0769, acc = 0.9800 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:44:35.197774: step 214000, loss = 0.0724, acc = 0.9880 (281.5 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 16:44:49.129797: step 214000, acc = 0.9643, f1 = 0.9632
[Test] 2017-05-09 16:44:58.876128: step 214000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 16:44:58.876199: step 214000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:45:03.293488: step 214020, loss = 0.0890, acc = 0.9740 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 16:45:07.956564: step 214040, loss = 0.0924, acc = 0.9780 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 16:45:12.742029: step 214060, loss = 0.0850, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 16:45:17.252322: step 214080, loss = 0.0823, acc = 0.9780 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 16:45:21.961474: step 214100, loss = 0.0612, acc = 0.9880 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 16:45:26.686362: step 214120, loss = 0.0761, acc = 0.9860 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 16:45:31.225880: step 214140, loss = 0.0642, acc = 0.9900 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 16:45:35.903369: step 214160, loss = 0.0894, acc = 0.9720 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 16:45:40.602269: step 214180, loss = 0.0667, acc = 0.9860 (236.1 examples/sec; 0.271 sec/batch)
2017-05-09 16:45:45.487199: step 214200, loss = 0.0731, acc = 0.9820 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 16:45:50.153218: step 214220, loss = 0.0817, acc = 0.9880 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 16:45:54.841931: step 214240, loss = 0.0664, acc = 0.9880 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 16:45:59.520692: step 214260, loss = 0.0764, acc = 0.9860 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 16:46:04.125147: step 214280, loss = 0.0913, acc = 0.9680 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:46:08.670585: step 214300, loss = 0.0819, acc = 0.9820 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 16:46:13.426143: step 214320, loss = 0.0748, acc = 0.9880 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 16:46:18.040769: step 214340, loss = 0.0711, acc = 0.9840 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 16:46:22.723100: step 214360, loss = 0.0987, acc = 0.9680 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 16:46:27.577073: step 214380, loss = 0.0787, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 16:46:32.255315: step 214400, loss = 0.0934, acc = 0.9760 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 16:46:36.871310: step 214420, loss = 0.0717, acc = 0.9840 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 16:46:41.580827: step 214440, loss = 0.0782, acc = 0.9800 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 16:46:46.062771: step 214460, loss = 0.1038, acc = 0.9720 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:46:50.566771: step 214480, loss = 0.1024, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:46:55.297972: step 214500, loss = 0.0943, acc = 0.9720 (247.1 examples/sec; 0.259 sec/batch)
2017-05-09 16:46:59.792736: step 214520, loss = 0.1079, acc = 0.9680 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:47:04.429234: step 214540, loss = 0.0654, acc = 0.9860 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 16:47:09.131004: step 214560, loss = 0.0677, acc = 0.9820 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 16:47:13.965315: step 214580, loss = 0.0889, acc = 0.9780 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 16:47:18.541131: step 214600, loss = 0.0859, acc = 0.9800 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 16:47:23.037156: step 214620, loss = 0.0828, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 16:47:27.646124: step 214640, loss = 0.0955, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:47:32.165507: step 214660, loss = 0.0826, acc = 0.9800 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 16:47:36.705551: step 214680, loss = 0.0855, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 16:47:42.462783: step 214700, loss = 0.0857, acc = 0.9720 (134.3 examples/sec; 0.477 sec/batch)
2017-05-09 16:47:47.342900: step 214720, loss = 0.0628, acc = 0.9880 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 16:47:51.848623: step 214740, loss = 0.1026, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 16:47:56.707898: step 214760, loss = 0.0975, acc = 0.9660 (207.7 examples/sec; 0.308 sec/batch)
2017-05-09 16:48:01.112319: step 214780, loss = 0.0798, acc = 0.9760 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 16:48:05.748620: step 214800, loss = 0.0732, acc = 0.9880 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 16:48:10.427060: step 214820, loss = 0.0733, acc = 0.9840 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 16:48:15.064781: step 214840, loss = 0.0863, acc = 0.9740 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 16:48:19.803058: step 214860, loss = 0.0751, acc = 0.9860 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 16:48:24.439630: step 214880, loss = 0.0777, acc = 0.9860 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 16:48:29.491930: step 214900, loss = 0.0806, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 16:48:33.953930: step 214920, loss = 0.0704, acc = 0.9840 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 16:48:38.588009: step 214940, loss = 0.0865, acc = 0.9800 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 16:48:43.563090: step 214960, loss = 0.0990, acc = 0.9660 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 16:48:48.124728: step 214980, loss = 0.0801, acc = 0.9740 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 16:48:52.741071: step 215000, loss = 0.0775, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 16:49:06.838984: step 215000, acc = 0.9594, f1 = 0.9583
[Test] 2017-05-09 16:49:16.484668: step 215000, acc = 0.9504, f1 = 0.9500
[Status] 2017-05-09 16:49:16.484754: step 215000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:49:21.045989: step 215020, loss = 0.0998, acc = 0.9640 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 16:49:25.679989: step 215040, loss = 0.1007, acc = 0.9740 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:49:30.318777: step 215060, loss = 0.1082, acc = 0.9580 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 16:49:34.824800: step 215080, loss = 0.0682, acc = 0.9880 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 16:49:39.556627: step 215100, loss = 0.0697, acc = 0.9820 (246.6 examples/sec; 0.260 sec/batch)
2017-05-09 16:49:44.137137: step 215120, loss = 0.0933, acc = 0.9760 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 16:49:48.653869: step 215140, loss = 0.0793, acc = 0.9780 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 16:49:53.244706: step 215160, loss = 0.1024, acc = 0.9720 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 16:49:58.038738: step 215180, loss = 0.0974, acc = 0.9700 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 16:50:02.708323: step 215200, loss = 0.1116, acc = 0.9660 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 16:50:07.371500: step 215220, loss = 0.0812, acc = 0.9800 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 16:50:12.098968: step 215240, loss = 0.0909, acc = 0.9760 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 16:50:16.632530: step 215260, loss = 0.0656, acc = 0.9860 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 16:50:21.271633: step 215280, loss = 0.0753, acc = 0.9840 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 16:50:26.015164: step 215300, loss = 0.0686, acc = 0.9880 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 16:50:30.730802: step 215320, loss = 0.0747, acc = 0.9840 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 16:50:35.305451: step 215340, loss = 0.0817, acc = 0.9780 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 16:50:39.901200: step 215360, loss = 0.0855, acc = 0.9800 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 16:50:44.514012: step 215380, loss = 0.1098, acc = 0.9620 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 16:50:49.106168: step 215400, loss = 0.1206, acc = 0.9580 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 16:50:53.834269: step 215420, loss = 0.0553, acc = 0.9900 (232.2 examples/sec; 0.276 sec/batch)
2017-05-09 16:50:58.403415: step 215440, loss = 0.0648, acc = 0.9900 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 16:51:03.000663: step 215460, loss = 0.1146, acc = 0.9620 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 16:51:07.578014: step 215480, loss = 0.0722, acc = 0.9840 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 16:51:12.309414: step 215500, loss = 0.0856, acc = 0.9800 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 16:51:16.867439: step 215520, loss = 0.0702, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 16:51:21.393093: step 215540, loss = 0.1148, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 16:51:26.197471: step 215560, loss = 0.0751, acc = 0.9940 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 16:51:30.699859: step 215580, loss = 0.0800, acc = 0.9880 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 16:51:35.438135: step 215600, loss = 0.0706, acc = 0.9760 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 16:51:40.256026: step 215620, loss = 0.0768, acc = 0.9840 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 16:51:45.097168: step 215640, loss = 0.0793, acc = 0.9860 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 16:51:49.696602: step 215660, loss = 0.0801, acc = 0.9900 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 16:51:54.531154: step 215680, loss = 0.0827, acc = 0.9820 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 16:51:59.237768: step 215700, loss = 0.0773, acc = 0.9780 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 16:52:04.473374: step 215720, loss = 0.0891, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 16:52:09.188782: step 215740, loss = 0.0834, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 16:52:14.000640: step 215760, loss = 0.0746, acc = 0.9800 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 16:52:18.724470: step 215780, loss = 0.0883, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 16:52:23.454350: step 215800, loss = 0.0756, acc = 0.9780 (301.0 examples/sec; 0.213 sec/batch)
2017-05-09 16:52:28.089166: step 215820, loss = 0.0744, acc = 0.9800 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 16:52:32.730423: step 215840, loss = 0.0824, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 16:52:37.494759: step 215860, loss = 0.0967, acc = 0.9700 (235.9 examples/sec; 0.271 sec/batch)
2017-05-09 16:52:42.082382: step 215880, loss = 0.0781, acc = 0.9760 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 16:52:46.664103: step 215900, loss = 0.0945, acc = 0.9680 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 16:52:51.246897: step 215920, loss = 0.1072, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 16:52:55.882518: step 215940, loss = 0.0821, acc = 0.9800 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 16:53:00.465366: step 215960, loss = 0.0699, acc = 0.9840 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 16:53:04.931995: step 215980, loss = 0.1280, acc = 0.9700 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 16:53:09.587849: step 216000, loss = 0.0793, acc = 0.9800 (271.0 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 16:53:23.571020: step 216000, acc = 0.9610, f1 = 0.9596
[Test] 2017-05-09 16:53:32.793649: step 216000, acc = 0.9507, f1 = 0.9503
[Status] 2017-05-09 16:53:32.793738: step 216000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:53:37.566652: step 216020, loss = 0.0770, acc = 0.9840 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 16:53:42.100124: step 216040, loss = 0.1125, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 16:53:46.799208: step 216060, loss = 0.0668, acc = 0.9820 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 16:53:51.444575: step 216080, loss = 0.0792, acc = 0.9760 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 16:53:56.057517: step 216100, loss = 0.0734, acc = 0.9880 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 16:54:00.582712: step 216120, loss = 0.0853, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:54:05.425478: step 216140, loss = 0.0849, acc = 0.9780 (246.6 examples/sec; 0.260 sec/batch)
2017-05-09 16:54:10.146134: step 216160, loss = 0.0763, acc = 0.9840 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 16:54:14.756218: step 216180, loss = 0.1036, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:54:19.345221: step 216200, loss = 0.0868, acc = 0.9740 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 16:54:24.006251: step 216220, loss = 0.0665, acc = 0.9840 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 16:54:28.599943: step 216240, loss = 0.1022, acc = 0.9800 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 16:54:33.175946: step 216260, loss = 0.0880, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 16:54:37.995311: step 216280, loss = 0.0921, acc = 0.9740 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 16:54:42.730178: step 216300, loss = 0.0713, acc = 0.9800 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 16:54:47.383793: step 216320, loss = 0.0607, acc = 0.9860 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 16:54:52.172925: step 216340, loss = 0.0854, acc = 0.9800 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 16:54:56.555020: step 216360, loss = 0.0744, acc = 0.9820 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 16:55:01.149800: step 216380, loss = 0.0856, acc = 0.9780 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 16:55:05.960563: step 216400, loss = 0.0677, acc = 0.9840 (217.8 examples/sec; 0.294 sec/batch)
2017-05-09 16:55:10.491503: step 216420, loss = 0.0924, acc = 0.9760 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 16:55:15.040998: step 216440, loss = 0.0915, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 16:55:19.608029: step 216460, loss = 0.0732, acc = 0.9840 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 16:55:24.245903: step 216480, loss = 0.0893, acc = 0.9780 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 16:55:28.711734: step 216500, loss = 0.0847, acc = 0.9760 (297.2 examples/sec; 0.215 sec/batch)
2017-05-09 16:55:33.249928: step 216520, loss = 0.0931, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 16:55:38.040221: step 216540, loss = 0.0819, acc = 0.9800 (253.1 examples/sec; 0.253 sec/batch)
2017-05-09 16:55:42.757961: step 216560, loss = 0.0885, acc = 0.9720 (242.3 examples/sec; 0.264 sec/batch)
2017-05-09 16:55:47.280511: step 216580, loss = 0.0665, acc = 0.9880 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 16:55:52.003663: step 216600, loss = 0.0823, acc = 0.9740 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 16:55:56.476971: step 216620, loss = 0.0706, acc = 0.9840 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:56:00.993148: step 216640, loss = 0.0952, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 16:56:05.789937: step 216660, loss = 0.0951, acc = 0.9700 (208.1 examples/sec; 0.308 sec/batch)
2017-05-09 16:56:10.385679: step 216680, loss = 0.0711, acc = 0.9920 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 16:56:14.885821: step 216700, loss = 0.0888, acc = 0.9680 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 16:56:20.675980: step 216720, loss = 0.0792, acc = 0.9760 (232.9 examples/sec; 0.275 sec/batch)
2017-05-09 16:56:25.146621: step 216740, loss = 0.0884, acc = 0.9800 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 16:56:29.724529: step 216760, loss = 0.0814, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:56:34.485680: step 216780, loss = 0.0758, acc = 0.9780 (242.1 examples/sec; 0.264 sec/batch)
2017-05-09 16:56:39.060136: step 216800, loss = 0.0923, acc = 0.9720 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 16:56:43.596570: step 216820, loss = 0.0960, acc = 0.9640 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 16:56:48.096423: step 216840, loss = 0.1040, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 16:56:52.775482: step 216860, loss = 0.0891, acc = 0.9780 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 16:56:57.366096: step 216880, loss = 0.0851, acc = 0.9780 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 16:57:01.903503: step 216900, loss = 0.0798, acc = 0.9780 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 16:57:06.808344: step 216920, loss = 0.1080, acc = 0.9760 (239.6 examples/sec; 0.267 sec/batch)
2017-05-09 16:57:11.301440: step 216940, loss = 0.0821, acc = 0.9780 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 16:57:15.861773: step 216960, loss = 0.0882, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 16:57:20.538135: step 216980, loss = 0.0813, acc = 0.9720 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 16:57:25.113974: step 217000, loss = 0.0843, acc = 0.9720 (273.8 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 16:57:39.238869: step 217000, acc = 0.9629, f1 = 0.9616
[Test] 2017-05-09 16:57:48.810145: step 217000, acc = 0.9533, f1 = 0.9529
[Status] 2017-05-09 16:57:48.810237: step 217000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 16:57:53.379877: step 217020, loss = 0.0797, acc = 0.9760 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 16:57:57.978807: step 217040, loss = 0.1064, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 16:58:02.550298: step 217060, loss = 0.0808, acc = 0.9840 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 16:58:07.207289: step 217080, loss = 0.0676, acc = 0.9880 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 16:58:11.851827: step 217100, loss = 0.0723, acc = 0.9840 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 16:58:16.434820: step 217120, loss = 0.0912, acc = 0.9700 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 16:58:21.294347: step 217140, loss = 0.0690, acc = 0.9860 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 16:58:25.751262: step 217160, loss = 0.0730, acc = 0.9860 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 16:58:30.229896: step 217180, loss = 0.0754, acc = 0.9840 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 16:58:35.040738: step 217200, loss = 0.0817, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:58:39.667058: step 217220, loss = 0.0872, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 16:58:44.302276: step 217240, loss = 0.0724, acc = 0.9860 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:58:48.935294: step 217260, loss = 0.0805, acc = 0.9800 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 16:58:53.646588: step 217280, loss = 0.0941, acc = 0.9780 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 16:58:58.230830: step 217300, loss = 0.0708, acc = 0.9840 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 16:59:03.154833: step 217320, loss = 0.0979, acc = 0.9660 (261.9 examples/sec; 0.244 sec/batch)
2017-05-09 16:59:07.672132: step 217340, loss = 0.0935, acc = 0.9800 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 16:59:12.291807: step 217360, loss = 0.0739, acc = 0.9760 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 16:59:17.416303: step 217380, loss = 0.0879, acc = 0.9820 (226.2 examples/sec; 0.283 sec/batch)
2017-05-09 16:59:22.013947: step 217400, loss = 0.0816, acc = 0.9740 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 16:59:26.582019: step 217420, loss = 0.1010, acc = 0.9660 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 16:59:31.346532: step 217440, loss = 0.0744, acc = 0.9820 (214.7 examples/sec; 0.298 sec/batch)
2017-05-09 16:59:35.827656: step 217460, loss = 0.0981, acc = 0.9720 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 16:59:40.388922: step 217480, loss = 0.0831, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 16:59:44.998876: step 217500, loss = 0.0818, acc = 0.9760 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 16:59:49.718026: step 217520, loss = 0.0709, acc = 0.9880 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 16:59:54.275890: step 217540, loss = 0.0894, acc = 0.9780 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 16:59:58.797157: step 217560, loss = 0.0852, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 17:00:03.496531: step 217580, loss = 0.0817, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 17:00:08.070793: step 217600, loss = 0.0803, acc = 0.9820 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 17:00:12.674648: step 217620, loss = 0.0891, acc = 0.9660 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 17:00:17.359775: step 217640, loss = 0.1073, acc = 0.9720 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:00:21.949330: step 217660, loss = 0.0922, acc = 0.9820 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 17:00:26.618605: step 217680, loss = 0.0971, acc = 0.9740 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 17:00:31.393742: step 217700, loss = 0.0867, acc = 0.9800 (235.2 examples/sec; 0.272 sec/batch)
2017-05-09 17:00:35.944088: step 217720, loss = 0.0690, acc = 0.9880 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 17:00:41.337889: step 217740, loss = 0.0765, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 17:00:45.973155: step 217760, loss = 0.0841, acc = 0.9820 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 17:00:50.722220: step 217780, loss = 0.0883, acc = 0.9780 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 17:00:55.309607: step 217800, loss = 0.0904, acc = 0.9680 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 17:00:59.868213: step 217820, loss = 0.0870, acc = 0.9760 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 17:01:04.712315: step 217840, loss = 0.1044, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 17:01:09.206686: step 217860, loss = 0.0838, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 17:01:13.854784: step 217880, loss = 0.0762, acc = 0.9780 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 17:01:18.520646: step 217900, loss = 0.0761, acc = 0.9800 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 17:01:23.112547: step 217920, loss = 0.0934, acc = 0.9840 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 17:01:27.615182: step 217940, loss = 0.0764, acc = 0.9820 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 17:01:32.296597: step 217960, loss = 0.0645, acc = 0.9840 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 17:01:36.889554: step 217980, loss = 0.0624, acc = 0.9920 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 17:01:41.488887: step 218000, loss = 0.0693, acc = 0.9920 (269.1 examples/sec; 0.238 sec/batch)
[Eval] 2017-05-09 17:01:55.665923: step 218000, acc = 0.9650, f1 = 0.9639
[Test] 2017-05-09 17:02:05.489499: step 218000, acc = 0.9566, f1 = 0.9562
[Status] 2017-05-09 17:02:05.489589: step 218000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:02:10.082010: step 218020, loss = 0.0926, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:02:14.790174: step 218040, loss = 0.0686, acc = 0.9780 (240.8 examples/sec; 0.266 sec/batch)
2017-05-09 17:02:19.448799: step 218060, loss = 0.0975, acc = 0.9780 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 17:02:23.944186: step 218080, loss = 0.0799, acc = 0.9820 (291.6 examples/sec; 0.219 sec/batch)
2017-05-09 17:02:28.593525: step 218100, loss = 0.0695, acc = 0.9860 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:02:33.291899: step 218120, loss = 0.0894, acc = 0.9780 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 17:02:37.896009: step 218140, loss = 0.0811, acc = 0.9780 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 17:02:42.397281: step 218160, loss = 0.0744, acc = 0.9860 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 17:02:47.071338: step 218180, loss = 0.0996, acc = 0.9680 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 17:02:51.695138: step 218200, loss = 0.0699, acc = 0.9820 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 17:02:56.265445: step 218220, loss = 0.0803, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 17:03:00.894133: step 218240, loss = 0.0956, acc = 0.9760 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 17:03:05.494869: step 218260, loss = 0.1035, acc = 0.9700 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 17:03:10.013551: step 218280, loss = 0.0928, acc = 0.9700 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 17:03:14.686166: step 218300, loss = 0.0799, acc = 0.9800 (236.9 examples/sec; 0.270 sec/batch)
2017-05-09 17:03:19.342324: step 218320, loss = 0.0843, acc = 0.9720 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 17:03:24.074489: step 218340, loss = 0.0847, acc = 0.9780 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 17:03:28.737644: step 218360, loss = 0.0840, acc = 0.9840 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 17:03:33.619214: step 218380, loss = 0.0765, acc = 0.9800 (292.9 examples/sec; 0.218 sec/batch)
2017-05-09 17:03:38.116118: step 218400, loss = 0.0904, acc = 0.9700 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 17:03:42.747648: step 218420, loss = 0.0840, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 17:03:47.522793: step 218440, loss = 0.0901, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 17:03:52.125151: step 218460, loss = 0.1040, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 17:03:56.639850: step 218480, loss = 0.1065, acc = 0.9700 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 17:04:01.265038: step 218500, loss = 0.0758, acc = 0.9800 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 17:04:05.795312: step 218520, loss = 0.0878, acc = 0.9780 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 17:04:10.321493: step 218540, loss = 0.0870, acc = 0.9720 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 17:04:14.992054: step 218560, loss = 0.0960, acc = 0.9740 (242.7 examples/sec; 0.264 sec/batch)
2017-05-09 17:04:19.553852: step 218580, loss = 0.0730, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:04:24.116518: step 218600, loss = 0.0677, acc = 0.9820 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 17:04:28.665466: step 218620, loss = 0.0857, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 17:04:33.328105: step 218640, loss = 0.1181, acc = 0.9700 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 17:04:37.836410: step 218660, loss = 0.1048, acc = 0.9720 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 17:04:42.392548: step 218680, loss = 0.0960, acc = 0.9820 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 17:04:47.099720: step 218700, loss = 0.1117, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 17:04:51.757375: step 218720, loss = 0.0990, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 17:04:57.442703: step 218740, loss = 0.0893, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 17:05:02.124025: step 218760, loss = 0.0774, acc = 0.9840 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:05:06.843444: step 218780, loss = 0.1036, acc = 0.9700 (260.7 examples/sec; 0.245 sec/batch)
2017-05-09 17:05:11.374350: step 218800, loss = 0.0764, acc = 0.9860 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 17:05:15.954887: step 218820, loss = 0.0805, acc = 0.9760 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 17:05:20.598983: step 218840, loss = 0.0788, acc = 0.9780 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 17:05:25.166719: step 218860, loss = 0.0758, acc = 0.9820 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 17:05:29.885873: step 218880, loss = 0.0675, acc = 0.9840 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 17:05:34.471393: step 218900, loss = 0.0932, acc = 0.9760 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 17:05:39.152712: step 218920, loss = 0.0945, acc = 0.9720 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 17:05:43.765294: step 218940, loss = 0.0838, acc = 0.9760 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 17:05:48.434766: step 218960, loss = 0.0845, acc = 0.9760 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 17:05:53.170057: step 218980, loss = 0.0954, acc = 0.9720 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 17:05:57.822457: step 219000, loss = 0.0737, acc = 0.9860 (237.3 examples/sec; 0.270 sec/batch)
[Eval] 2017-05-09 17:06:11.863448: step 219000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 17:06:21.645283: step 219000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 17:06:21.645361: step 219000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:06:26.379896: step 219020, loss = 0.1037, acc = 0.9700 (242.6 examples/sec; 0.264 sec/batch)
2017-05-09 17:06:31.154762: step 219040, loss = 0.1105, acc = 0.9600 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 17:06:35.812875: step 219060, loss = 0.0803, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 17:06:40.382272: step 219080, loss = 0.0735, acc = 0.9800 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:06:45.033484: step 219100, loss = 0.0689, acc = 0.9880 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 17:06:49.866552: step 219120, loss = 0.0929, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 17:06:54.423457: step 219140, loss = 0.1049, acc = 0.9680 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 17:06:58.906467: step 219160, loss = 0.0981, acc = 0.9700 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 17:07:03.685170: step 219180, loss = 0.0962, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 17:07:08.195588: step 219200, loss = 0.0913, acc = 0.9740 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 17:07:12.869017: step 219220, loss = 0.0866, acc = 0.9760 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 17:07:17.511640: step 219240, loss = 0.0766, acc = 0.9840 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 17:07:22.090597: step 219260, loss = 0.0953, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:07:26.645325: step 219280, loss = 0.0981, acc = 0.9780 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 17:07:31.466123: step 219300, loss = 0.0715, acc = 0.9860 (267.2 examples/sec; 0.239 sec/batch)
2017-05-09 17:07:36.077799: step 219320, loss = 0.0890, acc = 0.9720 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 17:07:40.841310: step 219340, loss = 0.0685, acc = 0.9920 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 17:07:45.638462: step 219360, loss = 0.0841, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 17:07:50.113131: step 219380, loss = 0.0913, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 17:07:54.690704: step 219400, loss = 0.0685, acc = 0.9840 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 17:07:59.423391: step 219420, loss = 0.1056, acc = 0.9760 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 17:08:03.918620: step 219440, loss = 0.0873, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 17:08:08.428929: step 219460, loss = 0.0876, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 17:08:13.097686: step 219480, loss = 0.0798, acc = 0.9780 (253.3 examples/sec; 0.253 sec/batch)
2017-05-09 17:08:17.908631: step 219500, loss = 0.0972, acc = 0.9740 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 17:08:22.514539: step 219520, loss = 0.0913, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 17:08:27.235501: step 219540, loss = 0.0819, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 17:08:32.265261: step 219560, loss = 0.0898, acc = 0.9740 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 17:08:36.847574: step 219580, loss = 0.0797, acc = 0.9840 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 17:08:41.375115: step 219600, loss = 0.0985, acc = 0.9620 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 17:08:46.147402: step 219620, loss = 0.0749, acc = 0.9840 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 17:08:50.904750: step 219640, loss = 0.1277, acc = 0.9640 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:08:55.458707: step 219660, loss = 0.0962, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 17:09:00.226542: step 219680, loss = 0.0705, acc = 0.9820 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 17:09:04.865188: step 219700, loss = 0.0881, acc = 0.9820 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 17:09:09.508937: step 219720, loss = 0.1105, acc = 0.9640 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 17:09:15.035295: step 219740, loss = 0.0764, acc = 0.9760 (135.9 examples/sec; 0.471 sec/batch)
2017-05-09 17:09:19.641386: step 219760, loss = 0.0681, acc = 0.9880 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 17:09:24.299891: step 219780, loss = 0.0666, acc = 0.9860 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 17:09:28.848391: step 219800, loss = 0.1017, acc = 0.9700 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 17:09:33.677993: step 219820, loss = 0.0873, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 17:09:38.175860: step 219840, loss = 0.0843, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 17:09:42.777701: step 219860, loss = 0.0752, acc = 0.9820 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 17:09:47.427357: step 219880, loss = 0.0787, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 17:09:51.972940: step 219900, loss = 0.0807, acc = 0.9780 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 17:09:56.701408: step 219920, loss = 0.0628, acc = 0.9900 (261.4 examples/sec; 0.245 sec/batch)
2017-05-09 17:10:01.298180: step 219940, loss = 0.0765, acc = 0.9860 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 17:10:05.995253: step 219960, loss = 0.0918, acc = 0.9780 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 17:10:10.747217: step 219980, loss = 0.0975, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 17:10:15.601033: step 220000, loss = 0.0791, acc = 0.9820 (277.5 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 17:10:29.619771: step 220000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 17:10:39.108333: step 220000, acc = 0.9565, f1 = 0.9562
[Status] 2017-05-09 17:10:39.108434: step 220000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:10:43.797839: step 220020, loss = 0.0709, acc = 0.9900 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 17:10:48.299139: step 220040, loss = 0.0882, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 17:10:52.825683: step 220060, loss = 0.0756, acc = 0.9860 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 17:10:57.639125: step 220080, loss = 0.0851, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 17:11:02.251921: step 220100, loss = 0.0752, acc = 0.9840 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 17:11:06.971754: step 220120, loss = 0.0970, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 17:11:11.693435: step 220140, loss = 0.0802, acc = 0.9800 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 17:11:16.202190: step 220160, loss = 0.0774, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 17:11:20.749675: step 220180, loss = 0.1208, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 17:11:25.530469: step 220200, loss = 0.0978, acc = 0.9740 (246.3 examples/sec; 0.260 sec/batch)
2017-05-09 17:11:30.266577: step 220220, loss = 0.0787, acc = 0.9820 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 17:11:34.958015: step 220240, loss = 0.0819, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 17:11:39.800455: step 220260, loss = 0.0683, acc = 0.9880 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 17:11:44.451979: step 220280, loss = 0.1093, acc = 0.9660 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 17:11:48.973357: step 220300, loss = 0.1073, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 17:11:53.609228: step 220320, loss = 0.0866, acc = 0.9820 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 17:11:58.445307: step 220340, loss = 0.0940, acc = 0.9740 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 17:12:03.015378: step 220360, loss = 0.0788, acc = 0.9820 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 17:12:07.490543: step 220380, loss = 0.0859, acc = 0.9840 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 17:12:12.168184: step 220400, loss = 0.0947, acc = 0.9800 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 17:12:16.791425: step 220420, loss = 0.0663, acc = 0.9920 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 17:12:21.424575: step 220440, loss = 0.0681, acc = 0.9880 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 17:12:26.293250: step 220460, loss = 0.0715, acc = 0.9860 (258.3 examples/sec; 0.248 sec/batch)
2017-05-09 17:12:30.907956: step 220480, loss = 0.0915, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 17:12:35.549580: step 220500, loss = 0.0799, acc = 0.9800 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 17:12:40.254430: step 220520, loss = 0.0715, acc = 0.9880 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 17:12:44.835716: step 220540, loss = 0.0992, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 17:12:49.320917: step 220560, loss = 0.0908, acc = 0.9760 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 17:12:53.951804: step 220580, loss = 0.0630, acc = 0.9920 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 17:12:58.528837: step 220600, loss = 0.1055, acc = 0.9700 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 17:13:03.165532: step 220620, loss = 0.0680, acc = 0.9900 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 17:13:07.708338: step 220640, loss = 0.0850, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:13:12.449783: step 220660, loss = 0.0717, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 17:13:16.984562: step 220680, loss = 0.0987, acc = 0.9680 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 17:13:21.685244: step 220700, loss = 0.1203, acc = 0.9600 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 17:13:26.352715: step 220720, loss = 0.0702, acc = 0.9860 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 17:13:31.002887: step 220740, loss = 0.0696, acc = 0.9860 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:13:36.529625: step 220760, loss = 0.0672, acc = 0.9860 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 17:13:41.316414: step 220780, loss = 0.0950, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 17:13:46.040433: step 220800, loss = 0.0851, acc = 0.9740 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 17:13:50.645820: step 220820, loss = 0.0955, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 17:13:55.418214: step 220840, loss = 0.0925, acc = 0.9780 (237.8 examples/sec; 0.269 sec/batch)
2017-05-09 17:13:59.900560: step 220860, loss = 0.0888, acc = 0.9800 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 17:14:04.528930: step 220880, loss = 0.0826, acc = 0.9720 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 17:14:09.084066: step 220900, loss = 0.0962, acc = 0.9820 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 17:14:13.783304: step 220920, loss = 0.0708, acc = 0.9880 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 17:14:18.290792: step 220940, loss = 0.0696, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 17:14:22.790093: step 220960, loss = 0.0939, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:14:27.493989: step 220980, loss = 0.0982, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 17:14:32.157635: step 221000, loss = 0.1357, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 17:14:46.268303: step 221000, acc = 0.9635, f1 = 0.9622
[Test] 2017-05-09 17:14:55.966366: step 221000, acc = 0.9538, f1 = 0.9534
[Status] 2017-05-09 17:14:55.966461: step 221000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:15:00.535617: step 221020, loss = 0.1033, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:15:05.074333: step 221040, loss = 0.0868, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 17:15:09.985292: step 221060, loss = 0.0741, acc = 0.9800 (209.9 examples/sec; 0.305 sec/batch)
2017-05-09 17:15:14.635401: step 221080, loss = 0.0759, acc = 0.9900 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 17:15:19.148961: step 221100, loss = 0.0932, acc = 0.9700 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:15:23.814625: step 221120, loss = 0.0906, acc = 0.9760 (255.3 examples/sec; 0.251 sec/batch)
2017-05-09 17:15:28.598731: step 221140, loss = 0.0862, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 17:15:33.138637: step 221160, loss = 0.0812, acc = 0.9800 (304.5 examples/sec; 0.210 sec/batch)
2017-05-09 17:15:37.786622: step 221180, loss = 0.0784, acc = 0.9820 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 17:15:42.893137: step 221200, loss = 0.0708, acc = 0.9840 (207.6 examples/sec; 0.308 sec/batch)
2017-05-09 17:15:47.602127: step 221220, loss = 0.0920, acc = 0.9780 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 17:15:52.164590: step 221240, loss = 0.0935, acc = 0.9760 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 17:15:56.856534: step 221260, loss = 0.0734, acc = 0.9840 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 17:16:01.314938: step 221280, loss = 0.0918, acc = 0.9660 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 17:16:05.850194: step 221300, loss = 0.0792, acc = 0.9720 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 17:16:10.508104: step 221320, loss = 0.0931, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 17:16:15.005664: step 221340, loss = 0.0925, acc = 0.9720 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 17:16:19.544779: step 221360, loss = 0.0787, acc = 0.9860 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 17:16:24.205248: step 221380, loss = 0.1260, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 17:16:28.818991: step 221400, loss = 0.0651, acc = 0.9880 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 17:16:33.501126: step 221420, loss = 0.0680, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 17:16:38.240405: step 221440, loss = 0.0874, acc = 0.9760 (231.9 examples/sec; 0.276 sec/batch)
2017-05-09 17:16:42.812614: step 221460, loss = 0.0894, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 17:16:47.571765: step 221480, loss = 0.1023, acc = 0.9780 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 17:16:52.136056: step 221500, loss = 0.0723, acc = 0.9800 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 17:16:57.268570: step 221520, loss = 0.0821, acc = 0.9800 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 17:17:01.849985: step 221540, loss = 0.0797, acc = 0.9820 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 17:17:06.595105: step 221560, loss = 0.1068, acc = 0.9720 (236.8 examples/sec; 0.270 sec/batch)
2017-05-09 17:17:11.417278: step 221580, loss = 0.0763, acc = 0.9800 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 17:17:16.089449: step 221600, loss = 0.0730, acc = 0.9800 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 17:17:20.778766: step 221620, loss = 0.1049, acc = 0.9700 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 17:17:25.606080: step 221640, loss = 0.0912, acc = 0.9740 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 17:17:30.206352: step 221660, loss = 0.0916, acc = 0.9780 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 17:17:35.011525: step 221680, loss = 0.0855, acc = 0.9760 (296.2 examples/sec; 0.216 sec/batch)
2017-05-09 17:17:39.965624: step 221700, loss = 0.0961, acc = 0.9620 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 17:17:44.500419: step 221720, loss = 0.0963, acc = 0.9660 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 17:17:49.102077: step 221740, loss = 0.1011, acc = 0.9700 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 17:17:54.782818: step 221760, loss = 0.0802, acc = 0.9800 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 17:17:59.381296: step 221780, loss = 0.0832, acc = 0.9780 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 17:18:03.961823: step 221800, loss = 0.1014, acc = 0.9740 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 17:18:08.688815: step 221820, loss = 0.0789, acc = 0.9800 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 17:18:13.363660: step 221840, loss = 0.0761, acc = 0.9800 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 17:18:18.029263: step 221860, loss = 0.0726, acc = 0.9840 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 17:18:22.757167: step 221880, loss = 0.0671, acc = 0.9860 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 17:18:27.256635: step 221900, loss = 0.0715, acc = 0.9840 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 17:18:31.742194: step 221920, loss = 0.0792, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 17:18:36.485026: step 221940, loss = 0.0704, acc = 0.9900 (233.1 examples/sec; 0.275 sec/batch)
2017-05-09 17:18:40.994442: step 221960, loss = 0.0925, acc = 0.9800 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 17:18:45.474458: step 221980, loss = 0.0866, acc = 0.9800 (297.8 examples/sec; 0.215 sec/batch)
2017-05-09 17:18:50.170550: step 222000, loss = 0.0876, acc = 0.9720 (236.1 examples/sec; 0.271 sec/batch)
[Eval] 2017-05-09 17:19:03.832756: step 222000, acc = 0.9631, f1 = 0.9620
[Test] 2017-05-09 17:19:13.384480: step 222000, acc = 0.9549, f1 = 0.9545
[Status] 2017-05-09 17:19:13.384548: step 222000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:19:18.032676: step 222020, loss = 0.0847, acc = 0.9800 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 17:19:22.698889: step 222040, loss = 0.0906, acc = 0.9760 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 17:19:27.262512: step 222060, loss = 0.0902, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 17:19:31.869440: step 222080, loss = 0.0736, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:19:36.519784: step 222100, loss = 0.0978, acc = 0.9640 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 17:19:41.651987: step 222120, loss = 0.0657, acc = 0.9900 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 17:19:46.205953: step 222140, loss = 0.0865, acc = 0.9800 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 17:19:50.752256: step 222160, loss = 0.0991, acc = 0.9720 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 17:19:55.441973: step 222180, loss = 0.0925, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 17:20:00.029222: step 222200, loss = 0.0762, acc = 0.9800 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 17:20:04.990757: step 222220, loss = 0.0682, acc = 0.9860 (252.8 examples/sec; 0.253 sec/batch)
2017-05-09 17:20:09.677047: step 222240, loss = 0.0730, acc = 0.9820 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 17:20:14.256316: step 222260, loss = 0.0807, acc = 0.9860 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 17:20:18.984738: step 222280, loss = 0.0900, acc = 0.9780 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 17:20:23.566057: step 222300, loss = 0.0875, acc = 0.9800 (269.5 examples/sec; 0.238 sec/batch)
2017-05-09 17:20:28.512396: step 222320, loss = 0.0813, acc = 0.9840 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 17:20:33.372867: step 222340, loss = 0.0784, acc = 0.9840 (229.0 examples/sec; 0.279 sec/batch)
2017-05-09 17:20:37.944751: step 222360, loss = 0.1069, acc = 0.9660 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 17:20:42.526744: step 222380, loss = 0.0703, acc = 0.9860 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:20:47.097419: step 222400, loss = 0.0874, acc = 0.9840 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 17:20:51.763449: step 222420, loss = 0.0728, acc = 0.9860 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 17:20:56.440423: step 222440, loss = 0.1136, acc = 0.9720 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 17:21:01.039679: step 222460, loss = 0.0922, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 17:21:05.729746: step 222480, loss = 0.0882, acc = 0.9720 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 17:21:10.292480: step 222500, loss = 0.0627, acc = 0.9900 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 17:21:14.859297: step 222520, loss = 0.0826, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 17:21:19.832117: step 222540, loss = 0.0867, acc = 0.9740 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 17:21:24.448627: step 222560, loss = 0.0922, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 17:21:28.923619: step 222580, loss = 0.0717, acc = 0.9800 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 17:21:33.580864: step 222600, loss = 0.0877, acc = 0.9800 (251.2 examples/sec; 0.255 sec/batch)
2017-05-09 17:21:38.099803: step 222620, loss = 0.0732, acc = 0.9800 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:21:42.725395: step 222640, loss = 0.0740, acc = 0.9820 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 17:21:47.285367: step 222660, loss = 0.0759, acc = 0.9840 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 17:21:51.952687: step 222680, loss = 0.0826, acc = 0.9800 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 17:21:56.514135: step 222700, loss = 0.1078, acc = 0.9640 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 17:22:01.034854: step 222720, loss = 0.0899, acc = 0.9820 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 17:22:05.814397: step 222740, loss = 0.0767, acc = 0.9780 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 17:22:10.380035: step 222760, loss = 0.1015, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 17:22:15.877686: step 222780, loss = 0.0812, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 17:22:20.624158: step 222800, loss = 0.0798, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 17:22:25.251563: step 222820, loss = 0.0559, acc = 0.9900 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 17:22:29.795354: step 222840, loss = 0.0837, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 17:22:34.550261: step 222860, loss = 0.0874, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 17:22:39.062500: step 222880, loss = 0.0928, acc = 0.9760 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:22:43.727927: step 222900, loss = 0.0654, acc = 0.9860 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 17:22:48.531990: step 222920, loss = 0.0992, acc = 0.9820 (225.1 examples/sec; 0.284 sec/batch)
2017-05-09 17:22:53.186230: step 222940, loss = 0.0890, acc = 0.9740 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 17:22:57.781470: step 222960, loss = 0.0770, acc = 0.9840 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 17:23:02.356163: step 222980, loss = 0.1009, acc = 0.9620 (303.0 examples/sec; 0.211 sec/batch)
2017-05-09 17:23:06.963926: step 223000, loss = 0.0862, acc = 0.9820 (269.8 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 17:23:20.997296: step 223000, acc = 0.9639, f1 = 0.9626
[Test] 2017-05-09 17:23:30.317042: step 223000, acc = 0.9547, f1 = 0.9544
[Status] 2017-05-09 17:23:30.317147: step 223000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:23:35.109906: step 223020, loss = 0.0694, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:23:39.659749: step 223040, loss = 0.0855, acc = 0.9780 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 17:23:44.226706: step 223060, loss = 0.0889, acc = 0.9720 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 17:23:48.816659: step 223080, loss = 0.0980, acc = 0.9680 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 17:23:53.260024: step 223100, loss = 0.0857, acc = 0.9780 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 17:23:57.789954: step 223120, loss = 0.0755, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 17:24:02.547467: step 223140, loss = 0.0932, acc = 0.9740 (249.9 examples/sec; 0.256 sec/batch)
2017-05-09 17:24:07.058440: step 223160, loss = 0.0956, acc = 0.9600 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 17:24:11.560758: step 223180, loss = 0.0710, acc = 0.9800 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 17:24:16.194768: step 223200, loss = 0.0671, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 17:24:20.831915: step 223220, loss = 0.0822, acc = 0.9780 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 17:24:25.395979: step 223240, loss = 0.0855, acc = 0.9840 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 17:24:29.998809: step 223260, loss = 0.0969, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:24:34.772047: step 223280, loss = 0.0782, acc = 0.9800 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 17:24:39.345642: step 223300, loss = 0.0955, acc = 0.9800 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 17:24:43.993244: step 223320, loss = 0.0744, acc = 0.9780 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:24:48.714402: step 223340, loss = 0.0812, acc = 0.9780 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 17:24:53.309280: step 223360, loss = 0.0750, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:24:58.021323: step 223380, loss = 0.0734, acc = 0.9880 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 17:25:02.961614: step 223400, loss = 0.1003, acc = 0.9720 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 17:25:07.464563: step 223420, loss = 0.0851, acc = 0.9680 (304.0 examples/sec; 0.211 sec/batch)
2017-05-09 17:25:12.204375: step 223440, loss = 0.0842, acc = 0.9760 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 17:25:16.928364: step 223460, loss = 0.0914, acc = 0.9660 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 17:25:21.418519: step 223480, loss = 0.0768, acc = 0.9740 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 17:25:26.008363: step 223500, loss = 0.0726, acc = 0.9840 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 17:25:30.748628: step 223520, loss = 0.0889, acc = 0.9740 (237.9 examples/sec; 0.269 sec/batch)
2017-05-09 17:25:35.489149: step 223540, loss = 0.0697, acc = 0.9900 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 17:25:40.110516: step 223560, loss = 0.0792, acc = 0.9760 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 17:25:44.672533: step 223580, loss = 0.0790, acc = 0.9840 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 17:25:49.439985: step 223600, loss = 0.0956, acc = 0.9740 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 17:25:53.974847: step 223620, loss = 0.0673, acc = 0.9900 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 17:25:58.545528: step 223640, loss = 0.0872, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 17:26:03.096589: step 223660, loss = 0.0846, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 17:26:07.747833: step 223680, loss = 0.0671, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 17:26:12.429079: step 223700, loss = 0.0834, acc = 0.9720 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 17:26:17.335760: step 223720, loss = 0.0727, acc = 0.9840 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 17:26:21.917348: step 223740, loss = 0.0765, acc = 0.9860 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:26:26.514492: step 223760, loss = 0.1042, acc = 0.9700 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 17:26:31.997260: step 223780, loss = 0.0783, acc = 0.9780 (250.5 examples/sec; 0.255 sec/batch)
2017-05-09 17:26:36.700812: step 223800, loss = 0.0767, acc = 0.9840 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 17:26:41.211749: step 223820, loss = 0.0960, acc = 0.9700 (303.5 examples/sec; 0.211 sec/batch)
2017-05-09 17:26:45.815911: step 223840, loss = 0.0768, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 17:26:50.593031: step 223860, loss = 0.0671, acc = 0.9860 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 17:26:55.508265: step 223880, loss = 0.1012, acc = 0.9660 (206.4 examples/sec; 0.310 sec/batch)
2017-05-09 17:27:00.180781: step 223900, loss = 0.0796, acc = 0.9800 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 17:27:04.813083: step 223920, loss = 0.0689, acc = 0.9840 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 17:27:09.361478: step 223940, loss = 0.0851, acc = 0.9740 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 17:27:14.018020: step 223960, loss = 0.0853, acc = 0.9800 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 17:27:18.814993: step 223980, loss = 0.0881, acc = 0.9720 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 17:27:23.301377: step 224000, loss = 0.0624, acc = 0.9880 (283.9 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 17:27:37.369389: step 224000, acc = 0.9643, f1 = 0.9632
[Test] 2017-05-09 17:27:47.097658: step 224000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 17:27:47.097747: step 224000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:27:51.594692: step 224020, loss = 0.0873, acc = 0.9820 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 17:27:56.076480: step 224040, loss = 0.0853, acc = 0.9800 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:28:00.945069: step 224060, loss = 0.0746, acc = 0.9780 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 17:28:05.520206: step 224080, loss = 0.0881, acc = 0.9760 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:28:10.140224: step 224100, loss = 0.1035, acc = 0.9660 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 17:28:14.700587: step 224120, loss = 0.0917, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 17:28:19.211362: step 224140, loss = 0.0709, acc = 0.9840 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:28:23.816549: step 224160, loss = 0.0705, acc = 0.9860 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 17:28:28.617869: step 224180, loss = 0.0858, acc = 0.9660 (225.8 examples/sec; 0.283 sec/batch)
2017-05-09 17:28:33.527601: step 224200, loss = 0.0831, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 17:28:38.059100: step 224220, loss = 0.0891, acc = 0.9780 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 17:28:42.612739: step 224240, loss = 0.0735, acc = 0.9860 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 17:28:47.276215: step 224260, loss = 0.1044, acc = 0.9700 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 17:28:51.836033: step 224280, loss = 0.0924, acc = 0.9800 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 17:28:56.535821: step 224300, loss = 0.0693, acc = 0.9840 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 17:29:01.161296: step 224320, loss = 0.1050, acc = 0.9700 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 17:29:05.771292: step 224340, loss = 0.0788, acc = 0.9880 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 17:29:10.340725: step 224360, loss = 0.0969, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:29:15.043316: step 224380, loss = 0.0798, acc = 0.9740 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 17:29:19.700205: step 224400, loss = 0.0837, acc = 0.9900 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 17:29:24.234240: step 224420, loss = 0.0747, acc = 0.9840 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 17:29:28.894237: step 224440, loss = 0.0804, acc = 0.9860 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 17:29:33.504088: step 224460, loss = 0.0758, acc = 0.9760 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 17:29:38.074832: step 224480, loss = 0.0848, acc = 0.9780 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:29:43.032988: step 224500, loss = 0.0961, acc = 0.9780 (230.5 examples/sec; 0.278 sec/batch)
2017-05-09 17:29:47.580360: step 224520, loss = 0.0626, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 17:29:52.162990: step 224540, loss = 0.0978, acc = 0.9740 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 17:29:56.701762: step 224560, loss = 0.0847, acc = 0.9800 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 17:30:01.282903: step 224580, loss = 0.0754, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 17:30:05.902276: step 224600, loss = 0.0692, acc = 0.9820 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 17:30:10.458875: step 224620, loss = 0.0845, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 17:30:15.254852: step 224640, loss = 0.0820, acc = 0.9740 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 17:30:19.785869: step 224660, loss = 0.0765, acc = 0.9800 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 17:30:24.289702: step 224680, loss = 0.0806, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 17:30:28.861954: step 224700, loss = 0.0982, acc = 0.9680 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 17:30:33.543247: step 224720, loss = 0.0694, acc = 0.9860 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 17:30:38.154832: step 224740, loss = 0.0833, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 17:30:43.018056: step 224760, loss = 0.0985, acc = 0.9740 (220.6 examples/sec; 0.290 sec/batch)
2017-05-09 17:30:48.587316: step 224780, loss = 0.0982, acc = 0.9760 (130.6 examples/sec; 0.490 sec/batch)
2017-05-09 17:30:53.218678: step 224800, loss = 0.0665, acc = 0.9920 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 17:30:57.953830: step 224820, loss = 0.0995, acc = 0.9680 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 17:31:02.390040: step 224840, loss = 0.0951, acc = 0.9780 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 17:31:06.832345: step 224860, loss = 0.0908, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 17:31:11.402729: step 224880, loss = 0.0828, acc = 0.9800 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:31:16.217195: step 224900, loss = 0.0967, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:31:20.931293: step 224920, loss = 0.0959, acc = 0.9740 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:31:25.549114: step 224940, loss = 0.0555, acc = 0.9900 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 17:31:30.212240: step 224960, loss = 0.0747, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 17:31:34.851147: step 224980, loss = 0.0829, acc = 0.9760 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:31:39.439290: step 225000, loss = 0.0762, acc = 0.9880 (281.3 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 17:31:53.455226: step 225000, acc = 0.9618, f1 = 0.9607
[Test] 2017-05-09 17:32:03.246796: step 225000, acc = 0.9525, f1 = 0.9521
[Status] 2017-05-09 17:32:03.246899: step 225000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:32:07.765692: step 225020, loss = 0.0769, acc = 0.9820 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 17:32:12.426270: step 225040, loss = 0.0994, acc = 0.9680 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 17:32:17.026580: step 225060, loss = 0.0876, acc = 0.9700 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 17:32:21.639806: step 225080, loss = 0.0851, acc = 0.9800 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 17:32:26.334081: step 225100, loss = 0.1108, acc = 0.9680 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 17:32:31.546956: step 225120, loss = 0.0960, acc = 0.9760 (212.3 examples/sec; 0.302 sec/batch)
2017-05-09 17:32:36.118790: step 225140, loss = 0.0885, acc = 0.9740 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:32:41.059568: step 225160, loss = 0.0916, acc = 0.9700 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 17:32:45.659168: step 225180, loss = 0.0882, acc = 0.9780 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:32:50.292349: step 225200, loss = 0.0798, acc = 0.9780 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 17:32:55.050435: step 225220, loss = 0.1097, acc = 0.9720 (228.4 examples/sec; 0.280 sec/batch)
2017-05-09 17:32:59.500977: step 225240, loss = 0.0976, acc = 0.9740 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 17:33:04.147884: step 225260, loss = 0.0929, acc = 0.9740 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 17:33:08.730364: step 225280, loss = 0.0724, acc = 0.9780 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 17:33:13.557708: step 225300, loss = 0.0813, acc = 0.9840 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 17:33:18.203794: step 225320, loss = 0.0676, acc = 0.9820 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:33:22.845764: step 225340, loss = 0.0915, acc = 0.9740 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 17:33:27.558888: step 225360, loss = 0.0756, acc = 0.9780 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 17:33:32.072050: step 225380, loss = 0.1043, acc = 0.9680 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:33:36.780233: step 225400, loss = 0.0735, acc = 0.9860 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 17:33:41.419328: step 225420, loss = 0.1016, acc = 0.9700 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 17:33:45.919468: step 225440, loss = 0.0879, acc = 0.9740 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 17:33:50.547724: step 225460, loss = 0.0853, acc = 0.9800 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:33:55.386832: step 225480, loss = 0.0730, acc = 0.9840 (227.0 examples/sec; 0.282 sec/batch)
2017-05-09 17:33:59.846066: step 225500, loss = 0.0983, acc = 0.9620 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 17:34:04.448886: step 225520, loss = 0.1218, acc = 0.9620 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 17:34:09.001224: step 225540, loss = 0.0946, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:34:13.703913: step 225560, loss = 0.0804, acc = 0.9860 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 17:34:18.188916: step 225580, loss = 0.0714, acc = 0.9840 (287.6 examples/sec; 0.222 sec/batch)
2017-05-09 17:34:22.807774: step 225600, loss = 0.0899, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 17:34:27.425145: step 225620, loss = 0.0900, acc = 0.9820 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 17:34:31.958934: step 225640, loss = 0.0791, acc = 0.9780 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 17:34:36.498681: step 225660, loss = 0.1016, acc = 0.9800 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 17:34:41.147089: step 225680, loss = 0.0997, acc = 0.9780 (301.8 examples/sec; 0.212 sec/batch)
2017-05-09 17:34:45.712519: step 225700, loss = 0.0740, acc = 0.9840 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 17:34:50.334148: step 225720, loss = 0.0822, acc = 0.9840 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 17:34:55.109302: step 225740, loss = 0.0828, acc = 0.9760 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 17:34:59.761008: step 225760, loss = 0.0767, acc = 0.9820 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 17:35:04.382441: step 225780, loss = 0.0813, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 17:35:09.887294: step 225800, loss = 0.0966, acc = 0.9800 (237.0 examples/sec; 0.270 sec/batch)
2017-05-09 17:35:14.496052: step 225820, loss = 0.0633, acc = 0.9880 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 17:35:18.995286: step 225840, loss = 0.0872, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 17:35:23.579476: step 225860, loss = 0.0831, acc = 0.9760 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 17:35:28.247598: step 225880, loss = 0.0854, acc = 0.9820 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 17:35:32.843458: step 225900, loss = 0.0697, acc = 0.9900 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 17:35:37.459126: step 225920, loss = 0.0726, acc = 0.9880 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 17:35:42.097936: step 225940, loss = 0.0909, acc = 0.9840 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 17:35:46.628254: step 225960, loss = 0.0848, acc = 0.9780 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:35:51.231594: step 225980, loss = 0.1129, acc = 0.9700 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 17:35:56.012753: step 226000, loss = 0.0687, acc = 0.9880 (225.2 examples/sec; 0.284 sec/batch)
[Eval] 2017-05-09 17:36:10.004555: step 226000, acc = 0.9624, f1 = 0.9611
[Test] 2017-05-09 17:36:19.496280: step 226000, acc = 0.9524, f1 = 0.9520
[Status] 2017-05-09 17:36:19.496363: step 226000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:36:24.223516: step 226020, loss = 0.0829, acc = 0.9780 (225.7 examples/sec; 0.284 sec/batch)
2017-05-09 17:36:28.770663: step 226040, loss = 0.0822, acc = 0.9700 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 17:36:33.387788: step 226060, loss = 0.0886, acc = 0.9780 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 17:36:38.025970: step 226080, loss = 0.0667, acc = 0.9880 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 17:36:42.651684: step 226100, loss = 0.0764, acc = 0.9880 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 17:36:47.224057: step 226120, loss = 0.1001, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 17:36:51.980965: step 226140, loss = 0.0685, acc = 0.9860 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 17:36:56.707040: step 226160, loss = 0.0990, acc = 0.9740 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 17:37:01.243243: step 226180, loss = 0.0950, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:37:05.826525: step 226200, loss = 0.0646, acc = 0.9900 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 17:37:10.528266: step 226220, loss = 0.0993, acc = 0.9680 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 17:37:15.112286: step 226240, loss = 0.0923, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 17:37:19.664685: step 226260, loss = 0.0886, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 17:37:24.316964: step 226280, loss = 0.0997, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 17:37:28.921108: step 226300, loss = 0.0827, acc = 0.9840 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 17:37:33.488606: step 226320, loss = 0.0779, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 17:37:38.091542: step 226340, loss = 0.0673, acc = 0.9860 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 17:37:42.740372: step 226360, loss = 0.0970, acc = 0.9780 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 17:37:47.290151: step 226380, loss = 0.1074, acc = 0.9620 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:37:51.856990: step 226400, loss = 0.0784, acc = 0.9780 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 17:37:56.663936: step 226420, loss = 0.0852, acc = 0.9780 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 17:38:01.149505: step 226440, loss = 0.0859, acc = 0.9780 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 17:38:05.751504: step 226460, loss = 0.0754, acc = 0.9860 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 17:38:10.564261: step 226480, loss = 0.0665, acc = 0.9820 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:38:15.173414: step 226500, loss = 0.0951, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 17:38:19.773179: step 226520, loss = 0.0965, acc = 0.9740 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 17:38:24.643636: step 226540, loss = 0.0820, acc = 0.9780 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 17:38:29.348982: step 226560, loss = 0.0901, acc = 0.9760 (288.9 examples/sec; 0.221 sec/batch)
2017-05-09 17:38:33.964582: step 226580, loss = 0.0869, acc = 0.9740 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 17:38:38.935683: step 226600, loss = 0.0670, acc = 0.9900 (253.3 examples/sec; 0.253 sec/batch)
2017-05-09 17:38:43.619003: step 226620, loss = 0.1046, acc = 0.9740 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 17:38:48.202285: step 226640, loss = 0.0853, acc = 0.9860 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 17:38:52.857019: step 226660, loss = 0.0873, acc = 0.9700 (245.2 examples/sec; 0.261 sec/batch)
2017-05-09 17:38:57.435092: step 226680, loss = 0.0891, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 17:39:01.995435: step 226700, loss = 0.0911, acc = 0.9720 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 17:39:06.592255: step 226720, loss = 0.1021, acc = 0.9620 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 17:39:11.376913: step 226740, loss = 0.0836, acc = 0.9800 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 17:39:15.860072: step 226760, loss = 0.0993, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 17:39:20.458434: step 226780, loss = 0.0735, acc = 0.9840 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 17:39:26.292309: step 226800, loss = 0.0910, acc = 0.9760 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 17:39:30.771953: step 226820, loss = 0.0854, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 17:39:35.326760: step 226840, loss = 0.0818, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 17:39:39.975325: step 226860, loss = 0.0634, acc = 0.9860 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 17:39:44.481760: step 226880, loss = 0.0589, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 17:39:48.981157: step 226900, loss = 0.1029, acc = 0.9680 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 17:39:53.689344: step 226920, loss = 0.0756, acc = 0.9860 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 17:39:58.185667: step 226940, loss = 0.0702, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 17:40:02.725136: step 226960, loss = 0.0779, acc = 0.9780 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 17:40:07.471497: step 226980, loss = 0.0724, acc = 0.9820 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 17:40:12.017685: step 227000, loss = 0.0650, acc = 0.9940 (268.1 examples/sec; 0.239 sec/batch)
[Eval] 2017-05-09 17:40:26.098125: step 227000, acc = 0.9646, f1 = 0.9633
[Test] 2017-05-09 17:40:36.314522: step 227000, acc = 0.9554, f1 = 0.9550
[Status] 2017-05-09 17:40:36.314616: step 227000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:40:40.878951: step 227020, loss = 0.0850, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 17:40:45.463562: step 227040, loss = 0.0990, acc = 0.9700 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 17:40:50.020079: step 227060, loss = 0.0739, acc = 0.9820 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 17:40:54.674885: step 227080, loss = 0.0800, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 17:40:59.181232: step 227100, loss = 0.0743, acc = 0.9840 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 17:41:03.850809: step 227120, loss = 0.0999, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 17:41:08.604290: step 227140, loss = 0.0722, acc = 0.9860 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 17:41:13.050164: step 227160, loss = 0.0802, acc = 0.9820 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 17:41:17.509317: step 227180, loss = 0.0797, acc = 0.9820 (297.6 examples/sec; 0.215 sec/batch)
2017-05-09 17:41:22.178349: step 227200, loss = 0.0810, acc = 0.9780 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 17:41:26.863701: step 227220, loss = 0.0822, acc = 0.9780 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 17:41:31.594823: step 227240, loss = 0.0833, acc = 0.9740 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 17:41:36.290846: step 227260, loss = 0.0844, acc = 0.9740 (236.1 examples/sec; 0.271 sec/batch)
2017-05-09 17:41:40.773826: step 227280, loss = 0.0762, acc = 0.9880 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 17:41:45.272514: step 227300, loss = 0.0853, acc = 0.9860 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 17:41:50.024144: step 227320, loss = 0.1089, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 17:41:54.766050: step 227340, loss = 0.1025, acc = 0.9620 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 17:41:59.360719: step 227360, loss = 0.0964, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 17:42:04.186294: step 227380, loss = 0.0794, acc = 0.9760 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 17:42:08.908787: step 227400, loss = 0.0928, acc = 0.9680 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 17:42:13.579860: step 227420, loss = 0.0949, acc = 0.9700 (263.3 examples/sec; 0.243 sec/batch)
2017-05-09 17:42:18.242681: step 227440, loss = 0.0945, acc = 0.9760 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 17:42:23.165690: step 227460, loss = 0.0923, acc = 0.9800 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 17:42:27.856146: step 227480, loss = 0.0952, acc = 0.9700 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 17:42:32.415494: step 227500, loss = 0.1255, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 17:42:37.047839: step 227520, loss = 0.0802, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 17:42:41.560535: step 227540, loss = 0.1264, acc = 0.9540 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 17:42:46.206944: step 227560, loss = 0.0888, acc = 0.9760 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 17:42:50.860119: step 227580, loss = 0.1067, acc = 0.9600 (240.0 examples/sec; 0.267 sec/batch)
2017-05-09 17:42:55.336121: step 227600, loss = 0.0839, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 17:42:59.737201: step 227620, loss = 0.0680, acc = 0.9820 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 17:43:04.476190: step 227640, loss = 0.0888, acc = 0.9760 (244.8 examples/sec; 0.261 sec/batch)
2017-05-09 17:43:09.109872: step 227660, loss = 0.0728, acc = 0.9840 (260.9 examples/sec; 0.245 sec/batch)
2017-05-09 17:43:13.884357: step 227680, loss = 0.0860, acc = 0.9740 (236.7 examples/sec; 0.270 sec/batch)
2017-05-09 17:43:18.465714: step 227700, loss = 0.0961, acc = 0.9740 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 17:43:23.112459: step 227720, loss = 0.0687, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 17:43:27.758962: step 227740, loss = 0.0745, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:43:32.319153: step 227760, loss = 0.0737, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 17:43:37.183102: step 227780, loss = 0.0687, acc = 0.9860 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 17:43:42.126527: step 227800, loss = 0.0624, acc = 0.9920 (218.0 examples/sec; 0.294 sec/batch)
2017-05-09 17:43:47.541730: step 227820, loss = 0.0787, acc = 0.9760 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 17:43:52.068070: step 227840, loss = 0.0986, acc = 0.9840 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:43:56.599056: step 227860, loss = 0.0804, acc = 0.9780 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 17:44:01.171241: step 227880, loss = 0.0948, acc = 0.9780 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 17:44:05.920776: step 227900, loss = 0.0758, acc = 0.9780 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 17:44:10.483420: step 227920, loss = 0.1023, acc = 0.9780 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 17:44:15.067078: step 227940, loss = 0.0849, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 17:44:19.766943: step 227960, loss = 0.1069, acc = 0.9640 (241.9 examples/sec; 0.265 sec/batch)
2017-05-09 17:44:24.404827: step 227980, loss = 0.0738, acc = 0.9820 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 17:44:29.002288: step 228000, loss = 0.0778, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 17:44:43.216873: step 228000, acc = 0.9632, f1 = 0.9619
[Test] 2017-05-09 17:44:53.073723: step 228000, acc = 0.9532, f1 = 0.9528
[Status] 2017-05-09 17:44:53.073795: step 228000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:44:57.621408: step 228020, loss = 0.0826, acc = 0.9780 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 17:45:02.181384: step 228040, loss = 0.1045, acc = 0.9700 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 17:45:06.848308: step 228060, loss = 0.0901, acc = 0.9800 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 17:45:11.396429: step 228080, loss = 0.0724, acc = 0.9840 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:45:16.141809: step 228100, loss = 0.0919, acc = 0.9700 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 17:45:21.150591: step 228120, loss = 0.0723, acc = 0.9800 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 17:45:25.905331: step 228140, loss = 0.0994, acc = 0.9840 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 17:45:30.501257: step 228160, loss = 0.1002, acc = 0.9740 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 17:45:35.128072: step 228180, loss = 0.0966, acc = 0.9740 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 17:45:39.780422: step 228200, loss = 0.0955, acc = 0.9660 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 17:45:44.349455: step 228220, loss = 0.1021, acc = 0.9820 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 17:45:48.961471: step 228240, loss = 0.0894, acc = 0.9760 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 17:45:53.722981: step 228260, loss = 0.0825, acc = 0.9800 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 17:45:58.327788: step 228280, loss = 0.0874, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 17:46:02.917077: step 228300, loss = 0.0943, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 17:46:07.913284: step 228320, loss = 0.0978, acc = 0.9700 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 17:46:12.337707: step 228340, loss = 0.0790, acc = 0.9860 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 17:46:16.883141: step 228360, loss = 0.0894, acc = 0.9780 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 17:46:21.450392: step 228380, loss = 0.0725, acc = 0.9780 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 17:46:25.943598: step 228400, loss = 0.0894, acc = 0.9760 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 17:46:30.505985: step 228420, loss = 0.1189, acc = 0.9640 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 17:46:35.203718: step 228440, loss = 0.0863, acc = 0.9800 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 17:46:39.845645: step 228460, loss = 0.0941, acc = 0.9660 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 17:46:44.509980: step 228480, loss = 0.0878, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 17:46:49.175881: step 228500, loss = 0.0715, acc = 0.9880 (258.4 examples/sec; 0.248 sec/batch)
2017-05-09 17:46:53.688667: step 228520, loss = 0.0771, acc = 0.9840 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 17:46:58.256813: step 228540, loss = 0.0889, acc = 0.9760 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 17:47:02.803137: step 228560, loss = 0.1019, acc = 0.9680 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 17:47:07.690719: step 228580, loss = 0.0744, acc = 0.9840 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 17:47:12.311673: step 228600, loss = 0.0889, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 17:47:16.928469: step 228620, loss = 0.0726, acc = 0.9840 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 17:47:21.844043: step 228640, loss = 0.0878, acc = 0.9820 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 17:47:26.383308: step 228660, loss = 0.0991, acc = 0.9640 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 17:47:30.962373: step 228680, loss = 0.0784, acc = 0.9760 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 17:47:35.818176: step 228700, loss = 0.0970, acc = 0.9640 (227.2 examples/sec; 0.282 sec/batch)
2017-05-09 17:47:40.541645: step 228720, loss = 0.0789, acc = 0.9720 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 17:47:45.046184: step 228740, loss = 0.0956, acc = 0.9720 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:47:49.542336: step 228760, loss = 0.0890, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 17:47:54.286445: step 228780, loss = 0.0722, acc = 0.9800 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 17:47:58.900971: step 228800, loss = 0.0986, acc = 0.9820 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 17:48:04.664278: step 228820, loss = 0.1016, acc = 0.9680 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 17:48:09.556211: step 228840, loss = 0.0907, acc = 0.9760 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 17:48:14.113200: step 228860, loss = 0.0631, acc = 0.9920 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 17:48:18.672050: step 228880, loss = 0.0868, acc = 0.9780 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 17:48:23.542922: step 228900, loss = 0.0790, acc = 0.9900 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 17:48:28.037026: step 228920, loss = 0.0871, acc = 0.9800 (294.4 examples/sec; 0.217 sec/batch)
2017-05-09 17:48:32.594620: step 228940, loss = 0.0699, acc = 0.9840 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 17:48:37.264163: step 228960, loss = 0.0805, acc = 0.9780 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 17:48:41.852154: step 228980, loss = 0.0909, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 17:48:46.481269: step 229000, loss = 0.0843, acc = 0.9740 (276.0 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 17:49:00.620391: step 229000, acc = 0.9644, f1 = 0.9633
[Test] 2017-05-09 17:49:10.488660: step 229000, acc = 0.9557, f1 = 0.9553
[Status] 2017-05-09 17:49:10.488763: step 229000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:49:15.051730: step 229020, loss = 0.0866, acc = 0.9720 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 17:49:19.845617: step 229040, loss = 0.0815, acc = 0.9780 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:49:24.607957: step 229060, loss = 0.0711, acc = 0.9860 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 17:49:29.113425: step 229080, loss = 0.0892, acc = 0.9780 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 17:49:33.611730: step 229100, loss = 0.0853, acc = 0.9740 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 17:49:38.125430: step 229120, loss = 0.0937, acc = 0.9740 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 17:49:42.769226: step 229140, loss = 0.0677, acc = 0.9820 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 17:49:47.266590: step 229160, loss = 0.0989, acc = 0.9720 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 17:49:52.100388: step 229180, loss = 0.1023, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 17:49:56.860163: step 229200, loss = 0.0695, acc = 0.9800 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 17:50:01.592943: step 229220, loss = 0.0711, acc = 0.9840 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 17:50:06.443248: step 229240, loss = 0.0751, acc = 0.9820 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 17:50:10.999931: step 229260, loss = 0.0904, acc = 0.9800 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 17:50:15.595182: step 229280, loss = 0.0790, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 17:50:20.289583: step 229300, loss = 0.0710, acc = 0.9880 (256.9 examples/sec; 0.249 sec/batch)
2017-05-09 17:50:25.131554: step 229320, loss = 0.0821, acc = 0.9800 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 17:50:29.779695: step 229340, loss = 0.0806, acc = 0.9880 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:50:34.396099: step 229360, loss = 0.0924, acc = 0.9760 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 17:50:39.096692: step 229380, loss = 0.0891, acc = 0.9800 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:50:43.659264: step 229400, loss = 0.0825, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 17:50:48.322121: step 229420, loss = 0.0724, acc = 0.9840 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 17:50:53.025679: step 229440, loss = 0.0677, acc = 0.9900 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 17:50:57.665094: step 229460, loss = 0.0852, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 17:51:02.263023: step 229480, loss = 0.0694, acc = 0.9820 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 17:51:07.090724: step 229500, loss = 0.0765, acc = 0.9820 (229.1 examples/sec; 0.279 sec/batch)
2017-05-09 17:51:11.605778: step 229520, loss = 0.0820, acc = 0.9760 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:51:16.351790: step 229540, loss = 0.0883, acc = 0.9780 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 17:51:21.211329: step 229560, loss = 0.0832, acc = 0.9760 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 17:51:25.875077: step 229580, loss = 0.0782, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 17:51:30.417636: step 229600, loss = 0.0824, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 17:51:34.982187: step 229620, loss = 0.0829, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 17:51:39.618418: step 229640, loss = 0.0603, acc = 0.9860 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 17:51:44.257347: step 229660, loss = 0.0817, acc = 0.9780 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 17:51:48.865594: step 229680, loss = 0.0840, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 17:51:53.563701: step 229700, loss = 0.0998, acc = 0.9760 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 17:51:58.284238: step 229720, loss = 0.0751, acc = 0.9820 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 17:52:02.902328: step 229740, loss = 0.1073, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 17:52:07.974236: step 229760, loss = 0.0785, acc = 0.9800 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 17:52:12.601935: step 229780, loss = 0.0744, acc = 0.9880 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 17:52:17.221686: step 229800, loss = 0.0760, acc = 0.9840 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 17:52:22.881670: step 229820, loss = 0.0817, acc = 0.9760 (130.2 examples/sec; 0.491 sec/batch)
2017-05-09 17:52:27.461761: step 229840, loss = 0.0757, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 17:52:32.157989: step 229860, loss = 0.1208, acc = 0.9580 (259.1 examples/sec; 0.247 sec/batch)
2017-05-09 17:52:36.912023: step 229880, loss = 0.0675, acc = 0.9860 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 17:52:41.470114: step 229900, loss = 0.0887, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 17:52:46.023744: step 229920, loss = 0.0664, acc = 0.9940 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 17:52:50.763955: step 229940, loss = 0.0827, acc = 0.9820 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 17:52:55.350058: step 229960, loss = 0.0759, acc = 0.9800 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 17:53:00.000442: step 229980, loss = 0.0660, acc = 0.9900 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 17:53:04.556860: step 230000, loss = 0.0665, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 17:53:18.571687: step 230000, acc = 0.9638, f1 = 0.9625
[Test] 2017-05-09 17:53:28.340078: step 230000, acc = 0.9546, f1 = 0.9542
[Status] 2017-05-09 17:53:28.340163: step 230000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:53:32.874199: step 230020, loss = 0.0859, acc = 0.9820 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 17:53:37.569085: step 230040, loss = 0.0863, acc = 0.9760 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:53:42.133512: step 230060, loss = 0.0729, acc = 0.9840 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 17:53:46.742035: step 230080, loss = 0.0828, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 17:53:51.411602: step 230100, loss = 0.0818, acc = 0.9860 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 17:53:55.901983: step 230120, loss = 0.0693, acc = 0.9820 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 17:54:00.413509: step 230140, loss = 0.0938, acc = 0.9800 (296.6 examples/sec; 0.216 sec/batch)
2017-05-09 17:54:05.155553: step 230160, loss = 0.0956, acc = 0.9780 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 17:54:09.733869: step 230180, loss = 0.1067, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 17:54:14.275550: step 230200, loss = 0.0890, acc = 0.9800 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 17:54:18.944975: step 230220, loss = 0.0895, acc = 0.9760 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 17:54:23.832489: step 230240, loss = 0.0726, acc = 0.9780 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 17:54:28.718132: step 230260, loss = 0.0781, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 17:54:33.347823: step 230280, loss = 0.0894, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 17:54:37.905291: step 230300, loss = 0.0618, acc = 0.9920 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 17:54:42.468548: step 230320, loss = 0.0920, acc = 0.9800 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 17:54:47.051509: step 230340, loss = 0.0847, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 17:54:51.773636: step 230360, loss = 0.0706, acc = 0.9840 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 17:54:56.403292: step 230380, loss = 0.0788, acc = 0.9820 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 17:55:00.973538: step 230400, loss = 0.0812, acc = 0.9860 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 17:55:05.757135: step 230420, loss = 0.0923, acc = 0.9740 (244.7 examples/sec; 0.262 sec/batch)
2017-05-09 17:55:10.368286: step 230440, loss = 0.0717, acc = 0.9900 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 17:55:14.949181: step 230460, loss = 0.0730, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 17:55:19.556545: step 230480, loss = 0.0879, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 17:55:24.330799: step 230500, loss = 0.1003, acc = 0.9760 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 17:55:28.840770: step 230520, loss = 0.0812, acc = 0.9780 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 17:55:33.538519: step 230540, loss = 0.0840, acc = 0.9760 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 17:55:38.216338: step 230560, loss = 0.0789, acc = 0.9780 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 17:55:42.788018: step 230580, loss = 0.0930, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 17:55:47.256211: step 230600, loss = 0.0811, acc = 0.9820 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 17:55:51.857535: step 230620, loss = 0.0652, acc = 0.9840 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 17:55:56.471798: step 230640, loss = 0.0873, acc = 0.9720 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 17:56:01.182253: step 230660, loss = 0.0851, acc = 0.9840 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 17:56:06.062546: step 230680, loss = 0.0826, acc = 0.9820 (228.9 examples/sec; 0.280 sec/batch)
2017-05-09 17:56:10.600573: step 230700, loss = 0.0888, acc = 0.9700 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 17:56:15.192963: step 230720, loss = 0.0946, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 17:56:19.782510: step 230740, loss = 0.0799, acc = 0.9840 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 17:56:24.687804: step 230760, loss = 0.0652, acc = 0.9820 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 17:56:29.259131: step 230780, loss = 0.0806, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 17:56:33.772219: step 230800, loss = 0.0884, acc = 0.9740 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 17:56:38.505784: step 230820, loss = 0.1067, acc = 0.9740 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 17:56:44.099407: step 230840, loss = 0.0808, acc = 0.9800 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 17:56:48.731739: step 230860, loss = 0.0913, acc = 0.9720 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 17:56:53.383964: step 230880, loss = 0.0699, acc = 0.9760 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 17:56:57.901498: step 230900, loss = 0.0884, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 17:57:02.472781: step 230920, loss = 0.0854, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 17:57:07.103106: step 230940, loss = 0.0849, acc = 0.9780 (235.3 examples/sec; 0.272 sec/batch)
2017-05-09 17:57:11.743579: step 230960, loss = 0.0855, acc = 0.9740 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 17:57:16.302290: step 230980, loss = 0.0901, acc = 0.9760 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 17:57:21.089759: step 231000, loss = 0.0814, acc = 0.9780 (244.3 examples/sec; 0.262 sec/batch)
[Eval] 2017-05-09 17:57:35.219529: step 231000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 17:57:44.771701: step 231000, acc = 0.9560, f1 = 0.9556
[Status] 2017-05-09 17:57:44.771789: step 231000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 17:57:49.511582: step 231020, loss = 0.0789, acc = 0.9840 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 17:57:54.099308: step 231040, loss = 0.0696, acc = 0.9820 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 17:57:58.616947: step 231060, loss = 0.0891, acc = 0.9780 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 17:58:03.195979: step 231080, loss = 0.0998, acc = 0.9680 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 17:58:08.021598: step 231100, loss = 0.0778, acc = 0.9760 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 17:58:12.660637: step 231120, loss = 0.0758, acc = 0.9880 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 17:58:17.237469: step 231140, loss = 0.0761, acc = 0.9820 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 17:58:22.006114: step 231160, loss = 0.0806, acc = 0.9880 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 17:58:26.476570: step 231180, loss = 0.1264, acc = 0.9680 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 17:58:31.181503: step 231200, loss = 0.0995, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 17:58:35.780954: step 231220, loss = 0.0766, acc = 0.9780 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 17:58:40.258782: step 231240, loss = 0.0934, acc = 0.9700 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 17:58:44.820850: step 231260, loss = 0.0751, acc = 0.9860 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 17:58:49.585637: step 231280, loss = 0.0768, acc = 0.9820 (232.5 examples/sec; 0.275 sec/batch)
2017-05-09 17:58:54.232262: step 231300, loss = 0.0810, acc = 0.9720 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 17:58:58.896756: step 231320, loss = 0.1014, acc = 0.9700 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 17:59:03.432420: step 231340, loss = 0.0944, acc = 0.9680 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 17:59:08.238456: step 231360, loss = 0.0719, acc = 0.9860 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 17:59:12.855859: step 231380, loss = 0.0749, acc = 0.9820 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 17:59:17.477098: step 231400, loss = 0.0830, acc = 0.9720 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 17:59:22.088232: step 231420, loss = 0.0791, acc = 0.9760 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 17:59:26.657719: step 231440, loss = 0.1011, acc = 0.9740 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 17:59:31.170358: step 231460, loss = 0.1057, acc = 0.9700 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 17:59:35.900102: step 231480, loss = 0.0920, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 17:59:40.497743: step 231500, loss = 0.0860, acc = 0.9800 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 17:59:45.215092: step 231520, loss = 0.0865, acc = 0.9780 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 17:59:50.038347: step 231540, loss = 0.0670, acc = 0.9840 (226.7 examples/sec; 0.282 sec/batch)
2017-05-09 17:59:54.838287: step 231560, loss = 0.0704, acc = 0.9880 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 17:59:59.444743: step 231580, loss = 0.0820, acc = 0.9800 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 18:00:04.256219: step 231600, loss = 0.0719, acc = 0.9820 (233.7 examples/sec; 0.274 sec/batch)
2017-05-09 18:00:08.706015: step 231620, loss = 0.0960, acc = 0.9760 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 18:00:13.366450: step 231640, loss = 0.0951, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 18:00:18.158120: step 231660, loss = 0.0885, acc = 0.9740 (227.8 examples/sec; 0.281 sec/batch)
2017-05-09 18:00:22.688529: step 231680, loss = 0.0963, acc = 0.9760 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 18:00:27.117180: step 231700, loss = 0.0947, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 18:00:32.006555: step 231720, loss = 0.1005, acc = 0.9740 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 18:00:36.685781: step 231740, loss = 0.0969, acc = 0.9740 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 18:00:41.258345: step 231760, loss = 0.0659, acc = 0.9820 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 18:00:45.823166: step 231780, loss = 0.0879, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 18:00:50.619336: step 231800, loss = 0.0704, acc = 0.9800 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 18:00:55.608856: step 231820, loss = 0.0923, acc = 0.9780 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:01:00.869199: step 231840, loss = 0.0952, acc = 0.9800 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 18:01:05.710408: step 231860, loss = 0.0768, acc = 0.9860 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 18:01:10.223275: step 231880, loss = 0.1122, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:01:14.817407: step 231900, loss = 0.0634, acc = 0.9840 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 18:01:19.419264: step 231920, loss = 0.0956, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:01:23.984808: step 231940, loss = 0.0846, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 18:01:28.465657: step 231960, loss = 0.0709, acc = 0.9820 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 18:01:33.065078: step 231980, loss = 0.0764, acc = 0.9780 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 18:01:37.576033: step 232000, loss = 0.0759, acc = 0.9820 (263.9 examples/sec; 0.243 sec/batch)
[Eval] 2017-05-09 18:01:51.742157: step 232000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 18:02:01.374973: step 232000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-09 18:02:01.375085: step 232000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:02:05.923122: step 232020, loss = 0.0685, acc = 0.9840 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 18:02:10.422849: step 232040, loss = 0.0792, acc = 0.9820 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 18:02:15.101311: step 232060, loss = 0.0739, acc = 0.9860 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:02:19.665974: step 232080, loss = 0.0967, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 18:02:24.253099: step 232100, loss = 0.0826, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:02:28.933781: step 232120, loss = 0.0787, acc = 0.9820 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 18:02:33.441959: step 232140, loss = 0.0863, acc = 0.9740 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:02:38.032044: step 232160, loss = 0.0780, acc = 0.9880 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 18:02:42.814769: step 232180, loss = 0.0740, acc = 0.9820 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 18:02:47.597911: step 232200, loss = 0.0743, acc = 0.9800 (247.5 examples/sec; 0.259 sec/batch)
2017-05-09 18:02:52.102391: step 232220, loss = 0.0788, acc = 0.9860 (297.0 examples/sec; 0.215 sec/batch)
2017-05-09 18:02:56.955033: step 232240, loss = 0.0846, acc = 0.9820 (234.8 examples/sec; 0.273 sec/batch)
2017-05-09 18:03:01.534540: step 232260, loss = 0.0842, acc = 0.9820 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 18:03:06.208746: step 232280, loss = 0.0745, acc = 0.9820 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 18:03:10.892525: step 232300, loss = 0.0849, acc = 0.9780 (236.2 examples/sec; 0.271 sec/batch)
2017-05-09 18:03:15.467978: step 232320, loss = 0.0772, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 18:03:20.336463: step 232340, loss = 0.0930, acc = 0.9700 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 18:03:24.903168: step 232360, loss = 0.0712, acc = 0.9860 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 18:03:29.712160: step 232380, loss = 0.0941, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 18:03:34.236656: step 232400, loss = 0.1100, acc = 0.9660 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 18:03:38.820535: step 232420, loss = 0.0774, acc = 0.9900 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 18:03:43.503779: step 232440, loss = 0.0801, acc = 0.9820 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 18:03:47.999578: step 232460, loss = 0.0652, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 18:03:52.487376: step 232480, loss = 0.0771, acc = 0.9880 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 18:03:57.356808: step 232500, loss = 0.0951, acc = 0.9700 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 18:04:02.019579: step 232520, loss = 0.0984, acc = 0.9680 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 18:04:06.642820: step 232540, loss = 0.1112, acc = 0.9640 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 18:04:11.655828: step 232560, loss = 0.0986, acc = 0.9680 (256.8 examples/sec; 0.249 sec/batch)
2017-05-09 18:04:16.419207: step 232580, loss = 0.0548, acc = 0.9960 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:04:21.026740: step 232600, loss = 0.0819, acc = 0.9800 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:04:25.924694: step 232620, loss = 0.0629, acc = 0.9900 (220.2 examples/sec; 0.291 sec/batch)
2017-05-09 18:04:30.645219: step 232640, loss = 0.0933, acc = 0.9740 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 18:04:35.218473: step 232660, loss = 0.1070, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 18:04:39.761685: step 232680, loss = 0.1093, acc = 0.9680 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:04:44.444223: step 232700, loss = 0.0851, acc = 0.9800 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 18:04:49.119675: step 232720, loss = 0.0891, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 18:04:53.679602: step 232740, loss = 0.1097, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:04:58.388927: step 232760, loss = 0.0727, acc = 0.9820 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 18:05:02.935055: step 232780, loss = 0.0906, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 18:05:07.570481: step 232800, loss = 0.1103, acc = 0.9660 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 18:05:12.450431: step 232820, loss = 0.0695, acc = 0.9880 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 18:05:16.997954: step 232840, loss = 0.0798, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 18:05:22.504222: step 232860, loss = 0.0828, acc = 0.9820 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:05:27.242388: step 232880, loss = 0.0862, acc = 0.9740 (223.1 examples/sec; 0.287 sec/batch)
2017-05-09 18:05:31.763047: step 232900, loss = 0.0782, acc = 0.9800 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 18:05:36.290034: step 232920, loss = 0.0793, acc = 0.9800 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:05:41.035853: step 232940, loss = 0.0799, acc = 0.9780 (256.1 examples/sec; 0.250 sec/batch)
2017-05-09 18:05:45.752806: step 232960, loss = 0.0971, acc = 0.9720 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 18:05:50.288234: step 232980, loss = 0.0751, acc = 0.9840 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 18:05:54.729718: step 233000, loss = 0.0816, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 18:06:08.982903: step 233000, acc = 0.9636, f1 = 0.9624
[Test] 2017-05-09 18:06:18.683850: step 233000, acc = 0.9547, f1 = 0.9543
[Status] 2017-05-09 18:06:18.683950: step 233000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:06:23.278851: step 233020, loss = 0.0935, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:06:27.977224: step 233040, loss = 0.0947, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:06:32.544480: step 233060, loss = 0.0885, acc = 0.9720 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 18:06:37.097388: step 233080, loss = 0.0933, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:06:41.685880: step 233100, loss = 0.0779, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 18:06:46.373751: step 233120, loss = 0.0861, acc = 0.9800 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 18:06:51.081387: step 233140, loss = 0.0903, acc = 0.9720 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 18:06:55.832137: step 233160, loss = 0.0967, acc = 0.9720 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 18:07:00.391273: step 233180, loss = 0.0779, acc = 0.9820 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 18:07:04.872909: step 233200, loss = 0.0843, acc = 0.9820 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 18:07:09.428267: step 233220, loss = 0.0660, acc = 0.9900 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 18:07:14.043616: step 233240, loss = 0.0872, acc = 0.9800 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 18:07:18.545130: step 233260, loss = 0.0900, acc = 0.9680 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 18:07:23.185385: step 233280, loss = 0.0882, acc = 0.9720 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 18:07:27.938851: step 233300, loss = 0.0935, acc = 0.9760 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 18:07:32.599958: step 233320, loss = 0.0834, acc = 0.9820 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 18:07:37.197888: step 233340, loss = 0.1015, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 18:07:41.903971: step 233360, loss = 0.0778, acc = 0.9800 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:07:46.478304: step 233380, loss = 0.1120, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 18:07:51.175784: step 233400, loss = 0.0816, acc = 0.9720 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 18:07:55.986717: step 233420, loss = 0.0621, acc = 0.9880 (242.3 examples/sec; 0.264 sec/batch)
2017-05-09 18:08:00.620165: step 233440, loss = 0.0851, acc = 0.9780 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 18:08:05.253835: step 233460, loss = 0.0878, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 18:08:09.861330: step 233480, loss = 0.0828, acc = 0.9760 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 18:08:14.552893: step 233500, loss = 0.0726, acc = 0.9860 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 18:08:19.139237: step 233520, loss = 0.0796, acc = 0.9780 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 18:08:23.909009: step 233540, loss = 0.0977, acc = 0.9640 (266.1 examples/sec; 0.240 sec/batch)
2017-05-09 18:08:28.766061: step 233560, loss = 0.0693, acc = 0.9880 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 18:08:33.439572: step 233580, loss = 0.0940, acc = 0.9700 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 18:08:37.907351: step 233600, loss = 0.1026, acc = 0.9760 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 18:08:42.583319: step 233620, loss = 0.0685, acc = 0.9880 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 18:08:47.261659: step 233640, loss = 0.0775, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 18:08:51.941816: step 233660, loss = 0.0748, acc = 0.9840 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 18:08:56.716343: step 233680, loss = 0.0773, acc = 0.9800 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 18:09:01.267636: step 233700, loss = 0.0703, acc = 0.9820 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 18:09:05.863171: step 233720, loss = 0.0884, acc = 0.9700 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 18:09:10.406269: step 233740, loss = 0.0844, acc = 0.9800 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 18:09:15.172171: step 233760, loss = 0.0816, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 18:09:19.648472: step 233780, loss = 0.0706, acc = 0.9880 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:09:24.187676: step 233800, loss = 0.0916, acc = 0.9740 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 18:09:28.816169: step 233820, loss = 0.0759, acc = 0.9780 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 18:09:33.529978: step 233840, loss = 0.0962, acc = 0.9800 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 18:09:38.882198: step 233860, loss = 0.0814, acc = 0.9860 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 18:09:43.640699: step 233880, loss = 0.0705, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 18:09:48.175157: step 233900, loss = 0.0672, acc = 0.9780 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:09:52.793716: step 233920, loss = 0.0682, acc = 0.9840 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 18:09:57.453326: step 233940, loss = 0.0992, acc = 0.9760 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 18:10:01.982385: step 233960, loss = 0.0843, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 18:10:06.708518: step 233980, loss = 0.0926, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:10:11.493770: step 234000, loss = 0.0817, acc = 0.9780 (294.6 examples/sec; 0.217 sec/batch)
[Eval] 2017-05-09 18:10:25.599445: step 234000, acc = 0.9632, f1 = 0.9619
[Test] 2017-05-09 18:10:35.145842: step 234000, acc = 0.9539, f1 = 0.9535
[Status] 2017-05-09 18:10:35.145928: step 234000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:10:39.887510: step 234020, loss = 0.0656, acc = 0.9900 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 18:10:44.655671: step 234040, loss = 0.0623, acc = 0.9920 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 18:10:49.225193: step 234060, loss = 0.0697, acc = 0.9800 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 18:10:54.001942: step 234080, loss = 0.0731, acc = 0.9800 (246.2 examples/sec; 0.260 sec/batch)
2017-05-09 18:10:58.519243: step 234100, loss = 0.0620, acc = 0.9900 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 18:11:03.082447: step 234120, loss = 0.0726, acc = 0.9880 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 18:11:07.684845: step 234140, loss = 0.0757, acc = 0.9840 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 18:11:12.601862: step 234160, loss = 0.0782, acc = 0.9780 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 18:11:17.246143: step 234180, loss = 0.0871, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 18:11:21.815922: step 234200, loss = 0.0879, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 18:11:26.567157: step 234220, loss = 0.0864, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:11:31.105430: step 234240, loss = 0.0796, acc = 0.9740 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 18:11:35.741636: step 234260, loss = 0.0912, acc = 0.9840 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 18:11:40.421786: step 234280, loss = 0.0784, acc = 0.9760 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 18:11:45.003542: step 234300, loss = 0.0988, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 18:11:49.619014: step 234320, loss = 0.1009, acc = 0.9740 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 18:11:54.505003: step 234340, loss = 0.0789, acc = 0.9840 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 18:11:59.040004: step 234360, loss = 0.0879, acc = 0.9760 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 18:12:03.643441: step 234380, loss = 0.0749, acc = 0.9800 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:12:08.730131: step 234400, loss = 0.0933, acc = 0.9780 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 18:12:13.352278: step 234420, loss = 0.0692, acc = 0.9800 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 18:12:17.956273: step 234440, loss = 0.0764, acc = 0.9800 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 18:12:22.527214: step 234460, loss = 0.1025, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 18:12:27.203932: step 234480, loss = 0.0767, acc = 0.9800 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 18:12:31.714584: step 234500, loss = 0.0717, acc = 0.9820 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 18:12:36.281394: step 234520, loss = 0.0930, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 18:12:41.088916: step 234540, loss = 0.0910, acc = 0.9740 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 18:12:45.712051: step 234560, loss = 0.0769, acc = 0.9900 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 18:12:50.254934: step 234580, loss = 0.0796, acc = 0.9820 (300.9 examples/sec; 0.213 sec/batch)
2017-05-09 18:12:54.893974: step 234600, loss = 0.0909, acc = 0.9720 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 18:12:59.507719: step 234620, loss = 0.0694, acc = 0.9780 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 18:13:04.135242: step 234640, loss = 0.1071, acc = 0.9600 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 18:13:08.973118: step 234660, loss = 0.0792, acc = 0.9880 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 18:13:13.712807: step 234680, loss = 0.0847, acc = 0.9760 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 18:13:18.271335: step 234700, loss = 0.0817, acc = 0.9840 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 18:13:23.133647: step 234720, loss = 0.0639, acc = 0.9860 (233.7 examples/sec; 0.274 sec/batch)
2017-05-09 18:13:27.663299: step 234740, loss = 0.1073, acc = 0.9680 (295.8 examples/sec; 0.216 sec/batch)
2017-05-09 18:13:32.178094: step 234760, loss = 0.0777, acc = 0.9800 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 18:13:36.930074: step 234780, loss = 0.0685, acc = 0.9780 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 18:13:41.811534: step 234800, loss = 0.0837, acc = 0.9780 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 18:13:46.465073: step 234820, loss = 0.0737, acc = 0.9820 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 18:13:51.090268: step 234840, loss = 0.0773, acc = 0.9780 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:13:56.901015: step 234860, loss = 0.0658, acc = 0.9900 (131.0 examples/sec; 0.488 sec/batch)
2017-05-09 18:14:01.446594: step 234880, loss = 0.0732, acc = 0.9880 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 18:14:06.033903: step 234900, loss = 0.1049, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:14:10.752365: step 234920, loss = 0.0933, acc = 0.9720 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 18:14:15.443799: step 234940, loss = 0.1144, acc = 0.9600 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 18:14:20.000088: step 234960, loss = 0.1028, acc = 0.9680 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 18:14:24.816073: step 234980, loss = 0.1025, acc = 0.9700 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 18:14:29.609012: step 235000, loss = 0.0935, acc = 0.9720 (261.3 examples/sec; 0.245 sec/batch)
[Eval] 2017-05-09 18:14:43.558300: step 235000, acc = 0.9601, f1 = 0.9587
[Test] 2017-05-09 18:14:53.175536: step 235000, acc = 0.9494, f1 = 0.9490
[Status] 2017-05-09 18:14:53.175740: step 235000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:14:58.017816: step 235020, loss = 0.0978, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:15:02.558136: step 235040, loss = 0.0850, acc = 0.9820 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 18:15:07.091414: step 235060, loss = 0.0638, acc = 0.9860 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 18:15:11.796650: step 235080, loss = 0.0637, acc = 0.9900 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 18:15:16.445871: step 235100, loss = 0.0952, acc = 0.9720 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 18:15:21.052457: step 235120, loss = 0.0704, acc = 0.9860 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:15:25.823992: step 235140, loss = 0.0753, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 18:15:30.383967: step 235160, loss = 0.1059, acc = 0.9720 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 18:15:35.055821: step 235180, loss = 0.0535, acc = 0.9960 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 18:15:39.744715: step 235200, loss = 0.0917, acc = 0.9720 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 18:15:44.255329: step 235220, loss = 0.1017, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:15:48.876194: step 235240, loss = 0.0771, acc = 0.9880 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 18:15:53.544010: step 235260, loss = 0.0734, acc = 0.9760 (243.2 examples/sec; 0.263 sec/batch)
2017-05-09 18:15:58.101325: step 235280, loss = 0.0803, acc = 0.9860 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 18:16:02.749187: step 235300, loss = 0.0934, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 18:16:07.414883: step 235320, loss = 0.1031, acc = 0.9560 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 18:16:12.086782: step 235340, loss = 0.0747, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 18:16:16.702181: step 235360, loss = 0.0701, acc = 0.9860 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:16:21.349411: step 235380, loss = 0.0903, acc = 0.9760 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 18:16:26.131356: step 235400, loss = 0.0872, acc = 0.9680 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 18:16:30.827098: step 235420, loss = 0.0695, acc = 0.9840 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:16:35.463008: step 235440, loss = 0.0787, acc = 0.9760 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 18:16:40.319539: step 235460, loss = 0.0852, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:16:44.918365: step 235480, loss = 0.0673, acc = 0.9900 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 18:16:49.516198: step 235500, loss = 0.0737, acc = 0.9820 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 18:16:54.314545: step 235520, loss = 0.0735, acc = 0.9860 (228.6 examples/sec; 0.280 sec/batch)
2017-05-09 18:16:58.940228: step 235540, loss = 0.0789, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 18:17:03.500707: step 235560, loss = 0.0778, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 18:17:08.248713: step 235580, loss = 0.0905, acc = 0.9760 (248.1 examples/sec; 0.258 sec/batch)
2017-05-09 18:17:12.748701: step 235600, loss = 0.0865, acc = 0.9760 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 18:17:17.330769: step 235620, loss = 0.0768, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 18:17:22.099205: step 235640, loss = 0.0760, acc = 0.9800 (248.5 examples/sec; 0.258 sec/batch)
2017-05-09 18:17:26.970286: step 235660, loss = 0.0741, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 18:17:31.624303: step 235680, loss = 0.1012, acc = 0.9800 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:17:36.150409: step 235700, loss = 0.0837, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 18:17:40.940946: step 235720, loss = 0.0640, acc = 0.9880 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 18:17:45.547070: step 235740, loss = 0.0919, acc = 0.9860 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 18:17:50.171068: step 235760, loss = 0.0790, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 18:17:54.929418: step 235780, loss = 0.0680, acc = 0.9900 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 18:17:59.436707: step 235800, loss = 0.0950, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 18:18:03.909071: step 235820, loss = 0.0906, acc = 0.9780 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 18:18:08.537801: step 235840, loss = 0.0986, acc = 0.9700 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 18:18:13.090461: step 235860, loss = 0.0746, acc = 0.9860 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 18:18:18.286813: step 235880, loss = 0.1058, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 18:18:23.020696: step 235900, loss = 0.0736, acc = 0.9880 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 18:18:27.495805: step 235920, loss = 0.0805, acc = 0.9800 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 18:18:32.165403: step 235940, loss = 0.0784, acc = 0.9800 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:18:36.871531: step 235960, loss = 0.0841, acc = 0.9740 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 18:18:41.506327: step 235980, loss = 0.0816, acc = 0.9760 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 18:18:46.057878: step 236000, loss = 0.0815, acc = 0.9820 (284.1 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 18:19:00.104868: step 236000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 18:19:09.897682: step 236000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 18:19:09.897769: step 236000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:19:14.347802: step 236020, loss = 0.0884, acc = 0.9760 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 18:19:18.836284: step 236040, loss = 0.1061, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 18:19:23.638975: step 236060, loss = 0.0820, acc = 0.9800 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 18:19:28.197251: step 236080, loss = 0.1051, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:19:32.797776: step 236100, loss = 0.0813, acc = 0.9780 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:19:37.742844: step 236120, loss = 0.0779, acc = 0.9780 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 18:19:42.372480: step 236140, loss = 0.1113, acc = 0.9740 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 18:19:46.991358: step 236160, loss = 0.0827, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 18:19:51.757038: step 236180, loss = 0.0859, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 18:19:56.354942: step 236200, loss = 0.0832, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:20:00.989314: step 236220, loss = 0.0755, acc = 0.9820 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 18:20:05.597770: step 236240, loss = 0.0868, acc = 0.9740 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 18:20:10.329585: step 236260, loss = 0.0844, acc = 0.9760 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 18:20:14.915545: step 236280, loss = 0.0878, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 18:20:19.499804: step 236300, loss = 0.0611, acc = 0.9920 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 18:20:24.139758: step 236320, loss = 0.0986, acc = 0.9820 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 18:20:28.744922: step 236340, loss = 0.0805, acc = 0.9800 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 18:20:33.333768: step 236360, loss = 0.0837, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 18:20:37.914856: step 236380, loss = 0.0747, acc = 0.9840 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 18:20:42.478780: step 236400, loss = 0.0829, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 18:20:47.145931: step 236420, loss = 0.0694, acc = 0.9820 (237.8 examples/sec; 0.269 sec/batch)
2017-05-09 18:20:51.868272: step 236440, loss = 0.0735, acc = 0.9800 (245.5 examples/sec; 0.261 sec/batch)
2017-05-09 18:20:56.693986: step 236460, loss = 0.0856, acc = 0.9760 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 18:21:01.375331: step 236480, loss = 0.0856, acc = 0.9800 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 18:21:05.931085: step 236500, loss = 0.0772, acc = 0.9820 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 18:21:10.643826: step 236520, loss = 0.0696, acc = 0.9920 (296.2 examples/sec; 0.216 sec/batch)
2017-05-09 18:21:15.204742: step 236540, loss = 0.0682, acc = 0.9880 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:21:19.897690: step 236560, loss = 0.1021, acc = 0.9680 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 18:21:24.630366: step 236580, loss = 0.0670, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 18:21:29.204015: step 236600, loss = 0.0894, acc = 0.9740 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 18:21:33.700456: step 236620, loss = 0.0937, acc = 0.9680 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 18:21:38.235380: step 236640, loss = 0.0928, acc = 0.9800 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 18:21:42.891946: step 236660, loss = 0.0971, acc = 0.9680 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:21:47.373460: step 236680, loss = 0.0661, acc = 0.9840 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:21:52.205797: step 236700, loss = 0.0862, acc = 0.9760 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 18:21:56.844397: step 236720, loss = 0.0837, acc = 0.9800 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 18:22:01.238440: step 236740, loss = 0.0747, acc = 0.9800 (301.1 examples/sec; 0.213 sec/batch)
2017-05-09 18:22:05.939971: step 236760, loss = 0.0953, acc = 0.9720 (251.9 examples/sec; 0.254 sec/batch)
2017-05-09 18:22:10.338421: step 236780, loss = 0.0874, acc = 0.9740 (296.3 examples/sec; 0.216 sec/batch)
2017-05-09 18:22:14.878074: step 236800, loss = 0.1114, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:22:19.639087: step 236820, loss = 0.0823, acc = 0.9820 (237.1 examples/sec; 0.270 sec/batch)
2017-05-09 18:22:24.181636: step 236840, loss = 0.0848, acc = 0.9780 (300.0 examples/sec; 0.213 sec/batch)
2017-05-09 18:22:28.788820: step 236860, loss = 0.0749, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 18:22:34.604595: step 236880, loss = 0.0776, acc = 0.9780 (250.3 examples/sec; 0.256 sec/batch)
2017-05-09 18:22:39.129530: step 236900, loss = 0.0875, acc = 0.9740 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 18:22:43.796768: step 236920, loss = 0.0922, acc = 0.9780 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 18:22:48.529547: step 236940, loss = 0.0973, acc = 0.9740 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 18:22:52.853978: step 236960, loss = 0.0729, acc = 0.9820 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 18:22:57.337240: step 236980, loss = 0.0934, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 18:23:01.913193: step 237000, loss = 0.0929, acc = 0.9780 (279.7 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 18:23:16.015337: step 237000, acc = 0.9621, f1 = 0.9609
[Test] 2017-05-09 18:23:25.735164: step 237000, acc = 0.9533, f1 = 0.9529
[Status] 2017-05-09 18:23:25.735264: step 237000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:23:30.276476: step 237020, loss = 0.0890, acc = 0.9780 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 18:23:35.054010: step 237040, loss = 0.0990, acc = 0.9680 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 18:23:39.693861: step 237060, loss = 0.0596, acc = 0.9900 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 18:23:44.199942: step 237080, loss = 0.0692, acc = 0.9840 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 18:23:48.683004: step 237100, loss = 0.0822, acc = 0.9760 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:23:53.277008: step 237120, loss = 0.0638, acc = 0.9880 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 18:23:57.880007: step 237140, loss = 0.0833, acc = 0.9860 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 18:24:02.572663: step 237160, loss = 0.0826, acc = 0.9840 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 18:24:07.330387: step 237180, loss = 0.0759, acc = 0.9820 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 18:24:11.920964: step 237200, loss = 0.0960, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 18:24:16.636803: step 237220, loss = 0.0871, acc = 0.9780 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 18:24:21.233829: step 237240, loss = 0.0713, acc = 0.9760 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 18:24:25.803297: step 237260, loss = 0.0757, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:24:30.483861: step 237280, loss = 0.0644, acc = 0.9880 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 18:24:35.339207: step 237300, loss = 0.0779, acc = 0.9800 (223.4 examples/sec; 0.286 sec/batch)
2017-05-09 18:24:39.955018: step 237320, loss = 0.0793, acc = 0.9780 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 18:24:44.554382: step 237340, loss = 0.0949, acc = 0.9700 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 18:24:49.237468: step 237360, loss = 0.0861, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 18:24:53.982688: step 237380, loss = 0.0722, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 18:24:58.480831: step 237400, loss = 0.0736, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 18:25:03.127789: step 237420, loss = 0.0777, acc = 0.9860 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 18:25:07.979471: step 237440, loss = 0.0913, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 18:25:12.544117: step 237460, loss = 0.0891, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 18:25:17.143923: step 237480, loss = 0.0876, acc = 0.9780 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 18:25:21.765058: step 237500, loss = 0.0782, acc = 0.9800 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 18:25:26.285100: step 237520, loss = 0.0755, acc = 0.9920 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 18:25:30.895157: step 237540, loss = 0.0890, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 18:25:35.711056: step 237560, loss = 0.0865, acc = 0.9760 (227.6 examples/sec; 0.281 sec/batch)
2017-05-09 18:25:40.272980: step 237580, loss = 0.0782, acc = 0.9820 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 18:25:44.755675: step 237600, loss = 0.0655, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:25:49.323995: step 237620, loss = 0.0756, acc = 0.9840 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 18:25:54.098995: step 237640, loss = 0.0996, acc = 0.9760 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 18:25:58.604867: step 237660, loss = 0.0920, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 18:26:03.210047: step 237680, loss = 0.0831, acc = 0.9780 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 18:26:07.939327: step 237700, loss = 0.0844, acc = 0.9780 (301.9 examples/sec; 0.212 sec/batch)
2017-05-09 18:26:12.512977: step 237720, loss = 0.0857, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:26:17.131740: step 237740, loss = 0.1398, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:26:21.883273: step 237760, loss = 0.0957, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:26:26.418082: step 237780, loss = 0.0791, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:26:31.044705: step 237800, loss = 0.0963, acc = 0.9680 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 18:26:35.748622: step 237820, loss = 0.0804, acc = 0.9860 (248.0 examples/sec; 0.258 sec/batch)
2017-05-09 18:26:40.271729: step 237840, loss = 0.0644, acc = 0.9880 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 18:26:44.840633: step 237860, loss = 0.0707, acc = 0.9860 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 18:26:49.500299: step 237880, loss = 0.0701, acc = 0.9840 (257.7 examples/sec; 0.248 sec/batch)
2017-05-09 18:26:55.005187: step 237900, loss = 0.0822, acc = 0.9780 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 18:26:59.644045: step 237920, loss = 0.0779, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:27:04.244453: step 237940, loss = 0.0678, acc = 0.9840 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 18:27:09.047805: step 237960, loss = 0.0690, acc = 0.9880 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 18:27:13.606675: step 237980, loss = 0.1092, acc = 0.9760 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 18:27:18.227025: step 238000, loss = 0.1001, acc = 0.9680 (271.3 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 18:27:32.379815: step 238000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 18:27:42.145260: step 238000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 18:27:42.145348: step 238000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:27:46.674862: step 238020, loss = 0.0741, acc = 0.9860 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 18:27:51.369297: step 238040, loss = 0.0922, acc = 0.9720 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 18:27:55.824360: step 238060, loss = 0.0787, acc = 0.9760 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:28:00.532503: step 238080, loss = 0.0800, acc = 0.9840 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 18:28:05.332035: step 238100, loss = 0.0648, acc = 0.9900 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 18:28:09.984113: step 238120, loss = 0.0760, acc = 0.9820 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 18:28:14.558942: step 238140, loss = 0.0973, acc = 0.9700 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 18:28:19.264608: step 238160, loss = 0.0715, acc = 0.9880 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 18:28:24.099901: step 238180, loss = 0.0816, acc = 0.9880 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 18:28:28.689088: step 238200, loss = 0.1129, acc = 0.9760 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 18:28:33.244611: step 238220, loss = 0.0660, acc = 0.9900 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 18:28:38.002315: step 238240, loss = 0.0744, acc = 0.9820 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 18:28:42.587447: step 238260, loss = 0.0643, acc = 0.9880 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 18:28:47.292720: step 238280, loss = 0.0638, acc = 0.9900 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 18:28:51.971160: step 238300, loss = 0.0686, acc = 0.9880 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 18:28:56.688231: step 238320, loss = 0.0781, acc = 0.9800 (258.3 examples/sec; 0.248 sec/batch)
2017-05-09 18:29:01.248690: step 238340, loss = 0.0800, acc = 0.9760 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 18:29:06.200325: step 238360, loss = 0.0656, acc = 0.9820 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 18:29:10.840639: step 238380, loss = 0.0746, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 18:29:15.454370: step 238400, loss = 0.0728, acc = 0.9780 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 18:29:20.219174: step 238420, loss = 0.1094, acc = 0.9560 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:29:24.761457: step 238440, loss = 0.0706, acc = 0.9860 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 18:29:29.339270: step 238460, loss = 0.0951, acc = 0.9700 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:29:33.834923: step 238480, loss = 0.0714, acc = 0.9840 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 18:29:38.514514: step 238500, loss = 0.0891, acc = 0.9800 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 18:29:43.328698: step 238520, loss = 0.0847, acc = 0.9700 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 18:29:47.920932: step 238540, loss = 0.0753, acc = 0.9800 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 18:29:52.793066: step 238560, loss = 0.0940, acc = 0.9740 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:29:57.282773: step 238580, loss = 0.0883, acc = 0.9860 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 18:30:01.892875: step 238600, loss = 0.0771, acc = 0.9800 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 18:30:06.651319: step 238620, loss = 0.0819, acc = 0.9800 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 18:30:11.140835: step 238640, loss = 0.0945, acc = 0.9760 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 18:30:15.809351: step 238660, loss = 0.0742, acc = 0.9800 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 18:30:20.528511: step 238680, loss = 0.0821, acc = 0.9820 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 18:30:25.145284: step 238700, loss = 0.0666, acc = 0.9900 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 18:30:29.796651: step 238720, loss = 0.1018, acc = 0.9700 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 18:30:34.511876: step 238740, loss = 0.0726, acc = 0.9820 (244.4 examples/sec; 0.262 sec/batch)
2017-05-09 18:30:39.210723: step 238760, loss = 0.0778, acc = 0.9800 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 18:30:43.838453: step 238780, loss = 0.0949, acc = 0.9660 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 18:30:48.430468: step 238800, loss = 0.0961, acc = 0.9720 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 18:30:53.000691: step 238820, loss = 0.0870, acc = 0.9760 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 18:30:57.537891: step 238840, loss = 0.0833, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 18:31:02.082486: step 238860, loss = 0.0684, acc = 0.9820 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 18:31:06.758427: step 238880, loss = 0.0659, acc = 0.9880 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 18:31:12.450656: step 238900, loss = 0.0714, acc = 0.9820 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 18:31:16.983376: step 238920, loss = 0.0609, acc = 0.9960 (297.8 examples/sec; 0.215 sec/batch)
2017-05-09 18:31:21.730154: step 238940, loss = 0.0979, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 18:31:26.297466: step 238960, loss = 0.0902, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 18:31:30.794321: step 238980, loss = 0.0818, acc = 0.9840 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 18:31:35.463258: step 239000, loss = 0.0858, acc = 0.9860 (277.6 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 18:31:49.519181: step 239000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 18:31:58.768039: step 239000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 18:31:58.768125: step 239000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:32:03.683214: step 239020, loss = 0.0752, acc = 0.9860 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 18:32:08.737406: step 239040, loss = 0.0862, acc = 0.9700 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 18:32:13.297412: step 239060, loss = 0.0879, acc = 0.9700 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 18:32:17.970830: step 239080, loss = 0.0819, acc = 0.9820 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:32:22.483737: step 239100, loss = 0.0797, acc = 0.9780 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 18:32:26.918934: step 239120, loss = 0.0889, acc = 0.9780 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:32:31.660580: step 239140, loss = 0.0932, acc = 0.9760 (224.7 examples/sec; 0.285 sec/batch)
2017-05-09 18:32:36.354315: step 239160, loss = 0.1168, acc = 0.9700 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 18:32:40.781906: step 239180, loss = 0.0812, acc = 0.9840 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 18:32:45.323891: step 239200, loss = 0.0738, acc = 0.9800 (298.3 examples/sec; 0.215 sec/batch)
2017-05-09 18:32:49.972716: step 239220, loss = 0.0630, acc = 0.9880 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 18:32:54.515804: step 239240, loss = 0.0815, acc = 0.9740 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 18:32:59.141331: step 239260, loss = 0.0823, acc = 0.9780 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 18:33:03.902536: step 239280, loss = 0.0760, acc = 0.9780 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 18:33:08.497660: step 239300, loss = 0.0792, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:33:13.111990: step 239320, loss = 0.0754, acc = 0.9860 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:33:17.884592: step 239340, loss = 0.0864, acc = 0.9800 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:33:22.535892: step 239360, loss = 0.0909, acc = 0.9740 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 18:33:27.205973: step 239380, loss = 0.0783, acc = 0.9820 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 18:33:32.059190: step 239400, loss = 0.0713, acc = 0.9800 (226.2 examples/sec; 0.283 sec/batch)
2017-05-09 18:33:36.703634: step 239420, loss = 0.0769, acc = 0.9840 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 18:33:41.272858: step 239440, loss = 0.0905, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:33:45.921474: step 239460, loss = 0.0753, acc = 0.9820 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 18:33:50.737725: step 239480, loss = 0.1022, acc = 0.9840 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 18:33:55.231640: step 239500, loss = 0.0694, acc = 0.9880 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 18:33:59.746493: step 239520, loss = 0.0699, acc = 0.9880 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 18:34:04.440304: step 239540, loss = 0.0906, acc = 0.9740 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 18:34:09.220102: step 239560, loss = 0.0749, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 18:34:13.826734: step 239580, loss = 0.0881, acc = 0.9860 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 18:34:18.464285: step 239600, loss = 0.1049, acc = 0.9640 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 18:34:23.110814: step 239620, loss = 0.0855, acc = 0.9760 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 18:34:27.719799: step 239640, loss = 0.0807, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:34:32.606957: step 239660, loss = 0.0898, acc = 0.9740 (217.6 examples/sec; 0.294 sec/batch)
2017-05-09 18:34:37.252804: step 239680, loss = 0.0864, acc = 0.9780 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 18:34:41.877266: step 239700, loss = 0.0814, acc = 0.9840 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 18:34:46.524187: step 239720, loss = 0.0690, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 18:34:51.401623: step 239740, loss = 0.0987, acc = 0.9780 (267.7 examples/sec; 0.239 sec/batch)
2017-05-09 18:34:55.924362: step 239760, loss = 0.0899, acc = 0.9740 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 18:35:00.495520: step 239780, loss = 0.0797, acc = 0.9820 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 18:35:05.224197: step 239800, loss = 0.0846, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 18:35:09.681640: step 239820, loss = 0.0863, acc = 0.9780 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 18:35:14.278346: step 239840, loss = 0.0960, acc = 0.9720 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:35:19.089324: step 239860, loss = 0.1056, acc = 0.9740 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 18:35:23.683009: step 239880, loss = 0.0690, acc = 0.9880 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 18:35:28.949350: step 239900, loss = 0.1002, acc = 0.9760 (161.0 examples/sec; 0.397 sec/batch)
2017-05-09 18:35:33.750096: step 239920, loss = 0.0682, acc = 0.9840 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 18:35:38.292786: step 239940, loss = 0.0822, acc = 0.9780 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 18:35:42.804167: step 239960, loss = 0.0703, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 18:35:47.591646: step 239980, loss = 0.0821, acc = 0.9780 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 18:35:52.205340: step 240000, loss = 0.0800, acc = 0.9820 (273.8 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 18:36:06.330709: step 240000, acc = 0.9640, f1 = 0.9627
[Test] 2017-05-09 18:36:15.964745: step 240000, acc = 0.9544, f1 = 0.9540
[Status] 2017-05-09 18:36:15.964814: step 240000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:36:20.552152: step 240020, loss = 0.0851, acc = 0.9840 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:36:25.201734: step 240040, loss = 0.0695, acc = 0.9940 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 18:36:29.866738: step 240060, loss = 0.0834, acc = 0.9860 (237.8 examples/sec; 0.269 sec/batch)
2017-05-09 18:36:34.591140: step 240080, loss = 0.0938, acc = 0.9820 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:36:39.148232: step 240100, loss = 0.0925, acc = 0.9780 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 18:36:43.734871: step 240120, loss = 0.0734, acc = 0.9840 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 18:36:48.561672: step 240140, loss = 0.0914, acc = 0.9740 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 18:36:53.122756: step 240160, loss = 0.0686, acc = 0.9880 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:36:57.693968: step 240180, loss = 0.0790, acc = 0.9840 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 18:37:02.350748: step 240200, loss = 0.0614, acc = 0.9880 (256.2 examples/sec; 0.250 sec/batch)
2017-05-09 18:37:06.972298: step 240220, loss = 0.0743, acc = 0.9800 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:37:11.600443: step 240240, loss = 0.0795, acc = 0.9840 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 18:37:16.274167: step 240260, loss = 0.0823, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 18:37:20.969118: step 240280, loss = 0.1174, acc = 0.9580 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 18:37:25.549197: step 240300, loss = 0.0800, acc = 0.9800 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:37:30.392127: step 240320, loss = 0.0912, acc = 0.9740 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 18:37:34.984107: step 240340, loss = 0.0948, acc = 0.9760 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 18:37:39.529327: step 240360, loss = 0.1015, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 18:37:44.314673: step 240380, loss = 0.0725, acc = 0.9820 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 18:37:48.906236: step 240400, loss = 0.0991, acc = 0.9800 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 18:37:53.459506: step 240420, loss = 0.0699, acc = 0.9820 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 18:37:58.082097: step 240440, loss = 0.0651, acc = 0.9860 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:38:02.828625: step 240460, loss = 0.0667, acc = 0.9880 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 18:38:07.356463: step 240480, loss = 0.0990, acc = 0.9700 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:38:11.804367: step 240500, loss = 0.0837, acc = 0.9800 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 18:38:16.476917: step 240520, loss = 0.0984, acc = 0.9640 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:38:20.993530: step 240540, loss = 0.0897, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 18:38:25.440348: step 240560, loss = 0.0748, acc = 0.9860 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 18:38:30.272794: step 240580, loss = 0.0903, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:38:34.752683: step 240600, loss = 0.0768, acc = 0.9860 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 18:38:39.446801: step 240620, loss = 0.0821, acc = 0.9800 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 18:38:44.247481: step 240640, loss = 0.0676, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 18:38:48.801197: step 240660, loss = 0.0755, acc = 0.9800 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 18:38:53.355501: step 240680, loss = 0.0847, acc = 0.9800 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 18:38:58.255510: step 240700, loss = 0.0794, acc = 0.9780 (221.2 examples/sec; 0.289 sec/batch)
2017-05-09 18:39:02.755510: step 240720, loss = 0.0776, acc = 0.9800 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 18:39:07.492902: step 240740, loss = 0.1025, acc = 0.9760 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 18:39:12.108301: step 240760, loss = 0.0853, acc = 0.9780 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 18:39:16.770951: step 240780, loss = 0.1000, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 18:39:21.351178: step 240800, loss = 0.0673, acc = 0.9900 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 18:39:25.880982: step 240820, loss = 0.0687, acc = 0.9900 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 18:39:31.253040: step 240840, loss = 0.0692, acc = 0.9860 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 18:39:35.873668: step 240860, loss = 0.0789, acc = 0.9800 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 18:39:40.488414: step 240880, loss = 0.0817, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 18:39:45.337900: step 240900, loss = 0.1072, acc = 0.9720 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 18:39:50.953591: step 240920, loss = 0.1095, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 18:39:55.618917: step 240940, loss = 0.0766, acc = 0.9860 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 18:40:00.425072: step 240960, loss = 0.0749, acc = 0.9800 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 18:40:05.091039: step 240980, loss = 0.0638, acc = 0.9860 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 18:40:09.706147: step 241000, loss = 0.0683, acc = 0.9800 (281.1 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 18:40:23.779430: step 241000, acc = 0.9611, f1 = 0.9599
[Test] 2017-05-09 18:40:33.488983: step 241000, acc = 0.9520, f1 = 0.9516
[Status] 2017-05-09 18:40:33.489047: step 241000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:40:38.074831: step 241020, loss = 0.0778, acc = 0.9800 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 18:40:42.796005: step 241040, loss = 0.0861, acc = 0.9780 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 18:40:47.445744: step 241060, loss = 0.0722, acc = 0.9860 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 18:40:51.979332: step 241080, loss = 0.0737, acc = 0.9840 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 18:40:56.820124: step 241100, loss = 0.0877, acc = 0.9780 (242.7 examples/sec; 0.264 sec/batch)
2017-05-09 18:41:01.325263: step 241120, loss = 0.0884, acc = 0.9740 (296.9 examples/sec; 0.216 sec/batch)
2017-05-09 18:41:05.927345: step 241140, loss = 0.0713, acc = 0.9800 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 18:41:10.598423: step 241160, loss = 0.0935, acc = 0.9760 (256.0 examples/sec; 0.250 sec/batch)
2017-05-09 18:41:15.478752: step 241180, loss = 0.0871, acc = 0.9820 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 18:41:20.194038: step 241200, loss = 0.0618, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 18:41:24.753468: step 241220, loss = 0.0995, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:41:29.516663: step 241240, loss = 0.1122, acc = 0.9720 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 18:41:34.062125: step 241260, loss = 0.0840, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 18:41:38.595744: step 241280, loss = 0.0765, acc = 0.9880 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 18:41:43.575147: step 241300, loss = 0.0962, acc = 0.9740 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 18:41:48.177035: step 241320, loss = 0.0810, acc = 0.9840 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 18:41:52.703397: step 241340, loss = 0.0950, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:41:57.399834: step 241360, loss = 0.0812, acc = 0.9780 (232.6 examples/sec; 0.275 sec/batch)
2017-05-09 18:42:02.021808: step 241380, loss = 0.0736, acc = 0.9880 (260.9 examples/sec; 0.245 sec/batch)
2017-05-09 18:42:06.674536: step 241400, loss = 0.0964, acc = 0.9700 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 18:42:11.313492: step 241420, loss = 0.0817, acc = 0.9820 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 18:42:16.223193: step 241440, loss = 0.0754, acc = 0.9780 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 18:42:20.770396: step 241460, loss = 0.0734, acc = 0.9840 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 18:42:25.437081: step 241480, loss = 0.1131, acc = 0.9640 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 18:42:30.326799: step 241500, loss = 0.0596, acc = 0.9920 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:42:34.963004: step 241520, loss = 0.0697, acc = 0.9820 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 18:42:39.545742: step 241540, loss = 0.1000, acc = 0.9680 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:42:44.378453: step 241560, loss = 0.1051, acc = 0.9760 (238.6 examples/sec; 0.268 sec/batch)
2017-05-09 18:42:48.862822: step 241580, loss = 0.0841, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 18:42:53.505633: step 241600, loss = 0.0757, acc = 0.9860 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:42:58.185299: step 241620, loss = 0.0724, acc = 0.9820 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 18:43:03.100849: step 241640, loss = 0.1008, acc = 0.9600 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 18:43:07.749809: step 241660, loss = 0.0729, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:43:12.436726: step 241680, loss = 0.0965, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 18:43:17.109015: step 241700, loss = 0.0983, acc = 0.9660 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 18:43:21.728064: step 241720, loss = 0.0707, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 18:43:26.278011: step 241740, loss = 0.0657, acc = 0.9820 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 18:43:30.955759: step 241760, loss = 0.0773, acc = 0.9820 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 18:43:35.506702: step 241780, loss = 0.0740, acc = 0.9760 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:43:39.950713: step 241800, loss = 0.0678, acc = 0.9880 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 18:43:44.630788: step 241820, loss = 0.0744, acc = 0.9840 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 18:43:49.326444: step 241840, loss = 0.0905, acc = 0.9820 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 18:43:53.886984: step 241860, loss = 0.1072, acc = 0.9600 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:43:58.581286: step 241880, loss = 0.0810, acc = 0.9800 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 18:44:03.429494: step 241900, loss = 0.0834, acc = 0.9740 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 18:44:08.646149: step 241920, loss = 0.0831, acc = 0.9780 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 18:44:13.300918: step 241940, loss = 0.0981, acc = 0.9720 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 18:44:18.055545: step 241960, loss = 0.0836, acc = 0.9760 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 18:44:22.648256: step 241980, loss = 0.0849, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 18:44:27.204461: step 242000, loss = 0.0902, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 18:44:41.360379: step 242000, acc = 0.9638, f1 = 0.9626
[Test] 2017-05-09 18:44:51.242704: step 242000, acc = 0.9553, f1 = 0.9549
[Status] 2017-05-09 18:44:51.242816: step 242000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:44:55.863703: step 242020, loss = 0.0786, acc = 0.9820 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:45:00.663281: step 242040, loss = 0.0879, acc = 0.9700 (236.2 examples/sec; 0.271 sec/batch)
2017-05-09 18:45:05.251752: step 242060, loss = 0.0685, acc = 0.9900 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 18:45:09.999308: step 242080, loss = 0.0965, acc = 0.9760 (258.2 examples/sec; 0.248 sec/batch)
2017-05-09 18:45:14.544878: step 242100, loss = 0.1072, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 18:45:19.329502: step 242120, loss = 0.0804, acc = 0.9740 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 18:45:23.857533: step 242140, loss = 0.0755, acc = 0.9800 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:45:28.624100: step 242160, loss = 0.0861, acc = 0.9800 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 18:45:33.232331: step 242180, loss = 0.0731, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 18:45:37.828725: step 242200, loss = 0.0886, acc = 0.9720 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 18:45:42.507718: step 242220, loss = 0.0651, acc = 0.9840 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 18:45:47.151733: step 242240, loss = 0.0662, acc = 0.9920 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 18:45:51.755777: step 242260, loss = 0.0659, acc = 0.9940 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 18:45:56.302722: step 242280, loss = 0.0732, acc = 0.9840 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:46:00.896064: step 242300, loss = 0.0732, acc = 0.9820 (245.6 examples/sec; 0.261 sec/batch)
2017-05-09 18:46:05.338474: step 242320, loss = 0.0871, acc = 0.9680 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 18:46:09.902371: step 242340, loss = 0.1057, acc = 0.9720 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 18:46:14.578323: step 242360, loss = 0.0652, acc = 0.9880 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 18:46:19.319293: step 242380, loss = 0.1110, acc = 0.9720 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 18:46:23.964956: step 242400, loss = 0.1025, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 18:46:28.630704: step 242420, loss = 0.0957, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 18:46:33.422236: step 242440, loss = 0.0834, acc = 0.9760 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 18:46:37.961438: step 242460, loss = 0.0827, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:46:42.577823: step 242480, loss = 0.0710, acc = 0.9840 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 18:46:47.284504: step 242500, loss = 0.0934, acc = 0.9720 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 18:46:51.879613: step 242520, loss = 0.0964, acc = 0.9760 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 18:46:56.494080: step 242540, loss = 0.0929, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 18:47:01.238360: step 242560, loss = 0.0677, acc = 0.9800 (248.2 examples/sec; 0.258 sec/batch)
2017-05-09 18:47:05.645675: step 242580, loss = 0.0668, acc = 0.9900 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 18:47:10.276326: step 242600, loss = 0.0879, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 18:47:14.863596: step 242620, loss = 0.0874, acc = 0.9820 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:47:19.654725: step 242640, loss = 0.0843, acc = 0.9820 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 18:47:24.214937: step 242660, loss = 0.0778, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 18:47:28.806136: step 242680, loss = 0.1067, acc = 0.9620 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 18:47:33.643531: step 242700, loss = 0.0832, acc = 0.9780 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 18:47:38.175947: step 242720, loss = 0.0704, acc = 0.9800 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 18:47:42.728789: step 242740, loss = 0.0803, acc = 0.9760 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:47:47.744797: step 242760, loss = 0.0746, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 18:47:52.346470: step 242780, loss = 0.0796, acc = 0.9800 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 18:47:57.119169: step 242800, loss = 0.0827, acc = 0.9720 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 18:48:01.712558: step 242820, loss = 0.0663, acc = 0.9860 (305.2 examples/sec; 0.210 sec/batch)
2017-05-09 18:48:06.275501: step 242840, loss = 0.0850, acc = 0.9780 (241.2 examples/sec; 0.265 sec/batch)
2017-05-09 18:48:10.837475: step 242860, loss = 0.0732, acc = 0.9840 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 18:48:15.556640: step 242880, loss = 0.0897, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 18:48:20.170653: step 242900, loss = 0.0955, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 18:48:24.742834: step 242920, loss = 0.0822, acc = 0.9780 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 18:48:30.355483: step 242940, loss = 0.0963, acc = 0.9680 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 18:48:34.804881: step 242960, loss = 0.1028, acc = 0.9720 (299.3 examples/sec; 0.214 sec/batch)
2017-05-09 18:48:39.426690: step 242980, loss = 0.0801, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:48:43.880415: step 243000, loss = 0.0920, acc = 0.9780 (290.6 examples/sec; 0.220 sec/batch)
[Eval] 2017-05-09 18:48:57.899274: step 243000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 18:49:07.412063: step 243000, acc = 0.9560, f1 = 0.9556
[Status] 2017-05-09 18:49:07.412146: step 243000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:49:12.041940: step 243020, loss = 0.0657, acc = 0.9900 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 18:49:16.864864: step 243040, loss = 0.0669, acc = 0.9860 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 18:49:21.347260: step 243060, loss = 0.0889, acc = 0.9760 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 18:49:25.904403: step 243080, loss = 0.0752, acc = 0.9820 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 18:49:30.698082: step 243100, loss = 0.0701, acc = 0.9880 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 18:49:35.203033: step 243120, loss = 0.0834, acc = 0.9820 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 18:49:39.778832: step 243140, loss = 0.0850, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 18:49:44.514990: step 243160, loss = 0.0752, acc = 0.9880 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 18:49:49.053270: step 243180, loss = 0.0675, acc = 0.9880 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 18:49:53.548329: step 243200, loss = 0.0920, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 18:49:58.137783: step 243220, loss = 0.0690, acc = 0.9880 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 18:50:02.709543: step 243240, loss = 0.0865, acc = 0.9720 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 18:50:07.300471: step 243260, loss = 0.1002, acc = 0.9800 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 18:50:11.887942: step 243280, loss = 0.0774, acc = 0.9780 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 18:50:16.540454: step 243300, loss = 0.0853, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 18:50:21.208814: step 243320, loss = 0.0778, acc = 0.9840 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 18:50:25.880507: step 243340, loss = 0.0702, acc = 0.9920 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 18:50:30.612777: step 243360, loss = 0.0858, acc = 0.9760 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 18:50:35.133074: step 243380, loss = 0.0802, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 18:50:39.673925: step 243400, loss = 0.0811, acc = 0.9800 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 18:50:44.436004: step 243420, loss = 0.0635, acc = 0.9840 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 18:50:48.938454: step 243440, loss = 0.0692, acc = 0.9880 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 18:50:53.562052: step 243460, loss = 0.0745, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 18:50:58.170645: step 243480, loss = 0.0820, acc = 0.9900 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 18:51:02.969802: step 243500, loss = 0.0762, acc = 0.9780 (223.8 examples/sec; 0.286 sec/batch)
2017-05-09 18:51:07.625382: step 243520, loss = 0.0747, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 18:51:12.445208: step 243540, loss = 0.0894, acc = 0.9740 (240.9 examples/sec; 0.266 sec/batch)
2017-05-09 18:51:17.034521: step 243560, loss = 0.0876, acc = 0.9700 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 18:51:21.645390: step 243580, loss = 0.0921, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 18:51:26.269523: step 243600, loss = 0.0760, acc = 0.9800 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 18:51:30.935543: step 243620, loss = 0.0905, acc = 0.9720 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:51:35.450896: step 243640, loss = 0.0832, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 18:51:39.992048: step 243660, loss = 0.0736, acc = 0.9820 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 18:51:44.743069: step 243680, loss = 0.0609, acc = 0.9840 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 18:51:49.309698: step 243700, loss = 0.1027, acc = 0.9740 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 18:51:53.880786: step 243720, loss = 0.0898, acc = 0.9780 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:51:58.746461: step 243740, loss = 0.0732, acc = 0.9840 (258.6 examples/sec; 0.247 sec/batch)
2017-05-09 18:52:03.409706: step 243760, loss = 0.0845, acc = 0.9780 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 18:52:08.063175: step 243780, loss = 0.0898, acc = 0.9700 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 18:52:12.925763: step 243800, loss = 0.0733, acc = 0.9880 (240.6 examples/sec; 0.266 sec/batch)
2017-05-09 18:52:17.521933: step 243820, loss = 0.0849, acc = 0.9820 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 18:52:22.015345: step 243840, loss = 0.0869, acc = 0.9720 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 18:52:26.639945: step 243860, loss = 0.0647, acc = 0.9900 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 18:52:31.304810: step 243880, loss = 0.0744, acc = 0.9820 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 18:52:36.044622: step 243900, loss = 0.0925, acc = 0.9740 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 18:52:40.690449: step 243920, loss = 0.1152, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 18:52:46.355816: step 243940, loss = 0.0893, acc = 0.9780 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 18:52:50.997223: step 243960, loss = 0.0644, acc = 0.9880 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 18:52:55.630705: step 243980, loss = 0.1059, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 18:53:00.500193: step 244000, loss = 0.0665, acc = 0.9820 (263.3 examples/sec; 0.243 sec/batch)
[Eval] 2017-05-09 18:53:14.616500: step 244000, acc = 0.9638, f1 = 0.9625
[Test] 2017-05-09 18:53:24.256963: step 244000, acc = 0.9546, f1 = 0.9543
[Status] 2017-05-09 18:53:24.257050: step 244000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:53:28.812028: step 244020, loss = 0.0753, acc = 0.9840 (299.5 examples/sec; 0.214 sec/batch)
2017-05-09 18:53:33.467590: step 244040, loss = 0.0582, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 18:53:37.981283: step 244060, loss = 0.0911, acc = 0.9800 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:53:42.866820: step 244080, loss = 0.0839, acc = 0.9780 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 18:53:47.649390: step 244100, loss = 0.0724, acc = 0.9820 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 18:53:52.227965: step 244120, loss = 0.0756, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 18:53:56.736774: step 244140, loss = 0.0891, acc = 0.9820 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 18:54:01.445001: step 244160, loss = 0.1008, acc = 0.9680 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 18:54:06.057788: step 244180, loss = 0.0706, acc = 0.9840 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 18:54:10.739014: step 244200, loss = 0.1024, acc = 0.9680 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 18:54:15.487100: step 244220, loss = 0.0715, acc = 0.9880 (237.1 examples/sec; 0.270 sec/batch)
2017-05-09 18:54:19.991325: step 244240, loss = 0.0690, acc = 0.9840 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:54:24.571810: step 244260, loss = 0.0755, acc = 0.9860 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 18:54:29.367810: step 244280, loss = 0.1029, acc = 0.9760 (248.9 examples/sec; 0.257 sec/batch)
2017-05-09 18:54:33.966124: step 244300, loss = 0.0748, acc = 0.9780 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 18:54:38.603807: step 244320, loss = 0.0985, acc = 0.9760 (261.3 examples/sec; 0.245 sec/batch)
2017-05-09 18:54:43.172361: step 244340, loss = 0.0644, acc = 0.9860 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 18:54:47.968307: step 244360, loss = 0.0602, acc = 0.9880 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 18:54:52.483823: step 244380, loss = 0.0869, acc = 0.9760 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 18:54:57.113428: step 244400, loss = 0.0905, acc = 0.9720 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 18:55:02.028906: step 244420, loss = 0.0882, acc = 0.9740 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 18:55:06.752903: step 244440, loss = 0.0736, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:55:11.431374: step 244460, loss = 0.0950, acc = 0.9820 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 18:55:16.330701: step 244480, loss = 0.0685, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 18:55:20.876142: step 244500, loss = 0.0685, acc = 0.9860 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 18:55:25.469201: step 244520, loss = 0.1039, acc = 0.9680 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 18:55:30.185414: step 244540, loss = 0.0811, acc = 0.9740 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 18:55:35.036638: step 244560, loss = 0.0692, acc = 0.9840 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 18:55:39.679932: step 244580, loss = 0.0702, acc = 0.9860 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 18:55:44.526896: step 244600, loss = 0.0489, acc = 0.9940 (227.3 examples/sec; 0.282 sec/batch)
2017-05-09 18:55:49.198220: step 244620, loss = 0.0849, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 18:55:53.825359: step 244640, loss = 0.0696, acc = 0.9800 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 18:55:58.357054: step 244660, loss = 0.1020, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 18:56:03.110440: step 244680, loss = 0.0901, acc = 0.9740 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 18:56:07.708143: step 244700, loss = 0.1006, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 18:56:12.295634: step 244720, loss = 0.0839, acc = 0.9760 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 18:56:16.970727: step 244740, loss = 0.0895, acc = 0.9760 (304.2 examples/sec; 0.210 sec/batch)
2017-05-09 18:56:21.642927: step 244760, loss = 0.0992, acc = 0.9660 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 18:56:26.242239: step 244780, loss = 0.1037, acc = 0.9660 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 18:56:31.246219: step 244800, loss = 0.0614, acc = 0.9900 (240.1 examples/sec; 0.267 sec/batch)
2017-05-09 18:56:35.771422: step 244820, loss = 0.1060, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 18:56:40.500423: step 244840, loss = 0.0725, acc = 0.9840 (264.8 examples/sec; 0.242 sec/batch)
2017-05-09 18:56:45.257243: step 244860, loss = 0.0835, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 18:56:49.913761: step 244880, loss = 0.0814, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 18:56:54.443331: step 244900, loss = 0.1016, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 18:56:59.225057: step 244920, loss = 0.1361, acc = 0.9420 (244.2 examples/sec; 0.262 sec/batch)
2017-05-09 18:57:04.736981: step 244940, loss = 0.0848, acc = 0.9780 (133.1 examples/sec; 0.481 sec/batch)
2017-05-09 18:57:09.351671: step 244960, loss = 0.0865, acc = 0.9840 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 18:57:14.062417: step 244980, loss = 0.0828, acc = 0.9740 (229.1 examples/sec; 0.279 sec/batch)
2017-05-09 18:57:18.538448: step 245000, loss = 0.1079, acc = 0.9600 (282.3 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 18:57:32.541627: step 245000, acc = 0.9639, f1 = 0.9627
[Test] 2017-05-09 18:57:42.718992: step 245000, acc = 0.9548, f1 = 0.9544
[Status] 2017-05-09 18:57:42.719088: step 245000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 18:57:47.234363: step 245020, loss = 0.0722, acc = 0.9820 (293.5 examples/sec; 0.218 sec/batch)
2017-05-09 18:57:51.764447: step 245040, loss = 0.0885, acc = 0.9720 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 18:57:56.315669: step 245060, loss = 0.0666, acc = 0.9880 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 18:58:01.054314: step 245080, loss = 0.0787, acc = 0.9780 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 18:58:05.700754: step 245100, loss = 0.0749, acc = 0.9900 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 18:58:10.272627: step 245120, loss = 0.0692, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 18:58:14.949037: step 245140, loss = 0.0762, acc = 0.9800 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 18:58:19.563889: step 245160, loss = 0.0694, acc = 0.9840 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 18:58:24.207733: step 245180, loss = 0.0795, acc = 0.9800 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 18:58:28.855294: step 245200, loss = 0.0784, acc = 0.9820 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 18:58:33.457853: step 245220, loss = 0.0749, acc = 0.9780 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 18:58:38.019569: step 245240, loss = 0.0760, acc = 0.9780 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 18:58:42.762031: step 245260, loss = 0.0951, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 18:58:47.318207: step 245280, loss = 0.0738, acc = 0.9820 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 18:58:51.930195: step 245300, loss = 0.0661, acc = 0.9840 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 18:58:56.518354: step 245320, loss = 0.0655, acc = 0.9860 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 18:59:01.275590: step 245340, loss = 0.0815, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 18:59:05.871419: step 245360, loss = 0.0715, acc = 0.9820 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 18:59:10.569034: step 245380, loss = 0.0674, acc = 0.9860 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 18:59:15.427847: step 245400, loss = 0.0758, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 18:59:20.109168: step 245420, loss = 0.0918, acc = 0.9820 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 18:59:24.804903: step 245440, loss = 0.0921, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 18:59:29.577419: step 245460, loss = 0.0754, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 18:59:34.112960: step 245480, loss = 0.0803, acc = 0.9840 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 18:59:38.663373: step 245500, loss = 0.0761, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 18:59:43.288828: step 245520, loss = 0.0769, acc = 0.9800 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 18:59:47.870951: step 245540, loss = 0.0779, acc = 0.9860 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 18:59:52.401978: step 245560, loss = 0.0695, acc = 0.9800 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 18:59:57.072777: step 245580, loss = 0.0740, acc = 0.9840 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 19:00:01.766646: step 245600, loss = 0.0966, acc = 0.9800 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 19:00:06.249736: step 245620, loss = 0.0786, acc = 0.9740 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 19:00:10.743524: step 245640, loss = 0.0968, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 19:00:15.630455: step 245660, loss = 0.0696, acc = 0.9880 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 19:00:20.160562: step 245680, loss = 0.0875, acc = 0.9740 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 19:00:24.893185: step 245700, loss = 0.0801, acc = 0.9780 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 19:00:29.636471: step 245720, loss = 0.0840, acc = 0.9860 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 19:00:34.154511: step 245740, loss = 0.0785, acc = 0.9820 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 19:00:38.781540: step 245760, loss = 0.0771, acc = 0.9860 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 19:00:43.679503: step 245780, loss = 0.0941, acc = 0.9700 (238.5 examples/sec; 0.268 sec/batch)
2017-05-09 19:00:48.232221: step 245800, loss = 0.0886, acc = 0.9760 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 19:00:52.839758: step 245820, loss = 0.0681, acc = 0.9880 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:00:57.412416: step 245840, loss = 0.0754, acc = 0.9820 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 19:01:02.215875: step 245860, loss = 0.0893, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 19:01:06.783597: step 245880, loss = 0.0679, acc = 0.9860 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 19:01:11.422795: step 245900, loss = 0.0726, acc = 0.9820 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 19:01:16.302032: step 245920, loss = 0.0891, acc = 0.9800 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 19:01:20.905119: step 245940, loss = 0.0775, acc = 0.9840 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 19:01:26.273094: step 245960, loss = 0.0962, acc = 0.9720 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 19:01:30.940420: step 245980, loss = 0.0646, acc = 0.9880 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 19:01:35.602090: step 246000, loss = 0.0762, acc = 0.9820 (274.9 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 19:01:49.631476: step 246000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 19:01:59.646819: step 246000, acc = 0.9557, f1 = 0.9553
[Status] 2017-05-09 19:01:59.646906: step 246000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:02:04.183725: step 246020, loss = 0.0778, acc = 0.9740 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 19:02:08.800417: step 246040, loss = 0.0783, acc = 0.9800 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 19:02:13.438544: step 246060, loss = 0.0890, acc = 0.9660 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 19:02:17.876495: step 246080, loss = 0.1004, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 19:02:22.333516: step 246100, loss = 0.1105, acc = 0.9680 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 19:02:26.856917: step 246120, loss = 0.0765, acc = 0.9840 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 19:02:31.501908: step 246140, loss = 0.0792, acc = 0.9820 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 19:02:36.003367: step 246160, loss = 0.1089, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 19:02:40.619901: step 246180, loss = 0.0737, acc = 0.9860 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 19:02:45.342014: step 246200, loss = 0.0851, acc = 0.9760 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 19:02:49.806737: step 246220, loss = 0.0765, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 19:02:54.324433: step 246240, loss = 0.1110, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 19:02:59.089768: step 246260, loss = 0.0777, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 19:03:03.724751: step 246280, loss = 0.0889, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 19:03:08.306972: step 246300, loss = 0.0733, acc = 0.9840 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 19:03:13.161052: step 246320, loss = 0.0878, acc = 0.9760 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 19:03:17.688617: step 246340, loss = 0.0760, acc = 0.9780 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 19:03:22.377411: step 246360, loss = 0.0867, acc = 0.9760 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:03:26.944657: step 246380, loss = 0.0739, acc = 0.9880 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:03:31.413273: step 246400, loss = 0.0984, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 19:03:35.976486: step 246420, loss = 0.0918, acc = 0.9760 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 19:03:40.701221: step 246440, loss = 0.0907, acc = 0.9760 (303.2 examples/sec; 0.211 sec/batch)
2017-05-09 19:03:45.128549: step 246460, loss = 0.0863, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 19:03:49.786864: step 246480, loss = 0.0725, acc = 0.9860 (254.3 examples/sec; 0.252 sec/batch)
2017-05-09 19:03:54.290082: step 246500, loss = 0.0890, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 19:03:58.874408: step 246520, loss = 0.0816, acc = 0.9800 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 19:04:03.331831: step 246540, loss = 0.0702, acc = 0.9800 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 19:04:07.893686: step 246560, loss = 0.0800, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 19:04:12.486676: step 246580, loss = 0.0932, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 19:04:17.056720: step 246600, loss = 0.0784, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 19:04:21.905775: step 246620, loss = 0.0833, acc = 0.9820 (234.7 examples/sec; 0.273 sec/batch)
2017-05-09 19:04:26.361171: step 246640, loss = 0.0910, acc = 0.9720 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 19:04:30.920978: step 246660, loss = 0.0699, acc = 0.9840 (299.3 examples/sec; 0.214 sec/batch)
2017-05-09 19:04:35.466712: step 246680, loss = 0.0835, acc = 0.9780 (296.2 examples/sec; 0.216 sec/batch)
2017-05-09 19:04:40.450948: step 246700, loss = 0.0713, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 19:04:45.174868: step 246720, loss = 0.0589, acc = 0.9900 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 19:04:49.788209: step 246740, loss = 0.0952, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 19:04:54.505506: step 246760, loss = 0.0950, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 19:04:59.073201: step 246780, loss = 0.0836, acc = 0.9800 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:05:03.708316: step 246800, loss = 0.0914, acc = 0.9780 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:05:08.446162: step 246820, loss = 0.0820, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 19:05:13.039820: step 246840, loss = 0.0817, acc = 0.9860 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 19:05:17.643799: step 246860, loss = 0.0903, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 19:05:22.455092: step 246880, loss = 0.0797, acc = 0.9820 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 19:05:26.999186: step 246900, loss = 0.0737, acc = 0.9860 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 19:05:31.665815: step 246920, loss = 0.0876, acc = 0.9800 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:05:36.317355: step 246940, loss = 0.0783, acc = 0.9820 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 19:05:42.156374: step 246960, loss = 0.0737, acc = 0.9820 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 19:05:46.804659: step 246980, loss = 0.0728, acc = 0.9800 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 19:05:51.380069: step 247000, loss = 0.0663, acc = 0.9900 (285.4 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 19:06:05.342651: step 247000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 19:06:15.272753: step 247000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 19:06:15.272840: step 247000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:06:19.865491: step 247020, loss = 0.0687, acc = 0.9840 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 19:06:24.567111: step 247040, loss = 0.1318, acc = 0.9580 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 19:06:29.138713: step 247060, loss = 0.0705, acc = 0.9800 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 19:06:33.666861: step 247080, loss = 0.0699, acc = 0.9820 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 19:06:38.397014: step 247100, loss = 0.0977, acc = 0.9720 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 19:06:42.962683: step 247120, loss = 0.1061, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 19:06:47.477280: step 247140, loss = 0.0835, acc = 0.9760 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 19:06:52.075496: step 247160, loss = 0.0727, acc = 0.9780 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 19:06:56.691832: step 247180, loss = 0.0680, acc = 0.9920 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 19:07:01.283283: step 247200, loss = 0.0766, acc = 0.9840 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 19:07:05.985197: step 247220, loss = 0.0771, acc = 0.9780 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 19:07:10.729849: step 247240, loss = 0.0849, acc = 0.9740 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 19:07:15.332719: step 247260, loss = 0.0703, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 19:07:19.919035: step 247280, loss = 0.0707, acc = 0.9880 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 19:07:24.658307: step 247300, loss = 0.0596, acc = 0.9920 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 19:07:29.368581: step 247320, loss = 0.0807, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 19:07:33.946938: step 247340, loss = 0.0902, acc = 0.9760 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 19:07:38.631551: step 247360, loss = 0.1018, acc = 0.9720 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 19:07:43.345902: step 247380, loss = 0.0979, acc = 0.9700 (255.9 examples/sec; 0.250 sec/batch)
2017-05-09 19:07:47.905729: step 247400, loss = 0.0636, acc = 0.9840 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 19:07:52.696042: step 247420, loss = 0.0907, acc = 0.9720 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 19:07:57.247673: step 247440, loss = 0.0999, acc = 0.9760 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 19:08:01.791602: step 247460, loss = 0.0815, acc = 0.9860 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:08:06.556345: step 247480, loss = 0.0809, acc = 0.9760 (238.9 examples/sec; 0.268 sec/batch)
2017-05-09 19:08:11.236576: step 247500, loss = 0.0829, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 19:08:15.796785: step 247520, loss = 0.0741, acc = 0.9800 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 19:08:20.473960: step 247540, loss = 0.1042, acc = 0.9720 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 19:08:24.971680: step 247560, loss = 0.0792, acc = 0.9780 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 19:08:29.569107: step 247580, loss = 0.0771, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 19:08:34.220924: step 247600, loss = 0.0735, acc = 0.9880 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:08:39.298720: step 247620, loss = 0.0664, acc = 0.9820 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 19:08:43.860650: step 247640, loss = 0.1097, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 19:08:48.423509: step 247660, loss = 0.0737, acc = 0.9860 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 19:08:53.106486: step 247680, loss = 0.0760, acc = 0.9780 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 19:08:57.711181: step 247700, loss = 0.0744, acc = 0.9840 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:09:02.363920: step 247720, loss = 0.0794, acc = 0.9820 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 19:09:07.288201: step 247740, loss = 0.0984, acc = 0.9640 (243.3 examples/sec; 0.263 sec/batch)
2017-05-09 19:09:11.884330: step 247760, loss = 0.0907, acc = 0.9700 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:09:16.378245: step 247780, loss = 0.0802, acc = 0.9780 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:09:21.136733: step 247800, loss = 0.0724, acc = 0.9820 (246.1 examples/sec; 0.260 sec/batch)
2017-05-09 19:09:25.852688: step 247820, loss = 0.0888, acc = 0.9740 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 19:09:30.456412: step 247840, loss = 0.0875, acc = 0.9760 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:09:35.195581: step 247860, loss = 0.0647, acc = 0.9880 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 19:09:39.860112: step 247880, loss = 0.1134, acc = 0.9780 (297.7 examples/sec; 0.215 sec/batch)
2017-05-09 19:09:44.379871: step 247900, loss = 0.0782, acc = 0.9780 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 19:09:48.948563: step 247920, loss = 0.1008, acc = 0.9760 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 19:09:53.722079: step 247940, loss = 0.0805, acc = 0.9740 (259.6 examples/sec; 0.247 sec/batch)
2017-05-09 19:09:58.162323: step 247960, loss = 0.0704, acc = 0.9820 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:10:03.566375: step 247980, loss = 0.0632, acc = 0.9860 (226.0 examples/sec; 0.283 sec/batch)
2017-05-09 19:10:08.097719: step 248000, loss = 0.0910, acc = 0.9720 (296.2 examples/sec; 0.216 sec/batch)
[Eval] 2017-05-09 19:10:21.967581: step 248000, acc = 0.9626, f1 = 0.9613
[Test] 2017-05-09 19:10:31.186636: step 248000, acc = 0.9527, f1 = 0.9524
[Status] 2017-05-09 19:10:31.186735: step 248000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:10:35.943284: step 248020, loss = 0.0889, acc = 0.9820 (228.4 examples/sec; 0.280 sec/batch)
2017-05-09 19:10:40.427223: step 248040, loss = 0.0716, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 19:10:45.001159: step 248060, loss = 0.0802, acc = 0.9780 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 19:10:49.806607: step 248080, loss = 0.0976, acc = 0.9740 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 19:10:54.759179: step 248100, loss = 0.0867, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 19:10:59.271346: step 248120, loss = 0.0659, acc = 0.9860 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 19:11:03.847133: step 248140, loss = 0.0960, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 19:11:08.692160: step 248160, loss = 0.0861, acc = 0.9800 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 19:11:13.352259: step 248180, loss = 0.0743, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 19:11:17.959688: step 248200, loss = 0.0580, acc = 0.9880 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 19:11:22.748488: step 248220, loss = 0.0959, acc = 0.9760 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 19:11:27.234558: step 248240, loss = 0.0713, acc = 0.9880 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 19:11:31.804311: step 248260, loss = 0.0962, acc = 0.9740 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 19:11:36.396279: step 248280, loss = 0.1207, acc = 0.9640 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 19:11:41.006520: step 248300, loss = 0.0755, acc = 0.9800 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 19:11:45.587804: step 248320, loss = 0.0870, acc = 0.9720 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 19:11:50.262957: step 248340, loss = 0.0781, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 19:11:54.857351: step 248360, loss = 0.0791, acc = 0.9840 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:11:59.502442: step 248380, loss = 0.0819, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 19:12:04.235917: step 248400, loss = 0.0624, acc = 0.9940 (241.3 examples/sec; 0.265 sec/batch)
2017-05-09 19:12:08.722852: step 248420, loss = 0.0667, acc = 0.9860 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 19:12:13.292076: step 248440, loss = 0.0746, acc = 0.9800 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 19:12:18.015688: step 248460, loss = 0.0768, acc = 0.9840 (258.6 examples/sec; 0.247 sec/batch)
2017-05-09 19:12:22.893347: step 248480, loss = 0.0808, acc = 0.9760 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 19:12:27.575042: step 248500, loss = 0.0836, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 19:12:32.125667: step 248520, loss = 0.0715, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 19:12:36.855811: step 248540, loss = 0.0718, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 19:12:41.309071: step 248560, loss = 0.0799, acc = 0.9820 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 19:12:45.951865: step 248580, loss = 0.0770, acc = 0.9720 (255.4 examples/sec; 0.251 sec/batch)
2017-05-09 19:12:50.487523: step 248600, loss = 0.0879, acc = 0.9760 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 19:12:55.648348: step 248620, loss = 0.0609, acc = 0.9900 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 19:13:00.195135: step 248640, loss = 0.0836, acc = 0.9800 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 19:13:04.890292: step 248660, loss = 0.0752, acc = 0.9800 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 19:13:09.463485: step 248680, loss = 0.0732, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 19:13:14.020577: step 248700, loss = 0.0926, acc = 0.9860 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 19:13:18.592449: step 248720, loss = 0.0740, acc = 0.9800 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:13:23.211290: step 248740, loss = 0.0867, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 19:13:27.776491: step 248760, loss = 0.0871, acc = 0.9800 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 19:13:32.321333: step 248780, loss = 0.0987, acc = 0.9700 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 19:13:37.005597: step 248800, loss = 0.0773, acc = 0.9860 (296.9 examples/sec; 0.216 sec/batch)
2017-05-09 19:13:41.567175: step 248820, loss = 0.0748, acc = 0.9780 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 19:13:46.127785: step 248840, loss = 0.0734, acc = 0.9800 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:13:50.934317: step 248860, loss = 0.0834, acc = 0.9720 (225.6 examples/sec; 0.284 sec/batch)
2017-05-09 19:13:55.532353: step 248880, loss = 0.0684, acc = 0.9880 (258.6 examples/sec; 0.247 sec/batch)
2017-05-09 19:14:00.014094: step 248900, loss = 0.0790, acc = 0.9780 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 19:14:04.608361: step 248920, loss = 0.0645, acc = 0.9840 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 19:14:09.370197: step 248940, loss = 0.0753, acc = 0.9780 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 19:14:13.887284: step 248960, loss = 0.0695, acc = 0.9920 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 19:14:19.496726: step 248980, loss = 0.0726, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:14:24.289957: step 249000, loss = 0.0705, acc = 0.9820 (266.6 examples/sec; 0.240 sec/batch)
[Eval] 2017-05-09 19:14:38.343483: step 249000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 19:14:47.689050: step 249000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 19:14:47.689142: step 249000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:14:52.317047: step 249020, loss = 0.0874, acc = 0.9720 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 19:14:56.888422: step 249040, loss = 0.0845, acc = 0.9800 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:15:01.533804: step 249060, loss = 0.0779, acc = 0.9780 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 19:15:06.297668: step 249080, loss = 0.0638, acc = 0.9860 (246.6 examples/sec; 0.260 sec/batch)
2017-05-09 19:15:10.841353: step 249100, loss = 0.0630, acc = 0.9900 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:15:15.333409: step 249120, loss = 0.0722, acc = 0.9840 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 19:15:19.895009: step 249140, loss = 0.0870, acc = 0.9760 (301.1 examples/sec; 0.213 sec/batch)
2017-05-09 19:15:24.442202: step 249160, loss = 0.0739, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 19:15:29.034168: step 249180, loss = 0.0778, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 19:15:33.667030: step 249200, loss = 0.1023, acc = 0.9680 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 19:15:38.505813: step 249220, loss = 0.0729, acc = 0.9820 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 19:15:43.219298: step 249240, loss = 0.0711, acc = 0.9880 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 19:15:47.821099: step 249260, loss = 0.0696, acc = 0.9880 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 19:15:52.647176: step 249280, loss = 0.0805, acc = 0.9800 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 19:15:57.147228: step 249300, loss = 0.0837, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 19:16:01.823360: step 249320, loss = 0.0716, acc = 0.9780 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 19:16:06.527935: step 249340, loss = 0.0780, acc = 0.9820 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 19:16:11.036492: step 249360, loss = 0.0922, acc = 0.9780 (300.1 examples/sec; 0.213 sec/batch)
2017-05-09 19:16:15.623025: step 249380, loss = 0.0770, acc = 0.9820 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:16:20.299641: step 249400, loss = 0.0736, acc = 0.9900 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:16:24.966471: step 249420, loss = 0.0940, acc = 0.9800 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 19:16:29.562035: step 249440, loss = 0.0810, acc = 0.9760 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 19:16:34.392984: step 249460, loss = 0.0843, acc = 0.9760 (298.9 examples/sec; 0.214 sec/batch)
2017-05-09 19:16:39.151355: step 249480, loss = 0.1013, acc = 0.9760 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 19:16:43.766239: step 249500, loss = 0.0783, acc = 0.9760 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 19:16:48.363823: step 249520, loss = 0.0683, acc = 0.9840 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 19:16:53.203211: step 249540, loss = 0.0966, acc = 0.9640 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 19:16:57.719231: step 249560, loss = 0.0763, acc = 0.9780 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 19:17:02.380271: step 249580, loss = 0.0742, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 19:17:07.117855: step 249600, loss = 0.0728, acc = 0.9820 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:17:11.751002: step 249620, loss = 0.0972, acc = 0.9760 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 19:17:16.291658: step 249640, loss = 0.0747, acc = 0.9840 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 19:17:20.880294: step 249660, loss = 0.0784, acc = 0.9820 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 19:17:25.554546: step 249680, loss = 0.1097, acc = 0.9680 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 19:17:30.138963: step 249700, loss = 0.0839, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 19:17:34.956391: step 249720, loss = 0.0961, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 19:17:39.909209: step 249740, loss = 0.0747, acc = 0.9820 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 19:17:44.539904: step 249760, loss = 0.0820, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 19:17:49.089593: step 249780, loss = 0.0870, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 19:17:53.727696: step 249800, loss = 0.0834, acc = 0.9780 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 19:17:58.244109: step 249820, loss = 0.0705, acc = 0.9820 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 19:18:02.840118: step 249840, loss = 0.0876, acc = 0.9780 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 19:18:07.661495: step 249860, loss = 0.0847, acc = 0.9820 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 19:18:12.288260: step 249880, loss = 0.0664, acc = 0.9800 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 19:18:16.918432: step 249900, loss = 0.0875, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 19:18:21.593752: step 249920, loss = 0.0718, acc = 0.9880 (259.6 examples/sec; 0.247 sec/batch)
2017-05-09 19:18:26.223477: step 249940, loss = 0.0883, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:18:30.919915: step 249960, loss = 0.0973, acc = 0.9760 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 19:18:36.192138: step 249980, loss = 0.0771, acc = 0.9820 (162.4 examples/sec; 0.394 sec/batch)
2017-05-09 19:18:40.896243: step 250000, loss = 0.1013, acc = 0.9760 (301.9 examples/sec; 0.212 sec/batch)
[Eval] 2017-05-09 19:18:54.939032: step 250000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 19:19:04.707960: step 250000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 19:19:04.708020: step 250000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:19:09.313008: step 250020, loss = 0.0711, acc = 0.9840 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 19:19:13.953926: step 250040, loss = 0.0855, acc = 0.9820 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 19:19:18.478138: step 250060, loss = 0.0914, acc = 0.9680 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 19:19:23.410685: step 250080, loss = 0.0939, acc = 0.9800 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 19:19:27.933802: step 250100, loss = 0.0668, acc = 0.9820 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 19:19:32.415250: step 250120, loss = 0.0817, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 19:19:37.212977: step 250140, loss = 0.0816, acc = 0.9780 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 19:19:41.769037: step 250160, loss = 0.0884, acc = 0.9820 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 19:19:46.597742: step 250180, loss = 0.1008, acc = 0.9640 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 19:19:51.342397: step 250200, loss = 0.0762, acc = 0.9800 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 19:19:55.872699: step 250220, loss = 0.0692, acc = 0.9860 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 19:20:00.385416: step 250240, loss = 0.0722, acc = 0.9940 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 19:20:05.078224: step 250260, loss = 0.0844, acc = 0.9820 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 19:20:09.642437: step 250280, loss = 0.0865, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 19:20:14.227613: step 250300, loss = 0.0809, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 19:20:18.985674: step 250320, loss = 0.0696, acc = 0.9880 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 19:20:23.726995: step 250340, loss = 0.0799, acc = 0.9840 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 19:20:28.479233: step 250360, loss = 0.0748, acc = 0.9760 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 19:20:33.037926: step 250380, loss = 0.0539, acc = 0.9920 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 19:20:37.779821: step 250400, loss = 0.0789, acc = 0.9740 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 19:20:42.404922: step 250420, loss = 0.0762, acc = 0.9840 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 19:20:47.053144: step 250440, loss = 0.0895, acc = 0.9740 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 19:20:51.716277: step 250460, loss = 0.0745, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 19:20:56.312605: step 250480, loss = 0.0706, acc = 0.9860 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 19:21:00.834611: step 250500, loss = 0.0850, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 19:21:05.468931: step 250520, loss = 0.0886, acc = 0.9740 (240.2 examples/sec; 0.266 sec/batch)
2017-05-09 19:21:10.000759: step 250540, loss = 0.0913, acc = 0.9700 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 19:21:14.762126: step 250560, loss = 0.0889, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:21:19.887117: step 250580, loss = 0.0938, acc = 0.9660 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 19:21:24.408942: step 250600, loss = 0.0789, acc = 0.9780 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 19:21:28.904835: step 250620, loss = 0.0766, acc = 0.9800 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 19:21:33.458199: step 250640, loss = 0.0702, acc = 0.9840 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:21:38.186713: step 250660, loss = 0.0799, acc = 0.9880 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 19:21:42.813923: step 250680, loss = 0.0887, acc = 0.9740 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:21:47.442793: step 250700, loss = 0.0747, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:21:52.165338: step 250720, loss = 0.0747, acc = 0.9740 (257.6 examples/sec; 0.248 sec/batch)
2017-05-09 19:21:56.706449: step 250740, loss = 0.0612, acc = 0.9940 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 19:22:01.471787: step 250760, loss = 0.0917, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 19:22:06.154543: step 250780, loss = 0.0873, acc = 0.9700 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 19:22:10.622040: step 250800, loss = 0.0942, acc = 0.9720 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:22:15.170772: step 250820, loss = 0.0694, acc = 0.9920 (263.9 examples/sec; 0.242 sec/batch)
2017-05-09 19:22:20.072593: step 250840, loss = 0.0911, acc = 0.9740 (224.2 examples/sec; 0.286 sec/batch)
2017-05-09 19:22:24.645235: step 250860, loss = 0.0692, acc = 0.9820 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 19:22:29.152086: step 250880, loss = 0.0955, acc = 0.9760 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 19:22:33.724207: step 250900, loss = 0.0961, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 19:22:38.405879: step 250920, loss = 0.1111, acc = 0.9740 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 19:22:43.061895: step 250940, loss = 0.0997, acc = 0.9660 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 19:22:47.679910: step 250960, loss = 0.0896, acc = 0.9700 (260.1 examples/sec; 0.246 sec/batch)
2017-05-09 19:22:52.463911: step 250980, loss = 0.0675, acc = 0.9900 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 19:22:58.352743: step 251000, loss = 0.0787, acc = 0.9780 (279.6 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 19:23:12.321542: step 251000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 19:23:21.872426: step 251000, acc = 0.9564, f1 = 0.9560
[Status] 2017-05-09 19:23:21.872507: step 251000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:23:26.301377: step 251020, loss = 0.0663, acc = 0.9880 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 19:23:30.719969: step 251040, loss = 0.0942, acc = 0.9780 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 19:23:35.424594: step 251060, loss = 0.0627, acc = 0.9920 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 19:23:40.035177: step 251080, loss = 0.0809, acc = 0.9760 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 19:23:44.684078: step 251100, loss = 0.0785, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 19:23:49.459547: step 251120, loss = 0.0706, acc = 0.9800 (246.9 examples/sec; 0.259 sec/batch)
2017-05-09 19:23:53.912199: step 251140, loss = 0.0830, acc = 0.9780 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 19:23:58.534642: step 251160, loss = 0.0815, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 19:24:03.314737: step 251180, loss = 0.0791, acc = 0.9820 (234.6 examples/sec; 0.273 sec/batch)
2017-05-09 19:24:07.980221: step 251200, loss = 0.0851, acc = 0.9780 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:24:12.555390: step 251220, loss = 0.0684, acc = 0.9860 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 19:24:17.124726: step 251240, loss = 0.0853, acc = 0.9800 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 19:24:21.809217: step 251260, loss = 0.0820, acc = 0.9760 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 19:24:26.322298: step 251280, loss = 0.0781, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 19:24:30.802436: step 251300, loss = 0.0796, acc = 0.9800 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 19:24:35.628119: step 251320, loss = 0.0810, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 19:24:40.223639: step 251340, loss = 0.0724, acc = 0.9840 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 19:24:45.041317: step 251360, loss = 0.0874, acc = 0.9740 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 19:24:49.838829: step 251380, loss = 0.0914, acc = 0.9720 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 19:24:54.417782: step 251400, loss = 0.0868, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 19:24:58.930801: step 251420, loss = 0.0614, acc = 0.9880 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 19:25:03.751752: step 251440, loss = 0.0658, acc = 0.9860 (225.9 examples/sec; 0.283 sec/batch)
2017-05-09 19:25:08.184608: step 251460, loss = 0.0642, acc = 0.9840 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:25:12.940687: step 251480, loss = 0.0838, acc = 0.9800 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 19:25:17.697370: step 251500, loss = 0.0818, acc = 0.9840 (219.6 examples/sec; 0.291 sec/batch)
2017-05-09 19:25:22.211237: step 251520, loss = 0.0785, acc = 0.9800 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 19:25:26.747521: step 251540, loss = 0.0786, acc = 0.9840 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 19:25:31.399283: step 251560, loss = 0.0717, acc = 0.9820 (252.1 examples/sec; 0.254 sec/batch)
2017-05-09 19:25:35.986898: step 251580, loss = 0.0925, acc = 0.9800 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:25:40.529290: step 251600, loss = 0.1053, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 19:25:45.099721: step 251620, loss = 0.0969, acc = 0.9680 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 19:25:49.922851: step 251640, loss = 0.0720, acc = 0.9880 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 19:25:54.583668: step 251660, loss = 0.1017, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 19:25:59.120366: step 251680, loss = 0.0714, acc = 0.9880 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 19:26:03.883581: step 251700, loss = 0.0868, acc = 0.9760 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 19:26:08.446312: step 251720, loss = 0.1078, acc = 0.9640 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 19:26:12.988811: step 251740, loss = 0.0752, acc = 0.9900 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 19:26:17.662992: step 251760, loss = 0.0762, acc = 0.9800 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 19:26:22.173554: step 251780, loss = 0.0796, acc = 0.9780 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 19:26:26.855044: step 251800, loss = 0.0878, acc = 0.9740 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 19:26:31.749406: step 251820, loss = 0.0770, acc = 0.9840 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 19:26:36.295195: step 251840, loss = 0.0733, acc = 0.9860 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:26:40.813886: step 251860, loss = 0.0818, acc = 0.9800 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 19:26:45.591750: step 251880, loss = 0.0870, acc = 0.9700 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 19:26:50.206613: step 251900, loss = 0.0761, acc = 0.9780 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 19:26:54.818191: step 251920, loss = 0.0776, acc = 0.9840 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 19:26:59.484352: step 251940, loss = 0.0664, acc = 0.9900 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 19:27:04.211352: step 251960, loss = 0.0750, acc = 0.9820 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:27:08.749962: step 251980, loss = 0.0889, acc = 0.9800 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 19:27:13.969017: step 252000, loss = 0.0606, acc = 0.9900 (293.1 examples/sec; 0.218 sec/batch)
[Eval] 2017-05-09 19:27:27.914105: step 252000, acc = 0.9643, f1 = 0.9632
[Test] 2017-05-09 19:27:37.640329: step 252000, acc = 0.9560, f1 = 0.9557
[Status] 2017-05-09 19:27:37.640400: step 252000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:27:42.154654: step 252020, loss = 0.1028, acc = 0.9680 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 19:27:47.286002: step 252040, loss = 0.0724, acc = 0.9900 (240.5 examples/sec; 0.266 sec/batch)
2017-05-09 19:27:51.838371: step 252060, loss = 0.0786, acc = 0.9860 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 19:27:56.433821: step 252080, loss = 0.0707, acc = 0.9780 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 19:28:01.165336: step 252100, loss = 0.0730, acc = 0.9840 (212.0 examples/sec; 0.302 sec/batch)
2017-05-09 19:28:05.721905: step 252120, loss = 0.0738, acc = 0.9880 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 19:28:10.474486: step 252140, loss = 0.0790, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 19:28:15.091774: step 252160, loss = 0.0841, acc = 0.9740 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 19:28:19.666873: step 252180, loss = 0.0759, acc = 0.9820 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 19:28:24.224188: step 252200, loss = 0.0700, acc = 0.9860 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 19:28:28.891910: step 252220, loss = 0.0880, acc = 0.9760 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 19:28:33.592564: step 252240, loss = 0.0507, acc = 0.9960 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 19:28:38.160446: step 252260, loss = 0.0961, acc = 0.9760 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:28:42.829504: step 252280, loss = 0.1019, acc = 0.9740 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 19:28:47.608920: step 252300, loss = 0.0938, acc = 0.9760 (238.7 examples/sec; 0.268 sec/batch)
2017-05-09 19:28:52.184983: step 252320, loss = 0.0736, acc = 0.9860 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 19:28:56.726429: step 252340, loss = 0.0792, acc = 0.9800 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 19:29:01.332659: step 252360, loss = 0.0833, acc = 0.9740 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 19:29:05.909515: step 252380, loss = 0.0720, acc = 0.9900 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 19:29:10.487702: step 252400, loss = 0.0848, acc = 0.9760 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 19:29:15.036144: step 252420, loss = 0.0723, acc = 0.9820 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 19:29:19.860906: step 252440, loss = 0.0719, acc = 0.9840 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 19:29:24.396982: step 252460, loss = 0.0900, acc = 0.9840 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 19:29:28.936818: step 252480, loss = 0.0974, acc = 0.9700 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:29:33.667111: step 252500, loss = 0.1002, acc = 0.9680 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 19:29:38.222888: step 252520, loss = 0.0707, acc = 0.9840 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 19:29:43.016168: step 252540, loss = 0.0699, acc = 0.9860 (243.8 examples/sec; 0.263 sec/batch)
2017-05-09 19:29:47.968646: step 252560, loss = 0.0680, acc = 0.9840 (263.2 examples/sec; 0.243 sec/batch)
2017-05-09 19:29:52.467396: step 252580, loss = 0.0686, acc = 0.9880 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 19:29:57.068863: step 252600, loss = 0.0785, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 19:30:01.730641: step 252620, loss = 0.0746, acc = 0.9840 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 19:30:06.433459: step 252640, loss = 0.0820, acc = 0.9820 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 19:30:11.169758: step 252660, loss = 0.0920, acc = 0.9780 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:30:15.828717: step 252680, loss = 0.0706, acc = 0.9860 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:30:20.354289: step 252700, loss = 0.0759, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 19:30:24.894422: step 252720, loss = 0.1081, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 19:30:29.560842: step 252740, loss = 0.1012, acc = 0.9760 (239.8 examples/sec; 0.267 sec/batch)
2017-05-09 19:30:34.458162: step 252760, loss = 0.1067, acc = 0.9680 (288.9 examples/sec; 0.221 sec/batch)
2017-05-09 19:30:38.900190: step 252780, loss = 0.0876, acc = 0.9780 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 19:30:43.565334: step 252800, loss = 0.0788, acc = 0.9760 (240.3 examples/sec; 0.266 sec/batch)
2017-05-09 19:30:48.185906: step 252820, loss = 0.0866, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 19:30:52.825873: step 252840, loss = 0.0942, acc = 0.9740 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 19:30:57.433363: step 252860, loss = 0.0770, acc = 0.9880 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 19:31:02.241171: step 252880, loss = 0.0951, acc = 0.9780 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:31:06.903799: step 252900, loss = 0.0818, acc = 0.9760 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 19:31:11.567218: step 252920, loss = 0.0654, acc = 0.9840 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 19:31:16.325865: step 252940, loss = 0.0844, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 19:31:20.952540: step 252960, loss = 0.0981, acc = 0.9640 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 19:31:25.611919: step 252980, loss = 0.0696, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 19:31:30.242119: step 253000, loss = 0.0840, acc = 0.9660 (286.4 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 19:31:44.319915: step 253000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 19:31:53.614477: step 253000, acc = 0.9556, f1 = 0.9553
[Status] 2017-05-09 19:31:53.614559: step 253000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:31:59.259465: step 253020, loss = 0.0915, acc = 0.9740 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 19:32:03.836192: step 253040, loss = 0.0911, acc = 0.9760 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 19:32:08.514714: step 253060, loss = 0.0779, acc = 0.9780 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 19:32:13.071801: step 253080, loss = 0.0887, acc = 0.9760 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 19:32:17.641148: step 253100, loss = 0.0833, acc = 0.9820 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 19:32:22.174283: step 253120, loss = 0.0676, acc = 0.9840 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 19:32:26.818961: step 253140, loss = 0.0702, acc = 0.9820 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 19:32:31.410036: step 253160, loss = 0.0879, acc = 0.9720 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 19:32:36.028079: step 253180, loss = 0.0879, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 19:32:40.684451: step 253200, loss = 0.0666, acc = 0.9840 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 19:32:45.453059: step 253220, loss = 0.0714, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 19:32:50.066252: step 253240, loss = 0.0797, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 19:32:54.727918: step 253260, loss = 0.0674, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 19:32:59.531034: step 253280, loss = 0.0908, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 19:33:04.076422: step 253300, loss = 0.0818, acc = 0.9760 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:33:08.654896: step 253320, loss = 0.0716, acc = 0.9840 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 19:33:13.259319: step 253340, loss = 0.0747, acc = 0.9780 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 19:33:17.823887: step 253360, loss = 0.0894, acc = 0.9800 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:33:22.306989: step 253380, loss = 0.0750, acc = 0.9860 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 19:33:27.018067: step 253400, loss = 0.0874, acc = 0.9740 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 19:33:31.613426: step 253420, loss = 0.0878, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 19:33:36.228423: step 253440, loss = 0.0918, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:33:40.931453: step 253460, loss = 0.0838, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 19:33:45.627735: step 253480, loss = 0.0790, acc = 0.9700 (256.7 examples/sec; 0.249 sec/batch)
2017-05-09 19:33:50.156180: step 253500, loss = 0.0771, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 19:33:54.738504: step 253520, loss = 0.0739, acc = 0.9800 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 19:33:59.373454: step 253540, loss = 0.0755, acc = 0.9840 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 19:34:03.974394: step 253560, loss = 0.0774, acc = 0.9780 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 19:34:08.708795: step 253580, loss = 0.0851, acc = 0.9800 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 19:34:13.369790: step 253600, loss = 0.0702, acc = 0.9780 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 19:34:17.986280: step 253620, loss = 0.0691, acc = 0.9900 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 19:34:22.455070: step 253640, loss = 0.1001, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 19:34:27.100237: step 253660, loss = 0.0572, acc = 0.9900 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:34:31.568568: step 253680, loss = 0.0920, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 19:34:36.104981: step 253700, loss = 0.0896, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 19:34:40.897022: step 253720, loss = 0.1035, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 19:34:45.484429: step 253740, loss = 0.0870, acc = 0.9760 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 19:34:50.061390: step 253760, loss = 0.0805, acc = 0.9840 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 19:34:54.849598: step 253780, loss = 0.0882, acc = 0.9760 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 19:34:59.406651: step 253800, loss = 0.0726, acc = 0.9820 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 19:35:03.894057: step 253820, loss = 0.0896, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 19:35:08.557112: step 253840, loss = 0.0733, acc = 0.9860 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 19:35:13.205202: step 253860, loss = 0.0764, acc = 0.9780 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 19:35:17.794787: step 253880, loss = 0.0742, acc = 0.9860 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 19:35:22.398887: step 253900, loss = 0.0757, acc = 0.9860 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:35:27.118708: step 253920, loss = 0.0815, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 19:35:31.779685: step 253940, loss = 0.0918, acc = 0.9780 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 19:35:36.368221: step 253960, loss = 0.0738, acc = 0.9840 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 19:35:41.025368: step 253980, loss = 0.0836, acc = 0.9760 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 19:35:45.545630: step 254000, loss = 0.0835, acc = 0.9880 (285.1 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 19:35:59.661597: step 254000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 19:36:09.543121: step 254000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 19:36:09.543202: step 254000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:36:14.860623: step 254020, loss = 0.0648, acc = 0.9860 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:36:19.542378: step 254040, loss = 0.0611, acc = 0.9860 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 19:36:24.349545: step 254060, loss = 0.0933, acc = 0.9700 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:36:28.977131: step 254080, loss = 0.0935, acc = 0.9720 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 19:36:33.596203: step 254100, loss = 0.0984, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:36:38.316794: step 254120, loss = 0.0746, acc = 0.9760 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 19:36:42.986747: step 254140, loss = 0.0684, acc = 0.9860 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 19:36:47.693380: step 254160, loss = 0.0874, acc = 0.9780 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 19:36:52.465806: step 254180, loss = 0.0795, acc = 0.9780 (237.5 examples/sec; 0.270 sec/batch)
2017-05-09 19:36:56.983526: step 254200, loss = 0.0836, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 19:37:01.667657: step 254220, loss = 0.0941, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 19:37:06.131213: step 254240, loss = 0.0744, acc = 0.9880 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:37:10.885925: step 254260, loss = 0.0562, acc = 0.9940 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 19:37:15.532570: step 254280, loss = 0.0744, acc = 0.9800 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 19:37:20.109783: step 254300, loss = 0.0811, acc = 0.9820 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 19:37:24.918133: step 254320, loss = 0.0918, acc = 0.9740 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 19:37:29.486115: step 254340, loss = 0.0851, acc = 0.9720 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 19:37:34.051039: step 254360, loss = 0.0787, acc = 0.9780 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 19:37:38.661673: step 254380, loss = 0.0729, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 19:37:43.218668: step 254400, loss = 0.0916, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 19:37:47.836144: step 254420, loss = 0.0770, acc = 0.9800 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 19:37:52.533643: step 254440, loss = 0.0851, acc = 0.9760 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 19:37:57.126835: step 254460, loss = 0.0699, acc = 0.9920 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 19:38:01.774329: step 254480, loss = 0.0736, acc = 0.9860 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 19:38:06.629236: step 254500, loss = 0.0941, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 19:38:11.288231: step 254520, loss = 0.0792, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 19:38:15.771412: step 254540, loss = 0.0622, acc = 0.9940 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 19:38:20.523390: step 254560, loss = 0.0861, acc = 0.9660 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:38:25.127131: step 254580, loss = 0.0923, acc = 0.9740 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 19:38:29.965651: step 254600, loss = 0.0779, acc = 0.9860 (245.7 examples/sec; 0.260 sec/batch)
2017-05-09 19:38:34.634645: step 254620, loss = 0.0821, acc = 0.9740 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 19:38:39.306953: step 254640, loss = 0.0882, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 19:38:43.965413: step 254660, loss = 0.0691, acc = 0.9880 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 19:38:48.480348: step 254680, loss = 0.0789, acc = 0.9780 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 19:38:53.266625: step 254700, loss = 0.0779, acc = 0.9860 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 19:38:57.998963: step 254720, loss = 0.1219, acc = 0.9580 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 19:39:02.600623: step 254740, loss = 0.0626, acc = 0.9880 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 19:39:07.495852: step 254760, loss = 0.0770, acc = 0.9700 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 19:39:12.116080: step 254780, loss = 0.1000, acc = 0.9600 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:39:16.735086: step 254800, loss = 0.0810, acc = 0.9800 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 19:39:21.578462: step 254820, loss = 0.1107, acc = 0.9640 (227.3 examples/sec; 0.282 sec/batch)
2017-05-09 19:39:26.208661: step 254840, loss = 0.0956, acc = 0.9820 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 19:39:30.899441: step 254860, loss = 0.1141, acc = 0.9600 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 19:39:35.330948: step 254880, loss = 0.0755, acc = 0.9800 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 19:39:40.115898: step 254900, loss = 0.1170, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 19:39:44.801409: step 254920, loss = 0.0850, acc = 0.9760 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 19:39:49.348081: step 254940, loss = 0.0664, acc = 0.9860 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 19:39:54.241583: step 254960, loss = 0.0854, acc = 0.9820 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:39:58.863283: step 254980, loss = 0.1020, acc = 0.9620 (276.5 examples/sec; 0.232 sec/batch)
2017-05-09 19:40:03.580085: step 255000, loss = 0.0813, acc = 0.9760 (270.7 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 19:40:17.633291: step 255000, acc = 0.9598, f1 = 0.9583
[Test] 2017-05-09 19:40:27.567038: step 255000, acc = 0.9486, f1 = 0.9481
[Status] 2017-05-09 19:40:27.567160: step 255000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:40:33.357105: step 255020, loss = 0.1033, acc = 0.9660 (130.4 examples/sec; 0.491 sec/batch)
2017-05-09 19:40:38.094929: step 255040, loss = 0.0687, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 19:40:42.767900: step 255060, loss = 0.0763, acc = 0.9860 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 19:40:47.469958: step 255080, loss = 0.0708, acc = 0.9820 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 19:40:52.191699: step 255100, loss = 0.0710, acc = 0.9860 (229.5 examples/sec; 0.279 sec/batch)
2017-05-09 19:40:56.791712: step 255120, loss = 0.0787, acc = 0.9780 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 19:41:01.432983: step 255140, loss = 0.0843, acc = 0.9860 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 19:41:05.967550: step 255160, loss = 0.1032, acc = 0.9800 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 19:41:10.467753: step 255180, loss = 0.0916, acc = 0.9700 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 19:41:15.245228: step 255200, loss = 0.0691, acc = 0.9780 (254.1 examples/sec; 0.252 sec/batch)
2017-05-09 19:41:19.779167: step 255220, loss = 0.0809, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:41:24.501215: step 255240, loss = 0.0637, acc = 0.9880 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 19:41:29.118952: step 255260, loss = 0.0798, acc = 0.9880 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 19:41:33.702038: step 255280, loss = 0.0716, acc = 0.9900 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 19:41:38.604630: step 255300, loss = 0.0756, acc = 0.9840 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:41:43.161029: step 255320, loss = 0.0772, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 19:41:47.759272: step 255340, loss = 0.0865, acc = 0.9760 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 19:41:52.379582: step 255360, loss = 0.0592, acc = 0.9900 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 19:41:56.956584: step 255380, loss = 0.0707, acc = 0.9840 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 19:42:01.481404: step 255400, loss = 0.0871, acc = 0.9840 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 19:42:06.355460: step 255420, loss = 0.0887, acc = 0.9780 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 19:42:11.330839: step 255440, loss = 0.0973, acc = 0.9720 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 19:42:15.816351: step 255460, loss = 0.0710, acc = 0.9840 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 19:42:20.368120: step 255480, loss = 0.0746, acc = 0.9840 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 19:42:24.996888: step 255500, loss = 0.1133, acc = 0.9700 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 19:42:29.661513: step 255520, loss = 0.0680, acc = 0.9860 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 19:42:34.286225: step 255540, loss = 0.0924, acc = 0.9740 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 19:42:39.031657: step 255560, loss = 0.0954, acc = 0.9720 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 19:42:43.559989: step 255580, loss = 0.1253, acc = 0.9660 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 19:42:48.192182: step 255600, loss = 0.0824, acc = 0.9800 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 19:42:52.862511: step 255620, loss = 0.0747, acc = 0.9780 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 19:42:57.477810: step 255640, loss = 0.0754, acc = 0.9800 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 19:43:02.114395: step 255660, loss = 0.0787, acc = 0.9820 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 19:43:06.939044: step 255680, loss = 0.0753, acc = 0.9860 (232.7 examples/sec; 0.275 sec/batch)
2017-05-09 19:43:11.612954: step 255700, loss = 0.0871, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:43:16.145746: step 255720, loss = 0.0732, acc = 0.9820 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 19:43:20.705436: step 255740, loss = 0.0788, acc = 0.9780 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 19:43:25.505496: step 255760, loss = 0.0909, acc = 0.9720 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 19:43:30.098224: step 255780, loss = 0.0951, acc = 0.9660 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 19:43:34.690636: step 255800, loss = 0.0733, acc = 0.9840 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:43:39.394012: step 255820, loss = 0.0805, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 19:43:44.051816: step 255840, loss = 0.0816, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 19:43:48.697014: step 255860, loss = 0.1183, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:43:53.384100: step 255880, loss = 0.0736, acc = 0.9860 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 19:43:57.974783: step 255900, loss = 0.0575, acc = 0.9900 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 19:44:02.577791: step 255920, loss = 0.0846, acc = 0.9820 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 19:44:07.297237: step 255940, loss = 0.0734, acc = 0.9820 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 19:44:11.799284: step 255960, loss = 0.0688, acc = 0.9820 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 19:44:16.337588: step 255980, loss = 0.0844, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 19:44:21.224749: step 256000, loss = 0.0911, acc = 0.9760 (207.2 examples/sec; 0.309 sec/batch)
[Eval] 2017-05-09 19:44:35.024915: step 256000, acc = 0.9629, f1 = 0.9617
[Test] 2017-05-09 19:44:44.549129: step 256000, acc = 0.9548, f1 = 0.9544
[Status] 2017-05-09 19:44:44.549213: step 256000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:44:49.156807: step 256020, loss = 0.0919, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:44:54.858608: step 256040, loss = 0.0902, acc = 0.9780 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 19:44:59.453061: step 256060, loss = 0.0878, acc = 0.9820 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 19:45:04.012861: step 256080, loss = 0.0771, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 19:45:08.782021: step 256100, loss = 0.0794, acc = 0.9820 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 19:45:13.345804: step 256120, loss = 0.0747, acc = 0.9800 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 19:45:17.838868: step 256140, loss = 0.0670, acc = 0.9880 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 19:45:22.750069: step 256160, loss = 0.0859, acc = 0.9700 (252.6 examples/sec; 0.253 sec/batch)
2017-05-09 19:45:27.359238: step 256180, loss = 0.0777, acc = 0.9840 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 19:45:31.836538: step 256200, loss = 0.0696, acc = 0.9820 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 19:45:36.542904: step 256220, loss = 0.0834, acc = 0.9820 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 19:45:41.213442: step 256240, loss = 0.0887, acc = 0.9800 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 19:45:46.370122: step 256260, loss = 0.0707, acc = 0.9840 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 19:45:50.961856: step 256280, loss = 0.0974, acc = 0.9680 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 19:45:55.610601: step 256300, loss = 0.1020, acc = 0.9700 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 19:46:00.144164: step 256320, loss = 0.0874, acc = 0.9800 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 19:46:04.649389: step 256340, loss = 0.0748, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 19:46:09.387665: step 256360, loss = 0.0787, acc = 0.9780 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 19:46:13.980101: step 256380, loss = 0.0804, acc = 0.9840 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 19:46:18.670996: step 256400, loss = 0.0705, acc = 0.9880 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 19:46:23.370836: step 256420, loss = 0.0722, acc = 0.9880 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 19:46:27.971694: step 256440, loss = 0.0859, acc = 0.9860 (276.5 examples/sec; 0.232 sec/batch)
2017-05-09 19:46:32.486104: step 256460, loss = 0.0803, acc = 0.9820 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 19:46:37.125515: step 256480, loss = 0.0723, acc = 0.9820 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:46:41.691937: step 256500, loss = 0.0697, acc = 0.9840 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 19:46:46.318196: step 256520, loss = 0.0918, acc = 0.9800 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:46:51.161530: step 256540, loss = 0.0787, acc = 0.9800 (298.2 examples/sec; 0.215 sec/batch)
2017-05-09 19:46:55.792009: step 256560, loss = 0.0696, acc = 0.9840 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 19:47:00.299527: step 256580, loss = 0.0679, acc = 0.9900 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 19:47:04.918250: step 256600, loss = 0.0787, acc = 0.9740 (251.8 examples/sec; 0.254 sec/batch)
2017-05-09 19:47:09.599961: step 256620, loss = 0.0985, acc = 0.9720 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 19:47:14.204774: step 256640, loss = 0.0733, acc = 0.9820 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 19:47:18.739887: step 256660, loss = 0.0576, acc = 0.9900 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 19:47:23.434775: step 256680, loss = 0.0852, acc = 0.9780 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 19:47:27.864330: step 256700, loss = 0.0639, acc = 0.9840 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 19:47:32.415441: step 256720, loss = 0.0718, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 19:47:37.129768: step 256740, loss = 0.0794, acc = 0.9760 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 19:47:41.839267: step 256760, loss = 0.0769, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 19:47:46.392577: step 256780, loss = 0.0790, acc = 0.9760 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 19:47:51.189177: step 256800, loss = 0.0761, acc = 0.9760 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 19:47:55.746666: step 256820, loss = 0.0778, acc = 0.9900 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 19:48:00.411686: step 256840, loss = 0.0865, acc = 0.9760 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 19:48:05.179769: step 256860, loss = 0.0669, acc = 0.9800 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 19:48:09.739916: step 256880, loss = 0.0827, acc = 0.9820 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:48:14.297837: step 256900, loss = 0.0685, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 19:48:18.857991: step 256920, loss = 0.0932, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:48:23.591120: step 256940, loss = 0.0836, acc = 0.9860 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 19:48:28.168707: step 256960, loss = 0.0775, acc = 0.9800 (278.9 examples/sec; 0.230 sec/batch)
2017-05-09 19:48:32.817044: step 256980, loss = 0.0629, acc = 0.9780 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 19:48:37.565182: step 257000, loss = 0.0984, acc = 0.9740 (274.6 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 19:48:51.587058: step 257000, acc = 0.9644, f1 = 0.9633
[Test] 2017-05-09 19:49:00.922725: step 257000, acc = 0.9557, f1 = 0.9553
[Status] 2017-05-09 19:49:00.922828: step 257000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:49:05.755985: step 257020, loss = 0.0726, acc = 0.9820 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 19:49:11.517281: step 257040, loss = 0.0846, acc = 0.9840 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 19:49:16.356820: step 257060, loss = 0.0659, acc = 0.9880 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 19:49:21.178466: step 257080, loss = 0.0721, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 19:49:25.765802: step 257100, loss = 0.0758, acc = 0.9780 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 19:49:30.233746: step 257120, loss = 0.0899, acc = 0.9680 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 19:49:35.024451: step 257140, loss = 0.0969, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 19:49:39.607530: step 257160, loss = 0.0909, acc = 0.9740 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 19:49:44.545042: step 257180, loss = 0.0791, acc = 0.9800 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 19:49:49.334190: step 257200, loss = 0.0887, acc = 0.9680 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 19:49:53.835420: step 257220, loss = 0.0719, acc = 0.9860 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 19:49:58.414949: step 257240, loss = 0.0784, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 19:50:03.049995: step 257260, loss = 0.0693, acc = 0.9780 (302.5 examples/sec; 0.212 sec/batch)
2017-05-09 19:50:07.471174: step 257280, loss = 0.0765, acc = 0.9940 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:50:11.993188: step 257300, loss = 0.0707, acc = 0.9840 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 19:50:16.800953: step 257320, loss = 0.0669, acc = 0.9900 (220.9 examples/sec; 0.290 sec/batch)
2017-05-09 19:50:21.283389: step 257340, loss = 0.0710, acc = 0.9860 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 19:50:25.843321: step 257360, loss = 0.0980, acc = 0.9760 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 19:50:30.626814: step 257380, loss = 0.0787, acc = 0.9820 (235.4 examples/sec; 0.272 sec/batch)
2017-05-09 19:50:35.176885: step 257400, loss = 0.1089, acc = 0.9680 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 19:50:39.742737: step 257420, loss = 0.0655, acc = 0.9880 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 19:50:44.358702: step 257440, loss = 0.0631, acc = 0.9940 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 19:50:49.154125: step 257460, loss = 0.0607, acc = 0.9860 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 19:50:53.830112: step 257480, loss = 0.0706, acc = 0.9880 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:50:58.466798: step 257500, loss = 0.1014, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 19:51:03.042055: step 257520, loss = 0.0684, acc = 0.9900 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 19:51:07.573008: step 257540, loss = 0.0972, acc = 0.9720 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 19:51:12.118912: step 257560, loss = 0.0587, acc = 0.9900 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 19:51:16.793479: step 257580, loss = 0.0817, acc = 0.9760 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 19:51:21.324143: step 257600, loss = 0.0774, acc = 0.9780 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 19:51:25.919092: step 257620, loss = 0.0700, acc = 0.9840 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 19:51:30.437449: step 257640, loss = 0.0688, acc = 0.9860 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 19:51:35.133602: step 257660, loss = 0.0867, acc = 0.9760 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 19:51:39.906799: step 257680, loss = 0.0735, acc = 0.9880 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 19:51:44.475022: step 257700, loss = 0.0852, acc = 0.9820 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 19:51:49.055508: step 257720, loss = 0.0761, acc = 0.9840 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 19:51:53.669852: step 257740, loss = 0.0838, acc = 0.9800 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 19:51:58.267630: step 257760, loss = 0.0956, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 19:52:02.994084: step 257780, loss = 0.0696, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 19:52:07.582181: step 257800, loss = 0.1107, acc = 0.9680 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 19:52:12.120170: step 257820, loss = 0.0937, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 19:52:16.817158: step 257840, loss = 0.0941, acc = 0.9880 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 19:52:21.405621: step 257860, loss = 0.0819, acc = 0.9680 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 19:52:25.996646: step 257880, loss = 0.0732, acc = 0.9820 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 19:52:30.582592: step 257900, loss = 0.0661, acc = 0.9860 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 19:52:35.268458: step 257920, loss = 0.0648, acc = 0.9900 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 19:52:39.876813: step 257940, loss = 0.0796, acc = 0.9820 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 19:52:44.345625: step 257960, loss = 0.0870, acc = 0.9780 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 19:52:49.186849: step 257980, loss = 0.0734, acc = 0.9820 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 19:52:53.783132: step 258000, loss = 0.0615, acc = 0.9900 (281.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 19:53:07.808657: step 258000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 19:53:17.490263: step 258000, acc = 0.9556, f1 = 0.9552
[Status] 2017-05-09 19:53:17.490370: step 258000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:53:22.149755: step 258020, loss = 0.0707, acc = 0.9880 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 19:53:26.773448: step 258040, loss = 0.0678, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 19:53:32.408543: step 258060, loss = 0.0673, acc = 0.9900 (217.2 examples/sec; 0.295 sec/batch)
2017-05-09 19:53:37.024970: step 258080, loss = 0.0596, acc = 0.9960 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 19:53:41.643392: step 258100, loss = 0.0818, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 19:53:46.363677: step 258120, loss = 0.0828, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:53:51.134678: step 258140, loss = 0.0626, acc = 0.9900 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:53:55.659404: step 258160, loss = 0.0741, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 19:54:00.193026: step 258180, loss = 0.0936, acc = 0.9780 (293.7 examples/sec; 0.218 sec/batch)
2017-05-09 19:54:04.954379: step 258200, loss = 0.0821, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 19:54:09.731947: step 258220, loss = 0.0905, acc = 0.9780 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 19:54:14.417122: step 258240, loss = 0.0738, acc = 0.9860 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 19:54:19.091120: step 258260, loss = 0.0921, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 19:54:23.544243: step 258280, loss = 0.0680, acc = 0.9860 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 19:54:28.229096: step 258300, loss = 0.0961, acc = 0.9740 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 19:54:33.000129: step 258320, loss = 0.0708, acc = 0.9800 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 19:54:37.655608: step 258340, loss = 0.0661, acc = 0.9880 (262.0 examples/sec; 0.244 sec/batch)
2017-05-09 19:54:42.211009: step 258360, loss = 0.0779, acc = 0.9840 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 19:54:46.848582: step 258380, loss = 0.0712, acc = 0.9860 (236.7 examples/sec; 0.270 sec/batch)
2017-05-09 19:54:51.346828: step 258400, loss = 0.0802, acc = 0.9800 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:54:55.968907: step 258420, loss = 0.0820, acc = 0.9740 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 19:55:00.544322: step 258440, loss = 0.0884, acc = 0.9760 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 19:55:05.206658: step 258460, loss = 0.0731, acc = 0.9780 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 19:55:09.882687: step 258480, loss = 0.0871, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 19:55:14.452855: step 258500, loss = 0.0789, acc = 0.9820 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 19:55:19.154751: step 258520, loss = 0.0730, acc = 0.9840 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 19:55:23.801949: step 258540, loss = 0.0688, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 19:55:28.434541: step 258560, loss = 0.0643, acc = 0.9900 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 19:55:33.197465: step 258580, loss = 0.0925, acc = 0.9660 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 19:55:37.852614: step 258600, loss = 0.0878, acc = 0.9780 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 19:55:42.463686: step 258620, loss = 0.0767, acc = 0.9800 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 19:55:47.262518: step 258640, loss = 0.0870, acc = 0.9700 (255.2 examples/sec; 0.251 sec/batch)
2017-05-09 19:55:51.990704: step 258660, loss = 0.0983, acc = 0.9780 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 19:55:56.626273: step 258680, loss = 0.0807, acc = 0.9800 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 19:56:01.164583: step 258700, loss = 0.0902, acc = 0.9760 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 19:56:05.889662: step 258720, loss = 0.0639, acc = 0.9900 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 19:56:10.436036: step 258740, loss = 0.0770, acc = 0.9820 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 19:56:14.960285: step 258760, loss = 0.0984, acc = 0.9640 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 19:56:19.700064: step 258780, loss = 0.0674, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 19:56:24.428420: step 258800, loss = 0.0774, acc = 0.9800 (252.5 examples/sec; 0.253 sec/batch)
2017-05-09 19:56:29.102666: step 258820, loss = 0.0912, acc = 0.9640 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 19:56:33.883167: step 258840, loss = 0.0692, acc = 0.9860 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 19:56:38.460577: step 258860, loss = 0.0744, acc = 0.9760 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 19:56:43.047037: step 258880, loss = 0.0802, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 19:56:47.882851: step 258900, loss = 0.0750, acc = 0.9840 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 19:56:52.380911: step 258920, loss = 0.1092, acc = 0.9700 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 19:56:56.982037: step 258940, loss = 0.0662, acc = 0.9860 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 19:57:01.676210: step 258960, loss = 0.0576, acc = 0.9900 (250.4 examples/sec; 0.256 sec/batch)
2017-05-09 19:57:06.225268: step 258980, loss = 0.0755, acc = 0.9860 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 19:57:11.005292: step 259000, loss = 0.0627, acc = 0.9900 (243.6 examples/sec; 0.263 sec/batch)
[Eval] 2017-05-09 19:57:25.008201: step 259000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 19:57:34.729330: step 259000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 19:57:34.729419: step 259000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 19:57:39.607120: step 259020, loss = 0.0943, acc = 0.9780 (212.6 examples/sec; 0.301 sec/batch)
2017-05-09 19:57:44.237078: step 259040, loss = 0.0863, acc = 0.9760 (249.8 examples/sec; 0.256 sec/batch)
2017-05-09 19:57:50.200468: step 259060, loss = 0.0989, acc = 0.9760 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 19:57:54.867256: step 259080, loss = 0.0810, acc = 0.9740 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 19:57:59.632744: step 259100, loss = 0.0671, acc = 0.9920 (242.4 examples/sec; 0.264 sec/batch)
2017-05-09 19:58:04.552220: step 259120, loss = 0.0942, acc = 0.9780 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 19:58:09.191008: step 259140, loss = 0.0749, acc = 0.9800 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 19:58:13.842454: step 259160, loss = 0.0815, acc = 0.9820 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 19:58:18.521644: step 259180, loss = 0.0896, acc = 0.9760 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 19:58:23.112751: step 259200, loss = 0.0792, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 19:58:27.710761: step 259220, loss = 0.0675, acc = 0.9800 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 19:58:32.556804: step 259240, loss = 0.0806, acc = 0.9760 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 19:58:37.156568: step 259260, loss = 0.0798, acc = 0.9860 (300.8 examples/sec; 0.213 sec/batch)
2017-05-09 19:58:41.749662: step 259280, loss = 0.0698, acc = 0.9820 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 19:58:46.434976: step 259300, loss = 0.0633, acc = 0.9860 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 19:58:50.975388: step 259320, loss = 0.0753, acc = 0.9800 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 19:58:55.589132: step 259340, loss = 0.0688, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 19:59:00.049290: step 259360, loss = 0.0662, acc = 0.9860 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 19:59:04.833354: step 259380, loss = 0.0670, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 19:59:09.397147: step 259400, loss = 0.0715, acc = 0.9840 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 19:59:13.980658: step 259420, loss = 0.0883, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 19:59:18.681549: step 259440, loss = 0.0777, acc = 0.9800 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 19:59:23.205710: step 259460, loss = 0.0734, acc = 0.9900 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 19:59:27.772661: step 259480, loss = 0.0773, acc = 0.9800 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 19:59:32.599568: step 259500, loss = 0.0658, acc = 0.9920 (233.9 examples/sec; 0.274 sec/batch)
2017-05-09 19:59:37.335106: step 259520, loss = 0.0676, acc = 0.9880 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 19:59:42.110038: step 259540, loss = 0.0751, acc = 0.9840 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 19:59:46.767237: step 259560, loss = 0.0800, acc = 0.9800 (249.9 examples/sec; 0.256 sec/batch)
2017-05-09 19:59:51.362756: step 259580, loss = 0.0885, acc = 0.9760 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 19:59:55.869313: step 259600, loss = 0.0805, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 20:00:00.522662: step 259620, loss = 0.0715, acc = 0.9820 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 20:00:05.323910: step 259640, loss = 0.0852, acc = 0.9740 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 20:00:10.010333: step 259660, loss = 0.1025, acc = 0.9740 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 20:00:14.657677: step 259680, loss = 0.0591, acc = 0.9860 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 20:00:19.534029: step 259700, loss = 0.0801, acc = 0.9780 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 20:00:24.149639: step 259720, loss = 0.0685, acc = 0.9820 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 20:00:28.746100: step 259740, loss = 0.1057, acc = 0.9680 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 20:00:33.290530: step 259760, loss = 0.0939, acc = 0.9760 (298.7 examples/sec; 0.214 sec/batch)
2017-05-09 20:00:37.826347: step 259780, loss = 0.0843, acc = 0.9840 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 20:00:42.403674: step 259800, loss = 0.0839, acc = 0.9780 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 20:00:47.009973: step 259820, loss = 0.0768, acc = 0.9820 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 20:00:51.549580: step 259840, loss = 0.0831, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 20:00:56.142512: step 259860, loss = 0.0795, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 20:01:00.891650: step 259880, loss = 0.0932, acc = 0.9740 (241.5 examples/sec; 0.265 sec/batch)
2017-05-09 20:01:05.603022: step 259900, loss = 0.0837, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:01:10.198013: step 259920, loss = 0.0984, acc = 0.9760 (300.0 examples/sec; 0.213 sec/batch)
2017-05-09 20:01:14.835797: step 259940, loss = 0.0649, acc = 0.9880 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 20:01:19.533282: step 259960, loss = 0.0804, acc = 0.9780 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 20:01:24.032856: step 259980, loss = 0.0702, acc = 0.9840 (298.4 examples/sec; 0.214 sec/batch)
2017-05-09 20:01:28.562379: step 260000, loss = 0.0710, acc = 0.9820 (286.9 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 20:01:42.763850: step 260000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 20:01:52.315677: step 260000, acc = 0.9566, f1 = 0.9562
[Status] 2017-05-09 20:01:52.315754: step 260000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:01:57.056724: step 260020, loss = 0.0796, acc = 0.9780 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 20:02:01.731411: step 260040, loss = 0.0809, acc = 0.9780 (260.2 examples/sec; 0.246 sec/batch)
2017-05-09 20:02:06.924512: step 260060, loss = 0.0770, acc = 0.9820 (165.9 examples/sec; 0.386 sec/batch)
2017-05-09 20:02:11.584185: step 260080, loss = 0.0802, acc = 0.9780 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 20:02:16.197133: step 260100, loss = 0.0774, acc = 0.9860 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 20:02:20.763258: step 260120, loss = 0.0933, acc = 0.9720 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:02:25.335674: step 260140, loss = 0.0747, acc = 0.9860 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 20:02:29.935825: step 260160, loss = 0.0658, acc = 0.9920 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 20:02:34.675402: step 260180, loss = 0.0721, acc = 0.9860 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 20:02:39.303176: step 260200, loss = 0.0750, acc = 0.9800 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 20:02:44.258569: step 260220, loss = 0.0783, acc = 0.9820 (203.9 examples/sec; 0.314 sec/batch)
2017-05-09 20:02:48.834266: step 260240, loss = 0.0868, acc = 0.9820 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 20:02:53.360259: step 260260, loss = 0.0942, acc = 0.9720 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 20:02:58.157508: step 260280, loss = 0.0727, acc = 0.9820 (209.9 examples/sec; 0.305 sec/batch)
2017-05-09 20:03:02.742575: step 260300, loss = 0.0786, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 20:03:07.217034: step 260320, loss = 0.0698, acc = 0.9820 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 20:03:11.826727: step 260340, loss = 0.0871, acc = 0.9700 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 20:03:16.556460: step 260360, loss = 0.0780, acc = 0.9800 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 20:03:21.221572: step 260380, loss = 0.0973, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 20:03:25.796589: step 260400, loss = 0.0922, acc = 0.9800 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 20:03:30.550013: step 260420, loss = 0.0809, acc = 0.9860 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:03:35.178541: step 260440, loss = 0.0670, acc = 0.9840 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:03:39.781814: step 260460, loss = 0.0714, acc = 0.9860 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:03:44.558640: step 260480, loss = 0.0855, acc = 0.9780 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 20:03:49.202916: step 260500, loss = 0.0589, acc = 0.9940 (267.4 examples/sec; 0.239 sec/batch)
2017-05-09 20:03:53.825009: step 260520, loss = 0.0729, acc = 0.9860 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:03:58.437294: step 260540, loss = 0.0763, acc = 0.9760 (241.3 examples/sec; 0.265 sec/batch)
2017-05-09 20:04:02.938952: step 260560, loss = 0.0782, acc = 0.9860 (299.7 examples/sec; 0.214 sec/batch)
2017-05-09 20:04:07.507724: step 260580, loss = 0.0978, acc = 0.9660 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 20:04:12.154511: step 260600, loss = 0.0751, acc = 0.9820 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 20:04:16.837281: step 260620, loss = 0.0778, acc = 0.9720 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 20:04:21.374842: step 260640, loss = 0.0641, acc = 0.9860 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 20:04:25.941212: step 260660, loss = 0.1077, acc = 0.9760 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 20:04:30.723975: step 260680, loss = 0.0662, acc = 0.9860 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 20:04:35.874615: step 260700, loss = 0.0826, acc = 0.9880 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 20:04:40.487398: step 260720, loss = 0.0766, acc = 0.9800 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:04:45.095679: step 260740, loss = 0.0713, acc = 0.9840 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 20:04:49.731824: step 260760, loss = 0.0753, acc = 0.9820 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 20:04:54.335604: step 260780, loss = 0.0779, acc = 0.9760 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 20:04:59.137277: step 260800, loss = 0.0717, acc = 0.9880 (260.1 examples/sec; 0.246 sec/batch)
2017-05-09 20:05:03.764792: step 260820, loss = 0.1003, acc = 0.9700 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 20:05:08.418827: step 260840, loss = 0.0627, acc = 0.9940 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 20:05:13.052691: step 260860, loss = 0.0762, acc = 0.9780 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 20:05:17.798789: step 260880, loss = 0.0898, acc = 0.9780 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 20:05:22.402624: step 260900, loss = 0.0855, acc = 0.9760 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 20:05:27.011030: step 260920, loss = 0.0821, acc = 0.9860 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 20:05:31.772099: step 260940, loss = 0.0857, acc = 0.9840 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 20:05:36.465354: step 260960, loss = 0.0761, acc = 0.9900 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 20:05:41.208184: step 260980, loss = 0.0843, acc = 0.9800 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 20:05:45.910653: step 261000, loss = 0.0783, acc = 0.9820 (285.4 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 20:05:59.857520: step 261000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 20:06:09.537717: step 261000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 20:06:09.537789: step 261000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:06:14.342554: step 261020, loss = 0.0845, acc = 0.9800 (234.1 examples/sec; 0.273 sec/batch)
2017-05-09 20:06:18.988164: step 261040, loss = 0.0910, acc = 0.9720 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 20:06:23.544190: step 261060, loss = 0.1012, acc = 0.9620 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 20:06:29.209181: step 261080, loss = 0.0693, acc = 0.9900 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:06:33.757967: step 261100, loss = 0.0732, acc = 0.9860 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 20:06:38.443691: step 261120, loss = 0.0859, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 20:06:42.984752: step 261140, loss = 0.0787, acc = 0.9800 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:06:47.636310: step 261160, loss = 0.0907, acc = 0.9740 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 20:06:52.292896: step 261180, loss = 0.0818, acc = 0.9800 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 20:06:56.971255: step 261200, loss = 0.0735, acc = 0.9860 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:07:01.601907: step 261220, loss = 0.1067, acc = 0.9640 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 20:07:06.191473: step 261240, loss = 0.0853, acc = 0.9760 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 20:07:10.873640: step 261260, loss = 0.0764, acc = 0.9780 (237.4 examples/sec; 0.270 sec/batch)
2017-05-09 20:07:15.521232: step 261280, loss = 0.0704, acc = 0.9820 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 20:07:20.163115: step 261300, loss = 0.0851, acc = 0.9740 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 20:07:24.850889: step 261320, loss = 0.0850, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 20:07:29.717666: step 261340, loss = 0.0865, acc = 0.9880 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 20:07:34.203363: step 261360, loss = 0.0807, acc = 0.9720 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:07:38.819508: step 261380, loss = 0.0586, acc = 0.9940 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 20:07:43.642495: step 261400, loss = 0.0882, acc = 0.9720 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:07:48.222137: step 261420, loss = 0.0751, acc = 0.9820 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 20:07:52.825460: step 261440, loss = 0.0770, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 20:07:57.607713: step 261460, loss = 0.0729, acc = 0.9820 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 20:08:02.131870: step 261480, loss = 0.0938, acc = 0.9740 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 20:08:06.818178: step 261500, loss = 0.0867, acc = 0.9860 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 20:08:11.520122: step 261520, loss = 0.0771, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 20:08:16.165080: step 261540, loss = 0.0953, acc = 0.9680 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 20:08:20.685269: step 261560, loss = 0.0808, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 20:08:25.264563: step 261580, loss = 0.0783, acc = 0.9760 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:08:29.951422: step 261600, loss = 0.0863, acc = 0.9760 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 20:08:34.622481: step 261620, loss = 0.0743, acc = 0.9840 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 20:08:39.264294: step 261640, loss = 0.0788, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 20:08:44.018632: step 261660, loss = 0.0613, acc = 0.9920 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:08:48.552255: step 261680, loss = 0.0832, acc = 0.9760 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 20:08:53.186162: step 261700, loss = 0.0746, acc = 0.9800 (255.6 examples/sec; 0.250 sec/batch)
2017-05-09 20:08:57.824510: step 261720, loss = 0.0931, acc = 0.9780 (298.2 examples/sec; 0.215 sec/batch)
2017-05-09 20:09:02.262389: step 261740, loss = 0.1002, acc = 0.9800 (299.1 examples/sec; 0.214 sec/batch)
2017-05-09 20:09:06.846405: step 261760, loss = 0.0785, acc = 0.9800 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 20:09:11.703029: step 261780, loss = 0.0795, acc = 0.9860 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 20:09:16.165214: step 261800, loss = 0.0894, acc = 0.9780 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 20:09:20.859425: step 261820, loss = 0.0630, acc = 0.9860 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 20:09:25.780902: step 261840, loss = 0.0827, acc = 0.9760 (237.0 examples/sec; 0.270 sec/batch)
2017-05-09 20:09:30.461495: step 261860, loss = 0.0786, acc = 0.9920 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 20:09:35.092274: step 261880, loss = 0.0935, acc = 0.9820 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 20:09:39.769352: step 261900, loss = 0.0869, acc = 0.9760 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 20:09:44.830978: step 261920, loss = 0.0829, acc = 0.9780 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 20:09:49.404191: step 261940, loss = 0.0873, acc = 0.9820 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 20:09:53.900385: step 261960, loss = 0.0632, acc = 0.9900 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 20:09:58.708921: step 261980, loss = 0.0761, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 20:10:03.351778: step 262000, loss = 0.0956, acc = 0.9780 (262.5 examples/sec; 0.244 sec/batch)
[Eval] 2017-05-09 20:10:17.379603: step 262000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 20:10:27.097840: step 262000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 20:10:27.097920: step 262000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:10:31.684430: step 262020, loss = 0.0720, acc = 0.9780 (299.7 examples/sec; 0.214 sec/batch)
2017-05-09 20:10:36.204403: step 262040, loss = 0.0829, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 20:10:40.982381: step 262060, loss = 0.0702, acc = 0.9800 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 20:10:46.292846: step 262080, loss = 0.0679, acc = 0.9880 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 20:10:50.785512: step 262100, loss = 0.0886, acc = 0.9800 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 20:10:55.456353: step 262120, loss = 0.0812, acc = 0.9780 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 20:11:00.119293: step 262140, loss = 0.0734, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 20:11:04.804312: step 262160, loss = 0.0860, acc = 0.9720 (258.5 examples/sec; 0.248 sec/batch)
2017-05-09 20:11:09.362866: step 262180, loss = 0.0788, acc = 0.9820 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 20:11:13.945941: step 262200, loss = 0.0803, acc = 0.9780 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 20:11:18.495548: step 262220, loss = 0.0734, acc = 0.9820 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:11:22.994664: step 262240, loss = 0.0894, acc = 0.9780 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 20:11:27.870982: step 262260, loss = 0.0840, acc = 0.9780 (239.2 examples/sec; 0.268 sec/batch)
2017-05-09 20:11:32.390277: step 262280, loss = 0.0745, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 20:11:37.042649: step 262300, loss = 0.0963, acc = 0.9720 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 20:11:41.583707: step 262320, loss = 0.0804, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 20:11:46.376838: step 262340, loss = 0.0662, acc = 0.9860 (260.4 examples/sec; 0.246 sec/batch)
2017-05-09 20:11:50.788645: step 262360, loss = 0.0814, acc = 0.9860 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 20:11:55.376266: step 262380, loss = 0.1094, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 20:12:00.216012: step 262400, loss = 0.0660, acc = 0.9880 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 20:12:04.965280: step 262420, loss = 0.0791, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:12:09.692668: step 262440, loss = 0.0900, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 20:12:14.592645: step 262460, loss = 0.0906, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 20:12:19.184279: step 262480, loss = 0.0813, acc = 0.9780 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:12:23.705930: step 262500, loss = 0.0679, acc = 0.9880 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:12:28.347736: step 262520, loss = 0.0813, acc = 0.9860 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 20:12:33.357116: step 262540, loss = 0.0625, acc = 0.9860 (258.0 examples/sec; 0.248 sec/batch)
2017-05-09 20:12:37.916140: step 262560, loss = 0.0841, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:12:42.706566: step 262580, loss = 0.1015, acc = 0.9760 (249.0 examples/sec; 0.257 sec/batch)
2017-05-09 20:12:47.305599: step 262600, loss = 0.0950, acc = 0.9800 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 20:12:51.894520: step 262620, loss = 0.0727, acc = 0.9840 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 20:12:56.577621: step 262640, loss = 0.0738, acc = 0.9820 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 20:13:01.476137: step 262660, loss = 0.0758, acc = 0.9860 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 20:13:06.125332: step 262680, loss = 0.0946, acc = 0.9760 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:13:10.627769: step 262700, loss = 0.0735, acc = 0.9840 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 20:13:15.262035: step 262720, loss = 0.0714, acc = 0.9860 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 20:13:19.822818: step 262740, loss = 0.0947, acc = 0.9720 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 20:13:24.447842: step 262760, loss = 0.0794, acc = 0.9800 (255.3 examples/sec; 0.251 sec/batch)
2017-05-09 20:13:29.319608: step 262780, loss = 0.0746, acc = 0.9860 (231.4 examples/sec; 0.277 sec/batch)
2017-05-09 20:13:33.993362: step 262800, loss = 0.0816, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 20:13:38.724285: step 262820, loss = 0.0773, acc = 0.9820 (243.7 examples/sec; 0.263 sec/batch)
2017-05-09 20:13:43.539571: step 262840, loss = 0.0730, acc = 0.9840 (248.3 examples/sec; 0.258 sec/batch)
2017-05-09 20:13:48.066759: step 262860, loss = 0.0698, acc = 0.9840 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 20:13:52.728751: step 262880, loss = 0.0632, acc = 0.9880 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 20:13:57.560239: step 262900, loss = 0.0757, acc = 0.9820 (251.4 examples/sec; 0.255 sec/batch)
2017-05-09 20:14:02.432268: step 262920, loss = 0.0735, acc = 0.9860 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:14:07.126084: step 262940, loss = 0.1032, acc = 0.9620 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 20:14:11.738889: step 262960, loss = 0.0837, acc = 0.9780 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 20:14:16.601263: step 262980, loss = 0.1059, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 20:14:21.160243: step 263000, loss = 0.0640, acc = 0.9840 (273.9 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 20:14:35.263761: step 263000, acc = 0.9640, f1 = 0.9627
[Test] 2017-05-09 20:14:44.948990: step 263000, acc = 0.9545, f1 = 0.9541
[Status] 2017-05-09 20:14:44.949078: step 263000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:14:49.522385: step 263020, loss = 0.1026, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 20:14:54.031375: step 263040, loss = 0.0757, acc = 0.9860 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 20:14:58.647565: step 263060, loss = 0.0774, acc = 0.9820 (248.5 examples/sec; 0.258 sec/batch)
2017-05-09 20:15:03.292234: step 263080, loss = 0.1081, acc = 0.9680 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 20:15:08.870190: step 263100, loss = 0.0792, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 20:15:13.557931: step 263120, loss = 0.0846, acc = 0.9780 (250.6 examples/sec; 0.255 sec/batch)
2017-05-09 20:15:17.952296: step 263140, loss = 0.0885, acc = 0.9760 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 20:15:22.616455: step 263160, loss = 0.0806, acc = 0.9840 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 20:15:27.152811: step 263180, loss = 0.0651, acc = 0.9860 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 20:15:31.926825: step 263200, loss = 0.0783, acc = 0.9800 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 20:15:36.543657: step 263220, loss = 0.0783, acc = 0.9800 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 20:15:41.218794: step 263240, loss = 0.0930, acc = 0.9820 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:15:46.058844: step 263260, loss = 0.1055, acc = 0.9740 (265.7 examples/sec; 0.241 sec/batch)
2017-05-09 20:15:50.588418: step 263280, loss = 0.0773, acc = 0.9800 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:15:55.132100: step 263300, loss = 0.0665, acc = 0.9820 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 20:15:59.771201: step 263320, loss = 0.1006, acc = 0.9700 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 20:16:04.365275: step 263340, loss = 0.1020, acc = 0.9700 (297.7 examples/sec; 0.215 sec/batch)
2017-05-09 20:16:08.997425: step 263360, loss = 0.1040, acc = 0.9740 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 20:16:13.590341: step 263380, loss = 0.0814, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 20:16:18.391710: step 263400, loss = 0.0949, acc = 0.9760 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 20:16:23.083078: step 263420, loss = 0.0787, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 20:16:27.747971: step 263440, loss = 0.0826, acc = 0.9760 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 20:16:32.592886: step 263460, loss = 0.0631, acc = 0.9840 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 20:16:37.118795: step 263480, loss = 0.0603, acc = 0.9880 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 20:16:41.671281: step 263500, loss = 0.0855, acc = 0.9760 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 20:16:46.332776: step 263520, loss = 0.0723, acc = 0.9860 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 20:16:50.964539: step 263540, loss = 0.0754, acc = 0.9840 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:16:55.540412: step 263560, loss = 0.0658, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 20:17:00.304624: step 263580, loss = 0.0892, acc = 0.9660 (224.2 examples/sec; 0.285 sec/batch)
2017-05-09 20:17:04.990641: step 263600, loss = 0.0849, acc = 0.9780 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 20:17:09.681265: step 263620, loss = 0.0692, acc = 0.9780 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 20:17:14.302804: step 263640, loss = 0.0688, acc = 0.9880 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 20:17:19.008041: step 263660, loss = 0.0732, acc = 0.9800 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 20:17:23.559089: step 263680, loss = 0.0791, acc = 0.9900 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 20:17:28.166527: step 263700, loss = 0.0658, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 20:17:32.878845: step 263720, loss = 0.0880, acc = 0.9800 (301.3 examples/sec; 0.212 sec/batch)
2017-05-09 20:17:37.498336: step 263740, loss = 0.0729, acc = 0.9820 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 20:17:42.159179: step 263760, loss = 0.0831, acc = 0.9800 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 20:17:47.150481: step 263780, loss = 0.0881, acc = 0.9740 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 20:17:51.839448: step 263800, loss = 0.0866, acc = 0.9740 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 20:17:56.401024: step 263820, loss = 0.0955, acc = 0.9740 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 20:18:01.134565: step 263840, loss = 0.0852, acc = 0.9860 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 20:18:05.634859: step 263860, loss = 0.0627, acc = 0.9860 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 20:18:10.174730: step 263880, loss = 0.0807, acc = 0.9760 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 20:18:14.981521: step 263900, loss = 0.0809, acc = 0.9780 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:18:19.586981: step 263920, loss = 0.0687, acc = 0.9840 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 20:18:24.223265: step 263940, loss = 0.0795, acc = 0.9840 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 20:18:29.053450: step 263960, loss = 0.1044, acc = 0.9640 (214.1 examples/sec; 0.299 sec/batch)
2017-05-09 20:18:33.620318: step 263980, loss = 0.0911, acc = 0.9760 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 20:18:38.049685: step 264000, loss = 0.0789, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 20:18:52.204310: step 264000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 20:19:02.319997: step 264000, acc = 0.9562, f1 = 0.9559
[Status] 2017-05-09 20:19:02.320100: step 264000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:19:06.901151: step 264020, loss = 0.0900, acc = 0.9840 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 20:19:11.674862: step 264040, loss = 0.0771, acc = 0.9840 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 20:19:16.385641: step 264060, loss = 0.0727, acc = 0.9820 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:19:20.973183: step 264080, loss = 0.0692, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 20:19:26.420428: step 264100, loss = 0.0582, acc = 0.9920 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 20:19:31.123423: step 264120, loss = 0.0751, acc = 0.9800 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 20:19:35.782447: step 264140, loss = 0.0827, acc = 0.9800 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 20:19:40.472821: step 264160, loss = 0.0735, acc = 0.9820 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 20:19:45.173736: step 264180, loss = 0.0865, acc = 0.9880 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 20:19:49.682823: step 264200, loss = 0.1036, acc = 0.9660 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 20:19:54.358075: step 264220, loss = 0.0930, acc = 0.9700 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 20:19:58.950928: step 264240, loss = 0.0860, acc = 0.9720 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 20:20:03.537049: step 264260, loss = 0.0972, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 20:20:08.143599: step 264280, loss = 0.0663, acc = 0.9920 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 20:20:12.859318: step 264300, loss = 0.0808, acc = 0.9780 (243.4 examples/sec; 0.263 sec/batch)
2017-05-09 20:20:17.525946: step 264320, loss = 0.0778, acc = 0.9880 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 20:20:22.179110: step 264340, loss = 0.0795, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 20:20:26.735785: step 264360, loss = 0.0643, acc = 0.9900 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 20:20:31.539184: step 264380, loss = 0.0735, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 20:20:36.117108: step 264400, loss = 0.0829, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 20:20:40.682249: step 264420, loss = 0.0815, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 20:20:45.603148: step 264440, loss = 0.0741, acc = 0.9860 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 20:20:50.172391: step 264460, loss = 0.0968, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:20:54.767909: step 264480, loss = 0.0875, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 20:20:59.339531: step 264500, loss = 0.0857, acc = 0.9800 (299.0 examples/sec; 0.214 sec/batch)
2017-05-09 20:21:03.863739: step 264520, loss = 0.0819, acc = 0.9840 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:21:08.478512: step 264540, loss = 0.0730, acc = 0.9780 (252.2 examples/sec; 0.254 sec/batch)
2017-05-09 20:21:13.086769: step 264560, loss = 0.0863, acc = 0.9780 (240.0 examples/sec; 0.267 sec/batch)
2017-05-09 20:21:17.767532: step 264580, loss = 0.0806, acc = 0.9800 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 20:21:22.421205: step 264600, loss = 0.0870, acc = 0.9700 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:21:26.962591: step 264620, loss = 0.0755, acc = 0.9840 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 20:21:31.652654: step 264640, loss = 0.0783, acc = 0.9820 (295.6 examples/sec; 0.217 sec/batch)
2017-05-09 20:21:36.211336: step 264660, loss = 0.0664, acc = 0.9920 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 20:21:40.848440: step 264680, loss = 0.0947, acc = 0.9780 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:21:45.613378: step 264700, loss = 0.0804, acc = 0.9800 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 20:21:50.183430: step 264720, loss = 0.0728, acc = 0.9840 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 20:21:54.888039: step 264740, loss = 0.0807, acc = 0.9780 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:21:59.536271: step 264760, loss = 0.0631, acc = 0.9920 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 20:22:04.081791: step 264780, loss = 0.0846, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 20:22:08.758086: step 264800, loss = 0.0733, acc = 0.9900 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 20:22:13.538648: step 264820, loss = 0.0804, acc = 0.9800 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 20:22:18.146603: step 264840, loss = 0.0867, acc = 0.9800 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 20:22:22.694207: step 264860, loss = 0.0703, acc = 0.9860 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 20:22:27.226118: step 264880, loss = 0.0578, acc = 0.9880 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 20:22:32.159981: step 264900, loss = 0.0843, acc = 0.9780 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 20:22:36.855978: step 264920, loss = 0.0589, acc = 0.9880 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 20:22:41.433782: step 264940, loss = 0.0935, acc = 0.9700 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 20:22:46.057192: step 264960, loss = 0.0792, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 20:22:50.683643: step 264980, loss = 0.0891, acc = 0.9720 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 20:22:55.385665: step 265000, loss = 0.0739, acc = 0.9820 (271.2 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 20:23:09.559598: step 265000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 20:23:19.243166: step 265000, acc = 0.9564, f1 = 0.9561
[Status] 2017-05-09 20:23:19.243266: step 265000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:23:23.824739: step 265020, loss = 0.0852, acc = 0.9820 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:23:28.401063: step 265040, loss = 0.0716, acc = 0.9820 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 20:23:33.078002: step 265060, loss = 0.0855, acc = 0.9760 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 20:23:37.775024: step 265080, loss = 0.0841, acc = 0.9860 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 20:23:43.426296: step 265100, loss = 0.0727, acc = 0.9820 (135.7 examples/sec; 0.472 sec/batch)
2017-05-09 20:23:48.415934: step 265120, loss = 0.0700, acc = 0.9880 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 20:23:53.142382: step 265140, loss = 0.0829, acc = 0.9840 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 20:23:57.791120: step 265160, loss = 0.0731, acc = 0.9800 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 20:24:02.560947: step 265180, loss = 0.0846, acc = 0.9800 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 20:24:06.996317: step 265200, loss = 0.0709, acc = 0.9780 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 20:24:11.716685: step 265220, loss = 0.0975, acc = 0.9640 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 20:24:16.421829: step 265240, loss = 0.0901, acc = 0.9720 (224.7 examples/sec; 0.285 sec/batch)
2017-05-09 20:24:20.909015: step 265260, loss = 0.0692, acc = 0.9840 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 20:24:25.446433: step 265280, loss = 0.0661, acc = 0.9900 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 20:24:30.145444: step 265300, loss = 0.0877, acc = 0.9800 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 20:24:34.853742: step 265320, loss = 0.0612, acc = 0.9920 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 20:24:39.402467: step 265340, loss = 0.0850, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 20:24:44.053546: step 265360, loss = 0.0770, acc = 0.9880 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 20:24:48.766808: step 265380, loss = 0.0769, acc = 0.9780 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 20:24:53.357526: step 265400, loss = 0.0649, acc = 0.9900 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 20:24:58.053152: step 265420, loss = 0.0910, acc = 0.9820 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 20:25:02.915590: step 265440, loss = 0.0839, acc = 0.9720 (269.2 examples/sec; 0.238 sec/batch)
2017-05-09 20:25:07.690904: step 265460, loss = 0.0730, acc = 0.9800 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:25:12.266639: step 265480, loss = 0.1023, acc = 0.9720 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 20:25:17.042795: step 265500, loss = 0.0771, acc = 0.9780 (230.8 examples/sec; 0.277 sec/batch)
2017-05-09 20:25:21.619537: step 265520, loss = 0.0751, acc = 0.9760 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 20:25:26.238190: step 265540, loss = 0.0786, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:25:30.774859: step 265560, loss = 0.0913, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 20:25:35.383575: step 265580, loss = 0.0742, acc = 0.9860 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 20:25:39.920003: step 265600, loss = 0.0831, acc = 0.9740 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 20:25:44.505826: step 265620, loss = 0.0757, acc = 0.9860 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:25:49.299926: step 265640, loss = 0.0982, acc = 0.9780 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 20:25:53.843100: step 265660, loss = 0.0526, acc = 0.9940 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:25:58.398526: step 265680, loss = 0.0831, acc = 0.9780 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 20:26:03.089081: step 265700, loss = 0.0640, acc = 0.9860 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 20:26:07.580023: step 265720, loss = 0.0736, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 20:26:12.282073: step 265740, loss = 0.0676, acc = 0.9860 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:26:16.928774: step 265760, loss = 0.0695, acc = 0.9800 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:26:21.684225: step 265780, loss = 0.0718, acc = 0.9860 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 20:26:26.267244: step 265800, loss = 0.0808, acc = 0.9760 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 20:26:30.766684: step 265820, loss = 0.0808, acc = 0.9760 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 20:26:35.353928: step 265840, loss = 0.0895, acc = 0.9680 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 20:26:39.947216: step 265860, loss = 0.0806, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:26:44.572523: step 265880, loss = 0.0863, acc = 0.9840 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 20:26:49.403391: step 265900, loss = 0.0910, acc = 0.9740 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 20:26:54.079468: step 265920, loss = 0.0866, acc = 0.9720 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 20:26:58.626187: step 265940, loss = 0.0863, acc = 0.9740 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:27:03.392601: step 265960, loss = 0.0852, acc = 0.9780 (223.7 examples/sec; 0.286 sec/batch)
2017-05-09 20:27:07.929033: step 265980, loss = 0.0881, acc = 0.9820 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 20:27:12.599025: step 266000, loss = 0.0838, acc = 0.9780 (276.7 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 20:27:26.711380: step 266000, acc = 0.9634, f1 = 0.9621
[Test] 2017-05-09 20:27:36.352996: step 266000, acc = 0.9543, f1 = 0.9539
[Status] 2017-05-09 20:27:36.353098: step 266000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:27:40.936461: step 266020, loss = 0.0971, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 20:27:45.520414: step 266040, loss = 0.0911, acc = 0.9720 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 20:27:50.469107: step 266060, loss = 0.0735, acc = 0.9840 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 20:27:54.943567: step 266080, loss = 0.0620, acc = 0.9880 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 20:27:59.645154: step 266100, loss = 0.0508, acc = 0.9980 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 20:28:05.210648: step 266120, loss = 0.0729, acc = 0.9840 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 20:28:09.701306: step 266140, loss = 0.0793, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 20:28:14.197810: step 266160, loss = 0.0735, acc = 0.9820 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 20:28:18.779595: step 266180, loss = 0.0656, acc = 0.9860 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 20:28:23.402627: step 266200, loss = 0.0903, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 20:28:28.046235: step 266220, loss = 0.0956, acc = 0.9700 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 20:28:32.724611: step 266240, loss = 0.0769, acc = 0.9800 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 20:28:37.284258: step 266260, loss = 0.0777, acc = 0.9780 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 20:28:41.836952: step 266280, loss = 0.0728, acc = 0.9860 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 20:28:46.433154: step 266300, loss = 0.0737, acc = 0.9840 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 20:28:51.127993: step 266320, loss = 0.0840, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:28:55.692725: step 266340, loss = 0.0696, acc = 0.9840 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 20:29:00.365397: step 266360, loss = 0.0835, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 20:29:05.191309: step 266380, loss = 0.0718, acc = 0.9820 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 20:29:09.873414: step 266400, loss = 0.0671, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 20:29:14.475408: step 266420, loss = 0.0854, acc = 0.9720 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 20:29:19.346159: step 266440, loss = 0.0955, acc = 0.9800 (230.3 examples/sec; 0.278 sec/batch)
2017-05-09 20:29:23.904240: step 266460, loss = 0.0629, acc = 0.9840 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 20:29:28.559655: step 266480, loss = 0.0837, acc = 0.9760 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 20:29:33.161665: step 266500, loss = 0.0671, acc = 0.9780 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 20:29:37.994446: step 266520, loss = 0.0850, acc = 0.9780 (239.8 examples/sec; 0.267 sec/batch)
2017-05-09 20:29:42.611279: step 266540, loss = 0.0830, acc = 0.9760 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 20:29:47.172298: step 266560, loss = 0.0794, acc = 0.9820 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 20:29:51.995181: step 266580, loss = 0.0766, acc = 0.9820 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 20:29:56.451856: step 266600, loss = 0.0859, acc = 0.9760 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 20:30:01.057013: step 266620, loss = 0.0754, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 20:30:05.879413: step 266640, loss = 0.0853, acc = 0.9800 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 20:30:10.451316: step 266660, loss = 0.0811, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 20:30:14.995215: step 266680, loss = 0.0838, acc = 0.9780 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 20:30:19.716616: step 266700, loss = 0.0951, acc = 0.9740 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 20:30:24.371503: step 266720, loss = 0.0689, acc = 0.9860 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 20:30:29.028108: step 266740, loss = 0.0671, acc = 0.9900 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 20:30:33.652459: step 266760, loss = 0.0649, acc = 0.9900 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 20:30:38.293053: step 266780, loss = 0.0789, acc = 0.9820 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 20:30:42.812264: step 266800, loss = 0.0941, acc = 0.9720 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 20:30:47.426769: step 266820, loss = 0.0767, acc = 0.9860 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 20:30:52.102538: step 266840, loss = 0.0823, acc = 0.9780 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 20:30:56.576233: step 266860, loss = 0.1047, acc = 0.9620 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 20:31:01.377498: step 266880, loss = 0.0989, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 20:31:06.148346: step 266900, loss = 0.0753, acc = 0.9760 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 20:31:10.874758: step 266920, loss = 0.1005, acc = 0.9720 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 20:31:15.515113: step 266940, loss = 0.0710, acc = 0.9840 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 20:31:20.519077: step 266960, loss = 0.0690, acc = 0.9880 (239.1 examples/sec; 0.268 sec/batch)
2017-05-09 20:31:25.126697: step 266980, loss = 0.0751, acc = 0.9820 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 20:31:29.830608: step 267000, loss = 0.0712, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 20:31:43.821342: step 267000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-09 20:31:53.509146: step 267000, acc = 0.9564, f1 = 0.9560
[Status] 2017-05-09 20:31:53.509234: step 267000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:31:58.007236: step 267020, loss = 0.0928, acc = 0.9820 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 20:32:02.691397: step 267040, loss = 0.1031, acc = 0.9760 (241.0 examples/sec; 0.266 sec/batch)
2017-05-09 20:32:07.327870: step 267060, loss = 0.0865, acc = 0.9820 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 20:32:12.081548: step 267080, loss = 0.0738, acc = 0.9800 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 20:32:16.734589: step 267100, loss = 0.0769, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 20:32:22.477553: step 267120, loss = 0.0827, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 20:32:26.992335: step 267140, loss = 0.0767, acc = 0.9820 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 20:32:31.669602: step 267160, loss = 0.0861, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:32:36.452635: step 267180, loss = 0.0660, acc = 0.9860 (291.6 examples/sec; 0.220 sec/batch)
2017-05-09 20:32:40.954839: step 267200, loss = 0.0809, acc = 0.9900 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 20:32:45.535163: step 267220, loss = 0.0643, acc = 0.9860 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:32:50.465076: step 267240, loss = 0.0933, acc = 0.9760 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:32:55.084272: step 267260, loss = 0.0765, acc = 0.9800 (288.9 examples/sec; 0.221 sec/batch)
2017-05-09 20:32:59.658700: step 267280, loss = 0.1173, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 20:33:04.388297: step 267300, loss = 0.0749, acc = 0.9800 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 20:33:08.869145: step 267320, loss = 0.0914, acc = 0.9780 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 20:33:13.381640: step 267340, loss = 0.0843, acc = 0.9820 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 20:33:18.078532: step 267360, loss = 0.0739, acc = 0.9800 (244.0 examples/sec; 0.262 sec/batch)
2017-05-09 20:33:22.461771: step 267380, loss = 0.0894, acc = 0.9700 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 20:33:26.962449: step 267400, loss = 0.0937, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:33:31.532085: step 267420, loss = 0.0750, acc = 0.9780 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 20:33:36.277872: step 267440, loss = 0.0682, acc = 0.9900 (299.2 examples/sec; 0.214 sec/batch)
2017-05-09 20:33:40.937991: step 267460, loss = 0.0799, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 20:33:45.501367: step 267480, loss = 0.0793, acc = 0.9760 (252.9 examples/sec; 0.253 sec/batch)
2017-05-09 20:33:50.268091: step 267500, loss = 0.0653, acc = 0.9840 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 20:33:54.904760: step 267520, loss = 0.0838, acc = 0.9800 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 20:33:59.567890: step 267540, loss = 0.0778, acc = 0.9760 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 20:34:04.247509: step 267560, loss = 0.0748, acc = 0.9800 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 20:34:08.843466: step 267580, loss = 0.0961, acc = 0.9740 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 20:34:13.440377: step 267600, loss = 0.0721, acc = 0.9780 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 20:34:18.046659: step 267620, loss = 0.0902, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 20:34:22.523526: step 267640, loss = 0.0646, acc = 0.9840 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 20:34:27.276810: step 267660, loss = 0.0703, acc = 0.9880 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 20:34:32.030704: step 267680, loss = 0.0787, acc = 0.9720 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 20:34:36.564043: step 267700, loss = 0.0848, acc = 0.9800 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 20:34:41.088203: step 267720, loss = 0.1049, acc = 0.9660 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 20:34:45.650658: step 267740, loss = 0.0798, acc = 0.9820 (252.3 examples/sec; 0.254 sec/batch)
2017-05-09 20:34:50.201095: step 267760, loss = 0.0632, acc = 0.9880 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 20:34:54.765256: step 267780, loss = 0.0766, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 20:34:59.394937: step 267800, loss = 0.0768, acc = 0.9840 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 20:35:04.052540: step 267820, loss = 0.0717, acc = 0.9820 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 20:35:08.629212: step 267840, loss = 0.0763, acc = 0.9860 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 20:35:13.115976: step 267860, loss = 0.0815, acc = 0.9740 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 20:35:17.922177: step 267880, loss = 0.0938, acc = 0.9740 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:35:22.585766: step 267900, loss = 0.0786, acc = 0.9820 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:35:27.168543: step 267920, loss = 0.0861, acc = 0.9780 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 20:35:31.971652: step 267940, loss = 0.0744, acc = 0.9880 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 20:35:36.664070: step 267960, loss = 0.0774, acc = 0.9740 (259.8 examples/sec; 0.246 sec/batch)
2017-05-09 20:35:41.298251: step 267980, loss = 0.0812, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 20:35:45.966536: step 268000, loss = 0.0893, acc = 0.9800 (247.0 examples/sec; 0.259 sec/batch)
[Eval] 2017-05-09 20:35:59.648696: step 268000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 20:36:09.140197: step 268000, acc = 0.9547, f1 = 0.9543
[Status] 2017-05-09 20:36:09.140263: step 268000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:36:13.600858: step 268020, loss = 0.0758, acc = 0.9840 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 20:36:18.334234: step 268040, loss = 0.0871, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:36:22.833845: step 268060, loss = 0.0780, acc = 0.9800 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 20:36:27.402866: step 268080, loss = 0.0605, acc = 0.9920 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 20:36:32.174846: step 268100, loss = 0.0792, acc = 0.9820 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 20:36:36.903675: step 268120, loss = 0.1068, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 20:36:42.156228: step 268140, loss = 0.0762, acc = 0.9820 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 20:36:47.018901: step 268160, loss = 0.0626, acc = 0.9920 (242.5 examples/sec; 0.264 sec/batch)
2017-05-09 20:36:51.405234: step 268180, loss = 0.0731, acc = 0.9880 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 20:36:55.865782: step 268200, loss = 0.0969, acc = 0.9740 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 20:37:00.540848: step 268220, loss = 0.0841, acc = 0.9800 (241.2 examples/sec; 0.265 sec/batch)
2017-05-09 20:37:05.038417: step 268240, loss = 0.0636, acc = 0.9960 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 20:37:09.575889: step 268260, loss = 0.0721, acc = 0.9840 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 20:37:14.105754: step 268280, loss = 0.0938, acc = 0.9820 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 20:37:18.809082: step 268300, loss = 0.0802, acc = 0.9780 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 20:37:23.377452: step 268320, loss = 0.0677, acc = 0.9880 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 20:37:27.974985: step 268340, loss = 0.0736, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 20:37:32.628502: step 268360, loss = 0.0912, acc = 0.9800 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 20:37:37.136757: step 268380, loss = 0.0809, acc = 0.9840 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 20:37:41.695246: step 268400, loss = 0.0931, acc = 0.9700 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 20:37:46.336995: step 268420, loss = 0.0865, acc = 0.9800 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 20:37:50.877268: step 268440, loss = 0.0795, acc = 0.9740 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 20:37:55.546221: step 268460, loss = 0.0868, acc = 0.9780 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 20:38:00.162848: step 268480, loss = 0.0864, acc = 0.9800 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 20:38:04.733726: step 268500, loss = 0.0788, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 20:38:09.303745: step 268520, loss = 0.0705, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 20:38:14.009270: step 268540, loss = 0.0868, acc = 0.9800 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 20:38:18.852503: step 268560, loss = 0.0786, acc = 0.9880 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 20:38:23.365364: step 268580, loss = 0.0901, acc = 0.9780 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 20:38:27.936130: step 268600, loss = 0.0746, acc = 0.9860 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 20:38:32.860575: step 268620, loss = 0.0653, acc = 0.9900 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 20:38:37.305292: step 268640, loss = 0.0771, acc = 0.9800 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 20:38:41.900301: step 268660, loss = 0.0685, acc = 0.9840 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 20:38:46.616950: step 268680, loss = 0.0880, acc = 0.9780 (260.9 examples/sec; 0.245 sec/batch)
2017-05-09 20:38:51.172981: step 268700, loss = 0.0834, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 20:38:55.944973: step 268720, loss = 0.0716, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 20:39:00.602983: step 268740, loss = 0.0894, acc = 0.9760 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 20:39:05.065660: step 268760, loss = 0.0703, acc = 0.9860 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 20:39:09.547402: step 268780, loss = 0.0718, acc = 0.9860 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 20:39:14.187992: step 268800, loss = 0.0649, acc = 0.9900 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 20:39:18.703625: step 268820, loss = 0.0688, acc = 0.9860 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 20:39:23.282088: step 268840, loss = 0.0847, acc = 0.9800 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 20:39:27.995141: step 268860, loss = 0.0713, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 20:39:32.679938: step 268880, loss = 0.0832, acc = 0.9800 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 20:39:37.258644: step 268900, loss = 0.1033, acc = 0.9680 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 20:39:42.020256: step 268920, loss = 0.0808, acc = 0.9820 (248.3 examples/sec; 0.258 sec/batch)
2017-05-09 20:39:46.681156: step 268940, loss = 0.0713, acc = 0.9820 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 20:39:51.321645: step 268960, loss = 0.0844, acc = 0.9780 (259.2 examples/sec; 0.247 sec/batch)
2017-05-09 20:39:55.994643: step 268980, loss = 0.0996, acc = 0.9740 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 20:40:00.883443: step 269000, loss = 0.0854, acc = 0.9740 (262.7 examples/sec; 0.244 sec/batch)
[Eval] 2017-05-09 20:40:14.912525: step 269000, acc = 0.9614, f1 = 0.9603
[Test] 2017-05-09 20:40:24.431555: step 269000, acc = 0.9524, f1 = 0.9520
[Status] 2017-05-09 20:40:24.431656: step 269000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:40:29.138423: step 269020, loss = 0.1090, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 20:40:33.759000: step 269040, loss = 0.0790, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 20:40:38.378228: step 269060, loss = 0.0839, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 20:40:43.372694: step 269080, loss = 0.0851, acc = 0.9860 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 20:40:48.077829: step 269100, loss = 0.0829, acc = 0.9780 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 20:40:52.840717: step 269120, loss = 0.0876, acc = 0.9820 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 20:40:58.917393: step 269140, loss = 0.0666, acc = 0.9900 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:41:03.477400: step 269160, loss = 0.0650, acc = 0.9820 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 20:41:08.032363: step 269180, loss = 0.0884, acc = 0.9820 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 20:41:12.741516: step 269200, loss = 0.0755, acc = 0.9840 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:41:17.431203: step 269220, loss = 0.0799, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 20:41:22.162600: step 269240, loss = 0.0893, acc = 0.9760 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 20:41:26.904905: step 269260, loss = 0.0716, acc = 0.9820 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 20:41:31.448031: step 269280, loss = 0.0836, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:41:36.024853: step 269300, loss = 0.0595, acc = 0.9900 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 20:41:40.704274: step 269320, loss = 0.0769, acc = 0.9860 (225.6 examples/sec; 0.284 sec/batch)
2017-05-09 20:41:45.295429: step 269340, loss = 0.1002, acc = 0.9800 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 20:41:49.947128: step 269360, loss = 0.1017, acc = 0.9700 (256.3 examples/sec; 0.250 sec/batch)
2017-05-09 20:41:54.423474: step 269380, loss = 0.0695, acc = 0.9860 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 20:41:59.061712: step 269400, loss = 0.0860, acc = 0.9800 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 20:42:03.689437: step 269420, loss = 0.0812, acc = 0.9780 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 20:42:08.271619: step 269440, loss = 0.0775, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 20:42:12.982815: step 269460, loss = 0.0749, acc = 0.9780 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 20:42:17.471787: step 269480, loss = 0.0939, acc = 0.9760 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 20:42:22.049100: step 269500, loss = 0.0727, acc = 0.9760 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 20:42:26.665258: step 269520, loss = 0.0731, acc = 0.9880 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 20:42:31.277144: step 269540, loss = 0.0654, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 20:42:35.989984: step 269560, loss = 0.0815, acc = 0.9720 (250.7 examples/sec; 0.255 sec/batch)
2017-05-09 20:42:40.767138: step 269580, loss = 0.0669, acc = 0.9880 (257.9 examples/sec; 0.248 sec/batch)
2017-05-09 20:42:45.230586: step 269600, loss = 0.0746, acc = 0.9840 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 20:42:49.791980: step 269620, loss = 0.0727, acc = 0.9820 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:42:54.467041: step 269640, loss = 0.0771, acc = 0.9840 (253.3 examples/sec; 0.253 sec/batch)
2017-05-09 20:42:59.043191: step 269660, loss = 0.0762, acc = 0.9840 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:43:03.639296: step 269680, loss = 0.1037, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 20:43:08.173371: step 269700, loss = 0.0730, acc = 0.9820 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 20:43:12.953876: step 269720, loss = 0.0676, acc = 0.9820 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 20:43:17.518808: step 269740, loss = 0.0750, acc = 0.9800 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 20:43:22.064627: step 269760, loss = 0.0652, acc = 0.9820 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 20:43:26.647118: step 269780, loss = 0.0821, acc = 0.9800 (299.4 examples/sec; 0.214 sec/batch)
2017-05-09 20:43:31.220275: step 269800, loss = 0.0841, acc = 0.9740 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 20:43:35.710177: step 269820, loss = 0.0907, acc = 0.9780 (296.6 examples/sec; 0.216 sec/batch)
2017-05-09 20:43:40.423924: step 269840, loss = 0.0834, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 20:43:45.154112: step 269860, loss = 0.0625, acc = 0.9920 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 20:43:49.787523: step 269880, loss = 0.0827, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 20:43:54.393650: step 269900, loss = 0.0674, acc = 0.9840 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 20:43:59.187216: step 269920, loss = 0.0801, acc = 0.9820 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 20:44:03.753781: step 269940, loss = 0.0680, acc = 0.9840 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 20:44:08.399633: step 269960, loss = 0.0671, acc = 0.9840 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 20:44:13.020054: step 269980, loss = 0.0683, acc = 0.9900 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 20:44:17.566257: step 270000, loss = 0.0826, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 20:44:31.675079: step 270000, acc = 0.9645, f1 = 0.9634
[Test] 2017-05-09 20:44:41.416876: step 270000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 20:44:41.416968: step 270000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:44:46.192586: step 270020, loss = 0.0699, acc = 0.9860 (242.2 examples/sec; 0.264 sec/batch)
2017-05-09 20:44:50.735616: step 270040, loss = 0.0887, acc = 0.9820 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 20:44:55.566123: step 270060, loss = 0.0592, acc = 0.9880 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 20:45:00.114036: step 270080, loss = 0.0802, acc = 0.9840 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 20:45:04.592872: step 270100, loss = 0.0687, acc = 0.9860 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 20:45:09.146654: step 270120, loss = 0.0927, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 20:45:14.354956: step 270140, loss = 0.0707, acc = 0.9900 (155.9 examples/sec; 0.411 sec/batch)
2017-05-09 20:45:18.961596: step 270160, loss = 0.0824, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 20:45:23.840555: step 270180, loss = 0.0739, acc = 0.9840 (227.7 examples/sec; 0.281 sec/batch)
2017-05-09 20:45:28.715364: step 270200, loss = 0.0826, acc = 0.9840 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 20:45:33.239278: step 270220, loss = 0.0744, acc = 0.9820 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 20:45:37.872479: step 270240, loss = 0.0868, acc = 0.9800 (249.5 examples/sec; 0.256 sec/batch)
2017-05-09 20:45:42.450526: step 270260, loss = 0.0799, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 20:45:46.922664: step 270280, loss = 0.0914, acc = 0.9820 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 20:45:51.621288: step 270300, loss = 0.0859, acc = 0.9780 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 20:45:56.120234: step 270320, loss = 0.0834, acc = 0.9800 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 20:46:00.652461: step 270340, loss = 0.0876, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 20:46:05.208631: step 270360, loss = 0.0845, acc = 0.9700 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 20:46:09.870064: step 270380, loss = 0.0835, acc = 0.9680 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 20:46:14.453235: step 270400, loss = 0.0786, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 20:46:18.983871: step 270420, loss = 0.0746, acc = 0.9840 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 20:46:23.674440: step 270440, loss = 0.0877, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 20:46:28.255843: step 270460, loss = 0.0670, acc = 0.9820 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 20:46:32.873545: step 270480, loss = 0.0887, acc = 0.9700 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 20:46:37.797713: step 270500, loss = 0.1016, acc = 0.9680 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 20:46:42.354411: step 270520, loss = 0.0731, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 20:46:46.929352: step 270540, loss = 0.0693, acc = 0.9800 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 20:46:51.816959: step 270560, loss = 0.0956, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 20:46:56.327245: step 270580, loss = 0.0611, acc = 0.9840 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 20:47:00.982267: step 270600, loss = 0.1070, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 20:47:05.688041: step 270620, loss = 0.0769, acc = 0.9820 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 20:47:10.142816: step 270640, loss = 0.0614, acc = 0.9880 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:47:14.839311: step 270660, loss = 0.0735, acc = 0.9860 (245.5 examples/sec; 0.261 sec/batch)
2017-05-09 20:47:19.434537: step 270680, loss = 0.0753, acc = 0.9800 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 20:47:24.166181: step 270700, loss = 0.0653, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 20:47:28.705169: step 270720, loss = 0.0742, acc = 0.9840 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 20:47:33.669943: step 270740, loss = 0.0825, acc = 0.9740 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 20:47:38.207668: step 270760, loss = 0.0654, acc = 0.9880 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 20:47:42.878579: step 270780, loss = 0.0783, acc = 0.9860 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 20:47:47.416699: step 270800, loss = 0.0929, acc = 0.9800 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 20:47:52.116774: step 270820, loss = 0.0888, acc = 0.9740 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 20:47:56.677646: step 270840, loss = 0.0733, acc = 0.9880 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 20:48:01.250602: step 270860, loss = 0.0753, acc = 0.9860 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 20:48:06.124433: step 270880, loss = 0.0816, acc = 0.9720 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 20:48:10.729245: step 270900, loss = 0.0827, acc = 0.9840 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 20:48:15.331445: step 270920, loss = 0.0849, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 20:48:20.090138: step 270940, loss = 0.0689, acc = 0.9800 (248.8 examples/sec; 0.257 sec/batch)
2017-05-09 20:48:24.720559: step 270960, loss = 0.0804, acc = 0.9800 (263.1 examples/sec; 0.243 sec/batch)
2017-05-09 20:48:29.419872: step 270980, loss = 0.0814, acc = 0.9780 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 20:48:34.010739: step 271000, loss = 0.0882, acc = 0.9780 (288.6 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 20:48:48.040926: step 271000, acc = 0.9645, f1 = 0.9634
[Test] 2017-05-09 20:48:57.840698: step 271000, acc = 0.9567, f1 = 0.9564
[Status] 2017-05-09 20:48:57.840783: step 271000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:49:02.498282: step 271020, loss = 0.0890, acc = 0.9840 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 20:49:07.498772: step 271040, loss = 0.0697, acc = 0.9840 (259.1 examples/sec; 0.247 sec/batch)
2017-05-09 20:49:12.085069: step 271060, loss = 0.0882, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 20:49:16.629688: step 271080, loss = 0.0757, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 20:49:21.276123: step 271100, loss = 0.0914, acc = 0.9740 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 20:49:25.950602: step 271120, loss = 0.0652, acc = 0.9920 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 20:49:30.599255: step 271140, loss = 0.0700, acc = 0.9840 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 20:49:36.574776: step 271160, loss = 0.0933, acc = 0.9780 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 20:49:41.319692: step 271180, loss = 0.0696, acc = 0.9860 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 20:49:46.022486: step 271200, loss = 0.0850, acc = 0.9840 (262.2 examples/sec; 0.244 sec/batch)
2017-05-09 20:49:50.707622: step 271220, loss = 0.0726, acc = 0.9860 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 20:49:55.257349: step 271240, loss = 0.0647, acc = 0.9880 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 20:49:59.903197: step 271260, loss = 0.0892, acc = 0.9840 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 20:50:04.655368: step 271280, loss = 0.0634, acc = 0.9840 (241.7 examples/sec; 0.265 sec/batch)
2017-05-09 20:50:09.287002: step 271300, loss = 0.0675, acc = 0.9860 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:50:14.047758: step 271320, loss = 0.0889, acc = 0.9740 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 20:50:18.679298: step 271340, loss = 0.0820, acc = 0.9740 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 20:50:23.285441: step 271360, loss = 0.0646, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 20:50:27.843487: step 271380, loss = 0.0751, acc = 0.9800 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:50:32.672793: step 271400, loss = 0.0907, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 20:50:37.434175: step 271420, loss = 0.0523, acc = 0.9960 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 20:50:42.084800: step 271440, loss = 0.0766, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 20:50:46.720878: step 271460, loss = 0.0817, acc = 0.9860 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 20:50:51.588323: step 271480, loss = 0.0816, acc = 0.9760 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 20:50:56.412399: step 271500, loss = 0.0915, acc = 0.9700 (227.2 examples/sec; 0.282 sec/batch)
2017-05-09 20:51:00.953739: step 271520, loss = 0.0687, acc = 0.9820 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:51:05.654824: step 271540, loss = 0.0920, acc = 0.9760 (237.8 examples/sec; 0.269 sec/batch)
2017-05-09 20:51:10.239274: step 271560, loss = 0.0672, acc = 0.9840 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 20:51:14.821233: step 271580, loss = 0.0774, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 20:51:19.508382: step 271600, loss = 0.0762, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:51:24.342847: step 271620, loss = 0.0962, acc = 0.9720 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 20:51:28.896988: step 271640, loss = 0.0650, acc = 0.9900 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 20:51:33.371041: step 271660, loss = 0.0835, acc = 0.9820 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 20:51:38.188133: step 271680, loss = 0.0605, acc = 0.9920 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 20:51:42.777447: step 271700, loss = 0.0639, acc = 0.9900 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 20:51:47.354850: step 271720, loss = 0.0898, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 20:51:52.027340: step 271740, loss = 0.0836, acc = 0.9800 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 20:51:56.712139: step 271760, loss = 0.1048, acc = 0.9700 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 20:52:01.309173: step 271780, loss = 0.0668, acc = 0.9820 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 20:52:05.838375: step 271800, loss = 0.0950, acc = 0.9680 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:52:10.530632: step 271820, loss = 0.1075, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 20:52:15.129305: step 271840, loss = 0.0757, acc = 0.9760 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 20:52:19.713044: step 271860, loss = 0.0634, acc = 0.9860 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 20:52:24.570619: step 271880, loss = 0.0713, acc = 0.9860 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 20:52:29.120233: step 271900, loss = 0.1029, acc = 0.9740 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 20:52:33.652217: step 271920, loss = 0.0825, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 20:52:38.515947: step 271940, loss = 0.0688, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 20:52:43.194193: step 271960, loss = 0.1056, acc = 0.9660 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 20:52:47.813425: step 271980, loss = 0.0980, acc = 0.9760 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 20:52:52.811993: step 272000, loss = 0.0772, acc = 0.9840 (213.4 examples/sec; 0.300 sec/batch)
[Eval] 2017-05-09 20:53:06.660000: step 272000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 20:53:16.251624: step 272000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 20:53:16.251708: step 272000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:53:20.898472: step 272020, loss = 0.0793, acc = 0.9800 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 20:53:25.799743: step 272040, loss = 0.0744, acc = 0.9920 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 20:53:30.496692: step 272060, loss = 0.1070, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 20:53:35.081153: step 272080, loss = 0.0740, acc = 0.9840 (261.1 examples/sec; 0.245 sec/batch)
2017-05-09 20:53:39.996494: step 272100, loss = 0.1153, acc = 0.9740 (246.7 examples/sec; 0.259 sec/batch)
2017-05-09 20:53:44.491498: step 272120, loss = 0.0788, acc = 0.9800 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 20:53:49.061022: step 272140, loss = 0.0625, acc = 0.9920 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 20:53:54.750937: step 272160, loss = 0.0674, acc = 0.9880 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 20:53:59.424710: step 272180, loss = 0.1113, acc = 0.9780 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 20:54:04.037779: step 272200, loss = 0.1008, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 20:54:08.906790: step 272220, loss = 0.0816, acc = 0.9780 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 20:54:13.396427: step 272240, loss = 0.0758, acc = 0.9780 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 20:54:18.008051: step 272260, loss = 0.0692, acc = 0.9820 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 20:54:22.914556: step 272280, loss = 0.0823, acc = 0.9820 (223.5 examples/sec; 0.286 sec/batch)
2017-05-09 20:54:27.383369: step 272300, loss = 0.0864, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 20:54:31.918483: step 272320, loss = 0.0752, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 20:54:36.462548: step 272340, loss = 0.0899, acc = 0.9700 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 20:54:41.143214: step 272360, loss = 0.0982, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:54:45.902990: step 272380, loss = 0.0731, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 20:54:50.569671: step 272400, loss = 0.0713, acc = 0.9860 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 20:54:55.283955: step 272420, loss = 0.0757, acc = 0.9840 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 20:54:59.974190: step 272440, loss = 0.0751, acc = 0.9840 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 20:55:04.603021: step 272460, loss = 0.0733, acc = 0.9800 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 20:55:09.323711: step 272480, loss = 0.0933, acc = 0.9760 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 20:55:13.908884: step 272500, loss = 0.0868, acc = 0.9680 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 20:55:18.467141: step 272520, loss = 0.0889, acc = 0.9680 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 20:55:23.417391: step 272540, loss = 0.0471, acc = 0.9940 (219.5 examples/sec; 0.292 sec/batch)
2017-05-09 20:55:28.136256: step 272560, loss = 0.0767, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 20:55:32.775889: step 272580, loss = 0.0796, acc = 0.9800 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 20:55:37.628624: step 272600, loss = 0.0663, acc = 0.9840 (222.4 examples/sec; 0.288 sec/batch)
2017-05-09 20:55:42.199386: step 272620, loss = 0.0714, acc = 0.9800 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 20:55:46.890586: step 272640, loss = 0.0800, acc = 0.9760 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 20:55:51.490069: step 272660, loss = 0.0748, acc = 0.9820 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 20:55:56.311677: step 272680, loss = 0.0538, acc = 0.9920 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 20:56:00.904695: step 272700, loss = 0.0762, acc = 0.9780 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 20:56:05.479193: step 272720, loss = 0.1112, acc = 0.9660 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 20:56:10.153859: step 272740, loss = 0.0968, acc = 0.9740 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 20:56:14.809323: step 272760, loss = 0.0792, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 20:56:19.385062: step 272780, loss = 0.0940, acc = 0.9720 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 20:56:24.148068: step 272800, loss = 0.1086, acc = 0.9700 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 20:56:28.719785: step 272820, loss = 0.0713, acc = 0.9880 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 20:56:33.324345: step 272840, loss = 0.0954, acc = 0.9740 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 20:56:37.970022: step 272860, loss = 0.0656, acc = 0.9840 (251.3 examples/sec; 0.255 sec/batch)
2017-05-09 20:56:42.734801: step 272880, loss = 0.0671, acc = 0.9820 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 20:56:47.309140: step 272900, loss = 0.0879, acc = 0.9720 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 20:56:51.879818: step 272920, loss = 0.0758, acc = 0.9860 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 20:56:56.669122: step 272940, loss = 0.0648, acc = 0.9940 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 20:57:01.213679: step 272960, loss = 0.0740, acc = 0.9840 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 20:57:05.747519: step 272980, loss = 0.1052, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 20:57:10.530699: step 273000, loss = 0.0578, acc = 0.9920 (292.2 examples/sec; 0.219 sec/batch)
[Eval] 2017-05-09 20:57:24.623046: step 273000, acc = 0.9602, f1 = 0.9588
[Test] 2017-05-09 20:57:33.881332: step 273000, acc = 0.9500, f1 = 0.9496
[Status] 2017-05-09 20:57:33.881422: step 273000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 20:57:38.639733: step 273020, loss = 0.0762, acc = 0.9800 (240.4 examples/sec; 0.266 sec/batch)
2017-05-09 20:57:43.230085: step 273040, loss = 0.1041, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 20:57:47.970995: step 273060, loss = 0.0830, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 20:57:52.616573: step 273080, loss = 0.0989, acc = 0.9680 (248.2 examples/sec; 0.258 sec/batch)
2017-05-09 20:57:57.346822: step 273100, loss = 0.0658, acc = 0.9880 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 20:58:01.890716: step 273120, loss = 0.0733, acc = 0.9780 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 20:58:06.467876: step 273140, loss = 0.0701, acc = 0.9900 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 20:58:11.226241: step 273160, loss = 0.0931, acc = 0.9720 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 20:58:16.961230: step 273180, loss = 0.0892, acc = 0.9720 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 20:58:21.567782: step 273200, loss = 0.0771, acc = 0.9800 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 20:58:26.380400: step 273220, loss = 0.0580, acc = 0.9920 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 20:58:30.911197: step 273240, loss = 0.0719, acc = 0.9800 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 20:58:35.448514: step 273260, loss = 0.0933, acc = 0.9800 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 20:58:40.154931: step 273280, loss = 0.0800, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 20:58:44.678089: step 273300, loss = 0.0683, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 20:58:49.213910: step 273320, loss = 0.0773, acc = 0.9780 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 20:58:54.052320: step 273340, loss = 0.0651, acc = 0.9820 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 20:58:58.685638: step 273360, loss = 0.0630, acc = 0.9920 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 20:59:03.199748: step 273380, loss = 0.0820, acc = 0.9820 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 20:59:07.701526: step 273400, loss = 0.0718, acc = 0.9840 (300.2 examples/sec; 0.213 sec/batch)
2017-05-09 20:59:12.483240: step 273420, loss = 0.0618, acc = 0.9860 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 20:59:17.107307: step 273440, loss = 0.0742, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 20:59:21.635342: step 273460, loss = 0.0747, acc = 0.9760 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 20:59:26.295452: step 273480, loss = 0.0734, acc = 0.9860 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 20:59:30.831998: step 273500, loss = 0.1044, acc = 0.9680 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 20:59:35.439130: step 273520, loss = 0.0696, acc = 0.9880 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 20:59:40.155803: step 273540, loss = 0.0878, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 20:59:44.785324: step 273560, loss = 0.0743, acc = 0.9740 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 20:59:49.649492: step 273580, loss = 0.0600, acc = 0.9820 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 20:59:54.341711: step 273600, loss = 0.0831, acc = 0.9780 (246.6 examples/sec; 0.260 sec/batch)
2017-05-09 20:59:58.916350: step 273620, loss = 0.0800, acc = 0.9740 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:00:03.478298: step 273640, loss = 0.0898, acc = 0.9680 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 21:00:08.003867: step 273660, loss = 0.0590, acc = 0.9900 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:00:12.761952: step 273680, loss = 0.0936, acc = 0.9760 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 21:00:17.369736: step 273700, loss = 0.0624, acc = 0.9880 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 21:00:22.159311: step 273720, loss = 0.0715, acc = 0.9820 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 21:00:27.026232: step 273740, loss = 0.0597, acc = 0.9880 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 21:00:31.586049: step 273760, loss = 0.0631, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 21:00:36.157660: step 273780, loss = 0.0679, acc = 0.9820 (266.3 examples/sec; 0.240 sec/batch)
2017-05-09 21:00:40.860868: step 273800, loss = 0.1195, acc = 0.9680 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 21:00:45.653036: step 273820, loss = 0.0794, acc = 0.9820 (253.0 examples/sec; 0.253 sec/batch)
2017-05-09 21:00:50.308493: step 273840, loss = 0.0825, acc = 0.9820 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:00:55.017314: step 273860, loss = 0.0929, acc = 0.9700 (250.6 examples/sec; 0.255 sec/batch)
2017-05-09 21:00:59.538770: step 273880, loss = 0.0630, acc = 0.9820 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 21:01:04.187008: step 273900, loss = 0.0746, acc = 0.9800 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 21:01:08.909251: step 273920, loss = 0.0618, acc = 0.9860 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 21:01:14.083387: step 273940, loss = 0.0853, acc = 0.9820 (208.7 examples/sec; 0.307 sec/batch)
2017-05-09 21:01:18.655465: step 273960, loss = 0.0723, acc = 0.9820 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 21:01:23.274629: step 273980, loss = 0.0681, acc = 0.9840 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 21:01:27.985600: step 274000, loss = 0.0845, acc = 0.9780 (273.6 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-09 21:01:42.077776: step 274000, acc = 0.9641, f1 = 0.9629
[Test] 2017-05-09 21:01:51.337873: step 274000, acc = 0.9554, f1 = 0.9550
[Status] 2017-05-09 21:01:51.337959: step 274000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:01:56.386644: step 274020, loss = 0.0862, acc = 0.9760 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 21:02:00.998987: step 274040, loss = 0.0685, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 21:02:05.629935: step 274060, loss = 0.0835, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 21:02:10.525831: step 274080, loss = 0.0744, acc = 0.9880 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 21:02:15.044424: step 274100, loss = 0.0690, acc = 0.9820 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 21:02:19.718364: step 274120, loss = 0.0823, acc = 0.9840 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 21:02:24.531417: step 274140, loss = 0.0926, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 21:02:29.090388: step 274160, loss = 0.0804, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:02:34.426015: step 274180, loss = 0.0741, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 21:02:39.287850: step 274200, loss = 0.0669, acc = 0.9860 (244.8 examples/sec; 0.261 sec/batch)
2017-05-09 21:02:43.809399: step 274220, loss = 0.0619, acc = 0.9860 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:02:48.432843: step 274240, loss = 0.0739, acc = 0.9860 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 21:02:52.979462: step 274260, loss = 0.0867, acc = 0.9860 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:02:57.765736: step 274280, loss = 0.1005, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:03:02.384483: step 274300, loss = 0.0983, acc = 0.9720 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 21:03:06.977605: step 274320, loss = 0.0765, acc = 0.9800 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 21:03:11.766977: step 274340, loss = 0.0741, acc = 0.9820 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 21:03:16.220208: step 274360, loss = 0.0888, acc = 0.9780 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 21:03:20.894226: step 274380, loss = 0.0666, acc = 0.9780 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 21:03:25.608450: step 274400, loss = 0.0749, acc = 0.9860 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 21:03:30.195412: step 274420, loss = 0.0762, acc = 0.9880 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 21:03:34.849326: step 274440, loss = 0.0788, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 21:03:39.659471: step 274460, loss = 0.0728, acc = 0.9800 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 21:03:44.294011: step 274480, loss = 0.0928, acc = 0.9760 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 21:03:48.945072: step 274500, loss = 0.0709, acc = 0.9820 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 21:03:53.796868: step 274520, loss = 0.0838, acc = 0.9800 (244.5 examples/sec; 0.262 sec/batch)
2017-05-09 21:03:58.336536: step 274540, loss = 0.0688, acc = 0.9900 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 21:04:03.099903: step 274560, loss = 0.0988, acc = 0.9800 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 21:04:07.731744: step 274580, loss = 0.0747, acc = 0.9840 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 21:04:12.483511: step 274600, loss = 0.0866, acc = 0.9800 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 21:04:17.130232: step 274620, loss = 0.0634, acc = 0.9880 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 21:04:21.618953: step 274640, loss = 0.0901, acc = 0.9760 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 21:04:26.503667: step 274660, loss = 0.1092, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 21:04:31.201997: step 274680, loss = 0.0960, acc = 0.9740 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 21:04:35.710070: step 274700, loss = 0.0669, acc = 0.9880 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 21:04:40.328089: step 274720, loss = 0.0720, acc = 0.9860 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 21:04:44.876670: step 274740, loss = 0.0842, acc = 0.9820 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:04:49.548303: step 274760, loss = 0.0688, acc = 0.9780 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 21:04:54.317488: step 274780, loss = 0.0731, acc = 0.9840 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 21:04:58.936387: step 274800, loss = 0.0633, acc = 0.9900 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:05:03.591356: step 274820, loss = 0.0828, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:05:08.282574: step 274840, loss = 0.0755, acc = 0.9800 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 21:05:12.812399: step 274860, loss = 0.0708, acc = 0.9840 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 21:05:17.373736: step 274880, loss = 0.0669, acc = 0.9900 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 21:05:21.931680: step 274900, loss = 0.0799, acc = 0.9840 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 21:05:26.794048: step 274920, loss = 0.0633, acc = 0.9880 (263.5 examples/sec; 0.243 sec/batch)
2017-05-09 21:05:31.397620: step 274940, loss = 0.0751, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 21:05:35.897638: step 274960, loss = 0.0766, acc = 0.9840 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 21:05:40.551685: step 274980, loss = 0.0756, acc = 0.9800 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 21:05:45.139640: step 275000, loss = 0.0796, acc = 0.9860 (250.7 examples/sec; 0.255 sec/batch)
[Eval] 2017-05-09 21:05:59.152484: step 275000, acc = 0.9649, f1 = 0.9638
[Test] 2017-05-09 21:06:08.823615: step 275000, acc = 0.9565, f1 = 0.9561
[Status] 2017-05-09 21:06:08.823704: step 275000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:06:13.533352: step 275020, loss = 0.1031, acc = 0.9640 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 21:06:18.139921: step 275040, loss = 0.1123, acc = 0.9740 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 21:06:22.884477: step 275060, loss = 0.0604, acc = 0.9860 (243.6 examples/sec; 0.263 sec/batch)
2017-05-09 21:06:27.448351: step 275080, loss = 0.0747, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 21:06:32.002940: step 275100, loss = 0.0786, acc = 0.9740 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 21:06:36.744069: step 275120, loss = 0.0803, acc = 0.9800 (255.9 examples/sec; 0.250 sec/batch)
2017-05-09 21:06:41.590647: step 275140, loss = 0.0815, acc = 0.9820 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 21:06:46.410766: step 275160, loss = 0.0819, acc = 0.9820 (264.9 examples/sec; 0.242 sec/batch)
2017-05-09 21:06:52.158323: step 275180, loss = 0.0629, acc = 0.9900 (126.6 examples/sec; 0.506 sec/batch)
2017-05-09 21:06:56.845229: step 275200, loss = 0.0769, acc = 0.9820 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:07:01.443936: step 275220, loss = 0.0729, acc = 0.9840 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 21:07:06.006800: step 275240, loss = 0.0752, acc = 0.9780 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:07:10.633489: step 275260, loss = 0.0835, acc = 0.9740 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 21:07:15.281465: step 275280, loss = 0.0754, acc = 0.9780 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 21:07:20.005007: step 275300, loss = 0.0865, acc = 0.9720 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 21:07:24.920420: step 275320, loss = 0.0818, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 21:07:29.614504: step 275340, loss = 0.0654, acc = 0.9860 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 21:07:34.176941: step 275360, loss = 0.0618, acc = 0.9860 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 21:07:39.060931: step 275380, loss = 0.0826, acc = 0.9800 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 21:07:43.683730: step 275400, loss = 0.0642, acc = 0.9860 (262.3 examples/sec; 0.244 sec/batch)
2017-05-09 21:07:48.296034: step 275420, loss = 0.0752, acc = 0.9840 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 21:07:53.075489: step 275440, loss = 0.0676, acc = 0.9900 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 21:07:57.749025: step 275460, loss = 0.0882, acc = 0.9740 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 21:08:02.677412: step 275480, loss = 0.0657, acc = 0.9820 (207.9 examples/sec; 0.308 sec/batch)
2017-05-09 21:08:07.509893: step 275500, loss = 0.0844, acc = 0.9840 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 21:08:12.100135: step 275520, loss = 0.0695, acc = 0.9860 (254.5 examples/sec; 0.251 sec/batch)
2017-05-09 21:08:16.679471: step 275540, loss = 0.0956, acc = 0.9840 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 21:08:21.305507: step 275560, loss = 0.0929, acc = 0.9680 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 21:08:26.015016: step 275580, loss = 0.0914, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:08:30.765666: step 275600, loss = 0.0928, acc = 0.9900 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 21:08:35.360605: step 275620, loss = 0.1156, acc = 0.9680 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 21:08:40.292359: step 275640, loss = 0.0749, acc = 0.9820 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 21:08:44.894815: step 275660, loss = 0.0803, acc = 0.9800 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:08:49.631301: step 275680, loss = 0.0626, acc = 0.9920 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 21:08:54.871967: step 275700, loss = 0.0928, acc = 0.9760 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 21:08:59.568647: step 275720, loss = 0.0726, acc = 0.9840 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 21:09:04.207937: step 275740, loss = 0.0641, acc = 0.9820 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:09:08.838781: step 275760, loss = 0.0989, acc = 0.9760 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 21:09:13.481376: step 275780, loss = 0.0775, acc = 0.9820 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 21:09:18.105478: step 275800, loss = 0.0826, acc = 0.9840 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 21:09:22.863128: step 275820, loss = 0.0789, acc = 0.9840 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 21:09:27.531926: step 275840, loss = 0.0720, acc = 0.9860 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 21:09:32.150749: step 275860, loss = 0.0720, acc = 0.9800 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 21:09:36.829404: step 275880, loss = 0.0940, acc = 0.9720 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 21:09:41.435435: step 275900, loss = 0.0683, acc = 0.9880 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 21:09:46.198340: step 275920, loss = 0.0769, acc = 0.9860 (287.6 examples/sec; 0.222 sec/batch)
2017-05-09 21:09:51.103879: step 275940, loss = 0.0897, acc = 0.9760 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 21:09:55.815306: step 275960, loss = 0.0612, acc = 0.9880 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 21:10:00.471819: step 275980, loss = 0.0829, acc = 0.9820 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 21:10:05.238921: step 276000, loss = 0.0634, acc = 0.9880 (281.7 examples/sec; 0.227 sec/batch)
[Eval] 2017-05-09 21:10:19.280335: step 276000, acc = 0.9644, f1 = 0.9633
[Test] 2017-05-09 21:10:28.596742: step 276000, acc = 0.9560, f1 = 0.9557
[Status] 2017-05-09 21:10:28.596819: step 276000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:10:33.223024: step 276020, loss = 0.0901, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 21:10:38.176256: step 276040, loss = 0.0931, acc = 0.9700 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:10:42.790643: step 276060, loss = 0.0767, acc = 0.9840 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 21:10:47.539706: step 276080, loss = 0.0841, acc = 0.9780 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 21:10:52.284324: step 276100, loss = 0.0758, acc = 0.9840 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 21:10:56.939725: step 276120, loss = 0.0930, acc = 0.9780 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 21:11:01.525623: step 276140, loss = 0.0944, acc = 0.9820 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 21:11:06.229312: step 276160, loss = 0.0890, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 21:11:10.854653: step 276180, loss = 0.0742, acc = 0.9820 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 21:11:16.129525: step 276200, loss = 0.0728, acc = 0.9820 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 21:11:20.820790: step 276220, loss = 0.0616, acc = 0.9880 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 21:11:25.349316: step 276240, loss = 0.0974, acc = 0.9740 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 21:11:29.793704: step 276260, loss = 0.0662, acc = 0.9900 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 21:11:34.430903: step 276280, loss = 0.0918, acc = 0.9620 (244.7 examples/sec; 0.262 sec/batch)
2017-05-09 21:11:39.054490: step 276300, loss = 0.0729, acc = 0.9820 (256.7 examples/sec; 0.249 sec/batch)
2017-05-09 21:11:43.714630: step 276320, loss = 0.0838, acc = 0.9760 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:11:48.385554: step 276340, loss = 0.0754, acc = 0.9880 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 21:11:53.102422: step 276360, loss = 0.0665, acc = 0.9860 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 21:11:57.801753: step 276380, loss = 0.0707, acc = 0.9860 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 21:12:02.467002: step 276400, loss = 0.0748, acc = 0.9860 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 21:12:07.287902: step 276420, loss = 0.0698, acc = 0.9860 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 21:12:11.870293: step 276440, loss = 0.0687, acc = 0.9880 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 21:12:16.453243: step 276460, loss = 0.0727, acc = 0.9820 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 21:12:21.227517: step 276480, loss = 0.0659, acc = 0.9900 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 21:12:25.920718: step 276500, loss = 0.0721, acc = 0.9880 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 21:12:30.505284: step 276520, loss = 0.0843, acc = 0.9820 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 21:12:35.107209: step 276540, loss = 0.0921, acc = 0.9740 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 21:12:39.885704: step 276560, loss = 0.0675, acc = 0.9840 (252.4 examples/sec; 0.254 sec/batch)
2017-05-09 21:12:44.474266: step 276580, loss = 0.0687, acc = 0.9880 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 21:12:49.028524: step 276600, loss = 0.0996, acc = 0.9740 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 21:12:53.985641: step 276620, loss = 0.0775, acc = 0.9840 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 21:12:58.577823: step 276640, loss = 0.0729, acc = 0.9880 (254.0 examples/sec; 0.252 sec/batch)
2017-05-09 21:13:03.115373: step 276660, loss = 0.0725, acc = 0.9860 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 21:13:07.902578: step 276680, loss = 0.0653, acc = 0.9900 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 21:13:12.454570: step 276700, loss = 0.0610, acc = 0.9880 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 21:13:17.132056: step 276720, loss = 0.1011, acc = 0.9820 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 21:13:22.275440: step 276740, loss = 0.0819, acc = 0.9840 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 21:13:26.885955: step 276760, loss = 0.0929, acc = 0.9680 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 21:13:31.483779: step 276780, loss = 0.0699, acc = 0.9840 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 21:13:36.122358: step 276800, loss = 0.0713, acc = 0.9800 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 21:13:40.594833: step 276820, loss = 0.0700, acc = 0.9800 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 21:13:45.104510: step 276840, loss = 0.1144, acc = 0.9600 (276.5 examples/sec; 0.232 sec/batch)
2017-05-09 21:13:49.759035: step 276860, loss = 0.0569, acc = 0.9900 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 21:13:54.293256: step 276880, loss = 0.1039, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 21:13:58.821989: step 276900, loss = 0.0687, acc = 0.9900 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 21:14:03.320958: step 276920, loss = 0.0844, acc = 0.9800 (296.8 examples/sec; 0.216 sec/batch)
2017-05-09 21:14:08.269046: step 276940, loss = 0.1050, acc = 0.9680 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 21:14:12.790486: step 276960, loss = 0.0748, acc = 0.9840 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 21:14:17.261446: step 276980, loss = 0.0894, acc = 0.9820 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 21:14:21.840617: step 277000, loss = 0.0701, acc = 0.9900 (284.0 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 21:14:35.903744: step 277000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 21:14:45.163599: step 277000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 21:14:45.163680: step 277000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:14:49.949151: step 277020, loss = 0.0835, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 21:14:54.655206: step 277040, loss = 0.0719, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 21:14:59.312131: step 277060, loss = 0.0672, acc = 0.9880 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 21:15:04.244898: step 277080, loss = 0.0829, acc = 0.9840 (253.8 examples/sec; 0.252 sec/batch)
2017-05-09 21:15:08.801293: step 277100, loss = 0.0748, acc = 0.9860 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 21:15:13.317340: step 277120, loss = 0.0856, acc = 0.9760 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:15:17.997315: step 277140, loss = 0.0611, acc = 0.9960 (247.1 examples/sec; 0.259 sec/batch)
2017-05-09 21:15:23.047171: step 277160, loss = 0.0796, acc = 0.9780 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 21:15:27.574760: step 277180, loss = 0.0754, acc = 0.9820 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 21:15:33.084684: step 277200, loss = 0.0576, acc = 0.9920 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 21:15:37.649575: step 277220, loss = 0.0624, acc = 0.9900 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 21:15:42.431745: step 277240, loss = 0.0719, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 21:15:46.973482: step 277260, loss = 0.0899, acc = 0.9820 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:15:51.675277: step 277280, loss = 0.0596, acc = 0.9940 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 21:15:56.227113: step 277300, loss = 0.0872, acc = 0.9760 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 21:16:00.798766: step 277320, loss = 0.0634, acc = 0.9880 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 21:16:05.662687: step 277340, loss = 0.0805, acc = 0.9760 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 21:16:10.233198: step 277360, loss = 0.0764, acc = 0.9820 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 21:16:15.002458: step 277380, loss = 0.0675, acc = 0.9860 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 21:16:19.710616: step 277400, loss = 0.0766, acc = 0.9720 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 21:16:24.412631: step 277420, loss = 0.1032, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 21:16:29.010096: step 277440, loss = 0.0749, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 21:16:34.030100: step 277460, loss = 0.0814, acc = 0.9820 (184.1 examples/sec; 0.348 sec/batch)
2017-05-09 21:16:38.595594: step 277480, loss = 0.0879, acc = 0.9800 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 21:16:43.246805: step 277500, loss = 0.0845, acc = 0.9800 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 21:16:47.845834: step 277520, loss = 0.0891, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 21:16:52.585258: step 277540, loss = 0.0634, acc = 0.9860 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 21:16:57.086667: step 277560, loss = 0.0951, acc = 0.9680 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 21:17:01.619115: step 277580, loss = 0.0743, acc = 0.9840 (296.3 examples/sec; 0.216 sec/batch)
2017-05-09 21:17:06.328034: step 277600, loss = 0.0920, acc = 0.9840 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 21:17:10.927337: step 277620, loss = 0.0636, acc = 0.9900 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 21:17:15.588567: step 277640, loss = 0.0837, acc = 0.9740 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 21:17:20.239986: step 277660, loss = 0.0820, acc = 0.9920 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:17:24.869789: step 277680, loss = 0.0674, acc = 0.9880 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 21:17:29.454410: step 277700, loss = 0.0762, acc = 0.9900 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 21:17:34.126388: step 277720, loss = 0.0858, acc = 0.9760 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 21:17:38.714679: step 277740, loss = 0.0937, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 21:17:43.390804: step 277760, loss = 0.0688, acc = 0.9780 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 21:17:48.188807: step 277780, loss = 0.0771, acc = 0.9800 (232.9 examples/sec; 0.275 sec/batch)
2017-05-09 21:17:52.805746: step 277800, loss = 0.0753, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:17:57.398529: step 277820, loss = 0.0733, acc = 0.9860 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 21:18:02.139847: step 277840, loss = 0.0691, acc = 0.9820 (242.9 examples/sec; 0.263 sec/batch)
2017-05-09 21:18:06.630104: step 277860, loss = 0.0824, acc = 0.9860 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 21:18:11.146211: step 277880, loss = 0.0732, acc = 0.9820 (261.2 examples/sec; 0.245 sec/batch)
2017-05-09 21:18:15.800383: step 277900, loss = 0.0725, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 21:18:20.438997: step 277920, loss = 0.0894, acc = 0.9820 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 21:18:24.971902: step 277940, loss = 0.0544, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 21:18:29.617666: step 277960, loss = 0.0895, acc = 0.9840 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 21:18:34.399088: step 277980, loss = 0.0777, acc = 0.9820 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 21:18:38.992747: step 278000, loss = 0.0953, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-09 21:18:53.045519: step 278000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 21:19:02.430583: step 278000, acc = 0.9558, f1 = 0.9555
[Status] 2017-05-09 21:19:02.430796: step 278000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:19:07.284691: step 278020, loss = 0.1075, acc = 0.9680 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 21:19:11.895354: step 278040, loss = 0.0693, acc = 0.9860 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 21:19:16.567107: step 278060, loss = 0.0845, acc = 0.9800 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 21:19:21.333795: step 278080, loss = 0.0774, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:19:25.919342: step 278100, loss = 0.1011, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 21:19:30.540128: step 278120, loss = 0.0750, acc = 0.9820 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 21:19:35.248712: step 278140, loss = 0.1022, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 21:19:39.724444: step 278160, loss = 0.0678, acc = 0.9900 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 21:19:44.685364: step 278180, loss = 0.0685, acc = 0.9860 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 21:19:49.353188: step 278200, loss = 0.0786, acc = 0.9860 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 21:19:54.744305: step 278220, loss = 0.0751, acc = 0.9760 (257.5 examples/sec; 0.249 sec/batch)
2017-05-09 21:19:59.385868: step 278240, loss = 0.0984, acc = 0.9780 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 21:20:04.273041: step 278260, loss = 0.0919, acc = 0.9780 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 21:20:08.850619: step 278280, loss = 0.0862, acc = 0.9800 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 21:20:13.303724: step 278300, loss = 0.0834, acc = 0.9740 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 21:20:18.007290: step 278320, loss = 0.0704, acc = 0.9860 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:20:22.647088: step 278340, loss = 0.0728, acc = 0.9840 (301.1 examples/sec; 0.213 sec/batch)
2017-05-09 21:20:27.239314: step 278360, loss = 0.0874, acc = 0.9720 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 21:20:31.942922: step 278380, loss = 0.0599, acc = 0.9840 (251.2 examples/sec; 0.255 sec/batch)
2017-05-09 21:20:36.430406: step 278400, loss = 0.0588, acc = 0.9920 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 21:20:41.062751: step 278420, loss = 0.0702, acc = 0.9840 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 21:20:45.634562: step 278440, loss = 0.0904, acc = 0.9720 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 21:20:50.280311: step 278460, loss = 0.0855, acc = 0.9720 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 21:20:54.796739: step 278480, loss = 0.0729, acc = 0.9800 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 21:20:59.426995: step 278500, loss = 0.1019, acc = 0.9680 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 21:21:04.041738: step 278520, loss = 0.0945, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 21:21:08.497644: step 278540, loss = 0.1021, acc = 0.9660 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 21:21:13.028020: step 278560, loss = 0.0612, acc = 0.9880 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 21:21:17.847668: step 278580, loss = 0.0695, acc = 0.9820 (247.6 examples/sec; 0.258 sec/batch)
2017-05-09 21:21:22.362336: step 278600, loss = 0.0934, acc = 0.9700 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 21:21:26.920122: step 278620, loss = 0.1049, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 21:21:31.611017: step 278640, loss = 0.0630, acc = 0.9840 (247.1 examples/sec; 0.259 sec/batch)
2017-05-09 21:21:36.146378: step 278660, loss = 0.0737, acc = 0.9840 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 21:21:40.602711: step 278680, loss = 0.0891, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 21:21:45.350087: step 278700, loss = 0.0711, acc = 0.9860 (227.9 examples/sec; 0.281 sec/batch)
2017-05-09 21:21:49.952460: step 278720, loss = 0.1025, acc = 0.9700 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 21:21:54.596199: step 278740, loss = 0.0705, acc = 0.9840 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 21:21:59.272297: step 278760, loss = 0.0691, acc = 0.9880 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 21:22:04.112287: step 278780, loss = 0.0678, acc = 0.9900 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:22:08.646237: step 278800, loss = 0.0568, acc = 0.9880 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 21:22:13.189292: step 278820, loss = 0.0811, acc = 0.9820 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 21:22:18.008360: step 278840, loss = 0.0841, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 21:22:22.660714: step 278860, loss = 0.0735, acc = 0.9820 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 21:22:27.268999: step 278880, loss = 0.0570, acc = 0.9960 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:22:32.035134: step 278900, loss = 0.0910, acc = 0.9740 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 21:22:36.702537: step 278920, loss = 0.0558, acc = 0.9960 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 21:22:41.277496: step 278940, loss = 0.0866, acc = 0.9680 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 21:22:45.948873: step 278960, loss = 0.0714, acc = 0.9860 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 21:22:50.605907: step 278980, loss = 0.0711, acc = 0.9820 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 21:22:55.151239: step 279000, loss = 0.0690, acc = 0.9800 (285.8 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 21:23:09.329267: step 279000, acc = 0.9616, f1 = 0.9605
[Test] 2017-05-09 21:23:18.963662: step 279000, acc = 0.9530, f1 = 0.9526
[Status] 2017-05-09 21:23:18.963744: step 279000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:23:23.444879: step 279020, loss = 0.0944, acc = 0.9740 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 21:23:28.175748: step 279040, loss = 0.0704, acc = 0.9820 (242.4 examples/sec; 0.264 sec/batch)
2017-05-09 21:23:32.785067: step 279060, loss = 0.0793, acc = 0.9780 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 21:23:37.430143: step 279080, loss = 0.0941, acc = 0.9740 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 21:23:42.020603: step 279100, loss = 0.1091, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:23:46.777028: step 279120, loss = 0.0883, acc = 0.9820 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 21:23:51.343899: step 279140, loss = 0.0894, acc = 0.9760 (300.7 examples/sec; 0.213 sec/batch)
2017-05-09 21:23:55.883488: step 279160, loss = 0.0885, acc = 0.9720 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 21:24:00.519253: step 279180, loss = 0.0707, acc = 0.9840 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 21:24:05.016538: step 279200, loss = 0.0762, acc = 0.9740 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 21:24:10.677281: step 279220, loss = 0.0941, acc = 0.9740 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 21:24:15.431081: step 279240, loss = 0.0764, acc = 0.9820 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 21:24:20.101038: step 279260, loss = 0.0744, acc = 0.9860 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:24:24.725858: step 279280, loss = 0.0783, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:24:29.614632: step 279300, loss = 0.1026, acc = 0.9700 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 21:24:34.128946: step 279320, loss = 0.0717, acc = 0.9800 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 21:24:38.679268: step 279340, loss = 0.0787, acc = 0.9840 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 21:24:43.418944: step 279360, loss = 0.0637, acc = 0.9860 (236.3 examples/sec; 0.271 sec/batch)
2017-05-09 21:24:48.028203: step 279380, loss = 0.0871, acc = 0.9840 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 21:24:52.641579: step 279400, loss = 0.0699, acc = 0.9860 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 21:24:57.328896: step 279420, loss = 0.0674, acc = 0.9840 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 21:25:02.104091: step 279440, loss = 0.0895, acc = 0.9800 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 21:25:06.724217: step 279460, loss = 0.0714, acc = 0.9800 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 21:25:11.300440: step 279480, loss = 0.0597, acc = 0.9920 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 21:25:16.031841: step 279500, loss = 0.0816, acc = 0.9800 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 21:25:20.562897: step 279520, loss = 0.0788, acc = 0.9840 (301.6 examples/sec; 0.212 sec/batch)
2017-05-09 21:25:25.093178: step 279540, loss = 0.1260, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:25:29.854202: step 279560, loss = 0.0823, acc = 0.9780 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 21:25:34.473404: step 279580, loss = 0.0838, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 21:25:39.095418: step 279600, loss = 0.0671, acc = 0.9860 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 21:25:44.153549: step 279620, loss = 0.1065, acc = 0.9700 (209.9 examples/sec; 0.305 sec/batch)
2017-05-09 21:25:48.634783: step 279640, loss = 0.0734, acc = 0.9860 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 21:25:53.254123: step 279660, loss = 0.0888, acc = 0.9700 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 21:25:57.791988: step 279680, loss = 0.0581, acc = 0.9920 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:26:02.465024: step 279700, loss = 0.1012, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 21:26:07.090054: step 279720, loss = 0.0878, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 21:26:12.208312: step 279740, loss = 0.0827, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 21:26:16.990066: step 279760, loss = 0.0733, acc = 0.9860 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 21:26:21.530882: step 279780, loss = 0.0753, acc = 0.9840 (266.1 examples/sec; 0.241 sec/batch)
2017-05-09 21:26:26.102512: step 279800, loss = 0.0606, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 21:26:30.783530: step 279820, loss = 0.0778, acc = 0.9820 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 21:26:35.379631: step 279840, loss = 0.0645, acc = 0.9860 (260.3 examples/sec; 0.246 sec/batch)
2017-05-09 21:26:39.987088: step 279860, loss = 0.0650, acc = 0.9880 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 21:26:44.879047: step 279880, loss = 0.0920, acc = 0.9760 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 21:26:49.453924: step 279900, loss = 0.0878, acc = 0.9800 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 21:26:54.353199: step 279920, loss = 0.0755, acc = 0.9820 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 21:26:59.186595: step 279940, loss = 0.0788, acc = 0.9800 (227.0 examples/sec; 0.282 sec/batch)
2017-05-09 21:27:03.737847: step 279960, loss = 0.0651, acc = 0.9860 (266.4 examples/sec; 0.240 sec/batch)
2017-05-09 21:27:08.544312: step 279980, loss = 0.0890, acc = 0.9780 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 21:27:13.263668: step 280000, loss = 0.0768, acc = 0.9820 (227.5 examples/sec; 0.281 sec/batch)
[Eval] 2017-05-09 21:27:27.236139: step 280000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 21:27:36.570784: step 280000, acc = 0.9563, f1 = 0.9559
[Status] 2017-05-09 21:27:36.570886: step 280000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:27:41.259554: step 280020, loss = 0.0770, acc = 0.9800 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 21:27:45.927417: step 280040, loss = 0.0742, acc = 0.9820 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 21:27:50.437446: step 280060, loss = 0.0924, acc = 0.9680 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 21:27:54.998391: step 280080, loss = 0.0729, acc = 0.9800 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 21:27:59.631600: step 280100, loss = 0.0898, acc = 0.9780 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 21:28:04.269071: step 280120, loss = 0.0984, acc = 0.9780 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 21:28:08.871275: step 280140, loss = 0.0985, acc = 0.9600 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 21:28:13.572084: step 280160, loss = 0.0724, acc = 0.9820 (243.1 examples/sec; 0.263 sec/batch)
2017-05-09 21:28:18.086415: step 280180, loss = 0.1112, acc = 0.9560 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 21:28:22.793479: step 280200, loss = 0.1090, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 21:28:28.223397: step 280220, loss = 0.0863, acc = 0.9780 (143.9 examples/sec; 0.445 sec/batch)
2017-05-09 21:28:32.844611: step 280240, loss = 0.0710, acc = 0.9860 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 21:28:37.359550: step 280260, loss = 0.0734, acc = 0.9800 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 21:28:41.972482: step 280280, loss = 0.1055, acc = 0.9680 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:28:46.609864: step 280300, loss = 0.0683, acc = 0.9840 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 21:28:51.185976: step 280320, loss = 0.0764, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 21:28:55.796137: step 280340, loss = 0.0797, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 21:29:00.485098: step 280360, loss = 0.0879, acc = 0.9720 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 21:29:04.982273: step 280380, loss = 0.0686, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 21:29:09.607221: step 280400, loss = 0.0668, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:29:14.277922: step 280420, loss = 0.0848, acc = 0.9800 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 21:29:18.899085: step 280440, loss = 0.0960, acc = 0.9720 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 21:29:23.468335: step 280460, loss = 0.0810, acc = 0.9780 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 21:29:27.952130: step 280480, loss = 0.0778, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:29:32.800732: step 280500, loss = 0.0496, acc = 0.9960 (277.7 examples/sec; 0.231 sec/batch)
2017-05-09 21:29:37.419492: step 280520, loss = 0.0838, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 21:29:42.073954: step 280540, loss = 0.0789, acc = 0.9820 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 21:29:46.706292: step 280560, loss = 0.0618, acc = 0.9840 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 21:29:51.261814: step 280580, loss = 0.1041, acc = 0.9700 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 21:29:55.875126: step 280600, loss = 0.1018, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 21:30:00.666825: step 280620, loss = 0.0679, acc = 0.9880 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 21:30:05.192028: step 280640, loss = 0.0653, acc = 0.9880 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:30:09.767848: step 280660, loss = 0.0746, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:30:14.626694: step 280680, loss = 0.0842, acc = 0.9760 (219.5 examples/sec; 0.292 sec/batch)
2017-05-09 21:30:19.119743: step 280700, loss = 0.0890, acc = 0.9800 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:30:23.874654: step 280720, loss = 0.0672, acc = 0.9820 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 21:30:28.516618: step 280740, loss = 0.1065, acc = 0.9680 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 21:30:33.133290: step 280760, loss = 0.0861, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 21:30:37.683590: step 280780, loss = 0.0846, acc = 0.9780 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:30:42.145205: step 280800, loss = 0.0696, acc = 0.9840 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 21:30:46.974217: step 280820, loss = 0.0906, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:30:51.521211: step 280840, loss = 0.0784, acc = 0.9820 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:30:56.127855: step 280860, loss = 0.0837, acc = 0.9820 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 21:31:00.889428: step 280880, loss = 0.0678, acc = 0.9880 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 21:31:05.476429: step 280900, loss = 0.0808, acc = 0.9820 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 21:31:10.111068: step 280920, loss = 0.1045, acc = 0.9700 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 21:31:14.883603: step 280940, loss = 0.0831, acc = 0.9780 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 21:31:19.575434: step 280960, loss = 0.0745, acc = 0.9860 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 21:31:24.256904: step 280980, loss = 0.0809, acc = 0.9740 (283.8 examples/sec; 0.225 sec/batch)
2017-05-09 21:31:28.916808: step 281000, loss = 0.0856, acc = 0.9760 (234.4 examples/sec; 0.273 sec/batch)
[Eval] 2017-05-09 21:31:42.561680: step 281000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 21:31:52.177411: step 281000, acc = 0.9558, f1 = 0.9555
[Status] 2017-05-09 21:31:52.177493: step 281000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:31:56.719026: step 281020, loss = 0.0739, acc = 0.9820 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 21:32:01.764784: step 281040, loss = 0.0704, acc = 0.9900 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 21:32:06.426727: step 281060, loss = 0.0659, acc = 0.9860 (262.8 examples/sec; 0.244 sec/batch)
2017-05-09 21:32:11.168009: step 281080, loss = 0.0922, acc = 0.9760 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 21:32:15.856503: step 281100, loss = 0.0780, acc = 0.9760 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 21:32:20.387778: step 281120, loss = 0.0686, acc = 0.9820 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 21:32:24.962811: step 281140, loss = 0.0821, acc = 0.9800 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:32:29.609402: step 281160, loss = 0.0761, acc = 0.9820 (297.6 examples/sec; 0.215 sec/batch)
2017-05-09 21:32:34.116613: step 281180, loss = 0.1027, acc = 0.9720 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 21:32:38.641999: step 281200, loss = 0.1039, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 21:32:43.324576: step 281220, loss = 0.0634, acc = 0.9840 (254.9 examples/sec; 0.251 sec/batch)
2017-05-09 21:32:49.178168: step 281240, loss = 0.0828, acc = 0.9900 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 21:32:53.933779: step 281260, loss = 0.0775, acc = 0.9840 (225.6 examples/sec; 0.284 sec/batch)
2017-05-09 21:32:58.525060: step 281280, loss = 0.0865, acc = 0.9740 (299.1 examples/sec; 0.214 sec/batch)
2017-05-09 21:33:03.356904: step 281300, loss = 0.0916, acc = 0.9760 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 21:33:07.911628: step 281320, loss = 0.0703, acc = 0.9780 (265.0 examples/sec; 0.242 sec/batch)
2017-05-09 21:33:12.490470: step 281340, loss = 0.0626, acc = 0.9880 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 21:33:17.371985: step 281360, loss = 0.0660, acc = 0.9860 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 21:33:21.912834: step 281380, loss = 0.0735, acc = 0.9820 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:33:26.483255: step 281400, loss = 0.0713, acc = 0.9840 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 21:33:31.097501: step 281420, loss = 0.0696, acc = 0.9860 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 21:33:35.582957: step 281440, loss = 0.0729, acc = 0.9840 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 21:33:40.216270: step 281460, loss = 0.0837, acc = 0.9780 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:33:45.027171: step 281480, loss = 0.0741, acc = 0.9860 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 21:33:49.699691: step 281500, loss = 0.0694, acc = 0.9900 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 21:33:54.415427: step 281520, loss = 0.0616, acc = 0.9860 (261.8 examples/sec; 0.244 sec/batch)
2017-05-09 21:33:59.363274: step 281540, loss = 0.0651, acc = 0.9900 (228.9 examples/sec; 0.280 sec/batch)
2017-05-09 21:34:03.835215: step 281560, loss = 0.0720, acc = 0.9820 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 21:34:08.586767: step 281580, loss = 0.0861, acc = 0.9760 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 21:34:13.346662: step 281600, loss = 0.0748, acc = 0.9800 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 21:34:18.224066: step 281620, loss = 0.0776, acc = 0.9820 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 21:34:22.828938: step 281640, loss = 0.0765, acc = 0.9800 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 21:34:27.650955: step 281660, loss = 0.1084, acc = 0.9780 (211.6 examples/sec; 0.302 sec/batch)
2017-05-09 21:34:32.436399: step 281680, loss = 0.0940, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:34:36.919144: step 281700, loss = 0.0912, acc = 0.9780 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 21:34:41.476061: step 281720, loss = 0.0778, acc = 0.9760 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 21:34:46.054682: step 281740, loss = 0.0776, acc = 0.9780 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 21:34:50.773443: step 281760, loss = 0.0696, acc = 0.9860 (257.7 examples/sec; 0.248 sec/batch)
2017-05-09 21:34:55.302517: step 281780, loss = 0.0912, acc = 0.9800 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 21:35:00.008430: step 281800, loss = 0.0799, acc = 0.9740 (249.9 examples/sec; 0.256 sec/batch)
2017-05-09 21:35:04.705547: step 281820, loss = 0.0704, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 21:35:09.327451: step 281840, loss = 0.0887, acc = 0.9860 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 21:35:13.846472: step 281860, loss = 0.0879, acc = 0.9720 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 21:35:18.789831: step 281880, loss = 0.0804, acc = 0.9840 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 21:35:23.290803: step 281900, loss = 0.0893, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 21:35:27.853619: step 281920, loss = 0.0973, acc = 0.9720 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 21:35:32.595197: step 281940, loss = 0.0739, acc = 0.9840 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 21:35:37.108411: step 281960, loss = 0.0840, acc = 0.9800 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 21:35:41.657717: step 281980, loss = 0.0779, acc = 0.9820 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:35:46.502804: step 282000, loss = 0.0703, acc = 0.9800 (270.5 examples/sec; 0.237 sec/batch)
[Eval] 2017-05-09 21:36:00.489387: step 282000, acc = 0.9645, f1 = 0.9634
[Test] 2017-05-09 21:36:09.808216: step 282000, acc = 0.9557, f1 = 0.9554
[Status] 2017-05-09 21:36:09.808301: step 282000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:36:14.529292: step 282020, loss = 0.0756, acc = 0.9820 (249.6 examples/sec; 0.256 sec/batch)
2017-05-09 21:36:18.997958: step 282040, loss = 0.0775, acc = 0.9840 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 21:36:23.578329: step 282060, loss = 0.0652, acc = 0.9880 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 21:36:28.055224: step 282080, loss = 0.1003, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 21:36:32.772279: step 282100, loss = 0.0870, acc = 0.9780 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 21:36:37.328524: step 282120, loss = 0.0650, acc = 0.9860 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 21:36:41.917127: step 282140, loss = 0.1129, acc = 0.9700 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 21:36:46.758524: step 282160, loss = 0.0708, acc = 0.9840 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 21:36:51.371872: step 282180, loss = 0.0942, acc = 0.9720 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:36:55.981686: step 282200, loss = 0.0741, acc = 0.9840 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 21:37:00.814946: step 282220, loss = 0.0834, acc = 0.9720 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 21:37:06.047714: step 282240, loss = 0.0768, acc = 0.9840 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 21:37:10.819642: step 282260, loss = 0.0810, acc = 0.9800 (255.5 examples/sec; 0.251 sec/batch)
2017-05-09 21:37:15.593842: step 282280, loss = 0.0820, acc = 0.9800 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 21:37:20.085898: step 282300, loss = 0.0591, acc = 0.9880 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:37:24.715122: step 282320, loss = 0.0655, acc = 0.9900 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 21:37:29.475381: step 282340, loss = 0.0688, acc = 0.9900 (250.2 examples/sec; 0.256 sec/batch)
2017-05-09 21:37:34.068057: step 282360, loss = 0.0664, acc = 0.9880 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 21:37:38.686621: step 282380, loss = 0.0861, acc = 0.9760 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 21:37:43.188378: step 282400, loss = 0.0702, acc = 0.9820 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:37:47.940586: step 282420, loss = 0.0815, acc = 0.9780 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 21:37:52.548379: step 282440, loss = 0.0930, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 21:37:57.134047: step 282460, loss = 0.0803, acc = 0.9820 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 21:38:01.794282: step 282480, loss = 0.0857, acc = 0.9840 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 21:38:06.494190: step 282500, loss = 0.0583, acc = 0.9900 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 21:38:11.113164: step 282520, loss = 0.0686, acc = 0.9880 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 21:38:15.863906: step 282540, loss = 0.0600, acc = 0.9900 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 21:38:20.491181: step 282560, loss = 0.0722, acc = 0.9860 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 21:38:25.214985: step 282580, loss = 0.0623, acc = 0.9920 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 21:38:30.400951: step 282600, loss = 0.0907, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 21:38:34.830618: step 282620, loss = 0.0897, acc = 0.9660 (292.9 examples/sec; 0.218 sec/batch)
2017-05-09 21:38:39.474050: step 282640, loss = 0.0806, acc = 0.9680 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 21:38:44.180101: step 282660, loss = 0.0868, acc = 0.9800 (239.7 examples/sec; 0.267 sec/batch)
2017-05-09 21:38:48.661807: step 282680, loss = 0.0604, acc = 0.9900 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 21:38:53.124221: step 282700, loss = 0.0860, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:38:58.099340: step 282720, loss = 0.0698, acc = 0.9840 (208.2 examples/sec; 0.307 sec/batch)
2017-05-09 21:39:02.671708: step 282740, loss = 0.0921, acc = 0.9800 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 21:39:07.214354: step 282760, loss = 0.0601, acc = 0.9920 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 21:39:11.738088: step 282780, loss = 0.0793, acc = 0.9800 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 21:39:16.292031: step 282800, loss = 0.0596, acc = 0.9920 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 21:39:20.800541: step 282820, loss = 0.1031, acc = 0.9620 (302.0 examples/sec; 0.212 sec/batch)
2017-05-09 21:39:25.386833: step 282840, loss = 0.0663, acc = 0.9840 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 21:39:30.069057: step 282860, loss = 0.0710, acc = 0.9820 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 21:39:34.642501: step 282880, loss = 0.0855, acc = 0.9680 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 21:39:39.226286: step 282900, loss = 0.1039, acc = 0.9660 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 21:39:44.122498: step 282920, loss = 0.0880, acc = 0.9720 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:39:48.730902: step 282940, loss = 0.0901, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 21:39:53.261287: step 282960, loss = 0.0763, acc = 0.9800 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:39:57.947965: step 282980, loss = 0.0857, acc = 0.9800 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 21:40:02.445425: step 283000, loss = 0.0949, acc = 0.9700 (279.9 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 21:40:16.452731: step 283000, acc = 0.9648, f1 = 0.9636
[Test] 2017-05-09 21:40:26.042311: step 283000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 21:40:26.042410: step 283000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:40:30.715833: step 283020, loss = 0.1104, acc = 0.9780 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 21:40:35.424432: step 283040, loss = 0.1040, acc = 0.9640 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 21:40:40.100334: step 283060, loss = 0.0584, acc = 0.9860 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 21:40:44.608055: step 283080, loss = 0.0906, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 21:40:49.127235: step 283100, loss = 0.0861, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 21:40:54.079811: step 283120, loss = 0.0768, acc = 0.9840 (224.5 examples/sec; 0.285 sec/batch)
2017-05-09 21:40:58.630331: step 283140, loss = 0.0856, acc = 0.9800 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 21:41:03.138314: step 283160, loss = 0.0773, acc = 0.9820 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 21:41:07.742811: step 283180, loss = 0.0805, acc = 0.9780 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 21:41:12.444807: step 283200, loss = 0.0655, acc = 0.9860 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 21:41:17.095835: step 283220, loss = 0.0879, acc = 0.9800 (257.4 examples/sec; 0.249 sec/batch)
2017-05-09 21:41:21.859140: step 283240, loss = 0.0717, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:41:27.797888: step 283260, loss = 0.0618, acc = 0.9880 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 21:41:32.351196: step 283280, loss = 0.0969, acc = 0.9720 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 21:41:37.029998: step 283300, loss = 0.0983, acc = 0.9660 (246.1 examples/sec; 0.260 sec/batch)
2017-05-09 21:41:41.519885: step 283320, loss = 0.0811, acc = 0.9780 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 21:41:46.039226: step 283340, loss = 0.0725, acc = 0.9840 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 21:41:50.630045: step 283360, loss = 0.0784, acc = 0.9800 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 21:41:55.395127: step 283380, loss = 0.0677, acc = 0.9840 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 21:42:00.031086: step 283400, loss = 0.0818, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 21:42:04.556320: step 283420, loss = 0.0828, acc = 0.9760 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 21:42:09.160575: step 283440, loss = 0.0827, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 21:42:13.614501: step 283460, loss = 0.0700, acc = 0.9860 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 21:42:18.270436: step 283480, loss = 0.0735, acc = 0.9820 (260.8 examples/sec; 0.245 sec/batch)
2017-05-09 21:42:23.061042: step 283500, loss = 0.0694, acc = 0.9840 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 21:42:27.669498: step 283520, loss = 0.0831, acc = 0.9740 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 21:42:32.376046: step 283540, loss = 0.0956, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 21:42:37.015173: step 283560, loss = 0.0931, acc = 0.9700 (251.6 examples/sec; 0.254 sec/batch)
2017-05-09 21:42:41.633616: step 283580, loss = 0.0833, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 21:42:46.191621: step 283600, loss = 0.0584, acc = 0.9840 (296.1 examples/sec; 0.216 sec/batch)
2017-05-09 21:42:50.622921: step 283620, loss = 0.0640, acc = 0.9880 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 21:42:55.364178: step 283640, loss = 0.0876, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:43:00.131330: step 283660, loss = 0.0747, acc = 0.9840 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 21:43:04.632123: step 283680, loss = 0.0880, acc = 0.9720 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 21:43:09.343427: step 283700, loss = 0.0848, acc = 0.9820 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 21:43:13.902380: step 283720, loss = 0.0833, acc = 0.9740 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 21:43:18.550484: step 283740, loss = 0.0674, acc = 0.9940 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:43:23.241463: step 283760, loss = 0.0707, acc = 0.9820 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 21:43:27.905266: step 283780, loss = 0.0855, acc = 0.9780 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 21:43:32.448093: step 283800, loss = 0.0759, acc = 0.9840 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 21:43:37.175352: step 283820, loss = 0.0895, acc = 0.9720 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 21:43:41.843245: step 283840, loss = 0.0682, acc = 0.9820 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 21:43:46.395494: step 283860, loss = 0.0832, acc = 0.9880 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 21:43:51.122574: step 283880, loss = 0.0819, acc = 0.9760 (247.5 examples/sec; 0.259 sec/batch)
2017-05-09 21:43:55.742859: step 283900, loss = 0.0613, acc = 0.9920 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 21:44:00.264780: step 283920, loss = 0.0839, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 21:44:04.875425: step 283940, loss = 0.0725, acc = 0.9860 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 21:44:09.699562: step 283960, loss = 0.0584, acc = 0.9940 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 21:44:14.192354: step 283980, loss = 0.0798, acc = 0.9800 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 21:44:18.841108: step 284000, loss = 0.0653, acc = 0.9880 (286.2 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 21:44:32.889438: step 284000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 21:44:42.633787: step 284000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 21:44:42.633885: step 284000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:44:47.269030: step 284020, loss = 0.0857, acc = 0.9800 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 21:44:51.929239: step 284040, loss = 0.0658, acc = 0.9820 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 21:44:56.561040: step 284060, loss = 0.0746, acc = 0.9800 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 21:45:01.179015: step 284080, loss = 0.0784, acc = 0.9780 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 21:45:05.737205: step 284100, loss = 0.0529, acc = 0.9960 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 21:45:10.530529: step 284120, loss = 0.0796, acc = 0.9780 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 21:45:14.983009: step 284140, loss = 0.0829, acc = 0.9760 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 21:45:19.556197: step 284160, loss = 0.0643, acc = 0.9840 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 21:45:24.262202: step 284180, loss = 0.0939, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 21:45:28.842880: step 284200, loss = 0.0727, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 21:45:33.492539: step 284220, loss = 0.0704, acc = 0.9800 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 21:45:38.138738: step 284240, loss = 0.0675, acc = 0.9820 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 21:45:43.488729: step 284260, loss = 0.0645, acc = 0.9920 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 21:45:48.107860: step 284280, loss = 0.0901, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 21:45:53.042036: step 284300, loss = 0.0886, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 21:45:57.635051: step 284320, loss = 0.0797, acc = 0.9880 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 21:46:02.194798: step 284340, loss = 0.1046, acc = 0.9660 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 21:46:07.175987: step 284360, loss = 0.0728, acc = 0.9800 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 21:46:11.880556: step 284380, loss = 0.0753, acc = 0.9820 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 21:46:16.427151: step 284400, loss = 0.0960, acc = 0.9720 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 21:46:21.073709: step 284420, loss = 0.0771, acc = 0.9880 (248.9 examples/sec; 0.257 sec/batch)
2017-05-09 21:46:25.758906: step 284440, loss = 0.0740, acc = 0.9800 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 21:46:30.278761: step 284460, loss = 0.0886, acc = 0.9720 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 21:46:34.776558: step 284480, loss = 0.0896, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
2017-05-09 21:46:39.524512: step 284500, loss = 0.0649, acc = 0.9840 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 21:46:44.054557: step 284520, loss = 0.0638, acc = 0.9880 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 21:46:48.734394: step 284540, loss = 0.0693, acc = 0.9880 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 21:46:53.931396: step 284560, loss = 0.0681, acc = 0.9840 (215.7 examples/sec; 0.297 sec/batch)
2017-05-09 21:46:58.393608: step 284580, loss = 0.1213, acc = 0.9580 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 21:47:03.002018: step 284600, loss = 0.0703, acc = 0.9860 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 21:47:07.771247: step 284620, loss = 0.0920, acc = 0.9720 (255.5 examples/sec; 0.250 sec/batch)
2017-05-09 21:47:12.382095: step 284640, loss = 0.0657, acc = 0.9860 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 21:47:16.935317: step 284660, loss = 0.1056, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 21:47:21.899589: step 284680, loss = 0.0961, acc = 0.9740 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 21:47:26.539838: step 284700, loss = 0.0679, acc = 0.9880 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:47:31.212857: step 284720, loss = 0.1017, acc = 0.9720 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 21:47:35.929395: step 284740, loss = 0.0708, acc = 0.9880 (302.8 examples/sec; 0.211 sec/batch)
2017-05-09 21:47:40.373680: step 284760, loss = 0.1111, acc = 0.9680 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 21:47:44.828325: step 284780, loss = 0.0774, acc = 0.9840 (286.4 examples/sec; 0.224 sec/batch)
2017-05-09 21:47:49.670616: step 284800, loss = 0.0832, acc = 0.9760 (219.0 examples/sec; 0.292 sec/batch)
2017-05-09 21:47:54.312278: step 284820, loss = 0.0714, acc = 0.9800 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:47:58.878531: step 284840, loss = 0.0681, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:48:03.493954: step 284860, loss = 0.0721, acc = 0.9840 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 21:48:08.171427: step 284880, loss = 0.0845, acc = 0.9760 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 21:48:12.785819: step 284900, loss = 0.0739, acc = 0.9860 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 21:48:17.416654: step 284920, loss = 0.0776, acc = 0.9800 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 21:48:22.337500: step 284940, loss = 0.0689, acc = 0.9880 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 21:48:26.872868: step 284960, loss = 0.0727, acc = 0.9800 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 21:48:31.419434: step 284980, loss = 0.0603, acc = 0.9860 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 21:48:36.157145: step 285000, loss = 0.0630, acc = 0.9860 (277.8 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 21:48:50.320142: step 285000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 21:48:59.534083: step 285000, acc = 0.9558, f1 = 0.9554
[Status] 2017-05-09 21:48:59.534165: step 285000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:49:04.237911: step 285020, loss = 0.0846, acc = 0.9720 (298.5 examples/sec; 0.214 sec/batch)
2017-05-09 21:49:08.822279: step 285040, loss = 0.0816, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:49:13.519360: step 285060, loss = 0.0769, acc = 0.9780 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 21:49:18.141160: step 285080, loss = 0.0805, acc = 0.9820 (255.3 examples/sec; 0.251 sec/batch)
2017-05-09 21:49:22.920579: step 285100, loss = 0.0654, acc = 0.9880 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 21:49:27.530421: step 285120, loss = 0.0802, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 21:49:32.097434: step 285140, loss = 0.0690, acc = 0.9800 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:49:36.866868: step 285160, loss = 0.0718, acc = 0.9820 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 21:49:41.474593: step 285180, loss = 0.0648, acc = 0.9900 (254.3 examples/sec; 0.252 sec/batch)
2017-05-09 21:49:46.323556: step 285200, loss = 0.0707, acc = 0.9840 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 21:49:51.045004: step 285220, loss = 0.0882, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 21:49:55.677329: step 285240, loss = 0.0656, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 21:50:01.390386: step 285260, loss = 0.0772, acc = 0.9840 (133.4 examples/sec; 0.480 sec/batch)
2017-05-09 21:50:06.255234: step 285280, loss = 0.0825, acc = 0.9780 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 21:50:10.767931: step 285300, loss = 0.0852, acc = 0.9820 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 21:50:15.376637: step 285320, loss = 0.0757, acc = 0.9820 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 21:50:20.049387: step 285340, loss = 0.0566, acc = 0.9920 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:50:24.614468: step 285360, loss = 0.0955, acc = 0.9720 (295.3 examples/sec; 0.217 sec/batch)
2017-05-09 21:50:29.161214: step 285380, loss = 0.0719, acc = 0.9840 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 21:50:33.942607: step 285400, loss = 0.1015, acc = 0.9720 (240.3 examples/sec; 0.266 sec/batch)
2017-05-09 21:50:38.511405: step 285420, loss = 0.0628, acc = 0.9920 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 21:50:43.016005: step 285440, loss = 0.0928, acc = 0.9800 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 21:50:47.495261: step 285460, loss = 0.0654, acc = 0.9880 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 21:50:52.161196: step 285480, loss = 0.0783, acc = 0.9780 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 21:50:56.621019: step 285500, loss = 0.0648, acc = 0.9880 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:51:01.161045: step 285520, loss = 0.0864, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 21:51:05.818631: step 285540, loss = 0.0852, acc = 0.9820 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 21:51:10.515964: step 285560, loss = 0.0742, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 21:51:15.103019: step 285580, loss = 0.0630, acc = 0.9900 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 21:51:19.968190: step 285600, loss = 0.0906, acc = 0.9740 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 21:51:24.556795: step 285620, loss = 0.0824, acc = 0.9780 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 21:51:29.128096: step 285640, loss = 0.0745, acc = 0.9860 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:51:33.927304: step 285660, loss = 0.0768, acc = 0.9760 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 21:51:38.583017: step 285680, loss = 0.0787, acc = 0.9760 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:51:43.172634: step 285700, loss = 0.1037, acc = 0.9740 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 21:51:47.940974: step 285720, loss = 0.0731, acc = 0.9780 (245.3 examples/sec; 0.261 sec/batch)
2017-05-09 21:51:52.471834: step 285740, loss = 0.0796, acc = 0.9820 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 21:51:56.977650: step 285760, loss = 0.0641, acc = 0.9880 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 21:52:01.535155: step 285780, loss = 0.0702, acc = 0.9820 (243.0 examples/sec; 0.263 sec/batch)
2017-05-09 21:52:06.015723: step 285800, loss = 0.0786, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 21:52:10.439321: step 285820, loss = 0.0720, acc = 0.9860 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 21:52:14.841745: step 285840, loss = 0.0677, acc = 0.9820 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 21:52:19.584415: step 285860, loss = 0.0669, acc = 0.9840 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 21:52:24.125399: step 285880, loss = 0.0725, acc = 0.9920 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 21:52:28.802748: step 285900, loss = 0.0691, acc = 0.9880 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 21:52:33.675886: step 285920, loss = 0.0711, acc = 0.9880 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 21:52:38.302022: step 285940, loss = 0.0788, acc = 0.9880 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 21:52:42.990731: step 285960, loss = 0.1029, acc = 0.9800 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 21:52:47.807718: step 285980, loss = 0.0582, acc = 0.9900 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 21:52:52.370396: step 286000, loss = 0.0862, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 21:53:06.336874: step 286000, acc = 0.9645, f1 = 0.9633
[Test] 2017-05-09 21:53:15.884714: step 286000, acc = 0.9561, f1 = 0.9558
[Status] 2017-05-09 21:53:15.884802: step 286000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:53:20.436547: step 286020, loss = 0.0790, acc = 0.9780 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 21:53:25.157620: step 286040, loss = 0.0698, acc = 0.9900 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 21:53:29.815866: step 286060, loss = 0.0777, acc = 0.9840 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 21:53:34.601422: step 286080, loss = 0.0659, acc = 0.9900 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 21:53:39.279141: step 286100, loss = 0.0753, acc = 0.9800 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 21:53:43.893455: step 286120, loss = 0.0767, acc = 0.9840 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 21:53:48.509042: step 286140, loss = 0.0717, acc = 0.9800 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 21:53:52.975575: step 286160, loss = 0.0780, acc = 0.9800 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 21:53:57.541214: step 286180, loss = 0.0714, acc = 0.9780 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 21:54:02.368781: step 286200, loss = 0.0750, acc = 0.9820 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 21:54:06.818796: step 286220, loss = 0.0928, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 21:54:11.399180: step 286240, loss = 0.0905, acc = 0.9740 (264.0 examples/sec; 0.242 sec/batch)
2017-05-09 21:54:16.019326: step 286260, loss = 0.0702, acc = 0.9860 (248.4 examples/sec; 0.258 sec/batch)
2017-05-09 21:54:21.237384: step 286280, loss = 0.0699, acc = 0.9840 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 21:54:25.833946: step 286300, loss = 0.0712, acc = 0.9820 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 21:54:30.610142: step 286320, loss = 0.0819, acc = 0.9780 (231.4 examples/sec; 0.277 sec/batch)
2017-05-09 21:54:35.062083: step 286340, loss = 0.0867, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 21:54:39.617792: step 286360, loss = 0.0562, acc = 0.9940 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 21:54:44.256024: step 286380, loss = 0.1069, acc = 0.9700 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 21:54:49.025446: step 286400, loss = 0.0743, acc = 0.9820 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 21:54:53.607332: step 286420, loss = 0.0626, acc = 0.9880 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 21:54:58.200307: step 286440, loss = 0.1025, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 21:55:02.900312: step 286460, loss = 0.0704, acc = 0.9820 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 21:55:07.454419: step 286480, loss = 0.0901, acc = 0.9760 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 21:55:11.919570: step 286500, loss = 0.0943, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 21:55:16.902503: step 286520, loss = 0.0735, acc = 0.9920 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 21:55:21.501606: step 286540, loss = 0.0672, acc = 0.9840 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 21:55:26.198501: step 286560, loss = 0.0855, acc = 0.9840 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 21:55:31.010919: step 286580, loss = 0.1013, acc = 0.9740 (223.5 examples/sec; 0.286 sec/batch)
2017-05-09 21:55:35.682989: step 286600, loss = 0.0777, acc = 0.9760 (250.0 examples/sec; 0.256 sec/batch)
2017-05-09 21:55:40.246252: step 286620, loss = 0.0740, acc = 0.9860 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 21:55:44.974125: step 286640, loss = 0.0701, acc = 0.9820 (236.2 examples/sec; 0.271 sec/batch)
2017-05-09 21:55:49.675037: step 286660, loss = 0.0694, acc = 0.9860 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:55:54.371315: step 286680, loss = 0.0712, acc = 0.9820 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 21:55:58.965809: step 286700, loss = 0.0833, acc = 0.9780 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 21:56:03.628781: step 286720, loss = 0.0725, acc = 0.9840 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 21:56:08.169692: step 286740, loss = 0.0781, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 21:56:12.757065: step 286760, loss = 0.0833, acc = 0.9700 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 21:56:17.583824: step 286780, loss = 0.0833, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 21:56:22.283609: step 286800, loss = 0.0653, acc = 0.9940 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 21:56:26.876217: step 286820, loss = 0.0726, acc = 0.9900 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 21:56:31.729865: step 286840, loss = 0.0730, acc = 0.9860 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 21:56:36.446094: step 286860, loss = 0.0625, acc = 0.9860 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 21:56:41.034888: step 286880, loss = 0.0767, acc = 0.9820 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 21:56:45.532369: step 286900, loss = 0.0860, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 21:56:50.123798: step 286920, loss = 0.0815, acc = 0.9800 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 21:56:54.759812: step 286940, loss = 0.1057, acc = 0.9760 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 21:56:59.338791: step 286960, loss = 0.0717, acc = 0.9860 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 21:57:04.032211: step 286980, loss = 0.1069, acc = 0.9740 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 21:57:08.545976: step 287000, loss = 0.0615, acc = 0.9920 (284.2 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 21:57:22.540489: step 287000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-09 21:57:32.304694: step 287000, acc = 0.9558, f1 = 0.9554
[Status] 2017-05-09 21:57:32.304764: step 287000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 21:57:36.867359: step 287020, loss = 0.0712, acc = 0.9940 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 21:57:41.529577: step 287040, loss = 0.0641, acc = 0.9900 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 21:57:46.233966: step 287060, loss = 0.0665, acc = 0.9840 (242.9 examples/sec; 0.263 sec/batch)
2017-05-09 21:57:50.921360: step 287080, loss = 0.0672, acc = 0.9900 (293.3 examples/sec; 0.218 sec/batch)
2017-05-09 21:57:55.512747: step 287100, loss = 0.0725, acc = 0.9820 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 21:58:00.122082: step 287120, loss = 0.0775, acc = 0.9820 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 21:58:04.827522: step 287140, loss = 0.0645, acc = 0.9920 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 21:58:09.359144: step 287160, loss = 0.0766, acc = 0.9700 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 21:58:14.140159: step 287180, loss = 0.0916, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 21:58:18.894811: step 287200, loss = 0.0759, acc = 0.9800 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 21:58:23.569355: step 287220, loss = 0.0730, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 21:58:28.241779: step 287240, loss = 0.0745, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 21:58:33.078361: step 287260, loss = 0.0786, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 21:58:38.588349: step 287280, loss = 0.0688, acc = 0.9780 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 21:58:43.117369: step 287300, loss = 0.1208, acc = 0.9540 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 21:58:47.960519: step 287320, loss = 0.0910, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 21:58:52.667027: step 287340, loss = 0.0689, acc = 0.9820 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 21:58:57.276210: step 287360, loss = 0.0780, acc = 0.9780 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 21:59:02.058172: step 287380, loss = 0.0661, acc = 0.9840 (267.5 examples/sec; 0.239 sec/batch)
2017-05-09 21:59:06.775373: step 287400, loss = 0.0822, acc = 0.9740 (270.0 examples/sec; 0.237 sec/batch)
2017-05-09 21:59:11.410616: step 287420, loss = 0.0778, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 21:59:16.250951: step 287440, loss = 0.0838, acc = 0.9720 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 21:59:20.933781: step 287460, loss = 0.0611, acc = 0.9880 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 21:59:25.593109: step 287480, loss = 0.0823, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 21:59:30.278860: step 287500, loss = 0.0622, acc = 0.9860 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 21:59:34.834320: step 287520, loss = 0.0965, acc = 0.9760 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 21:59:39.423446: step 287540, loss = 0.0968, acc = 0.9840 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 21:59:43.971264: step 287560, loss = 0.0674, acc = 0.9860 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 21:59:48.776294: step 287580, loss = 0.0813, acc = 0.9720 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 21:59:53.378486: step 287600, loss = 0.0776, acc = 0.9840 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 21:59:57.883914: step 287620, loss = 0.0618, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 22:00:02.525159: step 287640, loss = 0.0722, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 22:00:07.148617: step 287660, loss = 0.0783, acc = 0.9760 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 22:00:11.835765: step 287680, loss = 0.0901, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 22:00:16.562834: step 287700, loss = 0.0822, acc = 0.9800 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 22:00:21.160877: step 287720, loss = 0.0855, acc = 0.9740 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 22:00:25.756473: step 287740, loss = 0.0809, acc = 0.9780 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 22:00:30.584543: step 287760, loss = 0.0669, acc = 0.9860 (221.1 examples/sec; 0.289 sec/batch)
2017-05-09 22:00:35.274119: step 287780, loss = 0.0760, acc = 0.9820 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 22:00:39.868581: step 287800, loss = 0.0828, acc = 0.9820 (275.3 examples/sec; 0.232 sec/batch)
2017-05-09 22:00:44.423926: step 287820, loss = 0.0838, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 22:00:48.996332: step 287840, loss = 0.0756, acc = 0.9880 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 22:00:53.581351: step 287860, loss = 0.0714, acc = 0.9760 (269.7 examples/sec; 0.237 sec/batch)
2017-05-09 22:00:58.209804: step 287880, loss = 0.0793, acc = 0.9840 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 22:01:02.941008: step 287900, loss = 0.0799, acc = 0.9760 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 22:01:07.757446: step 287920, loss = 0.0879, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 22:01:12.344396: step 287940, loss = 0.1017, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 22:01:17.115612: step 287960, loss = 0.0992, acc = 0.9660 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:01:21.598379: step 287980, loss = 0.0860, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 22:01:26.185184: step 288000, loss = 0.0694, acc = 0.9880 (285.8 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 22:01:40.271401: step 288000, acc = 0.9558, f1 = 0.9546
[Test] 2017-05-09 22:01:49.875521: step 288000, acc = 0.9451, f1 = 0.9446
[Status] 2017-05-09 22:01:49.875616: step 288000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:01:54.508954: step 288020, loss = 0.0988, acc = 0.9680 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:01:59.183168: step 288040, loss = 0.0879, acc = 0.9740 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:02:03.914637: step 288060, loss = 0.0710, acc = 0.9860 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 22:02:08.456988: step 288080, loss = 0.0795, acc = 0.9820 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 22:02:13.042286: step 288100, loss = 0.0701, acc = 0.9860 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 22:02:17.768028: step 288120, loss = 0.0917, acc = 0.9660 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 22:02:22.279177: step 288140, loss = 0.0722, acc = 0.9740 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 22:02:26.757885: step 288160, loss = 0.0777, acc = 0.9820 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 22:02:31.507907: step 288180, loss = 0.0970, acc = 0.9740 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 22:02:36.126540: step 288200, loss = 0.0823, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 22:02:40.667508: step 288220, loss = 0.0875, acc = 0.9760 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 22:02:45.379519: step 288240, loss = 0.0637, acc = 0.9800 (244.9 examples/sec; 0.261 sec/batch)
2017-05-09 22:02:50.318671: step 288260, loss = 0.0916, acc = 0.9740 (256.1 examples/sec; 0.250 sec/batch)
2017-05-09 22:02:54.938824: step 288280, loss = 0.0872, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 22:03:00.201828: step 288300, loss = 0.0849, acc = 0.9700 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 22:03:04.852627: step 288320, loss = 0.0907, acc = 0.9800 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 22:03:09.359000: step 288340, loss = 0.0751, acc = 0.9820 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 22:03:13.860608: step 288360, loss = 0.0858, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 22:03:18.642451: step 288380, loss = 0.0818, acc = 0.9800 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 22:03:23.017426: step 288400, loss = 0.0814, acc = 0.9720 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 22:03:27.645704: step 288420, loss = 0.0989, acc = 0.9700 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:03:32.276184: step 288440, loss = 0.0873, acc = 0.9800 (297.1 examples/sec; 0.215 sec/batch)
2017-05-09 22:03:36.948344: step 288460, loss = 0.0802, acc = 0.9900 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 22:03:41.578664: step 288480, loss = 0.0941, acc = 0.9720 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 22:03:46.255684: step 288500, loss = 0.0874, acc = 0.9780 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 22:03:50.925416: step 288520, loss = 0.0790, acc = 0.9820 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 22:03:55.434426: step 288540, loss = 0.0639, acc = 0.9880 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 22:04:00.033318: step 288560, loss = 0.0639, acc = 0.9920 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:04:04.746954: step 288580, loss = 0.0865, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:04:09.489224: step 288600, loss = 0.0855, acc = 0.9860 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 22:04:14.010014: step 288620, loss = 0.0705, acc = 0.9780 (277.7 examples/sec; 0.231 sec/batch)
2017-05-09 22:04:18.693476: step 288640, loss = 0.0581, acc = 0.9860 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 22:04:23.187368: step 288660, loss = 0.0801, acc = 0.9780 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 22:04:27.804462: step 288680, loss = 0.0692, acc = 0.9840 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 22:04:32.487145: step 288700, loss = 0.0834, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 22:04:37.060173: step 288720, loss = 0.0848, acc = 0.9820 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 22:04:41.580201: step 288740, loss = 0.0814, acc = 0.9820 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 22:04:46.231311: step 288760, loss = 0.0825, acc = 0.9740 (250.8 examples/sec; 0.255 sec/batch)
2017-05-09 22:04:50.753238: step 288780, loss = 0.0791, acc = 0.9840 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 22:04:55.450927: step 288800, loss = 0.0678, acc = 0.9860 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 22:05:00.076940: step 288820, loss = 0.0767, acc = 0.9880 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 22:05:04.703411: step 288840, loss = 0.0933, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 22:05:09.206153: step 288860, loss = 0.1137, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 22:05:13.786667: step 288880, loss = 0.0881, acc = 0.9780 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:05:18.686300: step 288900, loss = 0.0720, acc = 0.9860 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:05:23.299202: step 288920, loss = 0.0781, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 22:05:27.839762: step 288940, loss = 0.0778, acc = 0.9760 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 22:05:32.608018: step 288960, loss = 0.0776, acc = 0.9800 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 22:05:37.147461: step 288980, loss = 0.0786, acc = 0.9840 (271.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:05:41.838333: step 289000, loss = 0.0807, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 22:05:55.917186: step 289000, acc = 0.9638, f1 = 0.9627
[Test] 2017-05-09 22:06:05.565298: step 289000, acc = 0.9548, f1 = 0.9544
[Status] 2017-05-09 22:06:05.565370: step 289000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:06:10.190403: step 289020, loss = 0.0758, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 22:06:14.759923: step 289040, loss = 0.0757, acc = 0.9860 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 22:06:19.476184: step 289060, loss = 0.0773, acc = 0.9780 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 22:06:24.164939: step 289080, loss = 0.0961, acc = 0.9700 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 22:06:29.115950: step 289100, loss = 0.0659, acc = 0.9840 (208.7 examples/sec; 0.307 sec/batch)
2017-05-09 22:06:34.008202: step 289120, loss = 0.0713, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:06:38.490927: step 289140, loss = 0.0681, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:06:43.098830: step 289160, loss = 0.0993, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 22:06:47.829298: step 289180, loss = 0.0796, acc = 0.9820 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 22:06:52.378281: step 289200, loss = 0.0669, acc = 0.9860 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 22:06:56.995494: step 289220, loss = 0.0784, acc = 0.9880 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 22:07:01.781495: step 289240, loss = 0.0663, acc = 0.9920 (240.7 examples/sec; 0.266 sec/batch)
2017-05-09 22:07:06.282473: step 289260, loss = 0.0943, acc = 0.9720 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 22:07:10.833640: step 289280, loss = 0.0831, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 22:07:16.631475: step 289300, loss = 0.0649, acc = 0.9880 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 22:07:21.152601: step 289320, loss = 0.0671, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 22:07:25.717109: step 289340, loss = 0.0992, acc = 0.9760 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 22:07:30.425374: step 289360, loss = 0.0538, acc = 0.9920 (238.6 examples/sec; 0.268 sec/batch)
2017-05-09 22:07:35.121710: step 289380, loss = 0.0663, acc = 0.9820 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 22:07:39.714690: step 289400, loss = 0.0770, acc = 0.9820 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 22:07:44.269940: step 289420, loss = 0.0670, acc = 0.9920 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 22:07:48.874676: step 289440, loss = 0.0585, acc = 0.9900 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 22:07:53.401951: step 289460, loss = 0.0725, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 22:07:57.997278: step 289480, loss = 0.0819, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 22:08:02.865297: step 289500, loss = 0.0844, acc = 0.9760 (265.1 examples/sec; 0.241 sec/batch)
2017-05-09 22:08:07.525948: step 289520, loss = 0.0757, acc = 0.9880 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:08:12.227599: step 289540, loss = 0.0993, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 22:08:16.912571: step 289560, loss = 0.0600, acc = 0.9880 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 22:08:21.467222: step 289580, loss = 0.0632, acc = 0.9860 (298.8 examples/sec; 0.214 sec/batch)
2017-05-09 22:08:26.007638: step 289600, loss = 0.0688, acc = 0.9900 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 22:08:30.801981: step 289620, loss = 0.0696, acc = 0.9900 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 22:08:35.470495: step 289640, loss = 0.0953, acc = 0.9720 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 22:08:40.052924: step 289660, loss = 0.0627, acc = 0.9900 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 22:08:44.679175: step 289680, loss = 0.0750, acc = 0.9820 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 22:08:49.267715: step 289700, loss = 0.0808, acc = 0.9840 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 22:08:53.866281: step 289720, loss = 0.0743, acc = 0.9780 (267.9 examples/sec; 0.239 sec/batch)
2017-05-09 22:08:58.416090: step 289740, loss = 0.0663, acc = 0.9880 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 22:09:03.053613: step 289760, loss = 0.0706, acc = 0.9760 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 22:09:07.651328: step 289780, loss = 0.0532, acc = 0.9960 (297.9 examples/sec; 0.215 sec/batch)
2017-05-09 22:09:12.256327: step 289800, loss = 0.0752, acc = 0.9760 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 22:09:17.078758: step 289820, loss = 0.0942, acc = 0.9760 (256.9 examples/sec; 0.249 sec/batch)
2017-05-09 22:09:21.891554: step 289840, loss = 0.0618, acc = 0.9920 (239.6 examples/sec; 0.267 sec/batch)
2017-05-09 22:09:26.428685: step 289860, loss = 0.0799, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 22:09:31.204653: step 289880, loss = 0.0787, acc = 0.9780 (300.3 examples/sec; 0.213 sec/batch)
2017-05-09 22:09:35.877298: step 289900, loss = 0.0682, acc = 0.9840 (258.4 examples/sec; 0.248 sec/batch)
2017-05-09 22:09:40.433585: step 289920, loss = 0.0829, acc = 0.9860 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:09:45.086452: step 289940, loss = 0.0607, acc = 0.9860 (245.5 examples/sec; 0.261 sec/batch)
2017-05-09 22:09:49.748515: step 289960, loss = 0.0699, acc = 0.9840 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 22:09:54.450988: step 289980, loss = 0.0829, acc = 0.9780 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 22:09:58.970017: step 290000, loss = 0.0775, acc = 0.9840 (283.6 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-09 22:10:13.075577: step 290000, acc = 0.9646, f1 = 0.9635
[Test] 2017-05-09 22:10:22.612773: step 290000, acc = 0.9563, f1 = 0.9560
[Status] 2017-05-09 22:10:22.612845: step 290000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:10:27.215457: step 290020, loss = 0.0937, acc = 0.9740 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 22:10:32.087898: step 290040, loss = 0.0661, acc = 0.9920 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 22:10:37.088438: step 290060, loss = 0.0731, acc = 0.9860 (181.1 examples/sec; 0.353 sec/batch)
2017-05-09 22:10:41.711307: step 290080, loss = 0.0828, acc = 0.9800 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 22:10:46.593035: step 290100, loss = 0.0692, acc = 0.9880 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 22:10:51.191326: step 290120, loss = 0.0905, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:10:55.713721: step 290140, loss = 0.0730, acc = 0.9800 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 22:11:00.582211: step 290160, loss = 0.0661, acc = 0.9840 (262.4 examples/sec; 0.244 sec/batch)
2017-05-09 22:11:05.120945: step 290180, loss = 0.0746, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 22:11:09.571306: step 290200, loss = 0.0905, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 22:11:14.455307: step 290220, loss = 0.0777, acc = 0.9840 (226.3 examples/sec; 0.283 sec/batch)
2017-05-09 22:11:19.055551: step 290240, loss = 0.0856, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 22:11:23.628076: step 290260, loss = 0.0728, acc = 0.9800 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 22:11:28.279945: step 290280, loss = 0.0766, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:11:33.780387: step 290300, loss = 0.0735, acc = 0.9820 (155.9 examples/sec; 0.411 sec/batch)
2017-05-09 22:11:38.439598: step 290320, loss = 0.0954, acc = 0.9740 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 22:11:43.110582: step 290340, loss = 0.0681, acc = 0.9840 (244.2 examples/sec; 0.262 sec/batch)
2017-05-09 22:11:47.679383: step 290360, loss = 0.0808, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:11:52.297844: step 290380, loss = 0.0924, acc = 0.9740 (263.0 examples/sec; 0.243 sec/batch)
2017-05-09 22:11:57.221391: step 290400, loss = 0.0860, acc = 0.9840 (299.1 examples/sec; 0.214 sec/batch)
2017-05-09 22:12:01.866116: step 290420, loss = 0.0898, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 22:12:06.424463: step 290440, loss = 0.1018, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:12:10.993923: step 290460, loss = 0.0698, acc = 0.9880 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 22:12:15.634717: step 290480, loss = 0.0925, acc = 0.9720 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 22:12:20.222586: step 290500, loss = 0.0979, acc = 0.9740 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:12:24.951169: step 290520, loss = 0.0824, acc = 0.9760 (236.6 examples/sec; 0.271 sec/batch)
2017-05-09 22:12:29.592912: step 290540, loss = 0.0554, acc = 0.9940 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 22:12:34.118535: step 290560, loss = 0.1129, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 22:12:38.779898: step 290580, loss = 0.0654, acc = 0.9900 (264.7 examples/sec; 0.242 sec/batch)
2017-05-09 22:12:43.500302: step 290600, loss = 0.0611, acc = 0.9880 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 22:12:48.111684: step 290620, loss = 0.0718, acc = 0.9900 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 22:12:52.775937: step 290640, loss = 0.1033, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 22:12:57.439956: step 290660, loss = 0.0664, acc = 0.9840 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 22:13:02.018129: step 290680, loss = 0.0713, acc = 0.9860 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 22:13:06.614930: step 290700, loss = 0.0759, acc = 0.9860 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 22:13:11.379115: step 290720, loss = 0.1036, acc = 0.9720 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 22:13:15.945504: step 290740, loss = 0.1043, acc = 0.9760 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 22:13:20.468053: step 290760, loss = 0.0607, acc = 0.9840 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:13:25.150303: step 290780, loss = 0.1471, acc = 0.9440 (251.6 examples/sec; 0.254 sec/batch)
2017-05-09 22:13:29.771303: step 290800, loss = 0.0751, acc = 0.9860 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:13:34.471311: step 290820, loss = 0.0833, acc = 0.9720 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 22:13:39.025254: step 290840, loss = 0.0766, acc = 0.9840 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 22:13:43.755877: step 290860, loss = 0.0744, acc = 0.9840 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 22:13:48.568673: step 290880, loss = 0.0767, acc = 0.9840 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 22:13:53.120005: step 290900, loss = 0.0830, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 22:13:57.863806: step 290920, loss = 0.0800, acc = 0.9780 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 22:14:02.330686: step 290940, loss = 0.0785, acc = 0.9740 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 22:14:07.006877: step 290960, loss = 0.0831, acc = 0.9820 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 22:14:11.737612: step 290980, loss = 0.0826, acc = 0.9780 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 22:14:16.323262: step 291000, loss = 0.0668, acc = 0.9880 (286.4 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 22:14:30.328988: step 291000, acc = 0.9647, f1 = 0.9635
[Test] 2017-05-09 22:14:39.968376: step 291000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 22:14:39.968447: step 291000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:14:44.592269: step 291020, loss = 0.0727, acc = 0.9820 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 22:14:49.120594: step 291040, loss = 0.0640, acc = 0.9840 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 22:14:53.826387: step 291060, loss = 0.0592, acc = 0.9900 (252.0 examples/sec; 0.254 sec/batch)
2017-05-09 22:14:58.525315: step 291080, loss = 0.0914, acc = 0.9820 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 22:15:03.136100: step 291100, loss = 0.0803, acc = 0.9820 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 22:15:07.755242: step 291120, loss = 0.0832, acc = 0.9720 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 22:15:12.389348: step 291140, loss = 0.0895, acc = 0.9800 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 22:15:17.014015: step 291160, loss = 0.0765, acc = 0.9740 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 22:15:21.562269: step 291180, loss = 0.1021, acc = 0.9600 (266.0 examples/sec; 0.241 sec/batch)
2017-05-09 22:15:26.246903: step 291200, loss = 0.0709, acc = 0.9880 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 22:15:30.713962: step 291220, loss = 0.0722, acc = 0.9820 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 22:15:35.288317: step 291240, loss = 0.0821, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 22:15:40.119708: step 291260, loss = 0.0812, acc = 0.9820 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 22:15:44.751497: step 291280, loss = 0.0812, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 22:15:49.322445: step 291300, loss = 0.0775, acc = 0.9800 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 22:15:54.979937: step 291320, loss = 0.0957, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 22:15:59.611803: step 291340, loss = 0.0889, acc = 0.9760 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:16:04.091914: step 291360, loss = 0.0856, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 22:16:08.809446: step 291380, loss = 0.0890, acc = 0.9740 (293.8 examples/sec; 0.218 sec/batch)
2017-05-09 22:16:13.474816: step 291400, loss = 0.0692, acc = 0.9900 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 22:16:18.064119: step 291420, loss = 0.0768, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 22:16:22.621523: step 291440, loss = 0.0636, acc = 0.9880 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 22:16:27.433063: step 291460, loss = 0.0740, acc = 0.9820 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 22:16:32.090418: step 291480, loss = 0.0839, acc = 0.9720 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 22:16:36.669144: step 291500, loss = 0.0987, acc = 0.9740 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 22:16:41.377842: step 291520, loss = 0.0774, acc = 0.9840 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 22:16:45.916627: step 291540, loss = 0.0752, acc = 0.9840 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 22:16:50.497972: step 291560, loss = 0.0773, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 22:16:55.298985: step 291580, loss = 0.0841, acc = 0.9700 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 22:16:59.992763: step 291600, loss = 0.0638, acc = 0.9900 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:17:04.608832: step 291620, loss = 0.0767, acc = 0.9860 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 22:17:09.296011: step 291640, loss = 0.0682, acc = 0.9800 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 22:17:14.216156: step 291660, loss = 0.0769, acc = 0.9820 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 22:17:18.862752: step 291680, loss = 0.0721, acc = 0.9820 (272.2 examples/sec; 0.235 sec/batch)
2017-05-09 22:17:23.748765: step 291700, loss = 0.0712, acc = 0.9840 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 22:17:28.404327: step 291720, loss = 0.0676, acc = 0.9860 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:17:33.033487: step 291740, loss = 0.0650, acc = 0.9840 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 22:17:38.000329: step 291760, loss = 0.1005, acc = 0.9740 (222.2 examples/sec; 0.288 sec/batch)
2017-05-09 22:17:42.529491: step 291780, loss = 0.0648, acc = 0.9920 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:17:47.086928: step 291800, loss = 0.0806, acc = 0.9820 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 22:17:51.840960: step 291820, loss = 0.0756, acc = 0.9860 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 22:17:56.463576: step 291840, loss = 0.0659, acc = 0.9860 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 22:18:01.182794: step 291860, loss = 0.0991, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:18:06.013549: step 291880, loss = 0.0497, acc = 0.9960 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 22:18:10.646622: step 291900, loss = 0.0791, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 22:18:15.200132: step 291920, loss = 0.0677, acc = 0.9860 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:18:19.806795: step 291940, loss = 0.0727, acc = 0.9740 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 22:18:24.602373: step 291960, loss = 0.0928, acc = 0.9760 (236.0 examples/sec; 0.271 sec/batch)
2017-05-09 22:18:29.192308: step 291980, loss = 0.1021, acc = 0.9720 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 22:18:33.772951: step 292000, loss = 0.0695, acc = 0.9860 (284.4 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 22:18:47.935429: step 292000, acc = 0.9635, f1 = 0.9622
[Test] 2017-05-09 22:18:57.757624: step 292000, acc = 0.9540, f1 = 0.9536
[Status] 2017-05-09 22:18:57.757709: step 292000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:19:02.348584: step 292020, loss = 0.0875, acc = 0.9820 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 22:19:06.850189: step 292040, loss = 0.0771, acc = 0.9780 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 22:19:11.660892: step 292060, loss = 0.0691, acc = 0.9900 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 22:19:16.537017: step 292080, loss = 0.0544, acc = 0.9860 (223.1 examples/sec; 0.287 sec/batch)
2017-05-09 22:19:21.198609: step 292100, loss = 0.0692, acc = 0.9940 (262.9 examples/sec; 0.243 sec/batch)
2017-05-09 22:19:25.925931: step 292120, loss = 0.0940, acc = 0.9760 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 22:19:30.509384: step 292140, loss = 0.0808, acc = 0.9800 (276.6 examples/sec; 0.231 sec/batch)
2017-05-09 22:19:35.044068: step 292160, loss = 0.0733, acc = 0.9760 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 22:19:39.640968: step 292180, loss = 0.0766, acc = 0.9880 (298.9 examples/sec; 0.214 sec/batch)
2017-05-09 22:19:44.204291: step 292200, loss = 0.0976, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 22:19:49.081052: step 292220, loss = 0.0793, acc = 0.9740 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 22:19:53.979710: step 292240, loss = 0.0915, acc = 0.9700 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:19:58.713371: step 292260, loss = 0.0777, acc = 0.9820 (260.5 examples/sec; 0.246 sec/batch)
2017-05-09 22:20:03.445819: step 292280, loss = 0.0834, acc = 0.9800 (266.6 examples/sec; 0.240 sec/batch)
2017-05-09 22:20:08.125900: step 292300, loss = 0.0771, acc = 0.9820 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 22:20:13.480511: step 292320, loss = 0.0826, acc = 0.9700 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 22:20:18.072557: step 292340, loss = 0.0822, acc = 0.9820 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 22:20:22.884901: step 292360, loss = 0.0918, acc = 0.9800 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 22:20:27.431459: step 292380, loss = 0.0979, acc = 0.9820 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 22:20:32.029927: step 292400, loss = 0.0794, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 22:20:36.665338: step 292420, loss = 0.0995, acc = 0.9820 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 22:20:41.390370: step 292440, loss = 0.0989, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 22:20:46.100428: step 292460, loss = 0.0747, acc = 0.9780 (257.7 examples/sec; 0.248 sec/batch)
2017-05-09 22:20:50.606937: step 292480, loss = 0.0699, acc = 0.9840 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:20:55.513990: step 292500, loss = 0.0572, acc = 0.9960 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 22:21:00.052591: step 292520, loss = 0.0939, acc = 0.9800 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 22:21:04.568488: step 292540, loss = 0.0707, acc = 0.9840 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 22:21:09.365009: step 292560, loss = 0.0802, acc = 0.9780 (268.0 examples/sec; 0.239 sec/batch)
2017-05-09 22:21:14.193722: step 292580, loss = 0.0640, acc = 0.9840 (263.4 examples/sec; 0.243 sec/batch)
2017-05-09 22:21:18.816353: step 292600, loss = 0.0653, acc = 0.9840 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 22:21:23.657198: step 292620, loss = 0.0798, acc = 0.9760 (264.1 examples/sec; 0.242 sec/batch)
2017-05-09 22:21:28.232139: step 292640, loss = 0.0618, acc = 0.9860 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 22:21:32.800326: step 292660, loss = 0.0711, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 22:21:37.551895: step 292680, loss = 0.0979, acc = 0.9720 (226.5 examples/sec; 0.283 sec/batch)
2017-05-09 22:21:42.221907: step 292700, loss = 0.0871, acc = 0.9820 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 22:21:46.734979: step 292720, loss = 0.0722, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 22:21:51.265512: step 292740, loss = 0.0817, acc = 0.9760 (303.2 examples/sec; 0.211 sec/batch)
2017-05-09 22:21:55.883309: step 292760, loss = 0.0787, acc = 0.9760 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 22:22:00.339221: step 292780, loss = 0.0803, acc = 0.9800 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 22:22:04.867347: step 292800, loss = 0.0571, acc = 0.9920 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:22:09.685725: step 292820, loss = 0.0817, acc = 0.9820 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 22:22:14.253451: step 292840, loss = 0.0878, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 22:22:18.864905: step 292860, loss = 0.0891, acc = 0.9820 (295.2 examples/sec; 0.217 sec/batch)
2017-05-09 22:22:23.777700: step 292880, loss = 0.0871, acc = 0.9860 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 22:22:28.327355: step 292900, loss = 0.0776, acc = 0.9800 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 22:22:32.946191: step 292920, loss = 0.0830, acc = 0.9760 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 22:22:37.818889: step 292940, loss = 0.0814, acc = 0.9880 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 22:22:42.401649: step 292960, loss = 0.0882, acc = 0.9740 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 22:22:47.070304: step 292980, loss = 0.0822, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 22:22:51.853464: step 293000, loss = 0.0657, acc = 0.9900 (288.7 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-09 22:23:05.618295: step 293000, acc = 0.9647, f1 = 0.9636
[Test] 2017-05-09 22:23:15.267349: step 293000, acc = 0.9564, f1 = 0.9560
[Status] 2017-05-09 22:23:15.267435: step 293000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:23:20.055972: step 293020, loss = 0.0728, acc = 0.9820 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 22:23:24.741919: step 293040, loss = 0.0715, acc = 0.9800 (298.2 examples/sec; 0.215 sec/batch)
2017-05-09 22:23:29.413387: step 293060, loss = 0.0752, acc = 0.9780 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 22:23:34.008406: step 293080, loss = 0.0800, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 22:23:38.809060: step 293100, loss = 0.0825, acc = 0.9800 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 22:23:43.474682: step 293120, loss = 0.0854, acc = 0.9800 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 22:23:48.095021: step 293140, loss = 0.0707, acc = 0.9800 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 22:23:52.751530: step 293160, loss = 0.0804, acc = 0.9860 (293.6 examples/sec; 0.218 sec/batch)
2017-05-09 22:23:57.277279: step 293180, loss = 0.0907, acc = 0.9780 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 22:24:01.885284: step 293200, loss = 0.0760, acc = 0.9720 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:24:06.783665: step 293220, loss = 0.0691, acc = 0.9880 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 22:24:11.193506: step 293240, loss = 0.0655, acc = 0.9820 (302.8 examples/sec; 0.211 sec/batch)
2017-05-09 22:24:15.810714: step 293260, loss = 0.0846, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:24:20.393324: step 293280, loss = 0.0723, acc = 0.9860 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 22:24:24.917495: step 293300, loss = 0.0871, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 22:24:29.558408: step 293320, loss = 0.0781, acc = 0.9800 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 22:24:35.182080: step 293340, loss = 0.0730, acc = 0.9880 (250.9 examples/sec; 0.255 sec/batch)
2017-05-09 22:24:40.042671: step 293360, loss = 0.0548, acc = 0.9900 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 22:24:44.555408: step 293380, loss = 0.0644, acc = 0.9900 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 22:24:49.117990: step 293400, loss = 0.0703, acc = 0.9840 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 22:24:53.799619: step 293420, loss = 0.0882, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:24:58.394266: step 293440, loss = 0.0675, acc = 0.9880 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 22:25:02.987893: step 293460, loss = 0.1019, acc = 0.9660 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 22:25:07.784999: step 293480, loss = 0.0583, acc = 0.9920 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 22:25:12.404151: step 293500, loss = 0.1040, acc = 0.9640 (274.9 examples/sec; 0.233 sec/batch)
2017-05-09 22:25:17.065134: step 293520, loss = 0.0879, acc = 0.9740 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 22:25:21.985483: step 293540, loss = 0.0795, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 22:25:26.529487: step 293560, loss = 0.1015, acc = 0.9620 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 22:25:31.157631: step 293580, loss = 0.0916, acc = 0.9760 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 22:25:35.666047: step 293600, loss = 0.0879, acc = 0.9780 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 22:25:40.455233: step 293620, loss = 0.0883, acc = 0.9720 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 22:25:45.097264: step 293640, loss = 0.0618, acc = 0.9880 (269.3 examples/sec; 0.238 sec/batch)
2017-05-09 22:25:49.654154: step 293660, loss = 0.0753, acc = 0.9880 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 22:25:54.369356: step 293680, loss = 0.0581, acc = 0.9900 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:25:58.952050: step 293700, loss = 0.0758, acc = 0.9800 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 22:26:03.579514: step 293720, loss = 0.0775, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:26:08.516249: step 293740, loss = 0.0857, acc = 0.9820 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 22:26:13.037691: step 293760, loss = 0.0673, acc = 0.9880 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 22:26:17.552048: step 293780, loss = 0.0668, acc = 0.9840 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 22:26:22.190302: step 293800, loss = 0.0818, acc = 0.9880 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 22:26:26.766565: step 293820, loss = 0.0845, acc = 0.9780 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 22:26:31.362979: step 293840, loss = 0.0823, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 22:26:36.015192: step 293860, loss = 0.0620, acc = 0.9880 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 22:26:40.596977: step 293880, loss = 0.0826, acc = 0.9800 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 22:26:45.187092: step 293900, loss = 0.1002, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 22:26:49.781872: step 293920, loss = 0.0694, acc = 0.9820 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:26:54.465637: step 293940, loss = 0.0805, acc = 0.9840 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 22:26:59.963065: step 293960, loss = 0.0825, acc = 0.9780 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 22:27:04.523542: step 293980, loss = 0.0743, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 22:27:09.239998: step 294000, loss = 0.0776, acc = 0.9720 (285.4 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 22:27:23.307504: step 294000, acc = 0.9630, f1 = 0.9619
[Test] 2017-05-09 22:27:32.888266: step 294000, acc = 0.9536, f1 = 0.9532
[Status] 2017-05-09 22:27:32.888348: step 294000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:27:37.653197: step 294020, loss = 0.0840, acc = 0.9740 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 22:27:42.091891: step 294040, loss = 0.0750, acc = 0.9800 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 22:27:46.727612: step 294060, loss = 0.0735, acc = 0.9800 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 22:27:51.351171: step 294080, loss = 0.0753, acc = 0.9860 (298.3 examples/sec; 0.215 sec/batch)
2017-05-09 22:27:56.022606: step 294100, loss = 0.0876, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:28:00.715494: step 294120, loss = 0.0938, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 22:28:05.557953: step 294140, loss = 0.0605, acc = 0.9880 (216.8 examples/sec; 0.295 sec/batch)
2017-05-09 22:28:10.281441: step 294160, loss = 0.0764, acc = 0.9840 (253.2 examples/sec; 0.253 sec/batch)
2017-05-09 22:28:14.884434: step 294180, loss = 0.0757, acc = 0.9800 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 22:28:19.471271: step 294200, loss = 0.0976, acc = 0.9700 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 22:28:24.124368: step 294220, loss = 0.0691, acc = 0.9860 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 22:28:28.616828: step 294240, loss = 0.0781, acc = 0.9740 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 22:28:33.142042: step 294260, loss = 0.0810, acc = 0.9800 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 22:28:37.777138: step 294280, loss = 0.0637, acc = 0.9880 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 22:28:42.361933: step 294300, loss = 0.0752, acc = 0.9820 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 22:28:46.944194: step 294320, loss = 0.0879, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 22:28:52.561684: step 294340, loss = 0.0967, acc = 0.9760 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 22:28:57.154984: step 294360, loss = 0.0617, acc = 0.9960 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 22:29:01.786365: step 294380, loss = 0.0551, acc = 0.9900 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 22:29:06.594760: step 294400, loss = 0.0771, acc = 0.9780 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 22:29:11.102769: step 294420, loss = 0.0681, acc = 0.9820 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 22:29:15.775489: step 294440, loss = 0.0805, acc = 0.9840 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 22:29:20.504639: step 294460, loss = 0.0979, acc = 0.9700 (240.3 examples/sec; 0.266 sec/batch)
2017-05-09 22:29:25.062746: step 294480, loss = 0.0663, acc = 0.9860 (260.9 examples/sec; 0.245 sec/batch)
2017-05-09 22:29:29.686868: step 294500, loss = 0.0954, acc = 0.9700 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 22:29:34.191682: step 294520, loss = 0.0688, acc = 0.9860 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 22:29:38.885135: step 294540, loss = 0.0754, acc = 0.9760 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 22:29:43.382412: step 294560, loss = 0.0550, acc = 0.9900 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 22:29:47.939776: step 294580, loss = 0.0922, acc = 0.9800 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 22:29:52.660199: step 294600, loss = 0.0841, acc = 0.9800 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 22:29:57.168738: step 294620, loss = 0.0978, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 22:30:01.811127: step 294640, loss = 0.0681, acc = 0.9880 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 22:30:06.443888: step 294660, loss = 0.0745, acc = 0.9880 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:30:10.944159: step 294680, loss = 0.0770, acc = 0.9780 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 22:30:15.547696: step 294700, loss = 0.0823, acc = 0.9820 (271.8 examples/sec; 0.236 sec/batch)
2017-05-09 22:30:20.238709: step 294720, loss = 0.0761, acc = 0.9860 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 22:30:24.707251: step 294740, loss = 0.0920, acc = 0.9780 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:30:29.831130: step 294760, loss = 0.0740, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:30:34.489606: step 294780, loss = 0.0461, acc = 0.9980 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 22:30:39.057928: step 294800, loss = 0.0664, acc = 0.9840 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 22:30:43.634428: step 294820, loss = 0.0652, acc = 0.9840 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 22:30:48.508140: step 294840, loss = 0.0705, acc = 0.9860 (232.8 examples/sec; 0.275 sec/batch)
2017-05-09 22:30:53.464776: step 294860, loss = 0.0679, acc = 0.9880 (230.1 examples/sec; 0.278 sec/batch)
2017-05-09 22:30:58.757183: step 294880, loss = 0.0622, acc = 0.9860 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 22:31:03.320628: step 294900, loss = 0.0687, acc = 0.9880 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 22:31:07.970297: step 294920, loss = 0.0905, acc = 0.9740 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 22:31:12.434845: step 294940, loss = 0.0639, acc = 0.9860 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 22:31:16.969263: step 294960, loss = 0.0741, acc = 0.9820 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 22:31:21.763667: step 294980, loss = 0.0744, acc = 0.9820 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 22:31:26.305254: step 295000, loss = 0.0760, acc = 0.9800 (294.1 examples/sec; 0.218 sec/batch)
[Eval] 2017-05-09 22:31:40.495665: step 295000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 22:31:50.107393: step 295000, acc = 0.9563, f1 = 0.9559
[Status] 2017-05-09 22:31:50.107515: step 295000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:31:54.663910: step 295020, loss = 0.0788, acc = 0.9760 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 22:31:59.302953: step 295040, loss = 0.0646, acc = 0.9880 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 22:32:04.162796: step 295060, loss = 0.0757, acc = 0.9800 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 22:32:08.743428: step 295080, loss = 0.0615, acc = 0.9820 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 22:32:13.301509: step 295100, loss = 0.0636, acc = 0.9920 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 22:32:17.982010: step 295120, loss = 0.0781, acc = 0.9820 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 22:32:22.512811: step 295140, loss = 0.0759, acc = 0.9760 (291.1 examples/sec; 0.220 sec/batch)
2017-05-09 22:32:27.091949: step 295160, loss = 0.0621, acc = 0.9880 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 22:32:31.829676: step 295180, loss = 0.0965, acc = 0.9760 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 22:32:36.634564: step 295200, loss = 0.0798, acc = 0.9860 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 22:32:41.243902: step 295220, loss = 0.0780, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 22:32:45.805697: step 295240, loss = 0.0962, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 22:32:50.749427: step 295260, loss = 0.0622, acc = 0.9940 (273.5 examples/sec; 0.234 sec/batch)
2017-05-09 22:32:55.352452: step 295280, loss = 0.0833, acc = 0.9760 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 22:33:00.010555: step 295300, loss = 0.0867, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 22:33:05.127044: step 295320, loss = 0.0820, acc = 0.9880 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 22:33:10.927589: step 295340, loss = 0.0897, acc = 0.9800 (133.2 examples/sec; 0.480 sec/batch)
2017-05-09 22:33:15.517441: step 295360, loss = 0.0629, acc = 0.9900 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:33:20.382338: step 295380, loss = 0.0860, acc = 0.9800 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 22:33:24.869676: step 295400, loss = 0.0667, acc = 0.9900 (287.6 examples/sec; 0.222 sec/batch)
2017-05-09 22:33:29.404117: step 295420, loss = 0.0825, acc = 0.9740 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 22:33:34.121771: step 295440, loss = 0.0760, acc = 0.9840 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 22:33:38.653505: step 295460, loss = 0.0762, acc = 0.9840 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 22:33:43.322644: step 295480, loss = 0.0818, acc = 0.9820 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 22:33:48.001447: step 295500, loss = 0.0824, acc = 0.9720 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 22:33:52.605854: step 295520, loss = 0.0885, acc = 0.9760 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:33:57.140376: step 295540, loss = 0.0707, acc = 0.9840 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:34:01.840523: step 295560, loss = 0.0942, acc = 0.9780 (234.2 examples/sec; 0.273 sec/batch)
2017-05-09 22:34:06.422251: step 295580, loss = 0.0703, acc = 0.9780 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 22:34:10.969265: step 295600, loss = 0.0770, acc = 0.9840 (293.4 examples/sec; 0.218 sec/batch)
2017-05-09 22:34:15.566187: step 295620, loss = 0.0703, acc = 0.9840 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 22:34:20.155233: step 295640, loss = 0.0733, acc = 0.9860 (303.3 examples/sec; 0.211 sec/batch)
2017-05-09 22:34:24.693652: step 295660, loss = 0.0691, acc = 0.9800 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 22:34:29.263349: step 295680, loss = 0.0867, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 22:34:34.021226: step 295700, loss = 0.0763, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 22:34:38.547005: step 295720, loss = 0.0779, acc = 0.9760 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 22:34:43.120014: step 295740, loss = 0.0661, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 22:34:48.049949: step 295760, loss = 0.0846, acc = 0.9800 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 22:34:52.634656: step 295780, loss = 0.0866, acc = 0.9780 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:34:57.202238: step 295800, loss = 0.0786, acc = 0.9880 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 22:35:01.997459: step 295820, loss = 0.0750, acc = 0.9820 (232.3 examples/sec; 0.276 sec/batch)
2017-05-09 22:35:06.444830: step 295840, loss = 0.0921, acc = 0.9660 (294.1 examples/sec; 0.218 sec/batch)
2017-05-09 22:35:11.000119: step 295860, loss = 0.0839, acc = 0.9780 (276.0 examples/sec; 0.232 sec/batch)
2017-05-09 22:35:15.527573: step 295880, loss = 0.0684, acc = 0.9880 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 22:35:20.182498: step 295900, loss = 0.0555, acc = 0.9920 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 22:35:24.767723: step 295920, loss = 0.0722, acc = 0.9880 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:35:29.416630: step 295940, loss = 0.0791, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 22:35:34.369082: step 295960, loss = 0.0753, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 22:35:38.966967: step 295980, loss = 0.0745, acc = 0.9760 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:35:43.567937: step 296000, loss = 0.0647, acc = 0.9840 (281.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 22:35:57.621011: step 296000, acc = 0.9629, f1 = 0.9618
[Test] 2017-05-09 22:36:07.368870: step 296000, acc = 0.9542, f1 = 0.9538
[Status] 2017-05-09 22:36:07.368947: step 296000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:36:11.947348: step 296020, loss = 0.0749, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 22:36:16.770222: step 296040, loss = 0.0895, acc = 0.9760 (247.5 examples/sec; 0.259 sec/batch)
2017-05-09 22:36:21.269002: step 296060, loss = 0.0928, acc = 0.9820 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 22:36:25.910974: step 296080, loss = 0.0737, acc = 0.9800 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 22:36:30.596924: step 296100, loss = 0.0784, acc = 0.9840 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 22:36:35.348917: step 296120, loss = 0.0943, acc = 0.9680 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:36:40.088445: step 296140, loss = 0.0788, acc = 0.9760 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 22:36:44.716201: step 296160, loss = 0.0870, acc = 0.9700 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 22:36:49.392936: step 296180, loss = 0.0941, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 22:36:53.935311: step 296200, loss = 0.0870, acc = 0.9800 (259.9 examples/sec; 0.246 sec/batch)
2017-05-09 22:36:58.538896: step 296220, loss = 0.0775, acc = 0.9740 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 22:37:03.410647: step 296240, loss = 0.0919, acc = 0.9720 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 22:37:07.992137: step 296260, loss = 0.0796, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 22:37:12.643759: step 296280, loss = 0.0767, acc = 0.9800 (273.0 examples/sec; 0.234 sec/batch)
2017-05-09 22:37:17.554138: step 296300, loss = 0.0732, acc = 0.9880 (224.1 examples/sec; 0.286 sec/batch)
2017-05-09 22:37:22.164119: step 296320, loss = 0.0758, acc = 0.9840 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 22:37:26.848806: step 296340, loss = 0.0669, acc = 0.9860 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 22:37:32.374810: step 296360, loss = 0.0829, acc = 0.9800 (213.9 examples/sec; 0.299 sec/batch)
2017-05-09 22:37:36.938382: step 296380, loss = 0.0778, acc = 0.9760 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 22:37:41.616424: step 296400, loss = 0.0858, acc = 0.9720 (264.5 examples/sec; 0.242 sec/batch)
2017-05-09 22:37:46.309174: step 296420, loss = 0.0918, acc = 0.9740 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 22:37:50.962401: step 296440, loss = 0.0704, acc = 0.9860 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:37:55.593322: step 296460, loss = 0.0800, acc = 0.9780 (268.3 examples/sec; 0.239 sec/batch)
2017-05-09 22:38:00.160297: step 296480, loss = 0.0921, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 22:38:04.797158: step 296500, loss = 0.0804, acc = 0.9740 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 22:38:09.399494: step 296520, loss = 0.0762, acc = 0.9880 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 22:38:13.910006: step 296540, loss = 0.0568, acc = 0.9940 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 22:38:18.778917: step 296560, loss = 0.0734, acc = 0.9860 (253.9 examples/sec; 0.252 sec/batch)
2017-05-09 22:38:23.445974: step 296580, loss = 0.0560, acc = 0.9920 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 22:38:28.074518: step 296600, loss = 0.0561, acc = 0.9940 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 22:38:33.029104: step 296620, loss = 0.0697, acc = 0.9860 (216.0 examples/sec; 0.296 sec/batch)
2017-05-09 22:38:37.634465: step 296640, loss = 0.0587, acc = 0.9920 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:38:42.127595: step 296660, loss = 0.0906, acc = 0.9780 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 22:38:46.783342: step 296680, loss = 0.0765, acc = 0.9760 (241.4 examples/sec; 0.265 sec/batch)
2017-05-09 22:38:51.354772: step 296700, loss = 0.0772, acc = 0.9860 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 22:38:55.810002: step 296720, loss = 0.0725, acc = 0.9840 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:39:00.204516: step 296740, loss = 0.0723, acc = 0.9820 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 22:39:04.860833: step 296760, loss = 0.0688, acc = 0.9860 (297.4 examples/sec; 0.215 sec/batch)
2017-05-09 22:39:09.474627: step 296780, loss = 0.0813, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:39:14.169817: step 296800, loss = 0.0766, acc = 0.9840 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:39:18.880060: step 296820, loss = 0.0877, acc = 0.9760 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:39:23.484668: step 296840, loss = 0.0623, acc = 0.9900 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 22:39:28.057050: step 296860, loss = 0.0869, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 22:39:32.822408: step 296880, loss = 0.0691, acc = 0.9840 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 22:39:37.435942: step 296900, loss = 0.0892, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:39:42.223263: step 296920, loss = 0.0653, acc = 0.9900 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 22:39:46.900578: step 296940, loss = 0.0789, acc = 0.9780 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 22:39:51.403185: step 296960, loss = 0.0925, acc = 0.9700 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 22:39:55.966355: step 296980, loss = 0.0796, acc = 0.9840 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 22:40:00.508179: step 297000, loss = 0.0962, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
[Eval] 2017-05-09 22:40:14.562413: step 297000, acc = 0.9646, f1 = 0.9634
[Test] 2017-05-09 22:40:24.371714: step 297000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-09 22:40:24.371818: step 297000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:40:28.951856: step 297020, loss = 0.0785, acc = 0.9840 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 22:40:33.719430: step 297040, loss = 0.0728, acc = 0.9840 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 22:40:38.214298: step 297060, loss = 0.0668, acc = 0.9860 (285.1 examples/sec; 0.225 sec/batch)
2017-05-09 22:40:42.760837: step 297080, loss = 0.0881, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-09 22:40:47.530272: step 297100, loss = 0.0765, acc = 0.9800 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 22:40:52.355562: step 297120, loss = 0.0770, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 22:40:56.947416: step 297140, loss = 0.0774, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 22:41:01.780330: step 297160, loss = 0.0663, acc = 0.9860 (221.7 examples/sec; 0.289 sec/batch)
2017-05-09 22:41:06.332077: step 297180, loss = 0.0629, acc = 0.9860 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 22:41:10.966277: step 297200, loss = 0.0680, acc = 0.9840 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 22:41:15.558529: step 297220, loss = 0.0730, acc = 0.9860 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 22:41:20.408266: step 297240, loss = 0.0777, acc = 0.9800 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 22:41:25.036212: step 297260, loss = 0.0729, acc = 0.9780 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 22:41:29.732082: step 297280, loss = 0.0801, acc = 0.9800 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 22:41:34.504370: step 297300, loss = 0.0727, acc = 0.9880 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 22:41:39.084627: step 297320, loss = 0.0965, acc = 0.9780 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 22:41:43.693836: step 297340, loss = 0.0653, acc = 0.9880 (260.7 examples/sec; 0.245 sec/batch)
2017-05-09 22:41:49.426365: step 297360, loss = 0.0632, acc = 0.9880 (282.2 examples/sec; 0.227 sec/batch)
2017-05-09 22:41:54.070995: step 297380, loss = 0.0867, acc = 0.9840 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 22:41:58.617418: step 297400, loss = 0.0650, acc = 0.9880 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 22:42:03.564464: step 297420, loss = 0.0905, acc = 0.9740 (258.8 examples/sec; 0.247 sec/batch)
2017-05-09 22:42:08.160423: step 297440, loss = 0.0720, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 22:42:12.696094: step 297460, loss = 0.0906, acc = 0.9740 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 22:42:17.377046: step 297480, loss = 0.1058, acc = 0.9700 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 22:42:21.873398: step 297500, loss = 0.0883, acc = 0.9780 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 22:42:26.479508: step 297520, loss = 0.0710, acc = 0.9820 (241.8 examples/sec; 0.265 sec/batch)
2017-05-09 22:42:31.138013: step 297540, loss = 0.0628, acc = 0.9860 (239.4 examples/sec; 0.267 sec/batch)
2017-05-09 22:42:35.596661: step 297560, loss = 0.0755, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-09 22:42:40.209159: step 297580, loss = 0.0807, acc = 0.9760 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 22:42:45.047609: step 297600, loss = 0.0852, acc = 0.9800 (237.9 examples/sec; 0.269 sec/batch)
2017-05-09 22:42:49.557855: step 297620, loss = 0.0921, acc = 0.9800 (266.8 examples/sec; 0.240 sec/batch)
2017-05-09 22:42:54.206198: step 297640, loss = 0.0910, acc = 0.9840 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 22:42:58.834808: step 297660, loss = 0.0672, acc = 0.9860 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 22:43:03.562058: step 297680, loss = 0.0892, acc = 0.9760 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 22:43:08.241797: step 297700, loss = 0.1026, acc = 0.9740 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 22:43:12.854448: step 297720, loss = 0.0569, acc = 0.9900 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 22:43:17.518939: step 297740, loss = 0.0794, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 22:43:22.058339: step 297760, loss = 0.0880, acc = 0.9740 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 22:43:26.690623: step 297780, loss = 0.0702, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 22:43:31.403327: step 297800, loss = 0.0745, acc = 0.9860 (238.3 examples/sec; 0.269 sec/batch)
2017-05-09 22:43:35.878162: step 297820, loss = 0.0900, acc = 0.9660 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:43:40.471606: step 297840, loss = 0.0615, acc = 0.9900 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 22:43:45.139941: step 297860, loss = 0.0881, acc = 0.9780 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 22:43:49.896333: step 297880, loss = 0.0852, acc = 0.9820 (265.3 examples/sec; 0.241 sec/batch)
2017-05-09 22:43:54.610441: step 297900, loss = 0.0959, acc = 0.9700 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:43:59.186807: step 297920, loss = 0.0831, acc = 0.9800 (286.2 examples/sec; 0.224 sec/batch)
2017-05-09 22:44:04.063895: step 297940, loss = 0.0780, acc = 0.9920 (265.5 examples/sec; 0.241 sec/batch)
2017-05-09 22:44:08.660141: step 297960, loss = 0.0714, acc = 0.9800 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 22:44:13.212787: step 297980, loss = 0.0785, acc = 0.9780 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 22:44:17.918847: step 298000, loss = 0.0737, acc = 0.9840 (274.5 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 22:44:31.960966: step 298000, acc = 0.9634, f1 = 0.9623
[Test] 2017-05-09 22:44:41.593180: step 298000, acc = 0.9544, f1 = 0.9541
[Status] 2017-05-09 22:44:41.593267: step 298000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:44:46.309734: step 298020, loss = 0.0619, acc = 0.9920 (308.7 examples/sec; 0.207 sec/batch)
2017-05-09 22:44:50.975588: step 298040, loss = 0.0875, acc = 0.9780 (250.0 examples/sec; 0.256 sec/batch)
2017-05-09 22:44:55.550472: step 298060, loss = 0.0833, acc = 0.9800 (294.0 examples/sec; 0.218 sec/batch)
2017-05-09 22:45:00.414320: step 298080, loss = 0.0542, acc = 0.9940 (242.7 examples/sec; 0.264 sec/batch)
2017-05-09 22:45:04.976180: step 298100, loss = 0.0773, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 22:45:09.576065: step 298120, loss = 0.0599, acc = 0.9840 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 22:45:14.111282: step 298140, loss = 0.0661, acc = 0.9860 (272.3 examples/sec; 0.235 sec/batch)
2017-05-09 22:45:18.849707: step 298160, loss = 0.0698, acc = 0.9880 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 22:45:23.369681: step 298180, loss = 0.0749, acc = 0.9860 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 22:45:27.981269: step 298200, loss = 0.0771, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 22:45:32.750642: step 298220, loss = 0.0824, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 22:45:37.330679: step 298240, loss = 0.0906, acc = 0.9820 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 22:45:41.911057: step 298260, loss = 0.0798, acc = 0.9860 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 22:45:46.626987: step 298280, loss = 0.0922, acc = 0.9780 (253.4 examples/sec; 0.253 sec/batch)
2017-05-09 22:45:51.219991: step 298300, loss = 0.0778, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 22:45:55.866014: step 298320, loss = 0.0738, acc = 0.9780 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:46:00.608740: step 298340, loss = 0.0782, acc = 0.9820 (257.8 examples/sec; 0.248 sec/batch)
2017-05-09 22:46:05.161960: step 298360, loss = 0.0688, acc = 0.9860 (293.0 examples/sec; 0.218 sec/batch)
2017-05-09 22:46:10.498592: step 298380, loss = 0.0845, acc = 0.9740 (262.6 examples/sec; 0.244 sec/batch)
2017-05-09 22:46:15.169337: step 298400, loss = 0.0613, acc = 0.9940 (245.6 examples/sec; 0.261 sec/batch)
2017-05-09 22:46:19.828575: step 298420, loss = 0.0804, acc = 0.9840 (250.3 examples/sec; 0.256 sec/batch)
2017-05-09 22:46:24.534107: step 298440, loss = 0.0636, acc = 0.9880 (254.5 examples/sec; 0.252 sec/batch)
2017-05-09 22:46:29.241700: step 298460, loss = 0.0807, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 22:46:33.995611: step 298480, loss = 0.0990, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 22:46:38.605816: step 298500, loss = 0.0951, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 22:46:43.197151: step 298520, loss = 0.0798, acc = 0.9740 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 22:46:47.830994: step 298540, loss = 0.0683, acc = 0.9840 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 22:46:52.302400: step 298560, loss = 0.0711, acc = 0.9900 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 22:46:56.773977: step 298580, loss = 0.0838, acc = 0.9780 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 22:47:01.876527: step 298600, loss = 0.0836, acc = 0.9760 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 22:47:06.486306: step 298620, loss = 0.0766, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:47:11.069889: step 298640, loss = 0.0871, acc = 0.9780 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 22:47:15.710898: step 298660, loss = 0.0672, acc = 0.9860 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 22:47:20.302641: step 298680, loss = 0.0678, acc = 0.9900 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 22:47:24.841748: step 298700, loss = 0.0781, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 22:47:29.478568: step 298720, loss = 0.0711, acc = 0.9920 (250.7 examples/sec; 0.255 sec/batch)
2017-05-09 22:47:34.111393: step 298740, loss = 0.0795, acc = 0.9840 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 22:47:38.609601: step 298760, loss = 0.1051, acc = 0.9720 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 22:47:43.110396: step 298780, loss = 0.0806, acc = 0.9880 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 22:47:47.986708: step 298800, loss = 0.0692, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 22:47:52.484340: step 298820, loss = 0.0815, acc = 0.9820 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 22:47:57.069429: step 298840, loss = 0.0645, acc = 0.9880 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 22:48:01.772353: step 298860, loss = 0.0726, acc = 0.9840 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 22:48:06.404444: step 298880, loss = 0.0645, acc = 0.9880 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 22:48:11.387079: step 298900, loss = 0.0773, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 22:48:16.090652: step 298920, loss = 0.0881, acc = 0.9780 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 22:48:20.633164: step 298940, loss = 0.0793, acc = 0.9860 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 22:48:25.281858: step 298960, loss = 0.0690, acc = 0.9860 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 22:48:29.984639: step 298980, loss = 0.0935, acc = 0.9680 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 22:48:34.458642: step 299000, loss = 0.0896, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 22:48:48.478361: step 299000, acc = 0.9613, f1 = 0.9599
[Test] 2017-05-09 22:48:58.419438: step 299000, acc = 0.9509, f1 = 0.9505
[Status] 2017-05-09 22:48:58.419523: step 299000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:49:02.986909: step 299020, loss = 0.0766, acc = 0.9860 (271.3 examples/sec; 0.236 sec/batch)
2017-05-09 22:49:07.541337: step 299040, loss = 0.0705, acc = 0.9860 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 22:49:12.250409: step 299060, loss = 0.0654, acc = 0.9880 (252.7 examples/sec; 0.253 sec/batch)
2017-05-09 22:49:16.733533: step 299080, loss = 0.0720, acc = 0.9840 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 22:49:21.273315: step 299100, loss = 0.0824, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 22:49:25.949142: step 299120, loss = 0.0877, acc = 0.9800 (269.4 examples/sec; 0.238 sec/batch)
2017-05-09 22:49:30.703748: step 299140, loss = 0.1046, acc = 0.9660 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 22:49:35.324043: step 299160, loss = 0.0719, acc = 0.9860 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 22:49:39.950808: step 299180, loss = 0.0800, acc = 0.9740 (261.0 examples/sec; 0.245 sec/batch)
2017-05-09 22:49:44.678864: step 299200, loss = 0.0848, acc = 0.9820 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 22:49:49.276470: step 299220, loss = 0.0656, acc = 0.9860 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 22:49:53.876280: step 299240, loss = 0.0708, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 22:49:58.889858: step 299260, loss = 0.0713, acc = 0.9840 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 22:50:03.436269: step 299280, loss = 0.0725, acc = 0.9840 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 22:50:08.088799: step 299300, loss = 0.0726, acc = 0.9800 (246.4 examples/sec; 0.260 sec/batch)
2017-05-09 22:50:12.599809: step 299320, loss = 0.0991, acc = 0.9620 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 22:50:17.304871: step 299340, loss = 0.0821, acc = 0.9840 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 22:50:21.969586: step 299360, loss = 0.0664, acc = 0.9860 (268.7 examples/sec; 0.238 sec/batch)
2017-05-09 22:50:27.569371: step 299380, loss = 0.0936, acc = 0.9760 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 22:50:32.256058: step 299400, loss = 0.0688, acc = 0.9880 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 22:50:36.842297: step 299420, loss = 0.0943, acc = 0.9680 (267.6 examples/sec; 0.239 sec/batch)
2017-05-09 22:50:41.417852: step 299440, loss = 0.0834, acc = 0.9720 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 22:50:46.168060: step 299460, loss = 0.0805, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 22:50:50.617931: step 299480, loss = 0.0876, acc = 0.9760 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 22:50:55.265506: step 299500, loss = 0.0820, acc = 0.9820 (271.0 examples/sec; 0.236 sec/batch)
2017-05-09 22:51:00.239480: step 299520, loss = 0.0892, acc = 0.9840 (231.2 examples/sec; 0.277 sec/batch)
2017-05-09 22:51:04.804200: step 299540, loss = 0.0577, acc = 0.9920 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 22:51:09.413292: step 299560, loss = 0.0900, acc = 0.9720 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 22:51:14.004063: step 299580, loss = 0.0700, acc = 0.9900 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 22:51:18.811904: step 299600, loss = 0.0754, acc = 0.9840 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 22:51:23.317444: step 299620, loss = 0.0791, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-09 22:51:27.910331: step 299640, loss = 0.0776, acc = 0.9860 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 22:51:32.673827: step 299660, loss = 0.0801, acc = 0.9840 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:51:37.468201: step 299680, loss = 0.0811, acc = 0.9840 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:51:42.129467: step 299700, loss = 0.0732, acc = 0.9860 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:51:46.952192: step 299720, loss = 0.0938, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 22:51:51.577329: step 299740, loss = 0.0758, acc = 0.9860 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 22:51:56.099573: step 299760, loss = 0.0788, acc = 0.9900 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 22:52:00.694881: step 299780, loss = 0.0755, acc = 0.9840 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 22:52:05.315294: step 299800, loss = 0.0747, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 22:52:09.953613: step 299820, loss = 0.0733, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 22:52:14.689252: step 299840, loss = 0.0824, acc = 0.9800 (254.0 examples/sec; 0.252 sec/batch)
2017-05-09 22:52:19.306455: step 299860, loss = 0.0668, acc = 0.9780 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 22:52:23.806324: step 299880, loss = 0.0754, acc = 0.9800 (301.6 examples/sec; 0.212 sec/batch)
2017-05-09 22:52:28.354237: step 299900, loss = 0.0820, acc = 0.9820 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 22:52:32.968323: step 299920, loss = 0.0600, acc = 0.9900 (300.4 examples/sec; 0.213 sec/batch)
2017-05-09 22:52:37.428645: step 299940, loss = 0.0739, acc = 0.9840 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 22:52:42.011815: step 299960, loss = 0.0638, acc = 0.9880 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 22:52:46.554735: step 299980, loss = 0.0751, acc = 0.9840 (304.4 examples/sec; 0.210 sec/batch)
2017-05-09 22:52:51.322520: step 300000, loss = 0.0708, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
[Eval] 2017-05-09 22:53:05.231244: step 300000, acc = 0.9641, f1 = 0.9630
[Test] 2017-05-09 22:53:15.030303: step 300000, acc = 0.9556, f1 = 0.9553
[Status] 2017-05-09 22:53:15.030417: step 300000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:53:19.612989: step 300020, loss = 0.0699, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 22:53:24.175637: step 300040, loss = 0.0856, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 22:53:28.929405: step 300060, loss = 0.0777, acc = 0.9840 (241.8 examples/sec; 0.265 sec/batch)
2017-05-09 22:53:33.578522: step 300080, loss = 0.0754, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 22:53:38.194896: step 300100, loss = 0.0655, acc = 0.9840 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 22:53:42.890752: step 300120, loss = 0.0927, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 22:53:47.540586: step 300140, loss = 0.0846, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 22:53:52.019147: step 300160, loss = 0.0810, acc = 0.9800 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 22:53:56.534265: step 300180, loss = 0.0691, acc = 0.9800 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 22:54:01.482139: step 300200, loss = 0.0702, acc = 0.9880 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 22:54:06.136603: step 300220, loss = 0.0763, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 22:54:10.716724: step 300240, loss = 0.0800, acc = 0.9840 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 22:54:15.385168: step 300260, loss = 0.0762, acc = 0.9860 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 22:54:19.863763: step 300280, loss = 0.0868, acc = 0.9680 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 22:54:24.482952: step 300300, loss = 0.0751, acc = 0.9760 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:54:29.263869: step 300320, loss = 0.0674, acc = 0.9820 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 22:54:33.934111: step 300340, loss = 0.0811, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 22:54:38.420050: step 300360, loss = 0.0858, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 22:54:43.736376: step 300380, loss = 0.0772, acc = 0.9840 (159.2 examples/sec; 0.402 sec/batch)
2017-05-09 22:54:48.451736: step 300400, loss = 0.0984, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 22:54:53.038148: step 300420, loss = 0.0882, acc = 0.9820 (271.2 examples/sec; 0.236 sec/batch)
2017-05-09 22:54:57.796664: step 300440, loss = 0.0809, acc = 0.9740 (236.8 examples/sec; 0.270 sec/batch)
2017-05-09 22:55:02.374209: step 300460, loss = 0.0831, acc = 0.9820 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 22:55:07.023339: step 300480, loss = 0.0735, acc = 0.9820 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 22:55:11.692844: step 300500, loss = 0.0678, acc = 0.9780 (249.0 examples/sec; 0.257 sec/batch)
2017-05-09 22:55:16.479355: step 300520, loss = 0.0662, acc = 0.9900 (285.1 examples/sec; 0.224 sec/batch)
2017-05-09 22:55:21.210048: step 300540, loss = 0.0784, acc = 0.9760 (281.3 examples/sec; 0.228 sec/batch)
2017-05-09 22:55:25.854663: step 300560, loss = 0.0750, acc = 0.9780 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 22:55:30.570451: step 300580, loss = 0.0549, acc = 0.9880 (283.6 examples/sec; 0.226 sec/batch)
2017-05-09 22:55:35.144738: step 300600, loss = 0.1018, acc = 0.9720 (280.5 examples/sec; 0.228 sec/batch)
2017-05-09 22:55:39.661470: step 300620, loss = 0.0738, acc = 0.9760 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 22:55:44.658215: step 300640, loss = 0.0837, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 22:55:49.207200: step 300660, loss = 0.0719, acc = 0.9800 (273.4 examples/sec; 0.234 sec/batch)
2017-05-09 22:55:53.752984: step 300680, loss = 0.0727, acc = 0.9820 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 22:55:58.401739: step 300700, loss = 0.0578, acc = 0.9920 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 22:56:02.886069: step 300720, loss = 0.0759, acc = 0.9820 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 22:56:07.491775: step 300740, loss = 0.0851, acc = 0.9860 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 22:56:12.154832: step 300760, loss = 0.0595, acc = 0.9880 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 22:56:16.723387: step 300780, loss = 0.0603, acc = 0.9920 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 22:56:21.249577: step 300800, loss = 0.0541, acc = 0.9900 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 22:56:26.020581: step 300820, loss = 0.0764, acc = 0.9820 (234.6 examples/sec; 0.273 sec/batch)
2017-05-09 22:56:30.641326: step 300840, loss = 0.0636, acc = 0.9880 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 22:56:35.238156: step 300860, loss = 0.0764, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 22:56:39.935292: step 300880, loss = 0.0606, acc = 0.9920 (257.0 examples/sec; 0.249 sec/batch)
2017-05-09 22:56:44.663396: step 300900, loss = 0.0798, acc = 0.9740 (265.8 examples/sec; 0.241 sec/batch)
2017-05-09 22:56:49.206168: step 300920, loss = 0.0598, acc = 0.9880 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 22:56:53.752724: step 300940, loss = 0.0821, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 22:56:58.503929: step 300960, loss = 0.0663, acc = 0.9900 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 22:57:03.120656: step 300980, loss = 0.0723, acc = 0.9780 (270.5 examples/sec; 0.237 sec/batch)
2017-05-09 22:57:07.715162: step 301000, loss = 0.0797, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 22:57:21.689374: step 301000, acc = 0.9635, f1 = 0.9622
[Test] 2017-05-09 22:57:31.523301: step 301000, acc = 0.9543, f1 = 0.9539
[Status] 2017-05-09 22:57:31.523406: step 301000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 22:57:36.077231: step 301020, loss = 0.0669, acc = 0.9820 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 22:57:40.859411: step 301040, loss = 0.0697, acc = 0.9840 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 22:57:45.453913: step 301060, loss = 0.0686, acc = 0.9920 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 22:57:50.056578: step 301080, loss = 0.0989, acc = 0.9600 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 22:57:54.630123: step 301100, loss = 0.0722, acc = 0.9860 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 22:57:59.380009: step 301120, loss = 0.0795, acc = 0.9780 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 22:58:03.963761: step 301140, loss = 0.0740, acc = 0.9880 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 22:58:08.536577: step 301160, loss = 0.0705, acc = 0.9780 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 22:58:13.261988: step 301180, loss = 0.0635, acc = 0.9900 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 22:58:17.914608: step 301200, loss = 0.0657, acc = 0.9880 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 22:58:22.543022: step 301220, loss = 0.0752, acc = 0.9840 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 22:58:27.225391: step 301240, loss = 0.0778, acc = 0.9800 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 22:58:31.954745: step 301260, loss = 0.0688, acc = 0.9860 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 22:58:36.492525: step 301280, loss = 0.0671, acc = 0.9900 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 22:58:41.294044: step 301300, loss = 0.0770, acc = 0.9800 (298.1 examples/sec; 0.215 sec/batch)
2017-05-09 22:58:46.075799: step 301320, loss = 0.0769, acc = 0.9840 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 22:58:50.681190: step 301340, loss = 0.0801, acc = 0.9820 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 22:58:55.307554: step 301360, loss = 0.0700, acc = 0.9860 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 22:58:59.916436: step 301380, loss = 0.0844, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 22:59:05.462236: step 301400, loss = 0.0562, acc = 0.9900 (295.0 examples/sec; 0.217 sec/batch)
2017-05-09 22:59:10.132099: step 301420, loss = 0.0781, acc = 0.9860 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 22:59:14.605186: step 301440, loss = 0.0699, acc = 0.9840 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 22:59:19.281078: step 301460, loss = 0.0788, acc = 0.9880 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 22:59:24.104875: step 301480, loss = 0.0651, acc = 0.9820 (228.5 examples/sec; 0.280 sec/batch)
2017-05-09 22:59:28.689815: step 301500, loss = 0.0691, acc = 0.9800 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 22:59:33.241261: step 301520, loss = 0.0689, acc = 0.9840 (283.0 examples/sec; 0.226 sec/batch)
2017-05-09 22:59:37.831173: step 301540, loss = 0.0618, acc = 0.9920 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 22:59:42.513851: step 301560, loss = 0.1118, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 22:59:47.043090: step 301580, loss = 0.0819, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 22:59:51.698536: step 301600, loss = 0.0783, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 22:59:56.319181: step 301620, loss = 0.0798, acc = 0.9880 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 23:00:00.836892: step 301640, loss = 0.0640, acc = 0.9900 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 23:00:05.392259: step 301660, loss = 0.0700, acc = 0.9760 (304.0 examples/sec; 0.211 sec/batch)
2017-05-09 23:00:09.913570: step 301680, loss = 0.0915, acc = 0.9740 (288.9 examples/sec; 0.222 sec/batch)
2017-05-09 23:00:14.464998: step 301700, loss = 0.0614, acc = 0.9900 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 23:00:19.062031: step 301720, loss = 0.0757, acc = 0.9820 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 23:00:23.781710: step 301740, loss = 0.0965, acc = 0.9760 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 23:00:28.310950: step 301760, loss = 0.0897, acc = 0.9740 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 23:00:32.853012: step 301780, loss = 0.0872, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:00:37.437094: step 301800, loss = 0.0650, acc = 0.9860 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 23:00:42.048580: step 301820, loss = 0.0889, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 23:00:46.716709: step 301840, loss = 0.0687, acc = 0.9800 (254.4 examples/sec; 0.252 sec/batch)
2017-05-09 23:00:51.261586: step 301860, loss = 0.0820, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:00:56.163718: step 301880, loss = 0.0844, acc = 0.9760 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 23:01:00.704589: step 301900, loss = 0.0888, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:01:05.350448: step 301920, loss = 0.0800, acc = 0.9840 (259.4 examples/sec; 0.247 sec/batch)
2017-05-09 23:01:10.290929: step 301940, loss = 0.0555, acc = 0.9960 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 23:01:14.742122: step 301960, loss = 0.0576, acc = 0.9920 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:01:19.238959: step 301980, loss = 0.0620, acc = 0.9840 (286.5 examples/sec; 0.223 sec/batch)
2017-05-09 23:01:23.904013: step 302000, loss = 0.0648, acc = 0.9840 (294.3 examples/sec; 0.217 sec/batch)
[Eval] 2017-05-09 23:01:37.887312: step 302000, acc = 0.9637, f1 = 0.9624
[Test] 2017-05-09 23:01:47.377318: step 302000, acc = 0.9546, f1 = 0.9542
[Status] 2017-05-09 23:01:47.377395: step 302000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:01:52.019761: step 302020, loss = 0.0634, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 23:01:56.514706: step 302040, loss = 0.0639, acc = 0.9860 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 23:02:01.122168: step 302060, loss = 0.0652, acc = 0.9820 (295.7 examples/sec; 0.216 sec/batch)
2017-05-09 23:02:05.901359: step 302080, loss = 0.0633, acc = 0.9900 (227.8 examples/sec; 0.281 sec/batch)
2017-05-09 23:02:10.595331: step 302100, loss = 0.0797, acc = 0.9800 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 23:02:15.146216: step 302120, loss = 0.0663, acc = 0.9880 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 23:02:19.703977: step 302140, loss = 0.0722, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:02:24.517925: step 302160, loss = 0.0730, acc = 0.9900 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 23:02:29.132553: step 302180, loss = 0.0610, acc = 0.9900 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 23:02:33.755109: step 302200, loss = 0.0793, acc = 0.9800 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:02:38.513101: step 302220, loss = 0.0589, acc = 0.9880 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 23:02:43.150106: step 302240, loss = 0.0794, acc = 0.9800 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 23:02:47.749750: step 302260, loss = 0.0777, acc = 0.9880 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 23:02:52.534462: step 302280, loss = 0.0746, acc = 0.9840 (255.7 examples/sec; 0.250 sec/batch)
2017-05-09 23:02:57.067970: step 302300, loss = 0.0689, acc = 0.9820 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 23:03:01.695906: step 302320, loss = 0.0699, acc = 0.9840 (259.6 examples/sec; 0.247 sec/batch)
2017-05-09 23:03:06.407945: step 302340, loss = 0.0625, acc = 0.9840 (246.7 examples/sec; 0.259 sec/batch)
2017-05-09 23:03:10.900962: step 302360, loss = 0.0847, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 23:03:15.601139: step 302380, loss = 0.0832, acc = 0.9800 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 23:03:20.937320: step 302400, loss = 0.0652, acc = 0.9800 (245.6 examples/sec; 0.261 sec/batch)
2017-05-09 23:03:25.455474: step 302420, loss = 0.0624, acc = 0.9880 (291.3 examples/sec; 0.220 sec/batch)
2017-05-09 23:03:30.048724: step 302440, loss = 0.0763, acc = 0.9840 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 23:03:34.660098: step 302460, loss = 0.0620, acc = 0.9880 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 23:03:39.373652: step 302480, loss = 0.0649, acc = 0.9880 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 23:03:43.960705: step 302500, loss = 0.0723, acc = 0.9880 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 23:03:48.490238: step 302520, loss = 0.0578, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 23:03:53.389189: step 302540, loss = 0.0813, acc = 0.9780 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:03:57.917178: step 302560, loss = 0.0799, acc = 0.9820 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 23:04:02.319530: step 302580, loss = 0.1063, acc = 0.9660 (298.0 examples/sec; 0.215 sec/batch)
2017-05-09 23:04:07.299829: step 302600, loss = 0.0711, acc = 0.9840 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 23:04:11.830746: step 302620, loss = 0.0920, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:04:16.502964: step 302640, loss = 0.0717, acc = 0.9860 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 23:04:21.120383: step 302660, loss = 0.0837, acc = 0.9740 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:04:25.606504: step 302680, loss = 0.0716, acc = 0.9840 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 23:04:30.079337: step 302700, loss = 0.0655, acc = 0.9880 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:04:34.798137: step 302720, loss = 0.0795, acc = 0.9840 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 23:04:39.383445: step 302740, loss = 0.0956, acc = 0.9740 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:04:43.971242: step 302760, loss = 0.0947, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 23:04:48.901815: step 302780, loss = 0.0815, acc = 0.9740 (230.9 examples/sec; 0.277 sec/batch)
2017-05-09 23:04:53.385285: step 302800, loss = 0.0636, acc = 0.9920 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 23:04:57.994689: step 302820, loss = 0.0848, acc = 0.9840 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 23:05:02.787263: step 302840, loss = 0.0642, acc = 0.9880 (249.2 examples/sec; 0.257 sec/batch)
2017-05-09 23:05:07.303930: step 302860, loss = 0.0737, acc = 0.9820 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 23:05:11.823095: step 302880, loss = 0.0647, acc = 0.9920 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 23:05:16.415061: step 302900, loss = 0.0799, acc = 0.9820 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 23:05:21.129250: step 302920, loss = 0.1131, acc = 0.9700 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 23:05:25.699184: step 302940, loss = 0.1012, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 23:05:30.235452: step 302960, loss = 0.0709, acc = 0.9880 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 23:05:35.156604: step 302980, loss = 0.0958, acc = 0.9740 (262.7 examples/sec; 0.244 sec/batch)
2017-05-09 23:05:39.767512: step 303000, loss = 0.0643, acc = 0.9880 (279.3 examples/sec; 0.229 sec/batch)
[Eval] 2017-05-09 23:05:53.784121: step 303000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 23:06:03.927575: step 303000, acc = 0.9564, f1 = 0.9561
[Status] 2017-05-09 23:06:03.927666: step 303000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:06:08.435477: step 303020, loss = 0.0881, acc = 0.9820 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 23:06:12.977132: step 303040, loss = 0.0707, acc = 0.9820 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 23:06:17.761830: step 303060, loss = 0.0865, acc = 0.9760 (237.7 examples/sec; 0.269 sec/batch)
2017-05-09 23:06:22.421670: step 303080, loss = 0.0870, acc = 0.9720 (289.0 examples/sec; 0.221 sec/batch)
2017-05-09 23:06:26.941017: step 303100, loss = 0.0903, acc = 0.9720 (300.4 examples/sec; 0.213 sec/batch)
2017-05-09 23:06:31.668470: step 303120, loss = 0.0811, acc = 0.9800 (245.0 examples/sec; 0.261 sec/batch)
2017-05-09 23:06:36.402307: step 303140, loss = 0.0672, acc = 0.9860 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:06:41.118237: step 303160, loss = 0.0615, acc = 0.9900 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 23:06:45.686617: step 303180, loss = 0.0716, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 23:06:50.383571: step 303200, loss = 0.0696, acc = 0.9820 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 23:06:54.987272: step 303220, loss = 0.0734, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 23:06:59.605985: step 303240, loss = 0.0707, acc = 0.9860 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 23:07:04.304126: step 303260, loss = 0.0538, acc = 0.9920 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 23:07:08.872727: step 303280, loss = 0.0774, acc = 0.9800 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 23:07:13.475680: step 303300, loss = 0.0838, acc = 0.9780 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 23:07:18.192717: step 303320, loss = 0.0943, acc = 0.9760 (267.3 examples/sec; 0.239 sec/batch)
2017-05-09 23:07:22.705260: step 303340, loss = 0.0632, acc = 0.9880 (297.5 examples/sec; 0.215 sec/batch)
2017-05-09 23:07:27.376758: step 303360, loss = 0.0848, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 23:07:32.215403: step 303380, loss = 0.0710, acc = 0.9800 (239.3 examples/sec; 0.267 sec/batch)
2017-05-09 23:07:36.693072: step 303400, loss = 0.0866, acc = 0.9760 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 23:07:42.387140: step 303420, loss = 0.0747, acc = 0.9820 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 23:07:47.132789: step 303440, loss = 0.0863, acc = 0.9740 (243.5 examples/sec; 0.263 sec/batch)
2017-05-09 23:07:51.702817: step 303460, loss = 0.0660, acc = 0.9800 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:07:56.340992: step 303480, loss = 0.0929, acc = 0.9780 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 23:08:01.172513: step 303500, loss = 0.0921, acc = 0.9680 (250.0 examples/sec; 0.256 sec/batch)
2017-05-09 23:08:05.650733: step 303520, loss = 0.0754, acc = 0.9780 (272.8 examples/sec; 0.235 sec/batch)
2017-05-09 23:08:10.224916: step 303540, loss = 0.0807, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 23:08:14.825194: step 303560, loss = 0.0861, acc = 0.9740 (289.3 examples/sec; 0.221 sec/batch)
2017-05-09 23:08:19.478223: step 303580, loss = 0.0728, acc = 0.9840 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 23:08:23.984239: step 303600, loss = 0.0694, acc = 0.9840 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 23:08:28.703346: step 303620, loss = 0.0597, acc = 0.9920 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 23:08:33.369131: step 303640, loss = 0.0669, acc = 0.9900 (289.7 examples/sec; 0.221 sec/batch)
2017-05-09 23:08:38.005956: step 303660, loss = 0.0684, acc = 0.9880 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 23:08:42.709078: step 303680, loss = 0.0658, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 23:08:47.454949: step 303700, loss = 0.0743, acc = 0.9840 (264.2 examples/sec; 0.242 sec/batch)
2017-05-09 23:08:52.085223: step 303720, loss = 0.0726, acc = 0.9860 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 23:08:56.645531: step 303740, loss = 0.1192, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 23:09:01.548591: step 303760, loss = 0.0623, acc = 0.9920 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 23:09:06.014850: step 303780, loss = 0.1153, acc = 0.9620 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 23:09:10.684960: step 303800, loss = 0.0950, acc = 0.9800 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 23:09:15.564044: step 303820, loss = 0.0589, acc = 0.9840 (225.8 examples/sec; 0.283 sec/batch)
2017-05-09 23:09:20.452469: step 303840, loss = 0.0687, acc = 0.9880 (294.7 examples/sec; 0.217 sec/batch)
2017-05-09 23:09:25.045887: step 303860, loss = 0.0803, acc = 0.9900 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 23:09:29.954695: step 303880, loss = 0.0698, acc = 0.9860 (213.8 examples/sec; 0.299 sec/batch)
2017-05-09 23:09:34.513952: step 303900, loss = 0.0940, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 23:09:39.270906: step 303920, loss = 0.0682, acc = 0.9860 (230.9 examples/sec; 0.277 sec/batch)
2017-05-09 23:09:43.984102: step 303940, loss = 0.0696, acc = 0.9840 (240.5 examples/sec; 0.266 sec/batch)
2017-05-09 23:09:48.719111: step 303960, loss = 0.0843, acc = 0.9760 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 23:09:53.318815: step 303980, loss = 0.0816, acc = 0.9760 (292.3 examples/sec; 0.219 sec/batch)
2017-05-09 23:09:58.007886: step 304000, loss = 0.0705, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 23:10:12.188086: step 304000, acc = 0.9628, f1 = 0.9615
[Test] 2017-05-09 23:10:21.810432: step 304000, acc = 0.9529, f1 = 0.9525
[Status] 2017-05-09 23:10:21.810524: step 304000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:10:26.326560: step 304020, loss = 0.0778, acc = 0.9800 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 23:10:31.038226: step 304040, loss = 0.0671, acc = 0.9880 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 23:10:35.551547: step 304060, loss = 0.0760, acc = 0.9840 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 23:10:40.128259: step 304080, loss = 0.0679, acc = 0.9900 (265.6 examples/sec; 0.241 sec/batch)
2017-05-09 23:10:44.895072: step 304100, loss = 0.0851, acc = 0.9700 (287.6 examples/sec; 0.223 sec/batch)
2017-05-09 23:10:49.600790: step 304120, loss = 0.0837, acc = 0.9840 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 23:10:54.146446: step 304140, loss = 0.0842, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 23:10:58.828437: step 304160, loss = 0.0610, acc = 0.9840 (248.8 examples/sec; 0.257 sec/batch)
2017-05-09 23:11:03.548049: step 304180, loss = 0.0717, acc = 0.9860 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 23:11:08.107471: step 304200, loss = 0.0654, acc = 0.9860 (300.9 examples/sec; 0.213 sec/batch)
2017-05-09 23:11:12.727203: step 304220, loss = 0.0791, acc = 0.9780 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 23:11:17.432382: step 304240, loss = 0.0709, acc = 0.9860 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 23:11:22.219580: step 304260, loss = 0.0676, acc = 0.9860 (270.3 examples/sec; 0.237 sec/batch)
2017-05-09 23:11:26.837397: step 304280, loss = 0.0690, acc = 0.9900 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 23:11:31.800243: step 304300, loss = 0.0672, acc = 0.9860 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:11:36.370445: step 304320, loss = 0.0721, acc = 0.9840 (278.1 examples/sec; 0.230 sec/batch)
2017-05-09 23:11:40.958079: step 304340, loss = 0.0869, acc = 0.9780 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 23:11:45.654015: step 304360, loss = 0.0803, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 23:11:50.158717: step 304380, loss = 0.0753, acc = 0.9860 (290.9 examples/sec; 0.220 sec/batch)
2017-05-09 23:11:54.714805: step 304400, loss = 0.0654, acc = 0.9920 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 23:12:00.245799: step 304420, loss = 0.0676, acc = 0.9880 (231.4 examples/sec; 0.277 sec/batch)
2017-05-09 23:12:04.751429: step 304440, loss = 0.0679, acc = 0.9880 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 23:12:09.651866: step 304460, loss = 0.0879, acc = 0.9660 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 23:12:14.306363: step 304480, loss = 0.0916, acc = 0.9760 (260.0 examples/sec; 0.246 sec/batch)
2017-05-09 23:12:19.028832: step 304500, loss = 0.0865, acc = 0.9820 (292.8 examples/sec; 0.219 sec/batch)
2017-05-09 23:12:23.667703: step 304520, loss = 0.0772, acc = 0.9840 (270.1 examples/sec; 0.237 sec/batch)
2017-05-09 23:12:28.226831: step 304540, loss = 0.0694, acc = 0.9900 (267.2 examples/sec; 0.240 sec/batch)
2017-05-09 23:12:33.341412: step 304560, loss = 0.0771, acc = 0.9840 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 23:12:37.875802: step 304580, loss = 0.0656, acc = 0.9920 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:12:42.390199: step 304600, loss = 0.0890, acc = 0.9700 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 23:12:47.150745: step 304620, loss = 0.0703, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 23:12:51.705860: step 304640, loss = 0.0745, acc = 0.9820 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 23:12:56.223146: step 304660, loss = 0.0820, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:13:00.875256: step 304680, loss = 0.0668, acc = 0.9800 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 23:13:05.384942: step 304700, loss = 0.0811, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 23:13:09.974362: step 304720, loss = 0.0775, acc = 0.9820 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 23:13:14.490046: step 304740, loss = 0.0795, acc = 0.9760 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 23:13:19.313795: step 304760, loss = 0.0640, acc = 0.9860 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 23:13:23.952515: step 304780, loss = 0.0816, acc = 0.9820 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:13:28.604093: step 304800, loss = 0.0771, acc = 0.9860 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 23:13:33.322051: step 304820, loss = 0.0698, acc = 0.9860 (274.2 examples/sec; 0.233 sec/batch)
2017-05-09 23:13:37.980249: step 304840, loss = 0.0890, acc = 0.9700 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 23:13:42.666109: step 304860, loss = 0.0746, acc = 0.9800 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 23:13:47.614244: step 304880, loss = 0.0660, acc = 0.9880 (270.6 examples/sec; 0.236 sec/batch)
2017-05-09 23:13:52.188712: step 304900, loss = 0.0720, acc = 0.9860 (282.4 examples/sec; 0.227 sec/batch)
2017-05-09 23:13:56.717581: step 304920, loss = 0.0691, acc = 0.9860 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 23:14:01.547660: step 304940, loss = 0.0804, acc = 0.9780 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 23:14:06.086023: step 304960, loss = 0.0875, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 23:14:10.703912: step 304980, loss = 0.0864, acc = 0.9860 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 23:14:15.209452: step 305000, loss = 0.0788, acc = 0.9760 (295.3 examples/sec; 0.217 sec/batch)
[Eval] 2017-05-09 23:14:29.243958: step 305000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 23:14:38.965189: step 305000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 23:14:38.965285: step 305000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:14:43.610072: step 305020, loss = 0.0898, acc = 0.9760 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 23:14:48.260050: step 305040, loss = 0.0921, acc = 0.9760 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 23:14:52.797361: step 305060, loss = 0.0889, acc = 0.9800 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 23:14:57.409710: step 305080, loss = 0.1004, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 23:15:02.143334: step 305100, loss = 0.0645, acc = 0.9820 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 23:15:06.753397: step 305120, loss = 0.0686, acc = 0.9920 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 23:15:11.319032: step 305140, loss = 0.0802, acc = 0.9800 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 23:15:16.051440: step 305160, loss = 0.1024, acc = 0.9620 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 23:15:20.685479: step 305180, loss = 0.0886, acc = 0.9800 (289.1 examples/sec; 0.221 sec/batch)
2017-05-09 23:15:25.319805: step 305200, loss = 0.0546, acc = 0.9880 (262.1 examples/sec; 0.244 sec/batch)
2017-05-09 23:15:30.015491: step 305220, loss = 0.0731, acc = 0.9840 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:15:34.679020: step 305240, loss = 0.0608, acc = 0.9900 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 23:15:39.235022: step 305260, loss = 0.0835, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:15:44.030680: step 305280, loss = 0.0713, acc = 0.9820 (237.5 examples/sec; 0.269 sec/batch)
2017-05-09 23:15:48.644973: step 305300, loss = 0.0662, acc = 0.9880 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 23:15:53.122476: step 305320, loss = 0.0785, acc = 0.9780 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 23:15:57.593049: step 305340, loss = 0.0789, acc = 0.9800 (290.4 examples/sec; 0.220 sec/batch)
2017-05-09 23:16:02.425148: step 305360, loss = 0.0707, acc = 0.9840 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 23:16:07.079808: step 305380, loss = 0.0788, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 23:16:11.682504: step 305400, loss = 0.0730, acc = 0.9800 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 23:16:17.357118: step 305420, loss = 0.0842, acc = 0.9760 (132.5 examples/sec; 0.483 sec/batch)
2017-05-09 23:16:21.951940: step 305440, loss = 0.0773, acc = 0.9780 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 23:16:26.433985: step 305460, loss = 0.0672, acc = 0.9840 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 23:16:31.207086: step 305480, loss = 0.0608, acc = 0.9900 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 23:16:35.816904: step 305500, loss = 0.0622, acc = 0.9900 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 23:16:40.439733: step 305520, loss = 0.0820, acc = 0.9780 (284.3 examples/sec; 0.225 sec/batch)
2017-05-09 23:16:45.170082: step 305540, loss = 0.0639, acc = 0.9900 (281.7 examples/sec; 0.227 sec/batch)
2017-05-09 23:16:49.748452: step 305560, loss = 0.0763, acc = 0.9760 (276.3 examples/sec; 0.232 sec/batch)
2017-05-09 23:16:54.467651: step 305580, loss = 0.0746, acc = 0.9880 (265.2 examples/sec; 0.241 sec/batch)
2017-05-09 23:16:59.346531: step 305600, loss = 0.0905, acc = 0.9760 (247.2 examples/sec; 0.259 sec/batch)
2017-05-09 23:17:03.967608: step 305620, loss = 0.0706, acc = 0.9880 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:17:08.659753: step 305640, loss = 0.0834, acc = 0.9720 (267.0 examples/sec; 0.240 sec/batch)
2017-05-09 23:17:13.222895: step 305660, loss = 0.0843, acc = 0.9720 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 23:17:17.901123: step 305680, loss = 0.0714, acc = 0.9840 (287.5 examples/sec; 0.223 sec/batch)
2017-05-09 23:17:22.575310: step 305700, loss = 0.0738, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 23:17:27.110852: step 305720, loss = 0.0700, acc = 0.9860 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 23:17:31.994454: step 305740, loss = 0.0775, acc = 0.9800 (272.9 examples/sec; 0.235 sec/batch)
2017-05-09 23:17:36.611512: step 305760, loss = 0.0714, acc = 0.9860 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 23:17:41.114266: step 305780, loss = 0.0583, acc = 0.9860 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 23:17:45.855025: step 305800, loss = 0.0694, acc = 0.9880 (295.6 examples/sec; 0.217 sec/batch)
2017-05-09 23:17:50.429704: step 305820, loss = 0.0846, acc = 0.9780 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 23:17:55.039739: step 305840, loss = 0.0814, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 23:17:59.718761: step 305860, loss = 0.0691, acc = 0.9860 (222.2 examples/sec; 0.288 sec/batch)
2017-05-09 23:18:04.373505: step 305880, loss = 0.0735, acc = 0.9840 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 23:18:09.049012: step 305900, loss = 0.0752, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:18:13.752508: step 305920, loss = 0.0629, acc = 0.9880 (266.2 examples/sec; 0.240 sec/batch)
2017-05-09 23:18:18.413268: step 305940, loss = 0.0551, acc = 0.9920 (294.5 examples/sec; 0.217 sec/batch)
2017-05-09 23:18:22.942679: step 305960, loss = 0.0778, acc = 0.9800 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 23:18:27.628811: step 305980, loss = 0.0648, acc = 0.9880 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 23:18:32.204157: step 306000, loss = 0.0740, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-09 23:18:46.295630: step 306000, acc = 0.9629, f1 = 0.9617
[Test] 2017-05-09 23:18:55.746790: step 306000, acc = 0.9543, f1 = 0.9540
[Status] 2017-05-09 23:18:55.746894: step 306000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:19:00.513869: step 306020, loss = 0.0778, acc = 0.9800 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:19:05.223861: step 306040, loss = 0.0813, acc = 0.9800 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 23:19:09.772982: step 306060, loss = 0.0764, acc = 0.9760 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 23:19:14.554978: step 306080, loss = 0.0654, acc = 0.9880 (264.4 examples/sec; 0.242 sec/batch)
2017-05-09 23:19:19.148355: step 306100, loss = 0.0773, acc = 0.9820 (284.2 examples/sec; 0.225 sec/batch)
2017-05-09 23:19:23.872548: step 306120, loss = 0.0701, acc = 0.9860 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 23:19:28.659999: step 306140, loss = 0.0751, acc = 0.9820 (226.9 examples/sec; 0.282 sec/batch)
2017-05-09 23:19:33.263393: step 306160, loss = 0.0839, acc = 0.9760 (268.5 examples/sec; 0.238 sec/batch)
2017-05-09 23:19:37.818899: step 306180, loss = 0.0835, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 23:19:42.355526: step 306200, loss = 0.0696, acc = 0.9800 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 23:19:47.342959: step 306220, loss = 0.0753, acc = 0.9820 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 23:19:51.890287: step 306240, loss = 0.0965, acc = 0.9740 (286.3 examples/sec; 0.224 sec/batch)
2017-05-09 23:19:56.568022: step 306260, loss = 0.0904, acc = 0.9740 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 23:20:01.326182: step 306280, loss = 0.0763, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 23:20:05.893007: step 306300, loss = 0.0621, acc = 0.9940 (284.7 examples/sec; 0.225 sec/batch)
2017-05-09 23:20:10.347796: step 306320, loss = 0.0645, acc = 0.9840 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 23:20:15.101362: step 306340, loss = 0.0985, acc = 0.9640 (277.5 examples/sec; 0.231 sec/batch)
2017-05-09 23:20:19.608509: step 306360, loss = 0.0851, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 23:20:24.093847: step 306380, loss = 0.0614, acc = 0.9920 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 23:20:28.831673: step 306400, loss = 0.0747, acc = 0.9840 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 23:20:33.391982: step 306420, loss = 0.0810, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
2017-05-09 23:20:38.708166: step 306440, loss = 0.0935, acc = 0.9740 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 23:20:43.569070: step 306460, loss = 0.0694, acc = 0.9840 (279.3 examples/sec; 0.229 sec/batch)
2017-05-09 23:20:48.106347: step 306480, loss = 0.0748, acc = 0.9760 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 23:20:52.728121: step 306500, loss = 0.0625, acc = 0.9860 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 23:20:57.459566: step 306520, loss = 0.0896, acc = 0.9760 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 23:21:02.090436: step 306540, loss = 0.0798, acc = 0.9740 (282.1 examples/sec; 0.227 sec/batch)
2017-05-09 23:21:06.707686: step 306560, loss = 0.0854, acc = 0.9740 (274.1 examples/sec; 0.233 sec/batch)
2017-05-09 23:21:11.448517: step 306580, loss = 0.0955, acc = 0.9780 (228.7 examples/sec; 0.280 sec/batch)
2017-05-09 23:21:15.959006: step 306600, loss = 0.0588, acc = 0.9940 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 23:21:20.517310: step 306620, loss = 0.0831, acc = 0.9740 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 23:21:25.203100: step 306640, loss = 0.0808, acc = 0.9820 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 23:21:30.001587: step 306660, loss = 0.0724, acc = 0.9880 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 23:21:34.688907: step 306680, loss = 0.0751, acc = 0.9880 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 23:21:39.332072: step 306700, loss = 0.0758, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 23:21:44.027163: step 306720, loss = 0.0621, acc = 0.9900 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 23:21:48.949848: step 306740, loss = 0.0700, acc = 0.9840 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 23:21:53.593457: step 306760, loss = 0.0610, acc = 0.9880 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 23:21:58.223661: step 306780, loss = 0.0657, acc = 0.9840 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 23:22:02.763042: step 306800, loss = 0.0898, acc = 0.9820 (263.6 examples/sec; 0.243 sec/batch)
2017-05-09 23:22:07.388613: step 306820, loss = 0.0820, acc = 0.9840 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 23:22:12.062862: step 306840, loss = 0.0972, acc = 0.9580 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 23:22:16.702684: step 306860, loss = 0.0588, acc = 0.9900 (265.9 examples/sec; 0.241 sec/batch)
2017-05-09 23:22:21.274614: step 306880, loss = 0.0882, acc = 0.9740 (277.3 examples/sec; 0.231 sec/batch)
2017-05-09 23:22:26.123906: step 306900, loss = 0.0772, acc = 0.9820 (228.3 examples/sec; 0.280 sec/batch)
2017-05-09 23:22:30.701308: step 306920, loss = 0.0816, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 23:22:35.379165: step 306940, loss = 0.0840, acc = 0.9780 (272.9 examples/sec; 0.234 sec/batch)
2017-05-09 23:22:39.929737: step 306960, loss = 0.0884, acc = 0.9740 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 23:22:44.881338: step 306980, loss = 0.0726, acc = 0.9780 (272.4 examples/sec; 0.235 sec/batch)
2017-05-09 23:22:49.472677: step 307000, loss = 0.0669, acc = 0.9820 (301.3 examples/sec; 0.212 sec/batch)
[Eval] 2017-05-09 23:23:03.543896: step 307000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 23:23:13.554577: step 307000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-09 23:23:13.554687: step 307000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:23:18.048857: step 307020, loss = 0.0921, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 23:23:22.649978: step 307040, loss = 0.0594, acc = 0.9900 (278.2 examples/sec; 0.230 sec/batch)
2017-05-09 23:23:27.510856: step 307060, loss = 0.0804, acc = 0.9720 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 23:23:32.194241: step 307080, loss = 0.0798, acc = 0.9800 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 23:23:36.776538: step 307100, loss = 0.0654, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 23:23:41.476833: step 307120, loss = 0.0784, acc = 0.9860 (237.4 examples/sec; 0.270 sec/batch)
2017-05-09 23:23:46.025058: step 307140, loss = 0.0774, acc = 0.9760 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 23:23:50.866465: step 307160, loss = 0.0803, acc = 0.9820 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 23:23:55.663771: step 307180, loss = 0.0913, acc = 0.9800 (253.6 examples/sec; 0.252 sec/batch)
2017-05-09 23:24:00.541370: step 307200, loss = 0.1132, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-09 23:24:05.186530: step 307220, loss = 0.0941, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 23:24:09.741920: step 307240, loss = 0.0749, acc = 0.9880 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 23:24:14.563418: step 307260, loss = 0.0688, acc = 0.9860 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 23:24:19.063508: step 307280, loss = 0.0666, acc = 0.9880 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 23:24:23.685255: step 307300, loss = 0.0599, acc = 0.9900 (284.5 examples/sec; 0.225 sec/batch)
2017-05-09 23:24:28.551619: step 307320, loss = 0.0560, acc = 0.9940 (294.8 examples/sec; 0.217 sec/batch)
2017-05-09 23:24:33.020034: step 307340, loss = 0.0749, acc = 0.9880 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:24:37.527413: step 307360, loss = 0.0768, acc = 0.9820 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:24:42.054558: step 307380, loss = 0.0720, acc = 0.9880 (303.7 examples/sec; 0.211 sec/batch)
2017-05-09 23:24:46.589659: step 307400, loss = 0.0686, acc = 0.9820 (260.6 examples/sec; 0.246 sec/batch)
2017-05-09 23:24:51.223466: step 307420, loss = 0.0739, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 23:24:56.957178: step 307440, loss = 0.0787, acc = 0.9840 (239.3 examples/sec; 0.267 sec/batch)
2017-05-09 23:25:01.679940: step 307460, loss = 0.0644, acc = 0.9880 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:25:06.323599: step 307480, loss = 0.0772, acc = 0.9820 (264.3 examples/sec; 0.242 sec/batch)
2017-05-09 23:25:10.922535: step 307500, loss = 0.0786, acc = 0.9820 (270.4 examples/sec; 0.237 sec/batch)
2017-05-09 23:25:15.768792: step 307520, loss = 0.0791, acc = 0.9820 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 23:25:20.264216: step 307540, loss = 0.0793, acc = 0.9800 (290.5 examples/sec; 0.220 sec/batch)
2017-05-09 23:25:24.795989: step 307560, loss = 0.0901, acc = 0.9720 (297.8 examples/sec; 0.215 sec/batch)
2017-05-09 23:25:29.611743: step 307580, loss = 0.0801, acc = 0.9780 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 23:25:34.065378: step 307600, loss = 0.0709, acc = 0.9880 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:25:38.622951: step 307620, loss = 0.0682, acc = 0.9840 (295.5 examples/sec; 0.217 sec/batch)
2017-05-09 23:25:43.370825: step 307640, loss = 0.0831, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 23:25:47.966079: step 307660, loss = 0.0683, acc = 0.9880 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 23:25:52.671785: step 307680, loss = 0.0784, acc = 0.9780 (272.5 examples/sec; 0.235 sec/batch)
2017-05-09 23:25:57.293656: step 307700, loss = 0.0675, acc = 0.9840 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 23:26:01.758943: step 307720, loss = 0.0948, acc = 0.9780 (287.1 examples/sec; 0.223 sec/batch)
2017-05-09 23:26:06.261278: step 307740, loss = 0.0909, acc = 0.9780 (287.3 examples/sec; 0.223 sec/batch)
2017-05-09 23:26:10.829866: step 307760, loss = 0.0755, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 23:26:15.476278: step 307780, loss = 0.0839, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 23:26:19.920801: step 307800, loss = 0.0639, acc = 0.9920 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 23:26:24.505123: step 307820, loss = 0.0799, acc = 0.9760 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 23:26:29.627900: step 307840, loss = 0.0962, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 23:26:34.270172: step 307860, loss = 0.0721, acc = 0.9800 (288.2 examples/sec; 0.222 sec/batch)
2017-05-09 23:26:38.931804: step 307880, loss = 0.0754, acc = 0.9860 (277.6 examples/sec; 0.231 sec/batch)
2017-05-09 23:26:43.570904: step 307900, loss = 0.0842, acc = 0.9800 (291.2 examples/sec; 0.220 sec/batch)
2017-05-09 23:26:48.286912: step 307920, loss = 0.0946, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 23:26:52.916032: step 307940, loss = 0.0507, acc = 0.9940 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:26:57.599955: step 307960, loss = 0.0917, acc = 0.9740 (258.7 examples/sec; 0.247 sec/batch)
2017-05-09 23:27:02.196627: step 307980, loss = 0.0770, acc = 0.9780 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 23:27:06.813989: step 308000, loss = 0.0746, acc = 0.9880 (272.0 examples/sec; 0.235 sec/batch)
[Eval] 2017-05-09 23:27:20.876483: step 308000, acc = 0.9630, f1 = 0.9617
[Test] 2017-05-09 23:27:30.908290: step 308000, acc = 0.9534, f1 = 0.9530
[Status] 2017-05-09 23:27:30.908359: step 308000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:27:35.554820: step 308020, loss = 0.0908, acc = 0.9740 (276.7 examples/sec; 0.231 sec/batch)
2017-05-09 23:27:40.114681: step 308040, loss = 0.0851, acc = 0.9800 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 23:27:44.903108: step 308060, loss = 0.0815, acc = 0.9820 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 23:27:49.428498: step 308080, loss = 0.0758, acc = 0.9820 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:27:53.974586: step 308100, loss = 0.0891, acc = 0.9780 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 23:27:58.618769: step 308120, loss = 0.0606, acc = 0.9920 (282.7 examples/sec; 0.226 sec/batch)
2017-05-09 23:28:03.290700: step 308140, loss = 0.0611, acc = 0.9900 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 23:28:07.900924: step 308160, loss = 0.0686, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:28:12.595613: step 308180, loss = 0.1009, acc = 0.9660 (292.6 examples/sec; 0.219 sec/batch)
2017-05-09 23:28:17.090336: step 308200, loss = 0.0718, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 23:28:21.713034: step 308220, loss = 0.0818, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 23:28:26.272992: step 308240, loss = 0.0873, acc = 0.9760 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:28:31.154164: step 308260, loss = 0.0865, acc = 0.9720 (272.6 examples/sec; 0.235 sec/batch)
2017-05-09 23:28:35.811857: step 308280, loss = 0.0725, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:28:40.400165: step 308300, loss = 0.0705, acc = 0.9860 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 23:28:44.986856: step 308320, loss = 0.0678, acc = 0.9880 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 23:28:49.650465: step 308340, loss = 0.0964, acc = 0.9700 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:28:54.269912: step 308360, loss = 0.0788, acc = 0.9820 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 23:28:59.008890: step 308380, loss = 0.0985, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:29:03.661343: step 308400, loss = 0.0850, acc = 0.9700 (273.1 examples/sec; 0.234 sec/batch)
2017-05-09 23:29:08.248132: step 308420, loss = 0.0724, acc = 0.9840 (288.3 examples/sec; 0.222 sec/batch)
2017-05-09 23:29:12.933126: step 308440, loss = 0.0650, acc = 0.9840 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 23:29:18.146474: step 308460, loss = 0.0818, acc = 0.9880 (275.8 examples/sec; 0.232 sec/batch)
2017-05-09 23:29:22.708753: step 308480, loss = 0.0793, acc = 0.9780 (261.5 examples/sec; 0.245 sec/batch)
2017-05-09 23:29:27.366418: step 308500, loss = 0.0776, acc = 0.9800 (245.3 examples/sec; 0.261 sec/batch)
2017-05-09 23:29:31.750979: step 308520, loss = 0.0735, acc = 0.9840 (290.7 examples/sec; 0.220 sec/batch)
2017-05-09 23:29:36.366240: step 308540, loss = 0.0868, acc = 0.9760 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 23:29:41.077973: step 308560, loss = 0.0696, acc = 0.9860 (249.8 examples/sec; 0.256 sec/batch)
2017-05-09 23:29:45.603168: step 308580, loss = 0.0684, acc = 0.9800 (290.8 examples/sec; 0.220 sec/batch)
2017-05-09 23:29:50.305838: step 308600, loss = 0.0832, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 23:29:54.861583: step 308620, loss = 0.0573, acc = 0.9900 (297.3 examples/sec; 0.215 sec/batch)
2017-05-09 23:29:59.708733: step 308640, loss = 0.0755, acc = 0.9820 (267.1 examples/sec; 0.240 sec/batch)
2017-05-09 23:30:04.324754: step 308660, loss = 0.0710, acc = 0.9840 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 23:30:08.966743: step 308680, loss = 0.0760, acc = 0.9780 (269.6 examples/sec; 0.237 sec/batch)
2017-05-09 23:30:13.611016: step 308700, loss = 0.0697, acc = 0.9900 (283.5 examples/sec; 0.226 sec/batch)
2017-05-09 23:30:18.130885: step 308720, loss = 0.0598, acc = 0.9900 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:30:22.671846: step 308740, loss = 0.0605, acc = 0.9840 (288.6 examples/sec; 0.222 sec/batch)
2017-05-09 23:30:27.420772: step 308760, loss = 0.1319, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-09 23:30:31.954951: step 308780, loss = 0.0657, acc = 0.9920 (273.7 examples/sec; 0.234 sec/batch)
2017-05-09 23:30:36.558274: step 308800, loss = 0.0968, acc = 0.9720 (290.1 examples/sec; 0.221 sec/batch)
2017-05-09 23:30:41.270750: step 308820, loss = 0.0709, acc = 0.9880 (244.4 examples/sec; 0.262 sec/batch)
2017-05-09 23:30:45.989477: step 308840, loss = 0.0766, acc = 0.9860 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 23:30:50.666393: step 308860, loss = 0.0795, acc = 0.9840 (273.3 examples/sec; 0.234 sec/batch)
2017-05-09 23:30:55.285884: step 308880, loss = 0.0682, acc = 0.9840 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 23:30:59.856023: step 308900, loss = 0.0910, acc = 0.9800 (289.2 examples/sec; 0.221 sec/batch)
2017-05-09 23:31:04.435675: step 308920, loss = 0.0676, acc = 0.9900 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:31:09.074885: step 308940, loss = 0.0923, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 23:31:13.697420: step 308960, loss = 0.0734, acc = 0.9820 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 23:31:18.320205: step 308980, loss = 0.0746, acc = 0.9860 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 23:31:22.991455: step 309000, loss = 0.0726, acc = 0.9820 (270.7 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-09 23:31:37.041630: step 309000, acc = 0.9640, f1 = 0.9628
[Test] 2017-05-09 23:31:46.799301: step 309000, acc = 0.9549, f1 = 0.9546
[Status] 2017-05-09 23:31:46.799407: step 309000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:31:51.402583: step 309020, loss = 0.0579, acc = 0.9880 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 23:31:56.080856: step 309040, loss = 0.0676, acc = 0.9840 (245.8 examples/sec; 0.260 sec/batch)
2017-05-09 23:32:00.885118: step 309060, loss = 0.0685, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:32:05.441447: step 309080, loss = 0.0687, acc = 0.9840 (285.8 examples/sec; 0.224 sec/batch)
2017-05-09 23:32:09.949504: step 309100, loss = 0.0779, acc = 0.9780 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 23:32:14.898029: step 309120, loss = 0.0847, acc = 0.9800 (268.6 examples/sec; 0.238 sec/batch)
2017-05-09 23:32:19.495759: step 309140, loss = 0.0658, acc = 0.9860 (275.0 examples/sec; 0.233 sec/batch)
2017-05-09 23:32:24.074944: step 309160, loss = 0.0717, acc = 0.9900 (292.2 examples/sec; 0.219 sec/batch)
2017-05-09 23:32:28.751057: step 309180, loss = 0.0700, acc = 0.9800 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 23:32:33.577481: step 309200, loss = 0.0725, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 23:32:38.135910: step 309220, loss = 0.0657, acc = 0.9880 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 23:32:42.818509: step 309240, loss = 0.0833, acc = 0.9780 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 23:32:47.497034: step 309260, loss = 0.0703, acc = 0.9900 (270.9 examples/sec; 0.236 sec/batch)
2017-05-09 23:32:52.122129: step 309280, loss = 0.0722, acc = 0.9780 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:32:56.751741: step 309300, loss = 0.0781, acc = 0.9840 (301.0 examples/sec; 0.213 sec/batch)
2017-05-09 23:33:01.389791: step 309320, loss = 0.0634, acc = 0.9880 (273.8 examples/sec; 0.234 sec/batch)
2017-05-09 23:33:05.948002: step 309340, loss = 0.0675, acc = 0.9820 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 23:33:10.519887: step 309360, loss = 0.1086, acc = 0.9700 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 23:33:15.379636: step 309380, loss = 0.0989, acc = 0.9800 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 23:33:20.007398: step 309400, loss = 0.0680, acc = 0.9860 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 23:33:24.507645: step 309420, loss = 0.0814, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 23:33:29.226202: step 309440, loss = 0.0913, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 23:33:34.838033: step 309460, loss = 0.0799, acc = 0.9860 (261.7 examples/sec; 0.245 sec/batch)
2017-05-09 23:33:39.423817: step 309480, loss = 0.0601, acc = 0.9960 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:33:43.936261: step 309500, loss = 0.0771, acc = 0.9820 (294.6 examples/sec; 0.217 sec/batch)
2017-05-09 23:33:48.508153: step 309520, loss = 0.0620, acc = 0.9920 (286.8 examples/sec; 0.223 sec/batch)
2017-05-09 23:33:53.062096: step 309540, loss = 0.0782, acc = 0.9840 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:33:57.926930: step 309560, loss = 0.0872, acc = 0.9720 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 23:34:02.496612: step 309580, loss = 0.0791, acc = 0.9800 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 23:34:06.991063: step 309600, loss = 0.1115, acc = 0.9700 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 23:34:11.653790: step 309620, loss = 0.0748, acc = 0.9820 (244.3 examples/sec; 0.262 sec/batch)
2017-05-09 23:34:16.216626: step 309640, loss = 0.0709, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:34:20.682422: step 309660, loss = 0.0744, acc = 0.9800 (296.7 examples/sec; 0.216 sec/batch)
2017-05-09 23:34:25.195956: step 309680, loss = 0.0696, acc = 0.9820 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 23:34:29.993266: step 309700, loss = 0.0985, acc = 0.9740 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:34:34.673170: step 309720, loss = 0.0673, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:34:39.380640: step 309740, loss = 0.0667, acc = 0.9900 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 23:34:43.990994: step 309760, loss = 0.0748, acc = 0.9760 (296.0 examples/sec; 0.216 sec/batch)
2017-05-09 23:34:48.566909: step 309780, loss = 0.0692, acc = 0.9840 (283.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:34:53.446794: step 309800, loss = 0.0802, acc = 0.9820 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 23:34:58.350605: step 309820, loss = 0.0855, acc = 0.9720 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 23:35:02.934001: step 309840, loss = 0.0770, acc = 0.9860 (287.2 examples/sec; 0.223 sec/batch)
2017-05-09 23:35:07.601097: step 309860, loss = 0.0770, acc = 0.9840 (270.7 examples/sec; 0.236 sec/batch)
2017-05-09 23:35:12.320960: step 309880, loss = 0.0832, acc = 0.9860 (234.7 examples/sec; 0.273 sec/batch)
2017-05-09 23:35:16.824042: step 309900, loss = 0.0767, acc = 0.9840 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 23:35:21.477246: step 309920, loss = 0.0739, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:35:26.037121: step 309940, loss = 0.1013, acc = 0.9700 (272.1 examples/sec; 0.235 sec/batch)
2017-05-09 23:35:30.653748: step 309960, loss = 0.0712, acc = 0.9880 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 23:35:35.293397: step 309980, loss = 0.0905, acc = 0.9760 (272.7 examples/sec; 0.235 sec/batch)
2017-05-09 23:35:39.800973: step 310000, loss = 0.0756, acc = 0.9800 (291.7 examples/sec; 0.219 sec/batch)
[Eval] 2017-05-09 23:35:53.916049: step 310000, acc = 0.9640, f1 = 0.9628
[Test] 2017-05-09 23:36:03.679107: step 310000, acc = 0.9545, f1 = 0.9542
[Status] 2017-05-09 23:36:03.679193: step 310000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:36:08.157825: step 310020, loss = 0.0848, acc = 0.9740 (305.3 examples/sec; 0.210 sec/batch)
2017-05-09 23:36:12.906890: step 310040, loss = 0.0734, acc = 0.9860 (238.4 examples/sec; 0.268 sec/batch)
2017-05-09 23:36:17.602709: step 310060, loss = 0.0846, acc = 0.9820 (290.6 examples/sec; 0.220 sec/batch)
2017-05-09 23:36:22.144462: step 310080, loss = 0.0692, acc = 0.9840 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 23:36:26.829429: step 310100, loss = 0.0785, acc = 0.9860 (273.2 examples/sec; 0.234 sec/batch)
2017-05-09 23:36:31.739065: step 310120, loss = 0.0656, acc = 0.9860 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 23:36:36.217860: step 310140, loss = 0.0745, acc = 0.9900 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 23:36:40.810869: step 310160, loss = 0.0715, acc = 0.9880 (263.7 examples/sec; 0.243 sec/batch)
2017-05-09 23:36:45.431102: step 310180, loss = 0.0847, acc = 0.9820 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 23:36:50.056152: step 310200, loss = 0.0707, acc = 0.9900 (271.9 examples/sec; 0.235 sec/batch)
2017-05-09 23:36:54.566021: step 310220, loss = 0.0661, acc = 0.9860 (278.0 examples/sec; 0.230 sec/batch)
2017-05-09 23:36:59.521395: step 310240, loss = 0.0666, acc = 0.9820 (218.2 examples/sec; 0.293 sec/batch)
2017-05-09 23:37:04.087772: step 310260, loss = 0.0872, acc = 0.9780 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 23:37:08.547996: step 310280, loss = 0.0841, acc = 0.9800 (284.0 examples/sec; 0.225 sec/batch)
2017-05-09 23:37:13.142361: step 310300, loss = 0.0732, acc = 0.9840 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 23:37:17.847067: step 310320, loss = 0.0824, acc = 0.9760 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:37:22.517981: step 310340, loss = 0.0561, acc = 0.9940 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:37:27.120540: step 310360, loss = 0.0616, acc = 0.9900 (300.0 examples/sec; 0.213 sec/batch)
2017-05-09 23:37:31.726940: step 310380, loss = 0.0719, acc = 0.9820 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 23:37:36.373215: step 310400, loss = 0.0737, acc = 0.9880 (267.8 examples/sec; 0.239 sec/batch)
2017-05-09 23:37:41.123654: step 310420, loss = 0.0894, acc = 0.9780 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 23:37:45.708399: step 310440, loss = 0.0660, acc = 0.9920 (289.6 examples/sec; 0.221 sec/batch)
2017-05-09 23:37:50.886510: step 310460, loss = 0.0764, acc = 0.9780 (162.4 examples/sec; 0.394 sec/batch)
2017-05-09 23:37:55.762782: step 310480, loss = 0.0780, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 23:38:00.447048: step 310500, loss = 0.0910, acc = 0.9700 (283.1 examples/sec; 0.226 sec/batch)
2017-05-09 23:38:05.003711: step 310520, loss = 0.0800, acc = 0.9820 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 23:38:09.759905: step 310540, loss = 0.0757, acc = 0.9820 (244.6 examples/sec; 0.262 sec/batch)
2017-05-09 23:38:14.410813: step 310560, loss = 0.0721, acc = 0.9840 (266.5 examples/sec; 0.240 sec/batch)
2017-05-09 23:38:18.906061: step 310580, loss = 0.0692, acc = 0.9860 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 23:38:23.689879: step 310600, loss = 0.0682, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:38:28.537815: step 310620, loss = 0.0673, acc = 0.9880 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 23:38:33.190706: step 310640, loss = 0.0826, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:38:37.827974: step 310660, loss = 0.0813, acc = 0.9760 (274.1 examples/sec; 0.234 sec/batch)
2017-05-09 23:38:42.404147: step 310680, loss = 0.0723, acc = 0.9780 (296.4 examples/sec; 0.216 sec/batch)
2017-05-09 23:38:46.974024: step 310700, loss = 0.0826, acc = 0.9780 (282.6 examples/sec; 0.227 sec/batch)
2017-05-09 23:38:51.517585: step 310720, loss = 0.0782, acc = 0.9860 (281.2 examples/sec; 0.228 sec/batch)
2017-05-09 23:38:56.397834: step 310740, loss = 0.0741, acc = 0.9840 (227.4 examples/sec; 0.281 sec/batch)
2017-05-09 23:39:00.961762: step 310760, loss = 0.0733, acc = 0.9800 (275.2 examples/sec; 0.233 sec/batch)
2017-05-09 23:39:05.543408: step 310780, loss = 0.0732, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 23:39:10.111286: step 310800, loss = 0.0644, acc = 0.9920 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 23:39:14.849137: step 310820, loss = 0.0694, acc = 0.9880 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 23:39:19.546705: step 310840, loss = 0.0548, acc = 0.9940 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 23:39:24.126290: step 310860, loss = 0.0562, acc = 0.9960 (279.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:39:28.754120: step 310880, loss = 0.0868, acc = 0.9740 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 23:39:33.249486: step 310900, loss = 0.0653, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
2017-05-09 23:39:37.807253: step 310920, loss = 0.0960, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 23:39:42.469424: step 310940, loss = 0.0701, acc = 0.9780 (291.4 examples/sec; 0.220 sec/batch)
2017-05-09 23:39:47.107549: step 310960, loss = 0.0681, acc = 0.9840 (274.7 examples/sec; 0.233 sec/batch)
2017-05-09 23:39:51.664723: step 310980, loss = 0.0748, acc = 0.9860 (280.2 examples/sec; 0.228 sec/batch)
2017-05-09 23:39:56.382795: step 311000, loss = 0.0776, acc = 0.9860 (296.1 examples/sec; 0.216 sec/batch)
[Eval] 2017-05-09 23:40:10.082752: step 311000, acc = 0.9625, f1 = 0.9612
[Test] 2017-05-09 23:40:19.607805: step 311000, acc = 0.9529, f1 = 0.9525
[Status] 2017-05-09 23:40:19.607885: step 311000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:40:24.192184: step 311020, loss = 0.0646, acc = 0.9860 (268.1 examples/sec; 0.239 sec/batch)
2017-05-09 23:40:28.973536: step 311040, loss = 0.0666, acc = 0.9840 (266.7 examples/sec; 0.240 sec/batch)
2017-05-09 23:40:33.743104: step 311060, loss = 0.0669, acc = 0.9900 (259.3 examples/sec; 0.247 sec/batch)
2017-05-09 23:40:38.346808: step 311080, loss = 0.0649, acc = 0.9820 (275.5 examples/sec; 0.232 sec/batch)
2017-05-09 23:40:43.117182: step 311100, loss = 0.0781, acc = 0.9900 (243.9 examples/sec; 0.262 sec/batch)
2017-05-09 23:40:47.663585: step 311120, loss = 0.0757, acc = 0.9780 (286.4 examples/sec; 0.223 sec/batch)
2017-05-09 23:40:52.328437: step 311140, loss = 0.0630, acc = 0.9880 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 23:40:57.214638: step 311160, loss = 0.0947, acc = 0.9840 (288.1 examples/sec; 0.222 sec/batch)
2017-05-09 23:41:01.805684: step 311180, loss = 0.0793, acc = 0.9780 (279.1 examples/sec; 0.229 sec/batch)
2017-05-09 23:41:06.295206: step 311200, loss = 0.0833, acc = 0.9800 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:41:11.058155: step 311220, loss = 0.0851, acc = 0.9760 (248.1 examples/sec; 0.258 sec/batch)
2017-05-09 23:41:15.618065: step 311240, loss = 0.0855, acc = 0.9800 (286.1 examples/sec; 0.224 sec/batch)
2017-05-09 23:41:20.299146: step 311260, loss = 0.0978, acc = 0.9780 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 23:41:24.822457: step 311280, loss = 0.0720, acc = 0.9840 (292.7 examples/sec; 0.219 sec/batch)
2017-05-09 23:41:29.566436: step 311300, loss = 0.0681, acc = 0.9880 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 23:41:34.165130: step 311320, loss = 0.0651, acc = 0.9820 (288.0 examples/sec; 0.222 sec/batch)
2017-05-09 23:41:38.906646: step 311340, loss = 0.0774, acc = 0.9840 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 23:41:43.735389: step 311360, loss = 0.0673, acc = 0.9820 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 23:41:48.381573: step 311380, loss = 0.0627, acc = 0.9940 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 23:41:52.950702: step 311400, loss = 0.0755, acc = 0.9880 (280.0 examples/sec; 0.229 sec/batch)
2017-05-09 23:41:57.538113: step 311420, loss = 0.0730, acc = 0.9820 (294.2 examples/sec; 0.218 sec/batch)
2017-05-09 23:42:02.051323: step 311440, loss = 0.0724, acc = 0.9820 (297.1 examples/sec; 0.215 sec/batch)
2017-05-09 23:42:06.475666: step 311460, loss = 0.0718, acc = 0.9780 (288.4 examples/sec; 0.222 sec/batch)
2017-05-09 23:42:12.203002: step 311480, loss = 0.0517, acc = 0.9920 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 23:42:16.836328: step 311500, loss = 0.0674, acc = 0.9900 (269.1 examples/sec; 0.238 sec/batch)
2017-05-09 23:42:21.477944: step 311520, loss = 0.1026, acc = 0.9700 (249.3 examples/sec; 0.257 sec/batch)
2017-05-09 23:42:26.252460: step 311540, loss = 0.0538, acc = 0.9920 (232.5 examples/sec; 0.275 sec/batch)
2017-05-09 23:42:30.744073: step 311560, loss = 0.0816, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 23:42:35.369100: step 311580, loss = 0.0935, acc = 0.9740 (276.2 examples/sec; 0.232 sec/batch)
2017-05-09 23:42:40.034894: step 311600, loss = 0.0709, acc = 0.9780 (281.1 examples/sec; 0.228 sec/batch)
2017-05-09 23:42:44.717946: step 311620, loss = 0.0880, acc = 0.9740 (293.1 examples/sec; 0.218 sec/batch)
2017-05-09 23:42:49.329479: step 311640, loss = 0.0882, acc = 0.9800 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 23:42:54.006387: step 311660, loss = 0.0775, acc = 0.9800 (274.8 examples/sec; 0.233 sec/batch)
2017-05-09 23:42:58.807944: step 311680, loss = 0.0705, acc = 0.9800 (286.6 examples/sec; 0.223 sec/batch)
2017-05-09 23:43:03.517361: step 311700, loss = 0.0655, acc = 0.9860 (270.2 examples/sec; 0.237 sec/batch)
2017-05-09 23:43:08.215825: step 311720, loss = 0.0724, acc = 0.9880 (275.6 examples/sec; 0.232 sec/batch)
2017-05-09 23:43:12.911562: step 311740, loss = 0.0714, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 23:43:17.535508: step 311760, loss = 0.0675, acc = 0.9900 (295.1 examples/sec; 0.217 sec/batch)
2017-05-09 23:43:22.153919: step 311780, loss = 0.0912, acc = 0.9760 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 23:43:26.852877: step 311800, loss = 0.0766, acc = 0.9840 (293.9 examples/sec; 0.218 sec/batch)
2017-05-09 23:43:31.326631: step 311820, loss = 0.0809, acc = 0.9740 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 23:43:35.937343: step 311840, loss = 0.0700, acc = 0.9900 (276.9 examples/sec; 0.231 sec/batch)
2017-05-09 23:43:40.703531: step 311860, loss = 0.1130, acc = 0.9740 (246.3 examples/sec; 0.260 sec/batch)
2017-05-09 23:43:45.386590: step 311880, loss = 0.0586, acc = 0.9940 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:43:50.074352: step 311900, loss = 0.0721, acc = 0.9800 (255.0 examples/sec; 0.251 sec/batch)
2017-05-09 23:43:54.618324: step 311920, loss = 0.0687, acc = 0.9880 (288.7 examples/sec; 0.222 sec/batch)
2017-05-09 23:43:59.343353: step 311940, loss = 0.0665, acc = 0.9900 (258.9 examples/sec; 0.247 sec/batch)
2017-05-09 23:44:03.994731: step 311960, loss = 0.0790, acc = 0.9840 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 23:44:08.594250: step 311980, loss = 0.0672, acc = 0.9840 (281.4 examples/sec; 0.227 sec/batch)
2017-05-09 23:44:13.327676: step 312000, loss = 0.0707, acc = 0.9880 (278.4 examples/sec; 0.230 sec/batch)
[Eval] 2017-05-09 23:44:27.218557: step 312000, acc = 0.9644, f1 = 0.9632
[Test] 2017-05-09 23:44:36.505260: step 312000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 23:44:36.505346: step 312000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:44:41.211960: step 312020, loss = 0.0806, acc = 0.9760 (278.8 examples/sec; 0.230 sec/batch)
2017-05-09 23:44:45.693959: step 312040, loss = 0.0701, acc = 0.9820 (295.9 examples/sec; 0.216 sec/batch)
2017-05-09 23:44:50.365980: step 312060, loss = 0.0752, acc = 0.9800 (268.8 examples/sec; 0.238 sec/batch)
2017-05-09 23:44:55.060859: step 312080, loss = 0.0820, acc = 0.9780 (226.4 examples/sec; 0.283 sec/batch)
2017-05-09 23:44:59.803364: step 312100, loss = 0.0612, acc = 0.9880 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 23:45:04.243799: step 312120, loss = 0.0756, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-09 23:45:08.859879: step 312140, loss = 0.0660, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-09 23:45:13.540015: step 312160, loss = 0.0642, acc = 0.9880 (278.7 examples/sec; 0.230 sec/batch)
2017-05-09 23:45:18.022713: step 312180, loss = 0.0557, acc = 0.9880 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 23:45:22.614333: step 312200, loss = 0.0803, acc = 0.9820 (280.1 examples/sec; 0.229 sec/batch)
2017-05-09 23:45:27.180359: step 312220, loss = 0.0951, acc = 0.9700 (266.9 examples/sec; 0.240 sec/batch)
2017-05-09 23:45:31.672673: step 312240, loss = 0.0906, acc = 0.9620 (290.3 examples/sec; 0.220 sec/batch)
2017-05-09 23:45:36.246932: step 312260, loss = 0.0711, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-09 23:45:40.845155: step 312280, loss = 0.0821, acc = 0.9820 (294.3 examples/sec; 0.217 sec/batch)
2017-05-09 23:45:45.448095: step 312300, loss = 0.0752, acc = 0.9840 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 23:45:50.079618: step 312320, loss = 0.0623, acc = 0.9900 (291.5 examples/sec; 0.220 sec/batch)
2017-05-09 23:45:54.847629: step 312340, loss = 0.0853, acc = 0.9780 (294.9 examples/sec; 0.217 sec/batch)
2017-05-09 23:45:59.475672: step 312360, loss = 0.0774, acc = 0.9780 (272.0 examples/sec; 0.235 sec/batch)
2017-05-09 23:46:04.099268: step 312380, loss = 0.0590, acc = 0.9920 (257.2 examples/sec; 0.249 sec/batch)
2017-05-09 23:46:08.753581: step 312400, loss = 0.1079, acc = 0.9700 (248.6 examples/sec; 0.257 sec/batch)
2017-05-09 23:46:13.197048: step 312420, loss = 0.0648, acc = 0.9840 (291.0 examples/sec; 0.220 sec/batch)
2017-05-09 23:46:17.698699: step 312440, loss = 0.0953, acc = 0.9720 (276.8 examples/sec; 0.231 sec/batch)
2017-05-09 23:46:22.262778: step 312460, loss = 0.0691, acc = 0.9780 (269.9 examples/sec; 0.237 sec/batch)
2017-05-09 23:46:27.717477: step 312480, loss = 0.0882, acc = 0.9860 (269.8 examples/sec; 0.237 sec/batch)
2017-05-09 23:46:32.306391: step 312500, loss = 0.0659, acc = 0.9860 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 23:46:36.928928: step 312520, loss = 0.0620, acc = 0.9860 (292.4 examples/sec; 0.219 sec/batch)
2017-05-09 23:46:41.724165: step 312540, loss = 0.0915, acc = 0.9760 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 23:46:46.283715: step 312560, loss = 0.0539, acc = 0.9960 (289.5 examples/sec; 0.221 sec/batch)
2017-05-09 23:46:50.752354: step 312580, loss = 0.0997, acc = 0.9780 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:46:55.381845: step 312600, loss = 0.0814, acc = 0.9780 (290.2 examples/sec; 0.221 sec/batch)
2017-05-09 23:46:59.920379: step 312620, loss = 0.0781, acc = 0.9860 (284.9 examples/sec; 0.225 sec/batch)
2017-05-09 23:47:04.475471: step 312640, loss = 0.1026, acc = 0.9580 (286.7 examples/sec; 0.223 sec/batch)
2017-05-09 23:47:09.214523: step 312660, loss = 0.0861, acc = 0.9740 (296.5 examples/sec; 0.216 sec/batch)
2017-05-09 23:47:13.872151: step 312680, loss = 0.1082, acc = 0.9740 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 23:47:18.416096: step 312700, loss = 0.0687, acc = 0.9820 (281.3 examples/sec; 0.227 sec/batch)
2017-05-09 23:47:23.210594: step 312720, loss = 0.0791, acc = 0.9760 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:47:27.779902: step 312740, loss = 0.0829, acc = 0.9780 (283.2 examples/sec; 0.226 sec/batch)
2017-05-09 23:47:32.419368: step 312760, loss = 0.0816, acc = 0.9780 (271.6 examples/sec; 0.236 sec/batch)
2017-05-09 23:47:37.306569: step 312780, loss = 0.0754, acc = 0.9840 (237.4 examples/sec; 0.270 sec/batch)
2017-05-09 23:47:41.724016: step 312800, loss = 0.0680, acc = 0.9880 (282.0 examples/sec; 0.227 sec/batch)
2017-05-09 23:47:46.295947: step 312820, loss = 0.0710, acc = 0.9820 (285.9 examples/sec; 0.224 sec/batch)
2017-05-09 23:47:51.097862: step 312840, loss = 0.0779, acc = 0.9780 (261.6 examples/sec; 0.245 sec/batch)
2017-05-09 23:47:55.793890: step 312860, loss = 0.0835, acc = 0.9780 (277.4 examples/sec; 0.231 sec/batch)
2017-05-09 23:48:00.439514: step 312880, loss = 0.0635, acc = 0.9900 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 23:48:05.299787: step 312900, loss = 0.0868, acc = 0.9820 (253.7 examples/sec; 0.252 sec/batch)
2017-05-09 23:48:09.848178: step 312920, loss = 0.0668, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 23:48:14.504465: step 312940, loss = 0.0655, acc = 0.9900 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 23:48:19.083282: step 312960, loss = 0.1025, acc = 0.9680 (281.6 examples/sec; 0.227 sec/batch)
2017-05-09 23:48:23.832437: step 312980, loss = 0.1087, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 23:48:28.388912: step 313000, loss = 0.0811, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-09 23:48:42.499361: step 313000, acc = 0.9610, f1 = 0.9596
[Test] 2017-05-09 23:48:52.133830: step 313000, acc = 0.9511, f1 = 0.9507
[Status] 2017-05-09 23:48:52.133902: step 313000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:48:56.647290: step 313020, loss = 0.0813, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-09 23:49:01.180558: step 313040, loss = 0.0984, acc = 0.9760 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 23:49:05.887352: step 313060, loss = 0.0959, acc = 0.9760 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 23:49:10.453857: step 313080, loss = 0.0814, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:49:15.160438: step 313100, loss = 0.0922, acc = 0.9700 (274.6 examples/sec; 0.233 sec/batch)
2017-05-09 23:49:20.057103: step 313120, loss = 0.0681, acc = 0.9860 (234.9 examples/sec; 0.272 sec/batch)
2017-05-09 23:49:24.559670: step 313140, loss = 0.0727, acc = 0.9800 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 23:49:29.282038: step 313160, loss = 0.0665, acc = 0.9880 (280.6 examples/sec; 0.228 sec/batch)
2017-05-09 23:49:34.004684: step 313180, loss = 0.0894, acc = 0.9780 (245.4 examples/sec; 0.261 sec/batch)
2017-05-09 23:49:38.567966: step 313200, loss = 0.0778, acc = 0.9820 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 23:49:43.077825: step 313220, loss = 0.0813, acc = 0.9800 (298.6 examples/sec; 0.214 sec/batch)
2017-05-09 23:49:47.538284: step 313240, loss = 0.0744, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 23:49:52.272330: step 313260, loss = 0.0759, acc = 0.9800 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 23:49:56.814903: step 313280, loss = 0.0752, acc = 0.9840 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 23:50:01.418966: step 313300, loss = 0.0571, acc = 0.9960 (282.8 examples/sec; 0.226 sec/batch)
2017-05-09 23:50:06.229589: step 313320, loss = 0.0715, acc = 0.9860 (255.9 examples/sec; 0.250 sec/batch)
2017-05-09 23:50:10.924109: step 313340, loss = 0.0676, acc = 0.9880 (269.0 examples/sec; 0.238 sec/batch)
2017-05-09 23:50:15.570106: step 313360, loss = 0.0988, acc = 0.9800 (270.6 examples/sec; 0.237 sec/batch)
2017-05-09 23:50:20.290889: step 313380, loss = 0.1038, acc = 0.9900 (292.0 examples/sec; 0.219 sec/batch)
2017-05-09 23:50:24.827787: step 313400, loss = 0.1040, acc = 0.9740 (292.1 examples/sec; 0.219 sec/batch)
2017-05-09 23:50:29.255171: step 313420, loss = 0.0506, acc = 0.9960 (295.4 examples/sec; 0.217 sec/batch)
2017-05-09 23:50:33.943011: step 313440, loss = 0.0753, acc = 0.9820 (246.2 examples/sec; 0.260 sec/batch)
2017-05-09 23:50:38.510515: step 313460, loss = 0.0820, acc = 0.9800 (275.3 examples/sec; 0.233 sec/batch)
2017-05-09 23:50:42.949760: step 313480, loss = 0.0989, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-09 23:50:48.705133: step 313500, loss = 0.0916, acc = 0.9700 (242.2 examples/sec; 0.264 sec/batch)
2017-05-09 23:50:53.362429: step 313520, loss = 0.0753, acc = 0.9900 (288.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:50:57.910990: step 313540, loss = 0.0916, acc = 0.9700 (275.1 examples/sec; 0.233 sec/batch)
2017-05-09 23:51:02.540891: step 313560, loss = 0.0817, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:51:07.266861: step 313580, loss = 0.0934, acc = 0.9720 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:51:11.778665: step 313600, loss = 0.0823, acc = 0.9760 (297.9 examples/sec; 0.215 sec/batch)
2017-05-09 23:51:16.335958: step 313620, loss = 0.0725, acc = 0.9840 (281.9 examples/sec; 0.227 sec/batch)
2017-05-09 23:51:21.006077: step 313640, loss = 0.0816, acc = 0.9760 (271.7 examples/sec; 0.236 sec/batch)
2017-05-09 23:51:25.731250: step 313660, loss = 0.0666, acc = 0.9920 (287.4 examples/sec; 0.223 sec/batch)
2017-05-09 23:51:30.360285: step 313680, loss = 0.0857, acc = 0.9760 (280.8 examples/sec; 0.228 sec/batch)
2017-05-09 23:51:35.063593: step 313700, loss = 0.0597, acc = 0.9880 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 23:51:39.553935: step 313720, loss = 0.0612, acc = 0.9920 (282.3 examples/sec; 0.227 sec/batch)
2017-05-09 23:51:44.216783: step 313740, loss = 0.0807, acc = 0.9780 (277.2 examples/sec; 0.231 sec/batch)
2017-05-09 23:51:48.915857: step 313760, loss = 0.0815, acc = 0.9820 (289.4 examples/sec; 0.221 sec/batch)
2017-05-09 23:51:53.644004: step 313780, loss = 0.0836, acc = 0.9800 (275.4 examples/sec; 0.232 sec/batch)
2017-05-09 23:51:58.240390: step 313800, loss = 0.0701, acc = 0.9880 (289.8 examples/sec; 0.221 sec/batch)
2017-05-09 23:52:02.834305: step 313820, loss = 0.0720, acc = 0.9900 (289.9 examples/sec; 0.221 sec/batch)
2017-05-09 23:52:07.567330: step 313840, loss = 0.0762, acc = 0.9800 (291.9 examples/sec; 0.219 sec/batch)
2017-05-09 23:52:11.980419: step 313860, loss = 0.0879, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 23:52:16.472573: step 313880, loss = 0.0818, acc = 0.9800 (268.9 examples/sec; 0.238 sec/batch)
2017-05-09 23:52:21.041046: step 313900, loss = 0.0841, acc = 0.9820 (271.1 examples/sec; 0.236 sec/batch)
2017-05-09 23:52:25.590171: step 313920, loss = 0.0786, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-09 23:52:30.074562: step 313940, loss = 0.0725, acc = 0.9840 (285.3 examples/sec; 0.224 sec/batch)
2017-05-09 23:52:34.796395: step 313960, loss = 0.0757, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-09 23:52:39.453106: step 313980, loss = 0.0855, acc = 0.9820 (276.5 examples/sec; 0.231 sec/batch)
2017-05-09 23:52:44.143627: step 314000, loss = 0.0665, acc = 0.9840 (274.7 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-09 23:52:58.259211: step 314000, acc = 0.9641, f1 = 0.9629
[Test] 2017-05-09 23:53:07.973293: step 314000, acc = 0.9562, f1 = 0.9558
[Status] 2017-05-09 23:53:07.973388: step 314000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:53:12.440243: step 314020, loss = 0.0743, acc = 0.9840 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:53:16.981209: step 314040, loss = 0.0696, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-09 23:53:21.756736: step 314060, loss = 0.0750, acc = 0.9840 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 23:53:26.259469: step 314080, loss = 0.0689, acc = 0.9880 (297.8 examples/sec; 0.215 sec/batch)
2017-05-09 23:53:30.847647: step 314100, loss = 0.0764, acc = 0.9780 (263.9 examples/sec; 0.243 sec/batch)
2017-05-09 23:53:35.546817: step 314120, loss = 0.0822, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:53:40.093370: step 314140, loss = 0.0844, acc = 0.9820 (274.5 examples/sec; 0.233 sec/batch)
2017-05-09 23:53:44.735420: step 314160, loss = 0.0657, acc = 0.9840 (273.6 examples/sec; 0.234 sec/batch)
2017-05-09 23:53:49.628370: step 314180, loss = 0.0819, acc = 0.9740 (240.0 examples/sec; 0.267 sec/batch)
2017-05-09 23:53:54.125353: step 314200, loss = 0.0800, acc = 0.9900 (287.7 examples/sec; 0.222 sec/batch)
2017-05-09 23:53:58.845520: step 314220, loss = 0.0881, acc = 0.9760 (258.2 examples/sec; 0.248 sec/batch)
2017-05-09 23:54:03.539776: step 314240, loss = 0.0621, acc = 0.9920 (246.3 examples/sec; 0.260 sec/batch)
2017-05-09 23:54:08.199680: step 314260, loss = 0.0799, acc = 0.9780 (263.8 examples/sec; 0.243 sec/batch)
2017-05-09 23:54:12.755824: step 314280, loss = 0.0533, acc = 0.9960 (283.4 examples/sec; 0.226 sec/batch)
2017-05-09 23:54:17.285637: step 314300, loss = 0.0693, acc = 0.9840 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 23:54:22.024400: step 314320, loss = 0.0574, acc = 0.9880 (287.8 examples/sec; 0.222 sec/batch)
2017-05-09 23:54:26.566407: step 314340, loss = 0.0909, acc = 0.9700 (288.5 examples/sec; 0.222 sec/batch)
2017-05-09 23:54:31.501838: step 314360, loss = 0.0552, acc = 0.9900 (279.4 examples/sec; 0.229 sec/batch)
2017-05-09 23:54:36.219616: step 314380, loss = 0.0703, acc = 0.9880 (276.4 examples/sec; 0.232 sec/batch)
2017-05-09 23:54:40.748004: step 314400, loss = 0.0609, acc = 0.9860 (290.0 examples/sec; 0.221 sec/batch)
2017-05-09 23:54:45.319699: step 314420, loss = 0.0895, acc = 0.9840 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 23:54:50.085496: step 314440, loss = 0.0646, acc = 0.9880 (277.7 examples/sec; 0.230 sec/batch)
2017-05-09 23:54:54.653327: step 314460, loss = 0.0753, acc = 0.9820 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 23:54:59.297594: step 314480, loss = 0.0840, acc = 0.9800 (274.4 examples/sec; 0.233 sec/batch)
2017-05-09 23:55:04.684991: step 314500, loss = 0.0850, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-09 23:55:09.191460: step 314520, loss = 0.0718, acc = 0.9800 (273.9 examples/sec; 0.234 sec/batch)
2017-05-09 23:55:13.629383: step 314540, loss = 0.0911, acc = 0.9760 (305.5 examples/sec; 0.209 sec/batch)
2017-05-09 23:55:18.203837: step 314560, loss = 0.0756, acc = 0.9760 (268.2 examples/sec; 0.239 sec/batch)
2017-05-09 23:55:22.959832: step 314580, loss = 0.0815, acc = 0.9740 (293.2 examples/sec; 0.218 sec/batch)
2017-05-09 23:55:27.548393: step 314600, loss = 0.0738, acc = 0.9820 (259.7 examples/sec; 0.246 sec/batch)
2017-05-09 23:55:32.112132: step 314620, loss = 0.0645, acc = 0.9860 (285.4 examples/sec; 0.224 sec/batch)
2017-05-09 23:55:36.863024: step 314640, loss = 0.0715, acc = 0.9860 (282.9 examples/sec; 0.226 sec/batch)
2017-05-09 23:55:41.464831: step 314660, loss = 0.0603, acc = 0.9880 (285.0 examples/sec; 0.225 sec/batch)
2017-05-09 23:55:46.021422: step 314680, loss = 0.0917, acc = 0.9840 (285.5 examples/sec; 0.224 sec/batch)
2017-05-09 23:55:50.728829: step 314700, loss = 0.0842, acc = 0.9820 (284.6 examples/sec; 0.225 sec/batch)
2017-05-09 23:55:55.277162: step 314720, loss = 0.0723, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-09 23:55:59.914999: step 314740, loss = 0.0819, acc = 0.9840 (278.6 examples/sec; 0.230 sec/batch)
2017-05-09 23:56:04.806328: step 314760, loss = 0.0615, acc = 0.9860 (286.9 examples/sec; 0.223 sec/batch)
2017-05-09 23:56:09.411154: step 314780, loss = 0.0651, acc = 0.9840 (277.8 examples/sec; 0.230 sec/batch)
2017-05-09 23:56:13.921055: step 314800, loss = 0.0648, acc = 0.9840 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 23:56:18.904484: step 314820, loss = 0.0863, acc = 0.9780 (206.1 examples/sec; 0.311 sec/batch)
2017-05-09 23:56:23.633640: step 314840, loss = 0.0732, acc = 0.9840 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 23:56:28.249138: step 314860, loss = 0.0754, acc = 0.9820 (255.5 examples/sec; 0.251 sec/batch)
2017-05-09 23:56:32.899496: step 314880, loss = 0.0969, acc = 0.9700 (287.0 examples/sec; 0.223 sec/batch)
2017-05-09 23:56:37.898480: step 314900, loss = 0.0794, acc = 0.9740 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 23:56:42.454252: step 314920, loss = 0.0740, acc = 0.9780 (279.2 examples/sec; 0.229 sec/batch)
2017-05-09 23:56:47.127815: step 314940, loss = 0.0793, acc = 0.9900 (274.0 examples/sec; 0.234 sec/batch)
2017-05-09 23:56:51.717701: step 314960, loss = 0.0816, acc = 0.9800 (297.8 examples/sec; 0.215 sec/batch)
2017-05-09 23:56:56.262854: step 314980, loss = 0.0560, acc = 0.9860 (271.5 examples/sec; 0.236 sec/batch)
2017-05-09 23:57:00.933395: step 315000, loss = 0.0658, acc = 0.9900 (280.2 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-09 23:57:15.044498: step 315000, acc = 0.9640, f1 = 0.9628
[Test] 2017-05-09 23:57:24.613942: step 315000, acc = 0.9552, f1 = 0.9548
[Status] 2017-05-09 23:57:24.614030: step 315000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-09 23:57:29.202183: step 315020, loss = 0.0623, acc = 0.9900 (277.0 examples/sec; 0.231 sec/batch)
2017-05-09 23:57:33.773618: step 315040, loss = 0.0645, acc = 0.9880 (292.5 examples/sec; 0.219 sec/batch)
2017-05-09 23:57:38.329401: step 315060, loss = 0.0757, acc = 0.9860 (274.3 examples/sec; 0.233 sec/batch)
2017-05-09 23:57:42.991224: step 315080, loss = 0.0755, acc = 0.9820 (264.6 examples/sec; 0.242 sec/batch)
2017-05-09 23:57:47.720857: step 315100, loss = 0.0652, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 23:57:52.335107: step 315120, loss = 0.0791, acc = 0.9820 (265.4 examples/sec; 0.241 sec/batch)
2017-05-09 23:57:56.826592: step 315140, loss = 0.0747, acc = 0.9800 (278.5 examples/sec; 0.230 sec/batch)
2017-05-09 23:58:01.565945: step 315160, loss = 0.0653, acc = 0.9820 (269.5 examples/sec; 0.237 sec/batch)
2017-05-09 23:58:06.126941: step 315180, loss = 0.0651, acc = 0.9880 (291.8 examples/sec; 0.219 sec/batch)
2017-05-09 23:58:10.698192: step 315200, loss = 0.1142, acc = 0.9700 (278.9 examples/sec; 0.229 sec/batch)
2017-05-09 23:58:15.270673: step 315220, loss = 0.0743, acc = 0.9780 (285.6 examples/sec; 0.224 sec/batch)
2017-05-09 23:58:19.931576: step 315240, loss = 0.0767, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-09 23:58:24.493035: step 315260, loss = 0.0831, acc = 0.9820 (280.4 examples/sec; 0.228 sec/batch)
2017-05-09 23:58:29.421603: step 315280, loss = 0.0789, acc = 0.9840 (283.3 examples/sec; 0.226 sec/batch)
2017-05-09 23:58:34.156272: step 315300, loss = 0.0886, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-09 23:58:38.705836: step 315320, loss = 0.0658, acc = 0.9820 (276.1 examples/sec; 0.232 sec/batch)
2017-05-09 23:58:43.319097: step 315340, loss = 0.0686, acc = 0.9840 (275.9 examples/sec; 0.232 sec/batch)
2017-05-09 23:58:48.159715: step 315360, loss = 0.0627, acc = 0.9880 (268.4 examples/sec; 0.238 sec/batch)
2017-05-09 23:58:52.732641: step 315380, loss = 0.0932, acc = 0.9700 (262.5 examples/sec; 0.244 sec/batch)
2017-05-09 23:58:57.275577: step 315400, loss = 0.0787, acc = 0.9780 (280.9 examples/sec; 0.228 sec/batch)
2017-05-09 23:59:02.052276: step 315420, loss = 0.1238, acc = 0.9620 (232.9 examples/sec; 0.275 sec/batch)
2017-05-09 23:59:07.023137: step 315440, loss = 0.0763, acc = 0.9820 (285.7 examples/sec; 0.224 sec/batch)
2017-05-09 23:59:11.613979: step 315460, loss = 0.0991, acc = 0.9680 (275.7 examples/sec; 0.232 sec/batch)
2017-05-09 23:59:16.529040: step 315480, loss = 0.0706, acc = 0.9760 (271.4 examples/sec; 0.236 sec/batch)
2017-05-09 23:59:22.331282: step 315500, loss = 0.0707, acc = 0.9860 (131.2 examples/sec; 0.488 sec/batch)
2017-05-09 23:59:26.883463: step 315520, loss = 0.0866, acc = 0.9780 (279.7 examples/sec; 0.229 sec/batch)
2017-05-09 23:59:31.729759: step 315540, loss = 0.0800, acc = 0.9760 (249.1 examples/sec; 0.257 sec/batch)
2017-05-09 23:59:36.489543: step 315560, loss = 0.0693, acc = 0.9880 (291.7 examples/sec; 0.219 sec/batch)
2017-05-09 23:59:41.066164: step 315580, loss = 0.0722, acc = 0.9820 (282.6 examples/sec; 0.226 sec/batch)
2017-05-09 23:59:45.628941: step 315600, loss = 0.0772, acc = 0.9780 (278.4 examples/sec; 0.230 sec/batch)
2017-05-09 23:59:50.376276: step 315620, loss = 0.0679, acc = 0.9900 (270.8 examples/sec; 0.236 sec/batch)
2017-05-09 23:59:54.976818: step 315640, loss = 0.0580, acc = 0.9940 (286.0 examples/sec; 0.224 sec/batch)
2017-05-09 23:59:59.706826: step 315660, loss = 0.0609, acc = 0.9900 (250.2 examples/sec; 0.256 sec/batch)
2017-05-10 00:00:04.487654: step 315680, loss = 0.0710, acc = 0.9840 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 00:00:09.036489: step 315700, loss = 0.0693, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-10 00:00:13.626818: step 315720, loss = 0.0825, acc = 0.9760 (269.9 examples/sec; 0.237 sec/batch)
2017-05-10 00:00:18.413873: step 315740, loss = 0.0592, acc = 0.9880 (283.8 examples/sec; 0.226 sec/batch)
2017-05-10 00:00:22.955995: step 315760, loss = 0.0931, acc = 0.9860 (283.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:00:27.593014: step 315780, loss = 0.0617, acc = 0.9880 (274.2 examples/sec; 0.233 sec/batch)
2017-05-10 00:00:32.576168: step 315800, loss = 0.0802, acc = 0.9800 (285.7 examples/sec; 0.224 sec/batch)
2017-05-10 00:00:37.090743: step 315820, loss = 0.0897, acc = 0.9740 (296.4 examples/sec; 0.216 sec/batch)
2017-05-10 00:00:41.711975: step 315840, loss = 0.0768, acc = 0.9800 (273.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:00:46.556084: step 315860, loss = 0.0638, acc = 0.9900 (234.3 examples/sec; 0.273 sec/batch)
2017-05-10 00:00:51.153244: step 315880, loss = 0.0652, acc = 0.9840 (269.0 examples/sec; 0.238 sec/batch)
2017-05-10 00:00:55.751665: step 315900, loss = 0.0707, acc = 0.9860 (267.5 examples/sec; 0.239 sec/batch)
2017-05-10 00:01:00.354184: step 315920, loss = 0.0892, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-10 00:01:05.013654: step 315940, loss = 0.0771, acc = 0.9820 (285.6 examples/sec; 0.224 sec/batch)
2017-05-10 00:01:09.714131: step 315960, loss = 0.0626, acc = 0.9880 (276.0 examples/sec; 0.232 sec/batch)
2017-05-10 00:01:14.318029: step 315980, loss = 0.0788, acc = 0.9820 (281.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:01:19.080070: step 316000, loss = 0.0864, acc = 0.9720 (240.7 examples/sec; 0.266 sec/batch)
[Eval] 2017-05-10 00:01:33.230503: step 316000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-10 00:01:42.504114: step 316000, acc = 0.9554, f1 = 0.9551
[Status] 2017-05-10 00:01:42.504218: step 316000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:01:47.159213: step 316020, loss = 0.0666, acc = 0.9880 (273.3 examples/sec; 0.234 sec/batch)
2017-05-10 00:01:51.889108: step 316040, loss = 0.0836, acc = 0.9800 (277.8 examples/sec; 0.230 sec/batch)
2017-05-10 00:01:56.390673: step 316060, loss = 0.0630, acc = 0.9900 (268.1 examples/sec; 0.239 sec/batch)
2017-05-10 00:02:01.023389: step 316080, loss = 0.0889, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-10 00:02:05.816327: step 316100, loss = 0.0945, acc = 0.9860 (267.9 examples/sec; 0.239 sec/batch)
2017-05-10 00:02:10.447723: step 316120, loss = 0.0816, acc = 0.9760 (266.4 examples/sec; 0.240 sec/batch)
2017-05-10 00:02:15.066994: step 316140, loss = 0.0769, acc = 0.9840 (265.4 examples/sec; 0.241 sec/batch)
2017-05-10 00:02:19.935233: step 316160, loss = 0.0769, acc = 0.9760 (233.9 examples/sec; 0.274 sec/batch)
2017-05-10 00:02:24.470924: step 316180, loss = 0.0738, acc = 0.9780 (276.8 examples/sec; 0.231 sec/batch)
2017-05-10 00:02:28.990086: step 316200, loss = 0.0681, acc = 0.9860 (282.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:02:33.661262: step 316220, loss = 0.0899, acc = 0.9700 (232.6 examples/sec; 0.275 sec/batch)
2017-05-10 00:02:38.302006: step 316240, loss = 0.0602, acc = 0.9940 (273.9 examples/sec; 0.234 sec/batch)
2017-05-10 00:02:42.841270: step 316260, loss = 0.0634, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:02:47.373407: step 316280, loss = 0.0808, acc = 0.9760 (286.2 examples/sec; 0.224 sec/batch)
2017-05-10 00:02:52.065838: step 316300, loss = 0.0649, acc = 0.9880 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 00:02:56.793273: step 316320, loss = 0.0990, acc = 0.9760 (281.9 examples/sec; 0.227 sec/batch)
2017-05-10 00:03:01.471637: step 316340, loss = 0.0501, acc = 0.9940 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 00:03:06.067286: step 316360, loss = 0.0873, acc = 0.9740 (291.8 examples/sec; 0.219 sec/batch)
2017-05-10 00:03:10.675413: step 316380, loss = 0.0874, acc = 0.9800 (276.2 examples/sec; 0.232 sec/batch)
2017-05-10 00:03:15.319963: step 316400, loss = 0.0795, acc = 0.9800 (270.4 examples/sec; 0.237 sec/batch)
2017-05-10 00:03:20.207297: step 316420, loss = 0.0779, acc = 0.9820 (239.8 examples/sec; 0.267 sec/batch)
2017-05-10 00:03:24.831239: step 316440, loss = 0.0682, acc = 0.9820 (275.0 examples/sec; 0.233 sec/batch)
2017-05-10 00:03:29.378299: step 316460, loss = 0.0647, acc = 0.9880 (285.3 examples/sec; 0.224 sec/batch)
2017-05-10 00:03:33.980970: step 316480, loss = 0.0642, acc = 0.9920 (296.3 examples/sec; 0.216 sec/batch)
2017-05-10 00:03:38.599675: step 316500, loss = 0.0857, acc = 0.9820 (283.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:03:43.883743: step 316520, loss = 0.0638, acc = 0.9860 (269.4 examples/sec; 0.238 sec/batch)
2017-05-10 00:03:48.846150: step 316540, loss = 0.0737, acc = 0.9860 (214.2 examples/sec; 0.299 sec/batch)
2017-05-10 00:03:53.306507: step 316560, loss = 0.0680, acc = 0.9820 (282.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:03:57.847456: step 316580, loss = 0.0954, acc = 0.9640 (280.3 examples/sec; 0.228 sec/batch)
2017-05-10 00:04:02.431153: step 316600, loss = 0.0641, acc = 0.9860 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:04:07.242690: step 316620, loss = 0.0816, acc = 0.9780 (263.1 examples/sec; 0.243 sec/batch)
2017-05-10 00:04:11.893960: step 316640, loss = 0.0672, acc = 0.9880 (255.1 examples/sec; 0.251 sec/batch)
2017-05-10 00:04:16.497412: step 316660, loss = 0.0614, acc = 0.9900 (289.5 examples/sec; 0.221 sec/batch)
2017-05-10 00:04:21.288413: step 316680, loss = 0.0626, acc = 0.9860 (279.2 examples/sec; 0.229 sec/batch)
2017-05-10 00:04:26.255497: step 316700, loss = 0.0733, acc = 0.9860 (192.9 examples/sec; 0.332 sec/batch)
2017-05-10 00:04:31.365282: step 316720, loss = 0.0885, acc = 0.9800 (271.2 examples/sec; 0.236 sec/batch)
2017-05-10 00:04:35.986547: step 316740, loss = 0.1261, acc = 0.9560 (291.2 examples/sec; 0.220 sec/batch)
2017-05-10 00:04:40.568892: step 316760, loss = 0.0619, acc = 0.9900 (269.8 examples/sec; 0.237 sec/batch)
2017-05-10 00:04:45.212636: step 316780, loss = 0.0567, acc = 0.9920 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 00:04:49.764659: step 316800, loss = 0.0692, acc = 0.9880 (292.0 examples/sec; 0.219 sec/batch)
2017-05-10 00:04:54.187027: step 316820, loss = 0.0744, acc = 0.9840 (287.2 examples/sec; 0.223 sec/batch)
2017-05-10 00:04:58.717141: step 316840, loss = 0.0730, acc = 0.9840 (280.3 examples/sec; 0.228 sec/batch)
2017-05-10 00:05:03.303333: step 316860, loss = 0.0812, acc = 0.9840 (253.0 examples/sec; 0.253 sec/batch)
2017-05-10 00:05:07.986614: step 316880, loss = 0.0653, acc = 0.9900 (266.7 examples/sec; 0.240 sec/batch)
2017-05-10 00:05:12.596548: step 316900, loss = 0.0706, acc = 0.9820 (271.4 examples/sec; 0.236 sec/batch)
2017-05-10 00:05:17.355712: step 316920, loss = 0.0868, acc = 0.9800 (237.4 examples/sec; 0.270 sec/batch)
2017-05-10 00:05:22.049754: step 316940, loss = 0.0916, acc = 0.9820 (290.4 examples/sec; 0.220 sec/batch)
2017-05-10 00:05:26.667009: step 316960, loss = 0.0658, acc = 0.9840 (269.3 examples/sec; 0.238 sec/batch)
2017-05-10 00:05:31.292205: step 316980, loss = 0.0891, acc = 0.9740 (271.3 examples/sec; 0.236 sec/batch)
2017-05-10 00:05:35.947901: step 317000, loss = 0.0644, acc = 0.9840 (290.6 examples/sec; 0.220 sec/batch)
[Eval] 2017-05-10 00:05:49.984512: step 317000, acc = 0.9629, f1 = 0.9618
[Test] 2017-05-10 00:05:59.275216: step 317000, acc = 0.9542, f1 = 0.9539
[Status] 2017-05-10 00:05:59.275295: step 317000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:06:03.918110: step 317020, loss = 0.0764, acc = 0.9840 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 00:06:08.398267: step 317040, loss = 0.0780, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 00:06:13.123182: step 317060, loss = 0.0796, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-10 00:06:17.840077: step 317080, loss = 0.0779, acc = 0.9820 (295.0 examples/sec; 0.217 sec/batch)
2017-05-10 00:06:22.382399: step 317100, loss = 0.0733, acc = 0.9940 (274.9 examples/sec; 0.233 sec/batch)
2017-05-10 00:06:27.044533: step 317120, loss = 0.0933, acc = 0.9800 (265.4 examples/sec; 0.241 sec/batch)
2017-05-10 00:06:31.679898: step 317140, loss = 0.0603, acc = 0.9880 (252.3 examples/sec; 0.254 sec/batch)
2017-05-10 00:06:36.212813: step 317160, loss = 0.0678, acc = 0.9840 (276.6 examples/sec; 0.231 sec/batch)
2017-05-10 00:06:40.794280: step 317180, loss = 0.0774, acc = 0.9800 (275.3 examples/sec; 0.232 sec/batch)
2017-05-10 00:06:45.303827: step 317200, loss = 0.0655, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 00:06:50.040353: step 317220, loss = 0.0764, acc = 0.9820 (284.8 examples/sec; 0.225 sec/batch)
2017-05-10 00:06:54.762994: step 317240, loss = 0.0807, acc = 0.9780 (291.9 examples/sec; 0.219 sec/batch)
2017-05-10 00:06:59.393935: step 317260, loss = 0.0682, acc = 0.9940 (272.1 examples/sec; 0.235 sec/batch)
2017-05-10 00:07:04.174316: step 317280, loss = 0.0713, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 00:07:08.724004: step 317300, loss = 0.1002, acc = 0.9700 (288.5 examples/sec; 0.222 sec/batch)
2017-05-10 00:07:13.350465: step 317320, loss = 0.0828, acc = 0.9780 (267.6 examples/sec; 0.239 sec/batch)
2017-05-10 00:07:18.037115: step 317340, loss = 0.0870, acc = 0.9720 (290.6 examples/sec; 0.220 sec/batch)
2017-05-10 00:07:22.642686: step 317360, loss = 0.0894, acc = 0.9820 (270.9 examples/sec; 0.236 sec/batch)
2017-05-10 00:07:27.313552: step 317380, loss = 0.0845, acc = 0.9800 (254.5 examples/sec; 0.251 sec/batch)
2017-05-10 00:07:32.161129: step 317400, loss = 0.0828, acc = 0.9740 (251.0 examples/sec; 0.255 sec/batch)
2017-05-10 00:07:36.712272: step 317420, loss = 0.0752, acc = 0.9840 (296.1 examples/sec; 0.216 sec/batch)
2017-05-10 00:07:41.296752: step 317440, loss = 0.0694, acc = 0.9860 (281.7 examples/sec; 0.227 sec/batch)
2017-05-10 00:07:46.254961: step 317460, loss = 0.0690, acc = 0.9760 (215.5 examples/sec; 0.297 sec/batch)
2017-05-10 00:07:50.892331: step 317480, loss = 0.0886, acc = 0.9740 (264.5 examples/sec; 0.242 sec/batch)
2017-05-10 00:07:55.427018: step 317500, loss = 0.0914, acc = 0.9660 (295.7 examples/sec; 0.216 sec/batch)
2017-05-10 00:08:01.224440: step 317520, loss = 0.0795, acc = 0.9780 (251.9 examples/sec; 0.254 sec/batch)
2017-05-10 00:08:05.666920: step 317540, loss = 0.0601, acc = 0.9880 (285.9 examples/sec; 0.224 sec/batch)
2017-05-10 00:08:10.243245: step 317560, loss = 0.0780, acc = 0.9780 (281.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:08:14.973399: step 317580, loss = 0.0837, acc = 0.9740 (269.6 examples/sec; 0.237 sec/batch)
2017-05-10 00:08:19.710816: step 317600, loss = 0.0727, acc = 0.9900 (283.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:08:24.250632: step 317620, loss = 0.0824, acc = 0.9820 (289.0 examples/sec; 0.221 sec/batch)
2017-05-10 00:08:28.864460: step 317640, loss = 0.0608, acc = 0.9920 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 00:08:33.489624: step 317660, loss = 0.0730, acc = 0.9860 (285.7 examples/sec; 0.224 sec/batch)
2017-05-10 00:08:37.966334: step 317680, loss = 0.0759, acc = 0.9860 (279.3 examples/sec; 0.229 sec/batch)
2017-05-10 00:08:42.491107: step 317700, loss = 0.0974, acc = 0.9780 (294.4 examples/sec; 0.217 sec/batch)
2017-05-10 00:08:47.226528: step 317720, loss = 0.0807, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-10 00:08:51.820792: step 317740, loss = 0.0899, acc = 0.9720 (272.7 examples/sec; 0.235 sec/batch)
2017-05-10 00:08:56.416075: step 317760, loss = 0.0668, acc = 0.9840 (285.1 examples/sec; 0.224 sec/batch)
2017-05-10 00:09:01.035057: step 317780, loss = 0.0858, acc = 0.9780 (298.1 examples/sec; 0.215 sec/batch)
2017-05-10 00:09:05.721794: step 317800, loss = 0.0708, acc = 0.9840 (263.9 examples/sec; 0.243 sec/batch)
2017-05-10 00:09:10.258549: step 317820, loss = 0.0763, acc = 0.9880 (291.1 examples/sec; 0.220 sec/batch)
2017-05-10 00:09:14.817908: step 317840, loss = 0.0647, acc = 0.9860 (285.3 examples/sec; 0.224 sec/batch)
2017-05-10 00:09:19.709928: step 317860, loss = 0.0538, acc = 0.9900 (287.5 examples/sec; 0.223 sec/batch)
2017-05-10 00:09:24.436855: step 317880, loss = 0.0814, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-10 00:09:29.039856: step 317900, loss = 0.0804, acc = 0.9840 (271.5 examples/sec; 0.236 sec/batch)
2017-05-10 00:09:33.876698: step 317920, loss = 0.0730, acc = 0.9800 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:09:38.514842: step 317940, loss = 0.0747, acc = 0.9840 (254.4 examples/sec; 0.252 sec/batch)
2017-05-10 00:09:43.116436: step 317960, loss = 0.0807, acc = 0.9800 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 00:09:47.788621: step 317980, loss = 0.0745, acc = 0.9800 (289.0 examples/sec; 0.221 sec/batch)
2017-05-10 00:09:52.376779: step 318000, loss = 0.0646, acc = 0.9800 (274.9 examples/sec; 0.233 sec/batch)
[Eval] 2017-05-10 00:10:06.415793: step 318000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-10 00:10:16.031716: step 318000, acc = 0.9559, f1 = 0.9555
[Status] 2017-05-10 00:10:16.031820: step 318000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:10:20.733061: step 318020, loss = 0.1143, acc = 0.9680 (267.1 examples/sec; 0.240 sec/batch)
2017-05-10 00:10:25.297303: step 318040, loss = 0.1029, acc = 0.9880 (282.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:10:29.889420: step 318060, loss = 0.0754, acc = 0.9880 (285.8 examples/sec; 0.224 sec/batch)
2017-05-10 00:10:34.679492: step 318080, loss = 0.0664, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 00:10:39.284018: step 318100, loss = 0.0909, acc = 0.9700 (285.5 examples/sec; 0.224 sec/batch)
2017-05-10 00:10:43.887108: step 318120, loss = 0.0882, acc = 0.9740 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 00:10:48.635822: step 318140, loss = 0.0654, acc = 0.9900 (293.8 examples/sec; 0.218 sec/batch)
2017-05-10 00:10:53.168380: step 318160, loss = 0.0696, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 00:10:57.788513: step 318180, loss = 0.0714, acc = 0.9860 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 00:11:02.582918: step 318200, loss = 0.0613, acc = 0.9840 (273.6 examples/sec; 0.234 sec/batch)
2017-05-10 00:11:07.099994: step 318220, loss = 0.0834, acc = 0.9780 (283.2 examples/sec; 0.226 sec/batch)
2017-05-10 00:11:11.727578: step 318240, loss = 0.0610, acc = 0.9900 (269.9 examples/sec; 0.237 sec/batch)
2017-05-10 00:11:16.401283: step 318260, loss = 0.0706, acc = 0.9800 (236.3 examples/sec; 0.271 sec/batch)
2017-05-10 00:11:20.956334: step 318280, loss = 0.0625, acc = 0.9840 (277.7 examples/sec; 0.230 sec/batch)
2017-05-10 00:11:25.665408: step 318300, loss = 0.0901, acc = 0.9840 (268.1 examples/sec; 0.239 sec/batch)
2017-05-10 00:11:30.320631: step 318320, loss = 0.0874, acc = 0.9760 (291.3 examples/sec; 0.220 sec/batch)
2017-05-10 00:11:35.190205: step 318340, loss = 0.0805, acc = 0.9780 (280.8 examples/sec; 0.228 sec/batch)
2017-05-10 00:11:39.847319: step 318360, loss = 0.0857, acc = 0.9740 (260.5 examples/sec; 0.246 sec/batch)
2017-05-10 00:11:44.395615: step 318380, loss = 0.0711, acc = 0.9860 (299.0 examples/sec; 0.214 sec/batch)
2017-05-10 00:11:49.301375: step 318400, loss = 0.0900, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
2017-05-10 00:11:53.912272: step 318420, loss = 0.0708, acc = 0.9840 (255.7 examples/sec; 0.250 sec/batch)
2017-05-10 00:11:58.484353: step 318440, loss = 0.0952, acc = 0.9740 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 00:12:03.347583: step 318460, loss = 0.0814, acc = 0.9840 (282.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:12:07.977147: step 318480, loss = 0.0818, acc = 0.9820 (269.6 examples/sec; 0.237 sec/batch)
2017-05-10 00:12:12.571075: step 318500, loss = 0.0623, acc = 0.9840 (283.2 examples/sec; 0.226 sec/batch)
2017-05-10 00:12:17.235439: step 318520, loss = 0.0827, acc = 0.9820 (291.1 examples/sec; 0.220 sec/batch)
2017-05-10 00:12:22.453054: step 318540, loss = 0.0553, acc = 0.9940 (286.8 examples/sec; 0.223 sec/batch)
2017-05-10 00:12:26.989777: step 318560, loss = 0.0834, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-10 00:12:31.596764: step 318580, loss = 0.0662, acc = 0.9880 (278.6 examples/sec; 0.230 sec/batch)
2017-05-10 00:12:36.179031: step 318600, loss = 0.0738, acc = 0.9880 (278.1 examples/sec; 0.230 sec/batch)
2017-05-10 00:12:40.769387: step 318620, loss = 0.0632, acc = 0.9880 (286.7 examples/sec; 0.223 sec/batch)
2017-05-10 00:12:45.472799: step 318640, loss = 0.0718, acc = 0.9880 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:12:50.042519: step 318660, loss = 0.0735, acc = 0.9860 (293.8 examples/sec; 0.218 sec/batch)
2017-05-10 00:12:54.576122: step 318680, loss = 0.0770, acc = 0.9760 (280.9 examples/sec; 0.228 sec/batch)
2017-05-10 00:12:59.505912: step 318700, loss = 0.0663, acc = 0.9920 (218.2 examples/sec; 0.293 sec/batch)
2017-05-10 00:13:03.978577: step 318720, loss = 0.0671, acc = 0.9820 (279.6 examples/sec; 0.229 sec/batch)
2017-05-10 00:13:08.593517: step 318740, loss = 0.0640, acc = 0.9920 (278.6 examples/sec; 0.230 sec/batch)
2017-05-10 00:13:13.398344: step 318760, loss = 0.0966, acc = 0.9780 (271.2 examples/sec; 0.236 sec/batch)
2017-05-10 00:13:18.524904: step 318780, loss = 0.0768, acc = 0.9820 (300.2 examples/sec; 0.213 sec/batch)
2017-05-10 00:13:23.174727: step 318800, loss = 0.0530, acc = 0.9920 (270.7 examples/sec; 0.236 sec/batch)
2017-05-10 00:13:27.671817: step 318820, loss = 0.0653, acc = 0.9920 (282.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:13:32.399533: step 318840, loss = 0.0769, acc = 0.9800 (286.3 examples/sec; 0.224 sec/batch)
2017-05-10 00:13:37.074407: step 318860, loss = 0.0805, acc = 0.9880 (258.9 examples/sec; 0.247 sec/batch)
2017-05-10 00:13:41.623571: step 318880, loss = 0.0687, acc = 0.9820 (288.9 examples/sec; 0.222 sec/batch)
2017-05-10 00:13:46.344736: step 318900, loss = 0.0837, acc = 0.9740 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 00:13:50.868807: step 318920, loss = 0.0815, acc = 0.9780 (283.2 examples/sec; 0.226 sec/batch)
2017-05-10 00:13:55.400696: step 318940, loss = 0.0654, acc = 0.9860 (272.1 examples/sec; 0.235 sec/batch)
2017-05-10 00:14:00.422739: step 318960, loss = 0.0633, acc = 0.9900 (284.6 examples/sec; 0.225 sec/batch)
2017-05-10 00:14:05.015396: step 318980, loss = 0.0632, acc = 0.9780 (271.4 examples/sec; 0.236 sec/batch)
2017-05-10 00:14:09.778437: step 319000, loss = 0.0840, acc = 0.9840 (280.4 examples/sec; 0.228 sec/batch)
[Eval] 2017-05-10 00:14:23.849244: step 319000, acc = 0.9634, f1 = 0.9621
[Test] 2017-05-10 00:14:33.434718: step 319000, acc = 0.9545, f1 = 0.9541
[Status] 2017-05-10 00:14:33.434817: step 319000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:14:38.065042: step 319020, loss = 0.0641, acc = 0.9780 (274.1 examples/sec; 0.233 sec/batch)
2017-05-10 00:14:42.755327: step 319040, loss = 0.0737, acc = 0.9800 (282.2 examples/sec; 0.227 sec/batch)
2017-05-10 00:14:47.423443: step 319060, loss = 0.0637, acc = 0.9880 (294.8 examples/sec; 0.217 sec/batch)
2017-05-10 00:14:52.033778: step 319080, loss = 0.0987, acc = 0.9720 (280.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:14:56.837382: step 319100, loss = 0.0883, acc = 0.9780 (226.7 examples/sec; 0.282 sec/batch)
2017-05-10 00:15:01.810173: step 319120, loss = 0.0693, acc = 0.9760 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 00:15:06.358231: step 319140, loss = 0.0669, acc = 0.9860 (287.5 examples/sec; 0.223 sec/batch)
2017-05-10 00:15:10.908541: step 319160, loss = 0.0845, acc = 0.9780 (278.3 examples/sec; 0.230 sec/batch)
2017-05-10 00:15:15.670763: step 319180, loss = 0.0785, acc = 0.9800 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 00:15:20.289158: step 319200, loss = 0.0724, acc = 0.9860 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 00:15:24.762066: step 319220, loss = 0.0974, acc = 0.9840 (290.5 examples/sec; 0.220 sec/batch)
2017-05-10 00:15:29.386942: step 319240, loss = 0.0848, acc = 0.9860 (281.1 examples/sec; 0.228 sec/batch)
2017-05-10 00:15:33.861324: step 319260, loss = 0.0629, acc = 0.9860 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 00:15:38.401445: step 319280, loss = 0.0891, acc = 0.9680 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 00:15:43.275327: step 319300, loss = 0.0923, acc = 0.9700 (273.5 examples/sec; 0.234 sec/batch)
2017-05-10 00:15:47.963411: step 319320, loss = 0.0728, acc = 0.9820 (286.0 examples/sec; 0.224 sec/batch)
2017-05-10 00:15:52.604801: step 319340, loss = 0.0809, acc = 0.9740 (281.8 examples/sec; 0.227 sec/batch)
2017-05-10 00:15:57.362442: step 319360, loss = 0.0820, acc = 0.9900 (249.5 examples/sec; 0.257 sec/batch)
2017-05-10 00:16:01.930767: step 319380, loss = 0.0757, acc = 0.9820 (273.3 examples/sec; 0.234 sec/batch)
2017-05-10 00:16:06.690958: step 319400, loss = 0.0766, acc = 0.9760 (273.6 examples/sec; 0.234 sec/batch)
2017-05-10 00:16:11.291562: step 319420, loss = 0.0749, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 00:16:16.036838: step 319440, loss = 0.0799, acc = 0.9820 (289.1 examples/sec; 0.221 sec/batch)
2017-05-10 00:16:20.509276: step 319460, loss = 0.0672, acc = 0.9860 (274.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:16:25.116178: step 319480, loss = 0.0788, acc = 0.9820 (273.9 examples/sec; 0.234 sec/batch)
2017-05-10 00:16:29.932665: step 319500, loss = 0.0651, acc = 0.9840 (259.9 examples/sec; 0.246 sec/batch)
2017-05-10 00:16:34.549382: step 319520, loss = 0.0734, acc = 0.9820 (277.5 examples/sec; 0.231 sec/batch)
2017-05-10 00:16:40.122353: step 319540, loss = 0.0676, acc = 0.9860 (265.1 examples/sec; 0.241 sec/batch)
2017-05-10 00:16:45.023830: step 319560, loss = 0.0691, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 00:16:49.583704: step 319580, loss = 0.0872, acc = 0.9840 (268.9 examples/sec; 0.238 sec/batch)
2017-05-10 00:16:54.069200: step 319600, loss = 0.0687, acc = 0.9820 (279.2 examples/sec; 0.229 sec/batch)
2017-05-10 00:16:58.862780: step 319620, loss = 0.0673, acc = 0.9920 (243.2 examples/sec; 0.263 sec/batch)
2017-05-10 00:17:03.371100: step 319640, loss = 0.0550, acc = 0.9980 (278.2 examples/sec; 0.230 sec/batch)
2017-05-10 00:17:08.040407: step 319660, loss = 0.0585, acc = 0.9880 (271.2 examples/sec; 0.236 sec/batch)
2017-05-10 00:17:12.673991: step 319680, loss = 0.0722, acc = 0.9820 (286.5 examples/sec; 0.223 sec/batch)
2017-05-10 00:17:17.362488: step 319700, loss = 0.0643, acc = 0.9880 (267.8 examples/sec; 0.239 sec/batch)
2017-05-10 00:17:22.065705: step 319720, loss = 0.0802, acc = 0.9800 (261.0 examples/sec; 0.245 sec/batch)
2017-05-10 00:17:26.710661: step 319740, loss = 0.1085, acc = 0.9620 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 00:17:31.180485: step 319760, loss = 0.0602, acc = 0.9880 (284.4 examples/sec; 0.225 sec/batch)
2017-05-10 00:17:35.784421: step 319780, loss = 0.0872, acc = 0.9780 (273.1 examples/sec; 0.234 sec/batch)
2017-05-10 00:17:40.431888: step 319800, loss = 0.0748, acc = 0.9820 (251.0 examples/sec; 0.255 sec/batch)
2017-05-10 00:17:45.082901: step 319820, loss = 0.0713, acc = 0.9820 (284.0 examples/sec; 0.225 sec/batch)
2017-05-10 00:17:49.677549: step 319840, loss = 0.0752, acc = 0.9840 (271.5 examples/sec; 0.236 sec/batch)
2017-05-10 00:17:54.257930: step 319860, loss = 0.0877, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
2017-05-10 00:17:58.917939: step 319880, loss = 0.0573, acc = 0.9900 (280.1 examples/sec; 0.229 sec/batch)
2017-05-10 00:18:03.717407: step 319900, loss = 0.0868, acc = 0.9680 (277.6 examples/sec; 0.231 sec/batch)
2017-05-10 00:18:08.377643: step 319920, loss = 0.0942, acc = 0.9720 (266.9 examples/sec; 0.240 sec/batch)
2017-05-10 00:18:13.045928: step 319940, loss = 0.0645, acc = 0.9880 (276.0 examples/sec; 0.232 sec/batch)
2017-05-10 00:18:17.645194: step 319960, loss = 0.0659, acc = 0.9840 (266.3 examples/sec; 0.240 sec/batch)
2017-05-10 00:18:22.291980: step 319980, loss = 0.0698, acc = 0.9900 (277.3 examples/sec; 0.231 sec/batch)
2017-05-10 00:18:27.028410: step 320000, loss = 0.0810, acc = 0.9840 (295.6 examples/sec; 0.216 sec/batch)
[Eval] 2017-05-10 00:18:41.219520: step 320000, acc = 0.9644, f1 = 0.9633
[Test] 2017-05-10 00:18:50.499251: step 320000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-10 00:18:50.499342: step 320000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:18:55.275580: step 320020, loss = 0.0857, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:19:00.040604: step 320040, loss = 0.0656, acc = 0.9880 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:19:04.814321: step 320060, loss = 0.0821, acc = 0.9780 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 00:19:09.692612: step 320080, loss = 0.0749, acc = 0.9720 (241.8 examples/sec; 0.265 sec/batch)
2017-05-10 00:19:14.279841: step 320100, loss = 0.0611, acc = 0.9920 (265.6 examples/sec; 0.241 sec/batch)
2017-05-10 00:19:18.722834: step 320120, loss = 0.0823, acc = 0.9760 (288.4 examples/sec; 0.222 sec/batch)
2017-05-10 00:19:23.329123: step 320140, loss = 0.0612, acc = 0.9900 (258.3 examples/sec; 0.248 sec/batch)
2017-05-10 00:19:28.061249: step 320160, loss = 0.0601, acc = 0.9900 (280.1 examples/sec; 0.228 sec/batch)
2017-05-10 00:19:32.545123: step 320180, loss = 0.0612, acc = 0.9880 (292.5 examples/sec; 0.219 sec/batch)
2017-05-10 00:19:37.084244: step 320200, loss = 0.0602, acc = 0.9840 (289.4 examples/sec; 0.221 sec/batch)
2017-05-10 00:19:41.689527: step 320220, loss = 0.0596, acc = 0.9920 (284.5 examples/sec; 0.225 sec/batch)
2017-05-10 00:19:46.334872: step 320240, loss = 0.0833, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-10 00:19:50.876523: step 320260, loss = 0.0663, acc = 0.9800 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 00:19:55.625121: step 320280, loss = 0.0642, acc = 0.9880 (286.2 examples/sec; 0.224 sec/batch)
2017-05-10 00:20:00.153624: step 320300, loss = 0.0705, acc = 0.9860 (273.3 examples/sec; 0.234 sec/batch)
2017-05-10 00:20:04.687744: step 320320, loss = 0.0630, acc = 0.9880 (298.1 examples/sec; 0.215 sec/batch)
2017-05-10 00:20:09.327674: step 320340, loss = 0.0996, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 00:20:13.928160: step 320360, loss = 0.0837, acc = 0.9760 (274.1 examples/sec; 0.234 sec/batch)
2017-05-10 00:20:18.496114: step 320380, loss = 0.0522, acc = 0.9900 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 00:20:23.366470: step 320400, loss = 0.0751, acc = 0.9840 (223.6 examples/sec; 0.286 sec/batch)
2017-05-10 00:20:27.967439: step 320420, loss = 0.0670, acc = 0.9840 (284.7 examples/sec; 0.225 sec/batch)
2017-05-10 00:20:32.801925: step 320440, loss = 0.0818, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 00:20:37.364670: step 320460, loss = 0.0716, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-10 00:20:42.138074: step 320480, loss = 0.0825, acc = 0.9820 (290.0 examples/sec; 0.221 sec/batch)
2017-05-10 00:20:46.770447: step 320500, loss = 0.0783, acc = 0.9820 (283.5 examples/sec; 0.226 sec/batch)
2017-05-10 00:20:51.578083: step 320520, loss = 0.0800, acc = 0.9800 (291.8 examples/sec; 0.219 sec/batch)
2017-05-10 00:20:57.149041: step 320540, loss = 0.0988, acc = 0.9740 (159.4 examples/sec; 0.402 sec/batch)
2017-05-10 00:21:01.710857: step 320560, loss = 0.0710, acc = 0.9840 (285.2 examples/sec; 0.224 sec/batch)
2017-05-10 00:21:06.324783: step 320580, loss = 0.0692, acc = 0.9900 (276.0 examples/sec; 0.232 sec/batch)
2017-05-10 00:21:10.956514: step 320600, loss = 0.0953, acc = 0.9780 (288.2 examples/sec; 0.222 sec/batch)
2017-05-10 00:21:15.895953: step 320620, loss = 0.0679, acc = 0.9860 (276.7 examples/sec; 0.231 sec/batch)
2017-05-10 00:21:20.507068: step 320640, loss = 0.0711, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-10 00:21:25.189014: step 320660, loss = 0.0739, acc = 0.9800 (297.9 examples/sec; 0.215 sec/batch)
2017-05-10 00:21:29.802234: step 320680, loss = 0.0563, acc = 0.9920 (282.1 examples/sec; 0.227 sec/batch)
2017-05-10 00:21:34.450846: step 320700, loss = 0.0744, acc = 0.9760 (270.1 examples/sec; 0.237 sec/batch)
2017-05-10 00:21:39.010979: step 320720, loss = 0.0591, acc = 0.9880 (282.6 examples/sec; 0.227 sec/batch)
2017-05-10 00:21:43.701659: step 320740, loss = 0.0731, acc = 0.9920 (283.6 examples/sec; 0.226 sec/batch)
2017-05-10 00:21:48.192186: step 320760, loss = 0.0726, acc = 0.9860 (284.4 examples/sec; 0.225 sec/batch)
2017-05-10 00:21:52.844802: step 320780, loss = 0.0738, acc = 0.9820 (294.5 examples/sec; 0.217 sec/batch)
2017-05-10 00:21:57.410130: step 320800, loss = 0.0815, acc = 0.9760 (284.0 examples/sec; 0.225 sec/batch)
2017-05-10 00:22:01.873055: step 320820, loss = 0.0661, acc = 0.9880 (279.4 examples/sec; 0.229 sec/batch)
2017-05-10 00:22:06.439278: step 320840, loss = 0.0894, acc = 0.9840 (282.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:22:11.238918: step 320860, loss = 0.0702, acc = 0.9880 (268.7 examples/sec; 0.238 sec/batch)
2017-05-10 00:22:15.830062: step 320880, loss = 0.0851, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
2017-05-10 00:22:20.406847: step 320900, loss = 0.0627, acc = 0.9880 (277.8 examples/sec; 0.230 sec/batch)
2017-05-10 00:22:25.269858: step 320920, loss = 0.0834, acc = 0.9740 (280.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:22:29.974356: step 320940, loss = 0.0939, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 00:22:34.649197: step 320960, loss = 0.0830, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 00:22:39.457000: step 320980, loss = 0.0751, acc = 0.9780 (295.6 examples/sec; 0.217 sec/batch)
2017-05-10 00:22:43.963044: step 321000, loss = 0.0896, acc = 0.9720 (266.7 examples/sec; 0.240 sec/batch)
[Eval] 2017-05-10 00:22:58.144602: step 321000, acc = 0.9623, f1 = 0.9609
[Test] 2017-05-10 00:23:07.870846: step 321000, acc = 0.9526, f1 = 0.9522
[Status] 2017-05-10 00:23:07.870953: step 321000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:23:12.465505: step 321020, loss = 0.1128, acc = 0.9600 (292.1 examples/sec; 0.219 sec/batch)
2017-05-10 00:23:16.988340: step 321040, loss = 0.0691, acc = 0.9860 (282.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:23:21.558977: step 321060, loss = 0.0655, acc = 0.9820 (278.4 examples/sec; 0.230 sec/batch)
2017-05-10 00:23:26.314982: step 321080, loss = 0.0726, acc = 0.9820 (274.2 examples/sec; 0.233 sec/batch)
2017-05-10 00:23:30.904041: step 321100, loss = 0.1073, acc = 0.9680 (273.5 examples/sec; 0.234 sec/batch)
2017-05-10 00:23:35.581475: step 321120, loss = 0.1068, acc = 0.9740 (276.2 examples/sec; 0.232 sec/batch)
2017-05-10 00:23:40.324500: step 321140, loss = 0.0674, acc = 0.9900 (286.9 examples/sec; 0.223 sec/batch)
2017-05-10 00:23:45.011334: step 321160, loss = 0.0640, acc = 0.9860 (258.9 examples/sec; 0.247 sec/batch)
2017-05-10 00:23:49.651973: step 321180, loss = 0.0771, acc = 0.9760 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 00:23:54.278321: step 321200, loss = 0.0640, acc = 0.9920 (281.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:23:58.890666: step 321220, loss = 0.0724, acc = 0.9840 (281.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:24:03.449717: step 321240, loss = 0.0762, acc = 0.9820 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 00:24:08.160954: step 321260, loss = 0.0777, acc = 0.9780 (300.4 examples/sec; 0.213 sec/batch)
2017-05-10 00:24:12.857108: step 321280, loss = 0.0891, acc = 0.9740 (257.2 examples/sec; 0.249 sec/batch)
2017-05-10 00:24:17.529297: step 321300, loss = 0.0696, acc = 0.9840 (275.0 examples/sec; 0.233 sec/batch)
2017-05-10 00:24:22.473667: step 321320, loss = 0.0594, acc = 0.9960 (230.8 examples/sec; 0.277 sec/batch)
2017-05-10 00:24:26.995647: step 321340, loss = 0.0632, acc = 0.9900 (286.7 examples/sec; 0.223 sec/batch)
2017-05-10 00:24:31.491694: step 321360, loss = 0.0675, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-10 00:24:36.293047: step 321380, loss = 0.0728, acc = 0.9860 (239.4 examples/sec; 0.267 sec/batch)
2017-05-10 00:24:40.884170: step 321400, loss = 0.0560, acc = 0.9900 (283.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:24:45.441057: step 321420, loss = 0.0630, acc = 0.9860 (287.4 examples/sec; 0.223 sec/batch)
2017-05-10 00:24:50.030555: step 321440, loss = 0.0763, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-10 00:24:54.763320: step 321460, loss = 0.0738, acc = 0.9820 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 00:24:59.377205: step 321480, loss = 0.0727, acc = 0.9800 (256.3 examples/sec; 0.250 sec/batch)
2017-05-10 00:25:04.025510: step 321500, loss = 0.0783, acc = 0.9760 (270.9 examples/sec; 0.236 sec/batch)
2017-05-10 00:25:08.765186: step 321520, loss = 0.0779, acc = 0.9800 (284.5 examples/sec; 0.225 sec/batch)
2017-05-10 00:25:13.336065: step 321540, loss = 0.0703, acc = 0.9880 (286.3 examples/sec; 0.224 sec/batch)
2017-05-10 00:25:18.936010: step 321560, loss = 0.0600, acc = 0.9920 (279.4 examples/sec; 0.229 sec/batch)
2017-05-10 00:25:23.659982: step 321580, loss = 0.0798, acc = 0.9780 (281.3 examples/sec; 0.227 sec/batch)
2017-05-10 00:25:28.183330: step 321600, loss = 0.0704, acc = 0.9840 (265.5 examples/sec; 0.241 sec/batch)
2017-05-10 00:25:32.785887: step 321620, loss = 0.0659, acc = 0.9880 (275.7 examples/sec; 0.232 sec/batch)
2017-05-10 00:25:37.370234: step 321640, loss = 0.0532, acc = 0.9940 (302.4 examples/sec; 0.212 sec/batch)
2017-05-10 00:25:41.933340: step 321660, loss = 0.0888, acc = 0.9660 (268.1 examples/sec; 0.239 sec/batch)
2017-05-10 00:25:46.520251: step 321680, loss = 0.0797, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-10 00:25:51.276834: step 321700, loss = 0.0660, acc = 0.9840 (296.0 examples/sec; 0.216 sec/batch)
2017-05-10 00:25:55.906482: step 321720, loss = 0.0731, acc = 0.9840 (271.4 examples/sec; 0.236 sec/batch)
2017-05-10 00:26:00.481878: step 321740, loss = 0.0776, acc = 0.9780 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 00:26:05.317331: step 321760, loss = 0.0733, acc = 0.9860 (235.7 examples/sec; 0.272 sec/batch)
2017-05-10 00:26:09.887423: step 321780, loss = 0.0625, acc = 0.9880 (279.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:26:14.481979: step 321800, loss = 0.0693, acc = 0.9860 (287.0 examples/sec; 0.223 sec/batch)
2017-05-10 00:26:18.998171: step 321820, loss = 0.0750, acc = 0.9820 (296.0 examples/sec; 0.216 sec/batch)
2017-05-10 00:26:23.747917: step 321840, loss = 0.0700, acc = 0.9800 (268.5 examples/sec; 0.238 sec/batch)
2017-05-10 00:26:28.351663: step 321860, loss = 0.0790, acc = 0.9820 (272.0 examples/sec; 0.235 sec/batch)
2017-05-10 00:26:33.003806: step 321880, loss = 0.0454, acc = 1.0000 (287.1 examples/sec; 0.223 sec/batch)
2017-05-10 00:26:37.695043: step 321900, loss = 0.0699, acc = 0.9780 (283.3 examples/sec; 0.226 sec/batch)
2017-05-10 00:26:42.377188: step 321920, loss = 0.0675, acc = 0.9900 (260.6 examples/sec; 0.246 sec/batch)
2017-05-10 00:26:46.982374: step 321940, loss = 0.0629, acc = 0.9860 (283.5 examples/sec; 0.226 sec/batch)
2017-05-10 00:26:51.893239: step 321960, loss = 0.0741, acc = 0.9820 (231.8 examples/sec; 0.276 sec/batch)
2017-05-10 00:26:56.443793: step 321980, loss = 0.0766, acc = 0.9860 (282.0 examples/sec; 0.227 sec/batch)
2017-05-10 00:27:01.097985: step 322000, loss = 0.0537, acc = 0.9940 (276.4 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-10 00:27:15.111343: step 322000, acc = 0.9610, f1 = 0.9596
[Test] 2017-05-10 00:27:24.864829: step 322000, acc = 0.9507, f1 = 0.9502
[Status] 2017-05-10 00:27:24.864910: step 322000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:27:29.387315: step 322020, loss = 0.0586, acc = 0.9880 (275.3 examples/sec; 0.232 sec/batch)
2017-05-10 00:27:33.881680: step 322040, loss = 0.0573, acc = 0.9900 (278.7 examples/sec; 0.230 sec/batch)
2017-05-10 00:27:38.710644: step 322060, loss = 0.0870, acc = 0.9780 (268.0 examples/sec; 0.239 sec/batch)
2017-05-10 00:27:43.493188: step 322080, loss = 0.1020, acc = 0.9900 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 00:27:48.072913: step 322100, loss = 0.0640, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 00:27:52.642699: step 322120, loss = 0.0771, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-10 00:27:57.353671: step 322140, loss = 0.0888, acc = 0.9720 (269.8 examples/sec; 0.237 sec/batch)
2017-05-10 00:28:01.959919: step 322160, loss = 0.0701, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-10 00:28:06.695054: step 322180, loss = 0.0724, acc = 0.9820 (263.0 examples/sec; 0.243 sec/batch)
2017-05-10 00:28:11.289482: step 322200, loss = 0.0905, acc = 0.9760 (264.0 examples/sec; 0.242 sec/batch)
2017-05-10 00:28:16.026936: step 322220, loss = 0.0832, acc = 0.9760 (280.5 examples/sec; 0.228 sec/batch)
2017-05-10 00:28:20.671449: step 322240, loss = 0.0797, acc = 0.9820 (250.4 examples/sec; 0.256 sec/batch)
2017-05-10 00:28:25.287265: step 322260, loss = 0.0769, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-10 00:28:29.938131: step 322280, loss = 0.0749, acc = 0.9780 (273.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:28:34.460806: step 322300, loss = 0.0669, acc = 0.9840 (274.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:28:39.266780: step 322320, loss = 0.0698, acc = 0.9820 (280.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:28:43.843670: step 322340, loss = 0.0802, acc = 0.9740 (270.2 examples/sec; 0.237 sec/batch)
2017-05-10 00:28:48.527891: step 322360, loss = 0.0598, acc = 0.9860 (265.5 examples/sec; 0.241 sec/batch)
2017-05-10 00:28:53.309212: step 322380, loss = 0.0808, acc = 0.9840 (293.6 examples/sec; 0.218 sec/batch)
2017-05-10 00:28:57.942634: step 322400, loss = 0.0873, acc = 0.9720 (279.8 examples/sec; 0.229 sec/batch)
2017-05-10 00:29:02.548370: step 322420, loss = 0.0759, acc = 0.9780 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 00:29:07.098532: step 322440, loss = 0.0653, acc = 0.9880 (289.8 examples/sec; 0.221 sec/batch)
2017-05-10 00:29:11.639051: step 322460, loss = 0.0731, acc = 0.9820 (285.4 examples/sec; 0.224 sec/batch)
2017-05-10 00:29:16.268450: step 322480, loss = 0.0683, acc = 0.9800 (276.7 examples/sec; 0.231 sec/batch)
2017-05-10 00:29:21.083706: step 322500, loss = 0.0669, acc = 0.9860 (252.2 examples/sec; 0.254 sec/batch)
2017-05-10 00:29:25.634934: step 322520, loss = 0.0607, acc = 0.9860 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 00:29:30.159834: step 322540, loss = 0.0817, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-10 00:29:35.603514: step 322560, loss = 0.0670, acc = 0.9880 (236.4 examples/sec; 0.271 sec/batch)
2017-05-10 00:29:40.310084: step 322580, loss = 0.0626, acc = 0.9900 (277.6 examples/sec; 0.231 sec/batch)
2017-05-10 00:29:44.979074: step 322600, loss = 0.0619, acc = 0.9880 (289.8 examples/sec; 0.221 sec/batch)
2017-05-10 00:29:49.537372: step 322620, loss = 0.0740, acc = 0.9840 (264.6 examples/sec; 0.242 sec/batch)
2017-05-10 00:29:54.223360: step 322640, loss = 0.0619, acc = 0.9940 (295.6 examples/sec; 0.216 sec/batch)
2017-05-10 00:29:58.694762: step 322660, loss = 0.0865, acc = 0.9780 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:30:03.272571: step 322680, loss = 0.0824, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-10 00:30:08.284177: step 322700, loss = 0.0990, acc = 0.9780 (275.2 examples/sec; 0.233 sec/batch)
2017-05-10 00:30:12.848317: step 322720, loss = 0.0695, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-10 00:30:17.551654: step 322740, loss = 0.0656, acc = 0.9800 (283.5 examples/sec; 0.226 sec/batch)
2017-05-10 00:30:22.205227: step 322760, loss = 0.0613, acc = 0.9880 (287.3 examples/sec; 0.223 sec/batch)
2017-05-10 00:30:26.744419: step 322780, loss = 0.0805, acc = 0.9700 (264.8 examples/sec; 0.242 sec/batch)
2017-05-10 00:30:31.410021: step 322800, loss = 0.0698, acc = 0.9900 (278.1 examples/sec; 0.230 sec/batch)
2017-05-10 00:30:36.101583: step 322820, loss = 0.0700, acc = 0.9820 (262.0 examples/sec; 0.244 sec/batch)
2017-05-10 00:30:40.677877: step 322840, loss = 0.0673, acc = 0.9820 (287.3 examples/sec; 0.223 sec/batch)
2017-05-10 00:30:45.204482: step 322860, loss = 0.0787, acc = 0.9820 (264.5 examples/sec; 0.242 sec/batch)
2017-05-10 00:30:49.933965: step 322880, loss = 0.0811, acc = 0.9780 (233.3 examples/sec; 0.274 sec/batch)
2017-05-10 00:30:54.474788: step 322900, loss = 0.0926, acc = 0.9700 (275.7 examples/sec; 0.232 sec/batch)
2017-05-10 00:30:59.010286: step 322920, loss = 0.0711, acc = 0.9880 (286.0 examples/sec; 0.224 sec/batch)
2017-05-10 00:31:03.695745: step 322940, loss = 0.0820, acc = 0.9820 (279.1 examples/sec; 0.229 sec/batch)
2017-05-10 00:31:08.465912: step 322960, loss = 0.0840, acc = 0.9720 (278.7 examples/sec; 0.230 sec/batch)
2017-05-10 00:31:13.041274: step 322980, loss = 0.0613, acc = 0.9900 (266.7 examples/sec; 0.240 sec/batch)
2017-05-10 00:31:17.600630: step 323000, loss = 0.0701, acc = 0.9840 (288.7 examples/sec; 0.222 sec/batch)
[Eval] 2017-05-10 00:31:31.667185: step 323000, acc = 0.9637, f1 = 0.9626
[Test] 2017-05-10 00:31:41.436836: step 323000, acc = 0.9550, f1 = 0.9547
[Status] 2017-05-10 00:31:41.436931: step 323000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:31:46.053623: step 323020, loss = 0.0892, acc = 0.9800 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 00:31:50.911604: step 323040, loss = 0.0848, acc = 0.9760 (275.6 examples/sec; 0.232 sec/batch)
2017-05-10 00:31:55.480249: step 323060, loss = 0.0796, acc = 0.9800 (285.5 examples/sec; 0.224 sec/batch)
2017-05-10 00:32:00.153942: step 323080, loss = 0.0725, acc = 0.9860 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 00:32:05.081637: step 323100, loss = 0.0662, acc = 0.9860 (267.0 examples/sec; 0.240 sec/batch)
2017-05-10 00:32:09.640245: step 323120, loss = 0.0722, acc = 0.9780 (285.8 examples/sec; 0.224 sec/batch)
2017-05-10 00:32:14.253882: step 323140, loss = 0.0778, acc = 0.9760 (282.3 examples/sec; 0.227 sec/batch)
2017-05-10 00:32:19.090723: step 323160, loss = 0.0705, acc = 0.9820 (272.8 examples/sec; 0.235 sec/batch)
2017-05-10 00:32:23.571374: step 323180, loss = 0.0864, acc = 0.9800 (279.2 examples/sec; 0.229 sec/batch)
2017-05-10 00:32:28.139211: step 323200, loss = 0.0798, acc = 0.9820 (273.8 examples/sec; 0.234 sec/batch)
2017-05-10 00:32:32.928718: step 323220, loss = 0.0761, acc = 0.9780 (226.6 examples/sec; 0.282 sec/batch)
2017-05-10 00:32:37.550621: step 323240, loss = 0.0640, acc = 0.9860 (290.9 examples/sec; 0.220 sec/batch)
2017-05-10 00:32:42.219526: step 323260, loss = 0.0681, acc = 0.9820 (273.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:32:46.799696: step 323280, loss = 0.0714, acc = 0.9860 (283.9 examples/sec; 0.225 sec/batch)
2017-05-10 00:32:51.722452: step 323300, loss = 0.0669, acc = 0.9900 (266.5 examples/sec; 0.240 sec/batch)
2017-05-10 00:32:56.187961: step 323320, loss = 0.0786, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-10 00:33:00.718627: step 323340, loss = 0.0743, acc = 0.9820 (286.8 examples/sec; 0.223 sec/batch)
2017-05-10 00:33:05.494590: step 323360, loss = 0.0871, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-10 00:33:10.105067: step 323380, loss = 0.0790, acc = 0.9840 (261.9 examples/sec; 0.244 sec/batch)
2017-05-10 00:33:14.626363: step 323400, loss = 0.0794, acc = 0.9760 (287.6 examples/sec; 0.223 sec/batch)
2017-05-10 00:33:19.212127: step 323420, loss = 0.0691, acc = 0.9820 (283.2 examples/sec; 0.226 sec/batch)
2017-05-10 00:33:23.700562: step 323440, loss = 0.0751, acc = 0.9780 (293.8 examples/sec; 0.218 sec/batch)
2017-05-10 00:33:28.320298: step 323460, loss = 0.0777, acc = 0.9800 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 00:33:33.213589: step 323480, loss = 0.0718, acc = 0.9860 (267.1 examples/sec; 0.240 sec/batch)
2017-05-10 00:33:37.768680: step 323500, loss = 0.0772, acc = 0.9820 (272.8 examples/sec; 0.235 sec/batch)
2017-05-10 00:33:42.594182: step 323520, loss = 0.0871, acc = 0.9760 (285.5 examples/sec; 0.224 sec/batch)
2017-05-10 00:33:47.358774: step 323540, loss = 0.0669, acc = 0.9860 (218.3 examples/sec; 0.293 sec/batch)
2017-05-10 00:33:51.884688: step 323560, loss = 0.0829, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-10 00:33:57.487815: step 323580, loss = 0.0805, acc = 0.9820 (265.8 examples/sec; 0.241 sec/batch)
2017-05-10 00:34:02.178627: step 323600, loss = 0.1035, acc = 0.9780 (292.3 examples/sec; 0.219 sec/batch)
2017-05-10 00:34:06.795740: step 323620, loss = 0.0781, acc = 0.9860 (274.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:34:11.386483: step 323640, loss = 0.0600, acc = 0.9920 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 00:34:15.932666: step 323660, loss = 0.0703, acc = 0.9820 (293.8 examples/sec; 0.218 sec/batch)
2017-05-10 00:34:20.526761: step 323680, loss = 0.0791, acc = 0.9780 (290.5 examples/sec; 0.220 sec/batch)
2017-05-10 00:34:25.137071: step 323700, loss = 0.0974, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:34:29.703682: step 323720, loss = 0.0914, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-10 00:34:34.323383: step 323740, loss = 0.0660, acc = 0.9840 (281.7 examples/sec; 0.227 sec/batch)
2017-05-10 00:34:38.822708: step 323760, loss = 0.0633, acc = 0.9860 (285.7 examples/sec; 0.224 sec/batch)
2017-05-10 00:34:43.373514: step 323780, loss = 0.0768, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-10 00:34:48.151061: step 323800, loss = 0.0685, acc = 0.9900 (279.2 examples/sec; 0.229 sec/batch)
2017-05-10 00:34:52.716218: step 323820, loss = 0.0895, acc = 0.9820 (282.3 examples/sec; 0.227 sec/batch)
2017-05-10 00:34:57.690041: step 323840, loss = 0.0749, acc = 0.9860 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 00:35:02.378004: step 323860, loss = 0.0916, acc = 0.9700 (293.8 examples/sec; 0.218 sec/batch)
2017-05-10 00:35:06.945811: step 323880, loss = 0.0692, acc = 0.9840 (282.0 examples/sec; 0.227 sec/batch)
2017-05-10 00:35:11.502785: step 323900, loss = 0.0813, acc = 0.9760 (289.5 examples/sec; 0.221 sec/batch)
2017-05-10 00:35:16.240061: step 323920, loss = 0.0686, acc = 0.9860 (247.7 examples/sec; 0.258 sec/batch)
2017-05-10 00:35:20.798855: step 323940, loss = 0.0753, acc = 0.9820 (283.6 examples/sec; 0.226 sec/batch)
2017-05-10 00:35:25.459521: step 323960, loss = 0.0597, acc = 0.9880 (276.4 examples/sec; 0.232 sec/batch)
2017-05-10 00:35:29.980854: step 323980, loss = 0.0889, acc = 0.9720 (284.5 examples/sec; 0.225 sec/batch)
2017-05-10 00:35:34.731496: step 324000, loss = 0.0786, acc = 0.9760 (285.7 examples/sec; 0.224 sec/batch)
[Eval] 2017-05-10 00:35:48.709356: step 324000, acc = 0.9636, f1 = 0.9624
[Test] 2017-05-10 00:35:58.085653: step 324000, acc = 0.9544, f1 = 0.9541
[Status] 2017-05-10 00:35:58.085734: step 324000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:36:02.828265: step 324020, loss = 0.0766, acc = 0.9820 (246.3 examples/sec; 0.260 sec/batch)
2017-05-10 00:36:07.304353: step 324040, loss = 0.0674, acc = 0.9820 (277.6 examples/sec; 0.231 sec/batch)
2017-05-10 00:36:11.969880: step 324060, loss = 0.0910, acc = 0.9780 (288.4 examples/sec; 0.222 sec/batch)
2017-05-10 00:36:16.631098: step 324080, loss = 0.0928, acc = 0.9740 (284.8 examples/sec; 0.225 sec/batch)
2017-05-10 00:36:21.196048: step 324100, loss = 0.0680, acc = 0.9880 (288.3 examples/sec; 0.222 sec/batch)
2017-05-10 00:36:25.718361: step 324120, loss = 0.0837, acc = 0.9860 (278.7 examples/sec; 0.230 sec/batch)
2017-05-10 00:36:30.359726: step 324140, loss = 0.0966, acc = 0.9760 (274.2 examples/sec; 0.233 sec/batch)
2017-05-10 00:36:35.123364: step 324160, loss = 0.0537, acc = 0.9900 (277.7 examples/sec; 0.230 sec/batch)
2017-05-10 00:36:39.760635: step 324180, loss = 0.0734, acc = 0.9840 (265.7 examples/sec; 0.241 sec/batch)
2017-05-10 00:36:44.371940: step 324200, loss = 0.0842, acc = 0.9860 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 00:36:49.081167: step 324220, loss = 0.0707, acc = 0.9780 (278.0 examples/sec; 0.230 sec/batch)
2017-05-10 00:36:54.039830: step 324240, loss = 0.0877, acc = 0.9780 (285.9 examples/sec; 0.224 sec/batch)
2017-05-10 00:36:58.587609: step 324260, loss = 0.0844, acc = 0.9800 (288.0 examples/sec; 0.222 sec/batch)
2017-05-10 00:37:03.355895: step 324280, loss = 0.0708, acc = 0.9840 (248.5 examples/sec; 0.258 sec/batch)
2017-05-10 00:37:07.910262: step 324300, loss = 0.0728, acc = 0.9840 (273.8 examples/sec; 0.234 sec/batch)
2017-05-10 00:37:12.672600: step 324320, loss = 0.1037, acc = 0.9780 (254.5 examples/sec; 0.251 sec/batch)
2017-05-10 00:37:17.289674: step 324340, loss = 0.1110, acc = 0.9660 (269.4 examples/sec; 0.238 sec/batch)
2017-05-10 00:37:22.037648: step 324360, loss = 0.0870, acc = 0.9760 (293.7 examples/sec; 0.218 sec/batch)
2017-05-10 00:37:26.704147: step 324380, loss = 0.0705, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 00:37:31.547879: step 324400, loss = 0.0584, acc = 0.9940 (218.4 examples/sec; 0.293 sec/batch)
2017-05-10 00:37:36.358123: step 324420, loss = 0.0648, acc = 0.9840 (279.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:37:40.863794: step 324440, loss = 0.0663, acc = 0.9880 (272.3 examples/sec; 0.235 sec/batch)
2017-05-10 00:37:45.421266: step 324460, loss = 0.0650, acc = 0.9900 (279.3 examples/sec; 0.229 sec/batch)
2017-05-10 00:37:50.046416: step 324480, loss = 0.0872, acc = 0.9840 (287.1 examples/sec; 0.223 sec/batch)
2017-05-10 00:37:54.650240: step 324500, loss = 0.0791, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-10 00:37:59.244439: step 324520, loss = 0.0703, acc = 0.9860 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 00:38:04.080237: step 324540, loss = 0.0843, acc = 0.9740 (233.4 examples/sec; 0.274 sec/batch)
2017-05-10 00:38:08.644661: step 324560, loss = 0.0735, acc = 0.9820 (284.4 examples/sec; 0.225 sec/batch)
2017-05-10 00:38:13.952698: step 324580, loss = 0.0649, acc = 0.9860 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 00:38:18.520142: step 324600, loss = 0.0605, acc = 0.9900 (301.4 examples/sec; 0.212 sec/batch)
2017-05-10 00:38:23.378951: step 324620, loss = 0.0808, acc = 0.9780 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 00:38:27.985662: step 324640, loss = 0.0712, acc = 0.9840 (268.5 examples/sec; 0.238 sec/batch)
2017-05-10 00:38:32.639759: step 324660, loss = 0.0794, acc = 0.9860 (272.6 examples/sec; 0.235 sec/batch)
2017-05-10 00:38:37.305984: step 324680, loss = 0.0667, acc = 0.9840 (284.4 examples/sec; 0.225 sec/batch)
2017-05-10 00:38:41.895186: step 324700, loss = 0.0837, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-10 00:38:46.474029: step 324720, loss = 0.0765, acc = 0.9820 (272.8 examples/sec; 0.235 sec/batch)
2017-05-10 00:38:51.395486: step 324740, loss = 0.0670, acc = 0.9900 (270.9 examples/sec; 0.236 sec/batch)
2017-05-10 00:38:56.007456: step 324760, loss = 0.0670, acc = 0.9840 (272.7 examples/sec; 0.235 sec/batch)
2017-05-10 00:39:00.631287: step 324780, loss = 0.0573, acc = 0.9880 (268.1 examples/sec; 0.239 sec/batch)
2017-05-10 00:39:05.407365: step 324800, loss = 0.0567, acc = 0.9880 (291.7 examples/sec; 0.219 sec/batch)
2017-05-10 00:39:10.033001: step 324820, loss = 0.0810, acc = 0.9840 (285.1 examples/sec; 0.224 sec/batch)
2017-05-10 00:39:14.574808: step 324840, loss = 0.0632, acc = 0.9920 (283.6 examples/sec; 0.226 sec/batch)
2017-05-10 00:39:19.458029: step 324860, loss = 0.0700, acc = 0.9940 (226.8 examples/sec; 0.282 sec/batch)
2017-05-10 00:39:24.085829: step 324880, loss = 0.0961, acc = 0.9660 (285.9 examples/sec; 0.224 sec/batch)
2017-05-10 00:39:28.707501: step 324900, loss = 0.0641, acc = 0.9900 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 00:39:33.550692: step 324920, loss = 0.1043, acc = 0.9620 (241.2 examples/sec; 0.265 sec/batch)
2017-05-10 00:39:37.977197: step 324940, loss = 0.0721, acc = 0.9860 (287.7 examples/sec; 0.222 sec/batch)
2017-05-10 00:39:42.538355: step 324960, loss = 0.0912, acc = 0.9800 (282.9 examples/sec; 0.226 sec/batch)
2017-05-10 00:39:47.068423: step 324980, loss = 0.0894, acc = 0.9740 (281.8 examples/sec; 0.227 sec/batch)
2017-05-10 00:39:51.697336: step 325000, loss = 0.0874, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-10 00:40:05.790477: step 325000, acc = 0.9631, f1 = 0.9620
[Test] 2017-05-10 00:40:15.052584: step 325000, acc = 0.9542, f1 = 0.9538
[Status] 2017-05-10 00:40:15.052653: step 325000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:40:19.759558: step 325020, loss = 0.0738, acc = 0.9840 (296.3 examples/sec; 0.216 sec/batch)
2017-05-10 00:40:24.358452: step 325040, loss = 0.1003, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-10 00:40:29.120924: step 325060, loss = 0.0735, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-10 00:40:33.917633: step 325080, loss = 0.0632, acc = 0.9900 (280.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:40:38.499207: step 325100, loss = 0.0876, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-10 00:40:43.125394: step 325120, loss = 0.0830, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-10 00:40:47.829765: step 325140, loss = 0.0698, acc = 0.9860 (268.2 examples/sec; 0.239 sec/batch)
2017-05-10 00:40:52.396898: step 325160, loss = 0.0642, acc = 0.9900 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 00:40:56.994791: step 325180, loss = 0.0916, acc = 0.9800 (287.6 examples/sec; 0.223 sec/batch)
2017-05-10 00:41:01.846432: step 325200, loss = 0.0766, acc = 0.9840 (285.2 examples/sec; 0.224 sec/batch)
2017-05-10 00:41:06.377504: step 325220, loss = 0.0796, acc = 0.9860 (274.4 examples/sec; 0.233 sec/batch)
2017-05-10 00:41:10.970952: step 325240, loss = 0.0570, acc = 0.9940 (260.3 examples/sec; 0.246 sec/batch)
2017-05-10 00:41:15.714252: step 325260, loss = 0.0631, acc = 0.9940 (238.2 examples/sec; 0.269 sec/batch)
2017-05-10 00:41:20.255211: step 325280, loss = 0.0638, acc = 0.9820 (265.7 examples/sec; 0.241 sec/batch)
2017-05-10 00:41:24.899853: step 325300, loss = 0.0748, acc = 0.9760 (275.3 examples/sec; 0.232 sec/batch)
2017-05-10 00:41:29.473452: step 325320, loss = 0.0718, acc = 0.9860 (276.9 examples/sec; 0.231 sec/batch)
2017-05-10 00:41:34.358724: step 325340, loss = 0.0944, acc = 0.9780 (278.3 examples/sec; 0.230 sec/batch)
2017-05-10 00:41:38.955221: step 325360, loss = 0.0768, acc = 0.9860 (287.5 examples/sec; 0.223 sec/batch)
2017-05-10 00:41:43.627598: step 325380, loss = 0.0739, acc = 0.9780 (262.2 examples/sec; 0.244 sec/batch)
2017-05-10 00:41:48.356748: step 325400, loss = 0.0651, acc = 0.9860 (272.7 examples/sec; 0.235 sec/batch)
2017-05-10 00:41:53.077302: step 325420, loss = 0.0619, acc = 0.9940 (276.8 examples/sec; 0.231 sec/batch)
2017-05-10 00:41:57.762408: step 325440, loss = 0.0676, acc = 0.9840 (269.8 examples/sec; 0.237 sec/batch)
2017-05-10 00:42:02.627945: step 325460, loss = 0.0734, acc = 0.9880 (224.8 examples/sec; 0.285 sec/batch)
2017-05-10 00:42:07.542544: step 325480, loss = 0.0756, acc = 0.9840 (271.4 examples/sec; 0.236 sec/batch)
2017-05-10 00:42:12.142060: step 325500, loss = 0.0716, acc = 0.9840 (275.7 examples/sec; 0.232 sec/batch)
2017-05-10 00:42:16.989847: step 325520, loss = 0.0794, acc = 0.9800 (220.1 examples/sec; 0.291 sec/batch)
2017-05-10 00:42:21.647473: step 325540, loss = 0.0850, acc = 0.9740 (265.2 examples/sec; 0.241 sec/batch)
2017-05-10 00:42:26.283728: step 325560, loss = 0.0779, acc = 0.9820 (293.3 examples/sec; 0.218 sec/batch)
2017-05-10 00:42:32.392441: step 325580, loss = 0.0596, acc = 0.9880 (108.3 examples/sec; 0.591 sec/batch)
2017-05-10 00:42:36.891048: step 325600, loss = 0.0664, acc = 0.9880 (279.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:42:41.455203: step 325620, loss = 0.0606, acc = 0.9860 (281.3 examples/sec; 0.228 sec/batch)
2017-05-10 00:42:46.155232: step 325640, loss = 0.0619, acc = 0.9900 (274.1 examples/sec; 0.234 sec/batch)
2017-05-10 00:42:50.780822: step 325660, loss = 0.0680, acc = 0.9840 (265.2 examples/sec; 0.241 sec/batch)
2017-05-10 00:42:55.358667: step 325680, loss = 0.0717, acc = 0.9780 (269.8 examples/sec; 0.237 sec/batch)
2017-05-10 00:42:59.957646: step 325700, loss = 0.0760, acc = 0.9860 (270.5 examples/sec; 0.237 sec/batch)
2017-05-10 00:43:04.775249: step 325720, loss = 0.0710, acc = 0.9880 (290.1 examples/sec; 0.221 sec/batch)
2017-05-10 00:43:09.403904: step 325740, loss = 0.0833, acc = 0.9800 (292.0 examples/sec; 0.219 sec/batch)
2017-05-10 00:43:13.963145: step 325760, loss = 0.0786, acc = 0.9840 (270.2 examples/sec; 0.237 sec/batch)
2017-05-10 00:43:18.779928: step 325780, loss = 0.0608, acc = 0.9900 (275.9 examples/sec; 0.232 sec/batch)
2017-05-10 00:43:23.455736: step 325800, loss = 0.0548, acc = 0.9880 (286.9 examples/sec; 0.223 sec/batch)
2017-05-10 00:43:28.028212: step 325820, loss = 0.0793, acc = 0.9820 (280.1 examples/sec; 0.229 sec/batch)
2017-05-10 00:43:32.724700: step 325840, loss = 0.0909, acc = 0.9740 (291.8 examples/sec; 0.219 sec/batch)
2017-05-10 00:43:37.260317: step 325860, loss = 0.0860, acc = 0.9820 (270.3 examples/sec; 0.237 sec/batch)
2017-05-10 00:43:41.935637: step 325880, loss = 0.0807, acc = 0.9860 (277.3 examples/sec; 0.231 sec/batch)
2017-05-10 00:43:46.938208: step 325900, loss = 0.0553, acc = 0.9940 (219.9 examples/sec; 0.291 sec/batch)
2017-05-10 00:43:51.451327: step 325920, loss = 0.0650, acc = 0.9860 (288.2 examples/sec; 0.222 sec/batch)
2017-05-10 00:43:55.991716: step 325940, loss = 0.0814, acc = 0.9840 (286.3 examples/sec; 0.224 sec/batch)
2017-05-10 00:44:00.525234: step 325960, loss = 0.0612, acc = 0.9900 (277.5 examples/sec; 0.231 sec/batch)
2017-05-10 00:44:05.212648: step 325980, loss = 0.0726, acc = 0.9840 (279.6 examples/sec; 0.229 sec/batch)
2017-05-10 00:44:09.900738: step 326000, loss = 0.0641, acc = 0.9880 (263.0 examples/sec; 0.243 sec/batch)
[Eval] 2017-05-10 00:44:24.050788: step 326000, acc = 0.9633, f1 = 0.9621
[Test] 2017-05-10 00:44:33.699037: step 326000, acc = 0.9549, f1 = 0.9545
[Status] 2017-05-10 00:44:33.699118: step 326000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:44:38.307363: step 326020, loss = 0.0688, acc = 0.9840 (287.6 examples/sec; 0.223 sec/batch)
2017-05-10 00:44:42.896400: step 326040, loss = 0.0724, acc = 0.9840 (267.6 examples/sec; 0.239 sec/batch)
2017-05-10 00:44:47.682768: step 326060, loss = 0.0675, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-10 00:44:52.616755: step 326080, loss = 0.0775, acc = 0.9800 (186.2 examples/sec; 0.344 sec/batch)
2017-05-10 00:44:57.327329: step 326100, loss = 0.0755, acc = 0.9820 (291.6 examples/sec; 0.219 sec/batch)
2017-05-10 00:45:02.090578: step 326120, loss = 0.0539, acc = 0.9900 (271.2 examples/sec; 0.236 sec/batch)
2017-05-10 00:45:06.611604: step 326140, loss = 0.0820, acc = 0.9740 (285.6 examples/sec; 0.224 sec/batch)
2017-05-10 00:45:11.143282: step 326160, loss = 0.0698, acc = 0.9840 (281.0 examples/sec; 0.228 sec/batch)
2017-05-10 00:45:15.850271: step 326180, loss = 0.0594, acc = 0.9880 (250.3 examples/sec; 0.256 sec/batch)
2017-05-10 00:45:20.451594: step 326200, loss = 0.0739, acc = 0.9840 (257.9 examples/sec; 0.248 sec/batch)
2017-05-10 00:45:25.225861: step 326220, loss = 0.0838, acc = 0.9800 (274.6 examples/sec; 0.233 sec/batch)
2017-05-10 00:45:29.851118: step 326240, loss = 0.0683, acc = 0.9880 (279.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:45:34.572174: step 326260, loss = 0.0669, acc = 0.9880 (265.9 examples/sec; 0.241 sec/batch)
2017-05-10 00:45:39.061591: step 326280, loss = 0.0651, acc = 0.9860 (293.2 examples/sec; 0.218 sec/batch)
2017-05-10 00:45:43.657133: step 326300, loss = 0.0939, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 00:45:48.504079: step 326320, loss = 0.0628, acc = 0.9860 (288.4 examples/sec; 0.222 sec/batch)
2017-05-10 00:45:53.214855: step 326340, loss = 0.0701, acc = 0.9780 (277.8 examples/sec; 0.230 sec/batch)
2017-05-10 00:45:57.849851: step 326360, loss = 0.0880, acc = 0.9800 (283.9 examples/sec; 0.225 sec/batch)
2017-05-10 00:46:02.634987: step 326380, loss = 0.0837, acc = 0.9780 (297.7 examples/sec; 0.215 sec/batch)
2017-05-10 00:46:07.146361: step 326400, loss = 0.0619, acc = 0.9880 (280.9 examples/sec; 0.228 sec/batch)
2017-05-10 00:46:12.688074: step 326420, loss = 0.0800, acc = 0.9800 (270.5 examples/sec; 0.237 sec/batch)
2017-05-10 00:46:17.434656: step 326440, loss = 0.0847, acc = 0.9780 (239.9 examples/sec; 0.267 sec/batch)
2017-05-10 00:46:21.969030: step 326460, loss = 0.0673, acc = 0.9920 (283.4 examples/sec; 0.226 sec/batch)
2017-05-10 00:46:26.649699: step 326480, loss = 0.0627, acc = 0.9880 (275.9 examples/sec; 0.232 sec/batch)
2017-05-10 00:46:31.195707: step 326500, loss = 0.0757, acc = 0.9820 (285.5 examples/sec; 0.224 sec/batch)
2017-05-10 00:46:35.902494: step 326520, loss = 0.0808, acc = 0.9800 (278.9 examples/sec; 0.229 sec/batch)
2017-05-10 00:46:40.542483: step 326540, loss = 0.0611, acc = 0.9880 (270.4 examples/sec; 0.237 sec/batch)
2017-05-10 00:46:45.143275: step 326560, loss = 0.0616, acc = 0.9900 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 00:46:49.901264: step 326580, loss = 0.0827, acc = 0.9760 (272.6 examples/sec; 0.235 sec/batch)
2017-05-10 00:46:55.150901: step 326600, loss = 0.0727, acc = 0.9820 (279.7 examples/sec; 0.229 sec/batch)
2017-05-10 00:46:59.730592: step 326620, loss = 0.0568, acc = 0.9940 (276.3 examples/sec; 0.232 sec/batch)
2017-05-10 00:47:04.598891: step 326640, loss = 0.0788, acc = 0.9840 (272.5 examples/sec; 0.235 sec/batch)
2017-05-10 00:47:09.185432: step 326660, loss = 0.0871, acc = 0.9720 (287.9 examples/sec; 0.222 sec/batch)
2017-05-10 00:47:13.803971: step 326680, loss = 0.0800, acc = 0.9760 (271.4 examples/sec; 0.236 sec/batch)
2017-05-10 00:47:18.454588: step 326700, loss = 0.0670, acc = 0.9900 (290.3 examples/sec; 0.220 sec/batch)
2017-05-10 00:47:23.031228: step 326720, loss = 0.0963, acc = 0.9720 (290.8 examples/sec; 0.220 sec/batch)
2017-05-10 00:47:27.620180: step 326740, loss = 0.0895, acc = 0.9800 (279.9 examples/sec; 0.229 sec/batch)
2017-05-10 00:47:32.569705: step 326760, loss = 0.0679, acc = 0.9800 (219.8 examples/sec; 0.291 sec/batch)
2017-05-10 00:47:37.124035: step 326780, loss = 0.0639, acc = 0.9860 (266.6 examples/sec; 0.240 sec/batch)
2017-05-10 00:47:41.728558: step 326800, loss = 0.0521, acc = 0.9980 (267.9 examples/sec; 0.239 sec/batch)
2017-05-10 00:47:46.492905: step 326820, loss = 0.1073, acc = 0.9660 (287.0 examples/sec; 0.223 sec/batch)
2017-05-10 00:47:51.018603: step 326840, loss = 0.0704, acc = 0.9860 (290.3 examples/sec; 0.220 sec/batch)
2017-05-10 00:47:55.534728: step 326860, loss = 0.0824, acc = 0.9820 (288.0 examples/sec; 0.222 sec/batch)
2017-05-10 00:48:00.353022: step 326880, loss = 0.0625, acc = 0.9920 (236.3 examples/sec; 0.271 sec/batch)
2017-05-10 00:48:04.983647: step 326900, loss = 0.0773, acc = 0.9780 (275.0 examples/sec; 0.233 sec/batch)
2017-05-10 00:48:09.628912: step 326920, loss = 0.0879, acc = 0.9740 (290.9 examples/sec; 0.220 sec/batch)
2017-05-10 00:48:14.187164: step 326940, loss = 0.0763, acc = 0.9840 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 00:48:19.008666: step 326960, loss = 0.0717, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:48:23.676464: step 326980, loss = 0.0755, acc = 0.9860 (278.7 examples/sec; 0.230 sec/batch)
2017-05-10 00:48:28.165702: step 327000, loss = 0.0828, acc = 0.9780 (284.9 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-10 00:48:42.103554: step 327000, acc = 0.9637, f1 = 0.9625
[Test] 2017-05-10 00:48:51.796252: step 327000, acc = 0.9551, f1 = 0.9547
[Status] 2017-05-10 00:48:51.796345: step 327000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:48:56.530382: step 327020, loss = 0.0945, acc = 0.9760 (264.9 examples/sec; 0.242 sec/batch)
2017-05-10 00:49:01.252793: step 327040, loss = 0.0724, acc = 0.9840 (280.5 examples/sec; 0.228 sec/batch)
2017-05-10 00:49:05.858595: step 327060, loss = 0.1029, acc = 0.9740 (278.1 examples/sec; 0.230 sec/batch)
2017-05-10 00:49:10.446628: step 327080, loss = 0.0773, acc = 0.9800 (269.9 examples/sec; 0.237 sec/batch)
2017-05-10 00:49:15.212955: step 327100, loss = 0.0628, acc = 0.9880 (288.5 examples/sec; 0.222 sec/batch)
2017-05-10 00:49:19.902811: step 327120, loss = 0.0973, acc = 0.9720 (280.8 examples/sec; 0.228 sec/batch)
2017-05-10 00:49:24.461463: step 327140, loss = 0.0689, acc = 0.9820 (289.3 examples/sec; 0.221 sec/batch)
2017-05-10 00:49:29.148081: step 327160, loss = 0.0724, acc = 0.9860 (272.2 examples/sec; 0.235 sec/batch)
2017-05-10 00:49:33.613700: step 327180, loss = 0.0715, acc = 0.9840 (291.3 examples/sec; 0.220 sec/batch)
2017-05-10 00:49:38.171362: step 327200, loss = 0.0870, acc = 0.9720 (289.5 examples/sec; 0.221 sec/batch)
2017-05-10 00:49:42.816797: step 327220, loss = 0.0687, acc = 0.9880 (253.2 examples/sec; 0.253 sec/batch)
2017-05-10 00:49:47.459272: step 327240, loss = 0.0796, acc = 0.9760 (283.0 examples/sec; 0.226 sec/batch)
2017-05-10 00:49:52.031425: step 327260, loss = 0.0831, acc = 0.9820 (279.4 examples/sec; 0.229 sec/batch)
2017-05-10 00:49:56.560250: step 327280, loss = 0.0743, acc = 0.9780 (292.4 examples/sec; 0.219 sec/batch)
2017-05-10 00:50:01.279145: step 327300, loss = 0.0633, acc = 0.9840 (271.9 examples/sec; 0.235 sec/batch)
2017-05-10 00:50:06.603319: step 327320, loss = 0.0968, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-10 00:50:11.107082: step 327340, loss = 0.0628, acc = 0.9940 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 00:50:15.856309: step 327360, loss = 0.0788, acc = 0.9740 (274.5 examples/sec; 0.233 sec/batch)
2017-05-10 00:50:20.521434: step 327380, loss = 0.0822, acc = 0.9820 (279.9 examples/sec; 0.229 sec/batch)
2017-05-10 00:50:25.137174: step 327400, loss = 0.0615, acc = 0.9880 (269.9 examples/sec; 0.237 sec/batch)
2017-05-10 00:50:29.833282: step 327420, loss = 0.0723, acc = 0.9860 (233.5 examples/sec; 0.274 sec/batch)
2017-05-10 00:50:34.366633: step 327440, loss = 0.0519, acc = 0.9980 (278.4 examples/sec; 0.230 sec/batch)
2017-05-10 00:50:38.891523: step 327460, loss = 0.0836, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 00:50:43.551411: step 327480, loss = 0.0680, acc = 0.9840 (247.1 examples/sec; 0.259 sec/batch)
2017-05-10 00:50:48.157017: step 327500, loss = 0.0674, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-10 00:50:52.717999: step 327520, loss = 0.0518, acc = 0.9940 (287.0 examples/sec; 0.223 sec/batch)
2017-05-10 00:50:57.229346: step 327540, loss = 0.0734, acc = 0.9900 (300.3 examples/sec; 0.213 sec/batch)
2017-05-10 00:51:01.893682: step 327560, loss = 0.0920, acc = 0.9780 (287.8 examples/sec; 0.222 sec/batch)
2017-05-10 00:51:06.429504: step 327580, loss = 0.0583, acc = 0.9920 (270.4 examples/sec; 0.237 sec/batch)
2017-05-10 00:51:12.008508: step 327600, loss = 0.0460, acc = 0.9980 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 00:51:16.817140: step 327620, loss = 0.0757, acc = 0.9800 (257.7 examples/sec; 0.248 sec/batch)
2017-05-10 00:51:21.520686: step 327640, loss = 0.0704, acc = 0.9820 (261.6 examples/sec; 0.245 sec/batch)
2017-05-10 00:51:26.178430: step 327660, loss = 0.0586, acc = 0.9880 (268.6 examples/sec; 0.238 sec/batch)
2017-05-10 00:51:30.970592: step 327680, loss = 0.0644, acc = 0.9840 (264.9 examples/sec; 0.242 sec/batch)
2017-05-10 00:51:35.537475: step 327700, loss = 0.0640, acc = 0.9820 (276.2 examples/sec; 0.232 sec/batch)
2017-05-10 00:51:40.118900: step 327720, loss = 0.0840, acc = 0.9840 (294.4 examples/sec; 0.217 sec/batch)
2017-05-10 00:51:44.826180: step 327740, loss = 0.0811, acc = 0.9820 (278.4 examples/sec; 0.230 sec/batch)
2017-05-10 00:51:49.447602: step 327760, loss = 0.0701, acc = 0.9820 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 00:51:53.994148: step 327780, loss = 0.0773, acc = 0.9860 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 00:51:58.671590: step 327800, loss = 0.0708, acc = 0.9860 (306.6 examples/sec; 0.209 sec/batch)
2017-05-10 00:52:03.290920: step 327820, loss = 0.0670, acc = 0.9880 (269.6 examples/sec; 0.237 sec/batch)
2017-05-10 00:52:07.831252: step 327840, loss = 0.1008, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 00:52:12.367524: step 327860, loss = 0.0719, acc = 0.9860 (292.9 examples/sec; 0.219 sec/batch)
2017-05-10 00:52:16.984163: step 327880, loss = 0.0656, acc = 0.9840 (292.6 examples/sec; 0.219 sec/batch)
2017-05-10 00:52:21.523082: step 327900, loss = 0.0797, acc = 0.9840 (296.3 examples/sec; 0.216 sec/batch)
2017-05-10 00:52:26.038611: step 327920, loss = 0.0723, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-10 00:52:30.719109: step 327940, loss = 0.0797, acc = 0.9860 (278.3 examples/sec; 0.230 sec/batch)
2017-05-10 00:52:35.177725: step 327960, loss = 0.0778, acc = 0.9860 (286.9 examples/sec; 0.223 sec/batch)
2017-05-10 00:52:39.874444: step 327980, loss = 0.0702, acc = 0.9820 (284.3 examples/sec; 0.225 sec/batch)
2017-05-10 00:52:44.562926: step 328000, loss = 0.0661, acc = 0.9900 (289.9 examples/sec; 0.221 sec/batch)
[Eval] 2017-05-10 00:52:58.551742: step 328000, acc = 0.9618, f1 = 0.9606
[Test] 2017-05-10 00:53:07.814697: step 328000, acc = 0.9527, f1 = 0.9523
[Status] 2017-05-10 00:53:07.814779: step 328000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:53:12.537103: step 328020, loss = 0.0800, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-10 00:53:17.295343: step 328040, loss = 0.1098, acc = 0.9700 (267.8 examples/sec; 0.239 sec/batch)
2017-05-10 00:53:21.888152: step 328060, loss = 0.0896, acc = 0.9640 (289.3 examples/sec; 0.221 sec/batch)
2017-05-10 00:53:26.742225: step 328080, loss = 0.0596, acc = 0.9860 (273.6 examples/sec; 0.234 sec/batch)
2017-05-10 00:53:31.273740: step 328100, loss = 0.0671, acc = 0.9800 (277.6 examples/sec; 0.231 sec/batch)
2017-05-10 00:53:35.920351: step 328120, loss = 0.0613, acc = 0.9860 (258.3 examples/sec; 0.248 sec/batch)
2017-05-10 00:53:40.533324: step 328140, loss = 0.0719, acc = 0.9820 (278.9 examples/sec; 0.229 sec/batch)
2017-05-10 00:53:45.143547: step 328160, loss = 0.0783, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 00:53:49.765602: step 328180, loss = 0.0860, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 00:53:54.722988: step 328200, loss = 0.0937, acc = 0.9700 (223.0 examples/sec; 0.287 sec/batch)
2017-05-10 00:53:59.404295: step 328220, loss = 0.0646, acc = 0.9900 (276.5 examples/sec; 0.231 sec/batch)
2017-05-10 00:54:03.972780: step 328240, loss = 0.0851, acc = 0.9820 (267.6 examples/sec; 0.239 sec/batch)
2017-05-10 00:54:08.636456: step 328260, loss = 0.0724, acc = 0.9860 (290.1 examples/sec; 0.221 sec/batch)
2017-05-10 00:54:13.368271: step 328280, loss = 0.0736, acc = 0.9800 (290.9 examples/sec; 0.220 sec/batch)
2017-05-10 00:54:17.906072: step 328300, loss = 0.0784, acc = 0.9800 (291.9 examples/sec; 0.219 sec/batch)
2017-05-10 00:54:22.564385: step 328320, loss = 0.0702, acc = 0.9780 (269.4 examples/sec; 0.238 sec/batch)
2017-05-10 00:54:27.198541: step 328340, loss = 0.0780, acc = 0.9780 (290.0 examples/sec; 0.221 sec/batch)
2017-05-10 00:54:31.714242: step 328360, loss = 0.0611, acc = 0.9880 (284.7 examples/sec; 0.225 sec/batch)
2017-05-10 00:54:36.347837: step 328380, loss = 0.0564, acc = 0.9900 (290.1 examples/sec; 0.221 sec/batch)
2017-05-10 00:54:40.977321: step 328400, loss = 0.0633, acc = 0.9780 (294.2 examples/sec; 0.218 sec/batch)
2017-05-10 00:54:45.435777: step 328420, loss = 0.0847, acc = 0.9820 (293.8 examples/sec; 0.218 sec/batch)
2017-05-10 00:54:50.011218: step 328440, loss = 0.0604, acc = 0.9880 (285.6 examples/sec; 0.224 sec/batch)
2017-05-10 00:54:54.689168: step 328460, loss = 0.0809, acc = 0.9760 (282.4 examples/sec; 0.227 sec/batch)
2017-05-10 00:54:59.223865: step 328480, loss = 0.0837, acc = 0.9720 (294.1 examples/sec; 0.218 sec/batch)
2017-05-10 00:55:03.809924: step 328500, loss = 0.0950, acc = 0.9780 (265.0 examples/sec; 0.241 sec/batch)
2017-05-10 00:55:08.523981: step 328520, loss = 0.0598, acc = 0.9920 (288.3 examples/sec; 0.222 sec/batch)
2017-05-10 00:55:13.148279: step 328540, loss = 0.0605, acc = 0.9900 (265.5 examples/sec; 0.241 sec/batch)
2017-05-10 00:55:17.786670: step 328560, loss = 0.0683, acc = 0.9860 (280.3 examples/sec; 0.228 sec/batch)
2017-05-10 00:55:22.510797: step 328580, loss = 0.0746, acc = 0.9820 (248.2 examples/sec; 0.258 sec/batch)
2017-05-10 00:55:26.976727: step 328600, loss = 0.0940, acc = 0.9760 (286.9 examples/sec; 0.223 sec/batch)
2017-05-10 00:55:32.204062: step 328620, loss = 0.0822, acc = 0.9820 (287.1 examples/sec; 0.223 sec/batch)
2017-05-10 00:55:36.855153: step 328640, loss = 0.0919, acc = 0.9780 (272.2 examples/sec; 0.235 sec/batch)
2017-05-10 00:55:41.448513: step 328660, loss = 0.0784, acc = 0.9780 (277.5 examples/sec; 0.231 sec/batch)
2017-05-10 00:55:46.114002: step 328680, loss = 0.0729, acc = 0.9880 (274.1 examples/sec; 0.233 sec/batch)
2017-05-10 00:55:50.684402: step 328700, loss = 0.0727, acc = 0.9860 (275.2 examples/sec; 0.233 sec/batch)
2017-05-10 00:55:55.388754: step 328720, loss = 0.0710, acc = 0.9920 (267.8 examples/sec; 0.239 sec/batch)
2017-05-10 00:55:59.998822: step 328740, loss = 0.0776, acc = 0.9840 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 00:56:04.591811: step 328760, loss = 0.0902, acc = 0.9800 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 00:56:09.363363: step 328780, loss = 0.0995, acc = 0.9620 (275.4 examples/sec; 0.232 sec/batch)
2017-05-10 00:56:13.924554: step 328800, loss = 0.0694, acc = 0.9800 (283.2 examples/sec; 0.226 sec/batch)
2017-05-10 00:56:18.553908: step 328820, loss = 0.0571, acc = 0.9900 (271.0 examples/sec; 0.236 sec/batch)
2017-05-10 00:56:23.196086: step 328840, loss = 0.0665, acc = 0.9860 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 00:56:27.902226: step 328860, loss = 0.1012, acc = 0.9680 (275.4 examples/sec; 0.232 sec/batch)
2017-05-10 00:56:32.484731: step 328880, loss = 0.0829, acc = 0.9740 (266.2 examples/sec; 0.240 sec/batch)
2017-05-10 00:56:37.166511: step 328900, loss = 0.0662, acc = 0.9860 (240.8 examples/sec; 0.266 sec/batch)
2017-05-10 00:56:41.661711: step 328920, loss = 0.0842, acc = 0.9800 (287.2 examples/sec; 0.223 sec/batch)
2017-05-10 00:56:46.171948: step 328940, loss = 0.0672, acc = 0.9820 (295.5 examples/sec; 0.217 sec/batch)
2017-05-10 00:56:50.622359: step 328960, loss = 0.0744, acc = 0.9840 (285.5 examples/sec; 0.224 sec/batch)
2017-05-10 00:56:55.341705: step 328980, loss = 0.0697, acc = 0.9800 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 00:56:59.919380: step 329000, loss = 0.0764, acc = 0.9900 (275.8 examples/sec; 0.232 sec/batch)
[Eval] 2017-05-10 00:57:13.864073: step 329000, acc = 0.9612, f1 = 0.9598
[Test] 2017-05-10 00:57:23.552131: step 329000, acc = 0.9517, f1 = 0.9513
[Status] 2017-05-10 00:57:23.552226: step 329000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 00:57:28.184579: step 329020, loss = 0.0860, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-10 00:57:32.769279: step 329040, loss = 0.0662, acc = 0.9860 (287.9 examples/sec; 0.222 sec/batch)
2017-05-10 00:57:37.399933: step 329060, loss = 0.0634, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-10 00:57:42.055150: step 329080, loss = 0.0690, acc = 0.9840 (257.9 examples/sec; 0.248 sec/batch)
2017-05-10 00:57:46.616285: step 329100, loss = 0.0796, acc = 0.9800 (285.8 examples/sec; 0.224 sec/batch)
2017-05-10 00:57:51.328996: step 329120, loss = 0.0723, acc = 0.9820 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 00:57:55.709073: step 329140, loss = 0.0841, acc = 0.9800 (287.7 examples/sec; 0.222 sec/batch)
2017-05-10 00:58:00.273626: step 329160, loss = 0.0906, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-10 00:58:05.312081: step 329180, loss = 0.0690, acc = 0.9880 (284.3 examples/sec; 0.225 sec/batch)
2017-05-10 00:58:09.874808: step 329200, loss = 0.0701, acc = 0.9840 (287.0 examples/sec; 0.223 sec/batch)
2017-05-10 00:58:14.395286: step 329220, loss = 0.0852, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-10 00:58:18.935928: step 329240, loss = 0.0783, acc = 0.9800 (297.4 examples/sec; 0.215 sec/batch)
2017-05-10 00:58:23.785713: step 329260, loss = 0.0796, acc = 0.9780 (258.6 examples/sec; 0.248 sec/batch)
2017-05-10 00:58:28.402654: step 329280, loss = 0.0863, acc = 0.9760 (276.0 examples/sec; 0.232 sec/batch)
2017-05-10 00:58:33.386973: step 329300, loss = 0.0680, acc = 0.9860 (284.6 examples/sec; 0.225 sec/batch)
2017-05-10 00:58:38.112705: step 329320, loss = 0.0648, acc = 0.9860 (276.5 examples/sec; 0.231 sec/batch)
2017-05-10 00:58:42.760683: step 329340, loss = 0.0666, acc = 0.9840 (281.2 examples/sec; 0.228 sec/batch)
2017-05-10 00:58:47.361276: step 329360, loss = 0.0748, acc = 0.9840 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 00:58:52.196120: step 329380, loss = 0.0534, acc = 0.9940 (241.0 examples/sec; 0.266 sec/batch)
2017-05-10 00:58:56.849011: step 329400, loss = 0.0587, acc = 0.9900 (274.7 examples/sec; 0.233 sec/batch)
2017-05-10 00:59:01.474671: step 329420, loss = 0.0844, acc = 0.9840 (281.8 examples/sec; 0.227 sec/batch)
2017-05-10 00:59:06.306318: step 329440, loss = 0.0580, acc = 0.9940 (279.4 examples/sec; 0.229 sec/batch)
2017-05-10 00:59:10.776947: step 329460, loss = 0.0919, acc = 0.9740 (273.3 examples/sec; 0.234 sec/batch)
2017-05-10 00:59:15.446084: step 329480, loss = 0.0653, acc = 0.9880 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 00:59:20.136552: step 329500, loss = 0.0810, acc = 0.9820 (248.8 examples/sec; 0.257 sec/batch)
2017-05-10 00:59:24.756903: step 329520, loss = 0.0758, acc = 0.9760 (288.2 examples/sec; 0.222 sec/batch)
2017-05-10 00:59:29.387838: step 329540, loss = 0.0752, acc = 0.9900 (267.9 examples/sec; 0.239 sec/batch)
2017-05-10 00:59:34.017080: step 329560, loss = 0.0818, acc = 0.9740 (272.5 examples/sec; 0.235 sec/batch)
2017-05-10 00:59:38.635824: step 329580, loss = 0.0823, acc = 0.9800 (278.1 examples/sec; 0.230 sec/batch)
2017-05-10 00:59:43.688409: step 329600, loss = 0.1186, acc = 0.9560 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 00:59:49.219815: step 329620, loss = 0.0998, acc = 0.9700 (293.0 examples/sec; 0.218 sec/batch)
2017-05-10 00:59:53.993195: step 329640, loss = 0.1058, acc = 0.9620 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 00:59:58.522606: step 329660, loss = 0.0706, acc = 0.9860 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 01:00:03.028585: step 329680, loss = 0.0751, acc = 0.9840 (292.3 examples/sec; 0.219 sec/batch)
2017-05-10 01:00:07.651527: step 329700, loss = 0.0653, acc = 0.9860 (280.8 examples/sec; 0.228 sec/batch)
2017-05-10 01:00:12.394831: step 329720, loss = 0.0912, acc = 0.9800 (267.0 examples/sec; 0.240 sec/batch)
2017-05-10 01:00:17.234182: step 329740, loss = 0.0744, acc = 0.9840 (234.6 examples/sec; 0.273 sec/batch)
2017-05-10 01:00:21.859528: step 329760, loss = 0.0657, acc = 0.9860 (284.5 examples/sec; 0.225 sec/batch)
2017-05-10 01:00:26.516759: step 329780, loss = 0.0808, acc = 0.9820 (276.4 examples/sec; 0.232 sec/batch)
2017-05-10 01:00:31.045157: step 329800, loss = 0.0751, acc = 0.9800 (293.9 examples/sec; 0.218 sec/batch)
2017-05-10 01:00:35.873426: step 329820, loss = 0.0719, acc = 0.9840 (271.5 examples/sec; 0.236 sec/batch)
2017-05-10 01:00:40.564874: step 329840, loss = 0.0624, acc = 0.9900 (277.8 examples/sec; 0.230 sec/batch)
2017-05-10 01:00:45.142380: step 329860, loss = 0.0669, acc = 0.9840 (274.4 examples/sec; 0.233 sec/batch)
2017-05-10 01:00:49.737300: step 329880, loss = 0.0658, acc = 0.9860 (289.3 examples/sec; 0.221 sec/batch)
2017-05-10 01:00:54.434682: step 329900, loss = 0.0645, acc = 0.9880 (286.6 examples/sec; 0.223 sec/batch)
2017-05-10 01:00:58.972894: step 329920, loss = 0.0720, acc = 0.9900 (276.8 examples/sec; 0.231 sec/batch)
2017-05-10 01:01:03.602789: step 329940, loss = 0.0822, acc = 0.9840 (271.2 examples/sec; 0.236 sec/batch)
2017-05-10 01:01:08.202981: step 329960, loss = 0.0701, acc = 0.9840 (288.6 examples/sec; 0.222 sec/batch)
2017-05-10 01:01:12.835470: step 329980, loss = 0.0654, acc = 0.9920 (273.1 examples/sec; 0.234 sec/batch)
2017-05-10 01:01:17.424511: step 330000, loss = 0.0604, acc = 0.9800 (273.9 examples/sec; 0.234 sec/batch)
[Eval] 2017-05-10 01:01:31.428485: step 330000, acc = 0.9643, f1 = 0.9631
[Test] 2017-05-10 01:01:41.223137: step 330000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-10 01:01:41.223230: step 330000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 01:01:45.809329: step 330020, loss = 0.0751, acc = 0.9780 (272.3 examples/sec; 0.235 sec/batch)
2017-05-10 01:01:50.709983: step 330040, loss = 0.0701, acc = 0.9820 (273.8 examples/sec; 0.234 sec/batch)
2017-05-10 01:01:55.315122: step 330060, loss = 0.0621, acc = 0.9900 (283.9 examples/sec; 0.225 sec/batch)
2017-05-10 01:01:59.918540: step 330080, loss = 0.0985, acc = 0.9740 (255.8 examples/sec; 0.250 sec/batch)
2017-05-10 01:02:04.634215: step 330100, loss = 0.0683, acc = 0.9860 (278.9 examples/sec; 0.229 sec/batch)
2017-05-10 01:02:09.169359: step 330120, loss = 0.1123, acc = 0.9700 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 01:02:13.596930: step 330140, loss = 0.0834, acc = 0.9760 (300.3 examples/sec; 0.213 sec/batch)
2017-05-10 01:02:18.227610: step 330160, loss = 0.0690, acc = 0.9860 (292.2 examples/sec; 0.219 sec/batch)
2017-05-10 01:02:22.720562: step 330180, loss = 0.0852, acc = 0.9800 (287.1 examples/sec; 0.223 sec/batch)
2017-05-10 01:02:27.281513: step 330200, loss = 0.0869, acc = 0.9860 (271.2 examples/sec; 0.236 sec/batch)
2017-05-10 01:02:31.882155: step 330220, loss = 0.0729, acc = 0.9780 (269.9 examples/sec; 0.237 sec/batch)
2017-05-10 01:02:36.760939: step 330240, loss = 0.0852, acc = 0.9800 (279.4 examples/sec; 0.229 sec/batch)
2017-05-10 01:02:41.349875: step 330260, loss = 0.0641, acc = 0.9860 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 01:02:45.893129: step 330280, loss = 0.0592, acc = 0.9900 (283.9 examples/sec; 0.225 sec/batch)
2017-05-10 01:02:50.647145: step 330300, loss = 0.0988, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 01:02:55.230813: step 330320, loss = 0.0797, acc = 0.9760 (283.0 examples/sec; 0.226 sec/batch)
2017-05-10 01:02:59.743712: step 330340, loss = 0.0792, acc = 0.9820 (286.2 examples/sec; 0.224 sec/batch)
2017-05-10 01:03:04.472323: step 330360, loss = 0.0820, acc = 0.9760 (278.9 examples/sec; 0.229 sec/batch)
2017-05-10 01:03:09.101884: step 330380, loss = 0.0613, acc = 0.9920 (252.3 examples/sec; 0.254 sec/batch)
2017-05-10 01:03:13.619902: step 330400, loss = 0.0759, acc = 0.9840 (282.3 examples/sec; 0.227 sec/batch)
2017-05-10 01:03:18.279816: step 330420, loss = 0.0773, acc = 0.9740 (297.3 examples/sec; 0.215 sec/batch)
2017-05-10 01:03:22.815138: step 330440, loss = 0.0608, acc = 0.9880 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 01:03:27.385446: step 330460, loss = 0.0813, acc = 0.9840 (281.1 examples/sec; 0.228 sec/batch)
2017-05-10 01:03:32.067456: step 330480, loss = 0.0729, acc = 0.9880 (242.2 examples/sec; 0.264 sec/batch)
2017-05-10 01:03:36.667444: step 330500, loss = 0.0741, acc = 0.9780 (275.7 examples/sec; 0.232 sec/batch)
2017-05-10 01:03:41.141936: step 330520, loss = 0.0944, acc = 0.9640 (289.4 examples/sec; 0.221 sec/batch)
2017-05-10 01:03:45.783401: step 330540, loss = 0.0685, acc = 0.9820 (268.3 examples/sec; 0.239 sec/batch)
2017-05-10 01:03:50.659490: step 330560, loss = 0.0600, acc = 0.9860 (287.3 examples/sec; 0.223 sec/batch)
2017-05-10 01:03:55.133086: step 330580, loss = 0.0869, acc = 0.9740 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 01:03:59.659209: step 330600, loss = 0.0760, acc = 0.9820 (282.8 examples/sec; 0.226 sec/batch)
2017-05-10 01:04:05.032708: step 330620, loss = 0.0602, acc = 0.9900 (153.6 examples/sec; 0.417 sec/batch)
2017-05-10 01:04:09.647869: step 330640, loss = 0.0748, acc = 0.9860 (272.8 examples/sec; 0.235 sec/batch)
2017-05-10 01:04:14.270665: step 330660, loss = 0.0632, acc = 0.9860 (284.2 examples/sec; 0.225 sec/batch)
2017-05-10 01:04:19.062309: step 330680, loss = 0.0710, acc = 0.9840 (282.8 examples/sec; 0.226 sec/batch)
2017-05-10 01:04:23.655252: step 330700, loss = 0.0909, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-10 01:04:28.291165: step 330720, loss = 0.0658, acc = 0.9900 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 01:04:33.041795: step 330740, loss = 0.0824, acc = 0.9780 (247.3 examples/sec; 0.259 sec/batch)
2017-05-10 01:04:37.655151: step 330760, loss = 0.0628, acc = 0.9900 (267.0 examples/sec; 0.240 sec/batch)
2017-05-10 01:04:42.254760: step 330780, loss = 0.0757, acc = 0.9780 (288.3 examples/sec; 0.222 sec/batch)
2017-05-10 01:04:46.921039: step 330800, loss = 0.0713, acc = 0.9860 (265.6 examples/sec; 0.241 sec/batch)
2017-05-10 01:04:51.680654: step 330820, loss = 0.0876, acc = 0.9720 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 01:04:56.243171: step 330840, loss = 0.0558, acc = 0.9900 (275.8 examples/sec; 0.232 sec/batch)
2017-05-10 01:05:00.825104: step 330860, loss = 0.0972, acc = 0.9740 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 01:05:05.467494: step 330880, loss = 0.0551, acc = 0.9940 (270.0 examples/sec; 0.237 sec/batch)
2017-05-10 01:05:10.225753: step 330900, loss = 0.0712, acc = 0.9820 (269.0 examples/sec; 0.238 sec/batch)
2017-05-10 01:05:14.874982: step 330920, loss = 0.0729, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-10 01:05:19.508255: step 330940, loss = 0.0703, acc = 0.9800 (300.3 examples/sec; 0.213 sec/batch)
2017-05-10 01:05:24.126724: step 330960, loss = 0.0862, acc = 0.9840 (277.3 examples/sec; 0.231 sec/batch)
2017-05-10 01:05:28.714217: step 330980, loss = 0.0559, acc = 0.9900 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 01:05:33.320289: step 331000, loss = 0.0777, acc = 0.9800 (283.6 examples/sec; 0.226 sec/batch)
[Eval] 2017-05-10 01:05:47.385635: step 331000, acc = 0.9636, f1 = 0.9625
[Test] 2017-05-10 01:05:57.179990: step 331000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-10 01:05:57.180061: step 331000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 01:06:01.706025: step 331020, loss = 0.0637, acc = 0.9860 (296.9 examples/sec; 0.216 sec/batch)
2017-05-10 01:06:06.684612: step 331040, loss = 0.0820, acc = 0.9820 (270.9 examples/sec; 0.236 sec/batch)
2017-05-10 01:06:11.187411: step 331060, loss = 0.0780, acc = 0.9800 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 01:06:15.815202: step 331080, loss = 0.0993, acc = 0.9660 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 01:06:20.698377: step 331100, loss = 0.0817, acc = 0.9740 (274.1 examples/sec; 0.233 sec/batch)
2017-05-10 01:06:25.321658: step 331120, loss = 0.0888, acc = 0.9700 (295.4 examples/sec; 0.217 sec/batch)
2017-05-10 01:06:29.996174: step 331140, loss = 0.0681, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 01:06:34.942474: step 331160, loss = 0.0778, acc = 0.9860 (282.4 examples/sec; 0.227 sec/batch)
2017-05-10 01:06:39.642985: step 331180, loss = 0.0796, acc = 0.9840 (274.9 examples/sec; 0.233 sec/batch)
2017-05-10 01:06:44.285976: step 331200, loss = 0.0727, acc = 0.9840 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 01:06:48.856757: step 331220, loss = 0.0928, acc = 0.9740 (295.2 examples/sec; 0.217 sec/batch)
2017-05-10 01:06:53.363768: step 331240, loss = 0.0694, acc = 0.9860 (281.0 examples/sec; 0.228 sec/batch)
2017-05-10 01:06:57.958472: step 331260, loss = 0.0607, acc = 0.9840 (278.9 examples/sec; 0.230 sec/batch)
2017-05-10 01:07:02.612674: step 331280, loss = 0.0750, acc = 0.9820 (234.3 examples/sec; 0.273 sec/batch)
2017-05-10 01:07:07.177940: step 331300, loss = 0.0760, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-10 01:07:11.676043: step 331320, loss = 0.0748, acc = 0.9800 (286.8 examples/sec; 0.223 sec/batch)
2017-05-10 01:07:16.226714: step 331340, loss = 0.0876, acc = 0.9740 (293.9 examples/sec; 0.218 sec/batch)
2017-05-10 01:07:20.873124: step 331360, loss = 0.0757, acc = 0.9800 (289.3 examples/sec; 0.221 sec/batch)
2017-05-10 01:07:25.376926: step 331380, loss = 0.0848, acc = 0.9840 (281.7 examples/sec; 0.227 sec/batch)
2017-05-10 01:07:30.129131: step 331400, loss = 0.0766, acc = 0.9820 (276.8 examples/sec; 0.231 sec/batch)
2017-05-10 01:07:34.843273: step 331420, loss = 0.0791, acc = 0.9780 (290.8 examples/sec; 0.220 sec/batch)
2017-05-10 01:07:39.366722: step 331440, loss = 0.0825, acc = 0.9760 (275.2 examples/sec; 0.233 sec/batch)
2017-05-10 01:07:43.870185: step 331460, loss = 0.0750, acc = 0.9820 (285.1 examples/sec; 0.224 sec/batch)
2017-05-10 01:07:48.537128: step 331480, loss = 0.0975, acc = 0.9680 (282.5 examples/sec; 0.227 sec/batch)
2017-05-10 01:07:53.209353: step 331500, loss = 0.0563, acc = 0.9900 (243.2 examples/sec; 0.263 sec/batch)
2017-05-10 01:07:57.801376: step 331520, loss = 0.1019, acc = 0.9640 (277.7 examples/sec; 0.230 sec/batch)
2017-05-10 01:08:02.495467: step 331540, loss = 0.0913, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-10 01:08:06.989096: step 331560, loss = 0.1053, acc = 0.9660 (280.1 examples/sec; 0.228 sec/batch)
2017-05-10 01:08:11.496615: step 331580, loss = 0.0968, acc = 0.9720 (289.2 examples/sec; 0.221 sec/batch)
2017-05-10 01:08:16.305974: step 331600, loss = 0.0803, acc = 0.9860 (288.5 examples/sec; 0.222 sec/batch)
2017-05-10 01:08:20.991012: step 331620, loss = 0.0535, acc = 0.9960 (248.4 examples/sec; 0.258 sec/batch)
2017-05-10 01:08:26.644661: step 331640, loss = 0.0863, acc = 0.9760 (278.7 examples/sec; 0.230 sec/batch)
2017-05-10 01:08:31.406815: step 331660, loss = 0.0707, acc = 0.9840 (285.7 examples/sec; 0.224 sec/batch)
2017-05-10 01:08:36.166256: step 331680, loss = 0.0601, acc = 0.9920 (275.1 examples/sec; 0.233 sec/batch)
2017-05-10 01:08:40.938562: step 331700, loss = 0.0789, acc = 0.9740 (273.2 examples/sec; 0.234 sec/batch)
2017-05-10 01:08:45.763861: step 331720, loss = 0.0694, acc = 0.9860 (225.7 examples/sec; 0.284 sec/batch)
2017-05-10 01:08:50.200284: step 331740, loss = 0.0670, acc = 0.9920 (282.0 examples/sec; 0.227 sec/batch)
2017-05-10 01:08:54.684529: step 331760, loss = 0.0758, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-10 01:08:59.332540: step 331780, loss = 0.0693, acc = 0.9860 (293.7 examples/sec; 0.218 sec/batch)
2017-05-10 01:09:04.037453: step 331800, loss = 0.0707, acc = 0.9840 (270.4 examples/sec; 0.237 sec/batch)
2017-05-10 01:09:08.517876: step 331820, loss = 0.0772, acc = 0.9800 (280.0 examples/sec; 0.229 sec/batch)
2017-05-10 01:09:13.004611: step 331840, loss = 0.0674, acc = 0.9840 (291.8 examples/sec; 0.219 sec/batch)
2017-05-10 01:09:17.649088: step 331860, loss = 0.0679, acc = 0.9860 (282.2 examples/sec; 0.227 sec/batch)
2017-05-10 01:09:22.214353: step 331880, loss = 0.0600, acc = 0.9860 (278.6 examples/sec; 0.230 sec/batch)
2017-05-10 01:09:26.808710: step 331900, loss = 0.0748, acc = 0.9860 (282.0 examples/sec; 0.227 sec/batch)
2017-05-10 01:09:31.420286: step 331920, loss = 0.0813, acc = 0.9700 (269.4 examples/sec; 0.238 sec/batch)
2017-05-10 01:09:36.010084: step 331940, loss = 0.0721, acc = 0.9860 (277.1 examples/sec; 0.231 sec/batch)
2017-05-10 01:09:40.585841: step 331960, loss = 0.0727, acc = 0.9860 (284.1 examples/sec; 0.225 sec/batch)
2017-05-10 01:09:45.271215: step 331980, loss = 0.0659, acc = 0.9880 (296.1 examples/sec; 0.216 sec/batch)
2017-05-10 01:09:49.928656: step 332000, loss = 0.0675, acc = 0.9840 (255.3 examples/sec; 0.251 sec/batch)
[Eval] 2017-05-10 01:10:03.926271: step 332000, acc = 0.9642, f1 = 0.9629
[Test] 2017-05-10 01:10:13.543285: step 332000, acc = 0.9556, f1 = 0.9553
[Status] 2017-05-10 01:10:13.543384: step 332000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 01:10:18.069676: step 332020, loss = 0.0810, acc = 0.9760 (273.9 examples/sec; 0.234 sec/batch)
2017-05-10 01:10:22.667778: step 332040, loss = 0.0791, acc = 0.9860 (279.3 examples/sec; 0.229 sec/batch)
2017-05-10 01:10:27.285150: step 332060, loss = 0.0689, acc = 0.9820 (276.0 examples/sec; 0.232 sec/batch)
2017-05-10 01:10:31.983767: step 332080, loss = 0.0837, acc = 0.9820 (258.8 examples/sec; 0.247 sec/batch)
2017-05-10 01:10:36.733053: step 332100, loss = 0.0886, acc = 0.9740 (272.8 examples/sec; 0.235 sec/batch)
2017-05-10 01:10:41.254045: step 332120, loss = 0.0634, acc = 0.9880 (273.5 examples/sec; 0.234 sec/batch)
2017-05-10 01:10:45.928773: step 332140, loss = 0.0732, acc = 0.9880 (278.1 examples/sec; 0.230 sec/batch)
2017-05-10 01:10:50.506787: step 332160, loss = 0.0790, acc = 0.9820 (282.6 examples/sec; 0.226 sec/batch)
2017-05-10 01:10:55.089356: step 332180, loss = 0.0886, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-10 01:11:00.076770: step 332200, loss = 0.0695, acc = 0.9880 (230.4 examples/sec; 0.278 sec/batch)
2017-05-10 01:11:04.738150: step 332220, loss = 0.0970, acc = 0.9700 (275.6 examples/sec; 0.232 sec/batch)
2017-05-10 01:11:09.345325: step 332240, loss = 0.0747, acc = 0.9820 (279.3 examples/sec; 0.229 sec/batch)
2017-05-10 01:11:14.098463: step 332260, loss = 0.0659, acc = 0.9880 (218.1 examples/sec; 0.293 sec/batch)
2017-05-10 01:11:18.672697: step 332280, loss = 0.0702, acc = 0.9820 (275.3 examples/sec; 0.232 sec/batch)
2017-05-10 01:11:23.362313: step 332300, loss = 0.0850, acc = 0.9700 (280.1 examples/sec; 0.228 sec/batch)
2017-05-10 01:11:28.164840: step 332320, loss = 0.0775, acc = 0.9820 (234.2 examples/sec; 0.273 sec/batch)
2017-05-10 01:11:32.822083: step 332340, loss = 0.0839, acc = 0.9860 (290.5 examples/sec; 0.220 sec/batch)
2017-05-10 01:11:37.286392: step 332360, loss = 0.0753, acc = 0.9840 (278.3 examples/sec; 0.230 sec/batch)
2017-05-10 01:11:41.898679: step 332380, loss = 0.0727, acc = 0.9860 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 01:11:46.737305: step 332400, loss = 0.0852, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-10 01:11:51.219027: step 332420, loss = 0.0571, acc = 0.9880 (287.9 examples/sec; 0.222 sec/batch)
2017-05-10 01:11:55.735668: step 332440, loss = 0.0702, acc = 0.9880 (297.4 examples/sec; 0.215 sec/batch)
2017-05-10 01:12:00.297322: step 332460, loss = 0.0757, acc = 0.9820 (281.0 examples/sec; 0.228 sec/batch)
2017-05-10 01:12:04.878413: step 332480, loss = 0.0795, acc = 0.9800 (295.5 examples/sec; 0.217 sec/batch)
2017-05-10 01:12:09.385843: step 332500, loss = 0.0707, acc = 0.9820 (280.5 examples/sec; 0.228 sec/batch)
2017-05-10 01:12:14.104048: step 332520, loss = 0.0827, acc = 0.9860 (272.1 examples/sec; 0.235 sec/batch)
2017-05-10 01:12:18.670659: step 332540, loss = 0.0789, acc = 0.9780 (279.7 examples/sec; 0.229 sec/batch)
2017-05-10 01:12:23.326306: step 332560, loss = 0.0901, acc = 0.9840 (273.6 examples/sec; 0.234 sec/batch)
2017-05-10 01:12:28.170308: step 332580, loss = 0.0863, acc = 0.9800 (284.2 examples/sec; 0.225 sec/batch)
2017-05-10 01:12:32.768088: step 332600, loss = 0.0941, acc = 0.9700 (281.8 examples/sec; 0.227 sec/batch)
2017-05-10 01:12:37.400177: step 332620, loss = 0.0579, acc = 0.9880 (276.4 examples/sec; 0.232 sec/batch)
2017-05-10 01:12:42.896034: step 332640, loss = 0.0686, acc = 0.9880 (218.9 examples/sec; 0.292 sec/batch)
2017-05-10 01:12:47.375987: step 332660, loss = 0.0671, acc = 0.9940 (276.4 examples/sec; 0.232 sec/batch)
2017-05-10 01:12:52.079032: step 332680, loss = 0.0686, acc = 0.9900 (281.5 examples/sec; 0.227 sec/batch)
2017-05-10 01:12:56.666541: step 332700, loss = 0.0708, acc = 0.9880 (265.8 examples/sec; 0.241 sec/batch)
2017-05-10 01:13:01.510425: step 332720, loss = 0.0687, acc = 0.9860 (278.6 examples/sec; 0.230 sec/batch)
2017-05-10 01:13:06.103913: step 332740, loss = 0.0690, acc = 0.9840 (267.5 examples/sec; 0.239 sec/batch)
2017-05-10 01:13:10.573866: step 332760, loss = 0.0731, acc = 0.9780 (283.2 examples/sec; 0.226 sec/batch)
2017-05-10 01:13:15.616864: step 332780, loss = 0.0803, acc = 0.9820 (253.6 examples/sec; 0.252 sec/batch)
2017-05-10 01:13:20.352568: step 332800, loss = 0.0767, acc = 0.9820 (284.9 examples/sec; 0.225 sec/batch)
2017-05-10 01:13:25.004058: step 332820, loss = 0.0738, acc = 0.9780 (289.6 examples/sec; 0.221 sec/batch)
2017-05-10 01:13:29.665400: step 332840, loss = 0.0715, acc = 0.9800 (298.1 examples/sec; 0.215 sec/batch)
2017-05-10 01:13:34.224373: step 332860, loss = 0.0763, acc = 0.9800 (293.3 examples/sec; 0.218 sec/batch)
2017-05-10 01:13:38.893082: step 332880, loss = 0.0856, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-10 01:13:43.526752: step 332900, loss = 0.0765, acc = 0.9860 (283.7 examples/sec; 0.226 sec/batch)
2017-05-10 01:13:48.093403: step 332920, loss = 0.0796, acc = 0.9860 (287.9 examples/sec; 0.222 sec/batch)
2017-05-10 01:13:52.584330: step 332940, loss = 0.0733, acc = 0.9820 (286.4 examples/sec; 0.223 sec/batch)
2017-05-10 01:13:57.190310: step 332960, loss = 0.0731, acc = 0.9900 (289.7 examples/sec; 0.221 sec/batch)
2017-05-10 01:14:01.769652: step 332980, loss = 0.1060, acc = 0.9620 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 01:14:06.366483: step 333000, loss = 0.0825, acc = 0.9800 (284.4 examples/sec; 0.225 sec/batch)
[Eval] 2017-05-10 01:14:20.443406: step 333000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-10 01:14:30.064734: step 333000, acc = 0.9561, f1 = 0.9557
[Status] 2017-05-10 01:14:30.064835: step 333000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 01:14:34.803191: step 333020, loss = 0.0773, acc = 0.9820 (268.4 examples/sec; 0.238 sec/batch)
2017-05-10 01:14:39.367330: step 333040, loss = 0.0691, acc = 0.9800 (274.0 examples/sec; 0.234 sec/batch)
2017-05-10 01:14:44.166363: step 333060, loss = 0.0570, acc = 0.9920 (278.7 examples/sec; 0.230 sec/batch)
2017-05-10 01:14:48.705710: step 333080, loss = 0.0538, acc = 0.9980 (269.8 examples/sec; 0.237 sec/batch)
2017-05-10 01:14:54.129073: step 333100, loss = 0.0862, acc = 0.9780 (173.3 examples/sec; 0.369 sec/batch)
2017-05-10 01:14:58.837321: step 333120, loss = 0.0655, acc = 0.9900 (272.0 examples/sec; 0.235 sec/batch)
2017-05-10 01:15:03.507373: step 333140, loss = 0.0785, acc = 0.9660 (276.2 examples/sec; 0.232 sec/batch)
2017-05-10 01:15:08.144711: step 333160, loss = 0.0916, acc = 0.9780 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 01:15:12.908140: step 333180, loss = 0.0704, acc = 0.9840 (298.0 examples/sec; 0.215 sec/batch)
2017-05-10 01:15:17.433358: step 333200, loss = 0.0960, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-10 01:15:22.009180: step 333220, loss = 0.0938, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-10 01:15:26.855119: step 333240, loss = 0.0628, acc = 0.9920 (268.3 examples/sec; 0.239 sec/batch)
2017-05-10 01:15:31.347104: step 333260, loss = 0.0770, acc = 0.9800 (288.5 examples/sec; 0.222 sec/batch)
2017-05-10 01:15:35.940141: step 333280, loss = 0.0674, acc = 0.9900 (285.7 examples/sec; 0.224 sec/batch)
2017-05-10 01:15:40.402785: step 333300, loss = 0.0781, acc = 0.9800 (286.0 examples/sec; 0.224 sec/batch)
2017-05-10 01:15:45.303406: step 333320, loss = 0.0539, acc = 0.9920 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 01:15:49.909550: step 333340, loss = 0.0716, acc = 0.9820 (278.2 examples/sec; 0.230 sec/batch)
2017-05-10 01:15:54.572073: step 333360, loss = 0.0477, acc = 0.9940 (266.8 examples/sec; 0.240 sec/batch)
2017-05-10 01:15:59.258837: step 333380, loss = 0.0701, acc = 0.9900 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 01:16:03.880966: step 333400, loss = 0.0873, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-10 01:16:08.557730: step 333420, loss = 0.0555, acc = 0.9920 (263.8 examples/sec; 0.243 sec/batch)
2017-05-10 01:16:13.165226: step 333440, loss = 0.0870, acc = 0.9840 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 01:16:17.707829: step 333460, loss = 0.0771, acc = 0.9880 (282.5 examples/sec; 0.227 sec/batch)
2017-05-10 01:16:22.122569: step 333480, loss = 0.0622, acc = 0.9920 (280.7 examples/sec; 0.228 sec/batch)
2017-05-10 01:16:26.847415: step 333500, loss = 0.0710, acc = 0.9800 (236.4 examples/sec; 0.271 sec/batch)
2017-05-10 01:16:31.354561: step 333520, loss = 0.0719, acc = 0.9860 (287.9 examples/sec; 0.222 sec/batch)
2017-05-10 01:16:35.932230: step 333540, loss = 0.0746, acc = 0.9780 (278.2 examples/sec; 0.230 sec/batch)
2017-05-10 01:16:40.553785: step 333560, loss = 0.0602, acc = 0.9880 (283.7 examples/sec; 0.226 sec/batch)
2017-05-10 01:16:45.086759: step 333580, loss = 0.0879, acc = 0.9760 (293.5 examples/sec; 0.218 sec/batch)
2017-05-10 01:16:49.682896: step 333600, loss = 0.0750, acc = 0.9820 (274.9 examples/sec; 0.233 sec/batch)
2017-05-10 01:16:54.275851: step 333620, loss = 0.0602, acc = 0.9860 (280.9 examples/sec; 0.228 sec/batch)
2017-05-10 01:16:58.924894: step 333640, loss = 0.0584, acc = 0.9960 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 01:17:04.435805: step 333660, loss = 0.0624, acc = 0.9840 (298.8 examples/sec; 0.214 sec/batch)
2017-05-10 01:17:08.980241: step 333680, loss = 0.0950, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-10 01:17:13.713143: step 333700, loss = 0.0629, acc = 0.9900 (280.4 examples/sec; 0.228 sec/batch)
2017-05-10 01:17:18.648769: step 333720, loss = 0.0818, acc = 0.9840 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 01:17:23.116893: step 333740, loss = 0.0762, acc = 0.9840 (287.8 examples/sec; 0.222 sec/batch)
2017-05-10 01:17:27.974896: step 333760, loss = 0.0617, acc = 0.9900 (259.7 examples/sec; 0.246 sec/batch)
2017-05-10 01:17:32.637304: step 333780, loss = 0.0779, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 01:17:37.277714: step 333800, loss = 0.0654, acc = 0.9880 (262.3 examples/sec; 0.244 sec/batch)
2017-05-10 01:17:42.167189: step 333820, loss = 0.0566, acc = 0.9880 (285.7 examples/sec; 0.224 sec/batch)
2017-05-10 01:17:46.870160: step 333840, loss = 0.0980, acc = 0.9600 (280.3 examples/sec; 0.228 sec/batch)
2017-05-10 01:17:51.270498: step 333860, loss = 0.0949, acc = 0.9680 (293.5 examples/sec; 0.218 sec/batch)
2017-05-10 01:17:56.083138: step 333880, loss = 0.0771, acc = 0.9840 (229.6 examples/sec; 0.279 sec/batch)
2017-05-10 01:18:00.866509: step 333900, loss = 0.0714, acc = 0.9840 (274.3 examples/sec; 0.233 sec/batch)
2017-05-10 01:18:05.419498: step 333920, loss = 0.0787, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-10 01:18:09.970560: step 333940, loss = 0.0814, acc = 0.9860 (278.1 examples/sec; 0.230 sec/batch)
2017-05-10 01:18:14.573997: step 333960, loss = 0.1045, acc = 0.9800 (291.4 examples/sec; 0.220 sec/batch)
2017-05-10 01:18:19.139986: step 333980, loss = 0.0599, acc = 0.9880 (282.0 examples/sec; 0.227 sec/batch)
2017-05-10 01:18:23.717227: step 334000, loss = 0.0720, acc = 0.9840 (271.2 examples/sec; 0.236 sec/batch)
[Eval] 2017-05-10 01:18:37.779571: step 334000, acc = 0.9642, f1 = 0.9630
[Test] 2017-05-10 01:18:47.486576: step 334000, acc = 0.9559, f1 = 0.9556
[Status] 2017-05-10 01:18:47.486670: step 334000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 01:18:52.056119: step 334020, loss = 0.0903, acc = 0.9800 (257.3 examples/sec; 0.249 sec/batch)
2017-05-10 01:18:56.692611: step 334040, loss = 0.0907, acc = 0.9780 (292.6 examples/sec; 0.219 sec/batch)
2017-05-10 01:19:01.382483: step 334060, loss = 0.0697, acc = 0.9800 (276.5 examples/sec; 0.231 sec/batch)
2017-05-10 01:19:05.823903: step 334080, loss = 0.0892, acc = 0.9740 (292.0 examples/sec; 0.219 sec/batch)
2017-05-10 01:19:10.417996: step 334100, loss = 0.0585, acc = 0.9860 (275.8 examples/sec; 0.232 sec/batch)
2017-05-10 01:19:15.048366: step 334120, loss = 0.0788, acc = 0.9820 (276.9 examples/sec; 0.231 sec/batch)
2017-05-10 01:19:19.599147: step 334140, loss = 0.0723, acc = 0.9780 (279.5 examples/sec; 0.229 sec/batch)
2017-05-10 01:19:24.181176: step 334160, loss = 0.0659, acc = 0.9860 (291.5 examples/sec; 0.220 sec/batch)
2017-05-10 01:19:28.754530: step 334180, loss = 0.0559, acc = 0.9980 (281.7 examples/sec; 0.227 sec/batch)
2017-05-10 01:19:33.348433: step 334200, loss = 0.0545, acc = 0.9920 (274.6 examples/sec; 0.233 sec/batch)
2017-05-10 01:19:38.057099: step 334220, loss = 0.0915, acc = 0.9740 (275.6 examples/sec; 0.232 sec/batch)
2017-05-10 01:19:42.765289: step 334240, loss = 0.0607, acc = 0.9860 (274.9 examples/sec; 0.233 sec/batch)
2017-05-10 01:19:47.315799: step 334260, loss = 0.0676, acc = 0.9800 (275.3 examples/sec; 0.232 sec/batch)
2017-05-10 01:19:51.978925: step 334280, loss = 0.0680, acc = 0.9840 (255.9 examples/sec; 0.250 sec/batch)
2017-05-10 01:19:56.511239: step 334300, loss = 0.0879, acc = 0.9800 (279.3 examples/sec; 0.229 sec/batch)
2017-05-10 01:20:01.103528: step 334320, loss = 0.0772, acc = 0.9800 (278.3 examples/sec; 0.230 sec/batch)
2017-05-10 01:20:05.940690: step 334340, loss = 0.0818, acc = 0.9780 (233.1 examples/sec; 0.275 sec/batch)
2017-05-10 01:20:10.465002: step 334360, loss = 0.0674, acc = 0.9840 (291.5 examples/sec; 0.220 sec/batch)
2017-05-10 01:20:15.152549: step 334380, loss = 0.0695, acc = 0.9900 (276.1 examples/sec; 0.232 sec/batch)
2017-05-10 01:20:19.750133: step 334400, loss = 0.0671, acc = 0.9880 (265.1 examples/sec; 0.241 sec/batch)
2017-05-10 01:20:24.460991: step 334420, loss = 0.0939, acc = 0.9740 (293.1 examples/sec; 0.218 sec/batch)
2017-05-10 01:20:28.948702: step 334440, loss = 0.0661, acc = 0.9900 (285.0 examples/sec; 0.225 sec/batch)
2017-05-10 01:20:33.622081: step 334460, loss = 0.0789, acc = 0.9840 (297.8 examples/sec; 0.215 sec/batch)
2017-05-10 01:20:38.438326: step 334480, loss = 0.0717, acc = 0.9820 (269.7 examples/sec; 0.237 sec/batch)
2017-05-10 01:20:43.230261: step 334500, loss = 0.0844, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-10 01:20:47.941351: step 334520, loss = 0.0610, acc = 0.9820 (285.2 examples/sec; 0.224 sec/batch)
2017-05-10 01:20:52.718630: step 334540, loss = 0.0702, acc = 0.9800 (257.7 examples/sec; 0.248 sec/batch)
2017-05-10 01:20:57.228400: step 334560, loss = 0.0830, acc = 0.9740 (291.1 examples/sec; 0.220 sec/batch)
2017-05-10 01:21:01.876960: step 334580, loss = 0.0676, acc = 0.9860 (273.2 examples/sec; 0.234 sec/batch)
2017-05-10 01:21:06.609659: step 334600, loss = 0.0630, acc = 0.9900 (224.0 examples/sec; 0.286 sec/batch)
2017-05-10 01:21:11.134937: step 334620, loss = 0.0777, acc = 0.9840 (281.6 examples/sec; 0.227 sec/batch)
2017-05-10 01:21:15.851930: step 334640, loss = 0.1012, acc = 0.9720 (288.5 examples/sec; 0.222 sec/batch)
2017-05-10 01:21:21.465237: step 334660, loss = 0.0762, acc = 0.9840 (218.2 examples/sec; 0.293 sec/batch)
2017-05-10 01:21:26.108581: step 334680, loss = 0.0890, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 01:21:30.752194: step 334700, loss = 0.1037, acc = 0.9680 (285.8 examples/sec; 0.224 sec/batch)
2017-05-10 01:21:35.429633: step 334720, loss = 0.0568, acc = 0.9900 (256.1 examples/sec; 0.250 sec/batch)
2017-05-10 01:21:39.910095: step 334740, loss = 0.0599, acc = 0.9940 (281.3 examples/sec; 0.228 sec/batch)
2017-05-10 01:21:44.528522: step 334760, loss = 0.0767, acc = 0.9860 (279.1 examples/sec; 0.229 sec/batch)
2017-05-10 01:21:49.135971: step 334780, loss = 0.0852, acc = 0.9820 (293.4 examples/sec; 0.218 sec/batch)
2017-05-10 01:21:54.114028: step 334800, loss = 0.0737, acc = 0.9840 (273.1 examples/sec; 0.234 sec/batch)
2017-05-10 01:21:58.645115: step 334820, loss = 0.0804, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-10 01:22:03.164877: step 334840, loss = 0.0740, acc = 0.9820 (271.9 examples/sec; 0.235 sec/batch)
2017-05-10 01:22:07.918371: step 334860, loss = 0.0770, acc = 0.9800 (264.5 examples/sec; 0.242 sec/batch)
2017-05-10 01:22:12.542673: step 334880, loss = 0.0671, acc = 0.9900 (277.0 examples/sec; 0.231 sec/batch)
2017-05-10 01:22:17.014381: step 334900, loss = 0.0762, acc = 0.9840 (285.5 examples/sec; 0.224 sec/batch)
2017-05-10 01:22:21.771191: step 334920, loss = 0.0901, acc = 0.9780 (284.7 examples/sec; 0.225 sec/batch)
2017-05-10 01:22:26.355141: step 334940, loss = 0.0888, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-10 01:22:30.896215: step 334960, loss = 0.0616, acc = 0.9840 (284.9 examples/sec; 0.225 sec/batch)
2017-05-10 01:22:35.624301: step 334980, loss = 0.0722, acc = 0.9760 (222.2 examples/sec; 0.288 sec/batch)
2017-05-10 01:22:40.156187: step 335000, loss = 0.0706, acc = 0.9820 (296.0 examples/sec; 0.216 sec/batch)
[Eval] 2017-05-10 01:22:54.100374: step 335000, acc = 0.9581, f1 = 0.9566
[Test] 2017-05-10 01:23:03.391911: step 335000, acc = 0.9467, f1 = 0.9462
[Status] 2017-05-10 01:23:03.391987: step 335000, maxindex = 196000, maxdev = 0.9651, maxtst = 0.9558
2017-05-10 01:23:08.253998: step 335020, loss = 0.0731, acc = 0.9840 (282.6 examples/sec; 0.226 sec/batch)
2017-05-10 01:23:12.844533: step 335040, loss = 0.0885, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-10 01:23:17.373023: step 335060, loss = 0.0598, acc = 0.9900 (281.8 examples/sec; 0.227 sec/batch)
2017-05-10 01:23:22.036517: step 335080, loss = 0.0667, acc = 0.9880 (297.7 examples/sec; 0.215 sec/batch)
2017-05-10 01:23:26.605865: step 335100, loss = 0.1029, acc = 0.9780 (274.4 examples/sec; 0.233 sec/batch)
2017-05-10 01:23:31.331347: step 335120, loss = 0.0929, acc = 0.9780 (280.0 examples/sec; 0.229 sec/batch)
2017-05-10 01:23:36.244892: step 335140, loss = 0.0728, acc = 0.9840 (210.6 examples/sec; 0.304 sec/batch)
2017-05-10 01:23:40.712200: step 335160, loss = 0.0632, acc = 0.9820 (281.9 examples/sec; 0.227 sec/batch)
2017-05-10 01:23:45.177534: step 335180, loss = 0.0554, acc = 0.9900 (293.4 examples/sec; 0.218 sec/batch)
2017-05-10 01:23:49.878788: step 335200, loss = 0.0594, acc = 0.9940 (239.4 examples/sec; 0.267 sec/batch)
2017-05-10 01:23:54.470659: step 335220, loss = 0.0680, acc = 0.9840 (274.1 examples/sec; 0.234 sec/batch)
2017-05-10 01:23:59.070064: step 335240, loss = 0.0764, acc = 0.9820 (275.1 examples/sec; 0.233 sec/batch)
2017-05-10 01:24:03.678413: step 335260, loss = 0.0672, acc = 0.9840 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 01:24:08.300715: step 335280, loss = 0.0677, acc = 0.9840 (292.0 examples/sec; 0.219 sec/batch)
2017-05-10 01:24:12.872537: step 335300, loss = 0.0636, acc = 0.9860 (291.4 examples/sec; 0.220 sec/batch)
2017-05-10 01:24:17.260003: step 335320, loss = 0.0646, acc = 0.9920 (293.9 examples/sec; 0.218 sec/batch)
2017-05-10 01:24:22.142706: step 335340, loss = 0.0815, acc = 0.9820 (259.6 examples/sec; 0.247 sec/batch)
2017-05-10 01:24:26.850666: step 335360, loss = 0.0669, acc = 0.9920 (271.8 examples/sec; 0.235 sec/batch)
2017-05-10 01:24:31.491715: step 335380, loss = 0.0782, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-10 01:24:36.241788: step 335400, loss = 0.0700, acc = 0.9840 (275.5 examples/sec; 0.232 sec/batch)
2017-05-10 01:24:40.845166: step 335420, loss = 0.1005, acc = 0.9680 (274.6 examples/sec; 0.233 sec/batch)
2017-05-10 01:24:45.552705: step 335440, loss = 0.0688, acc = 0.9880 (275.1 examples/sec; 0.233 sec/batch)
2017-05-10 01:24:50.259523: step 335460, loss = 0.0919, acc = 0.9720 (273.3 examples/sec; 0.234 sec/batch)
2017-05-10 01:24:54.852838: step 335480, loss = 0.0726, acc = 0.9840 (277.7 examples/sec; 0.230 sec/batch)
2017-05-10 01:24:59.351327: step 335500, loss = 0.0616, acc = 0.9900 (282.7 examples/sec; 0.226 sec/batch)
2017-05-10 01:25:04.145697: step 335520, loss = 0.0676, acc = 0.9840 (231.0 examples/sec; 0.277 sec/batch)
2017-05-10 01:25:08.737538: step 335540, loss = 0.0760, acc = 0.9860 (274.4 examples/sec; 0.233 sec/batch)
2017-05-10 01:25:13.365006: step 335560, loss = 0.0877, acc = 0.9800 (267.0 examples/sec; 0.240 sec/batch)
2017-05-10 01:25:17.861796: step 335580, loss = 0.0612, acc = 0.9880 (300.2 examples/sec; 0.213 sec/batch)
2017-05-10 01:25:22.518427: step 335600, loss = 0.0651, acc = 0.9840 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 01:25:27.076413: step 335620, loss = 0.0804, acc = 0.9740 (271.9 examples/sec; 0.235 sec/batch)
2017-05-10 01:25:31.582233: step 335640, loss = 0.0828, acc = 0.9800 (276.9 examples/sec; 0.231 sec/batch)
2017-05-10 01:25:37.457793: step 335660, loss = 0.0580, acc = 0.9920 (132.2 examples/sec; 0.484 sec/batch)
2017-05-10 01:25:42.175136: step 335680, loss = 0.0797, acc = 0.9840 (261.8 examples/sec; 0.244 sec/batch)
2017-05-10 01:25:47.031225: step 335700, loss = 0.0602, acc = 0.9880 (225.0 examples/sec; 0.284 sec/batch)
2017-05-10 01:25:51.547706: step 335720, loss = 0.0647, acc = 0.9880 (271.1 examples/sec; 0.236 sec/batch)
2017-05-10 01:25:56.035454: step 335740, loss = 0.0798, acc = 0.9840 (279.8 examples/sec; 0.229 sec/batch)
2017-05-10 01:26:00.503994: step 335760, loss = 0.0743, acc = 0.9880 (281.9 examples/sec; 0.227 sec/batch)
2017-05-10 01:26:05.123231: step 335780, loss = 0.0637, acc = 0.9880 (288.0 examples/sec; 0.222 sec/batch)
2017-05-10 01:26:09.740335: step 335800, loss = 0.0802, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-10 01:26:14.286560: step 335820, loss = 0.0724, acc = 0.9860 (279.2 examples/sec; 0.229 sec/batch)
2017-05-10 01:26:19.028385: step 335840, loss = 0.0815, acc = 0.9800 (269.2 examples/sec; 0.238 sec/batch)
2017-05-10 01:26:23.672027: step 335860, loss = 0.0731, acc = 0.9800 (279.5 examples/sec; 0.229 sec/batch)
2017-05-10 01:26:28.182966: step 335880, loss = 0.0796, acc = 0.9760 (275.4 examples/sec; 0.232 sec/batch)
2017-05-10 01:26:32.799129: step 335900, loss = 0.0712, acc = 0.9840 (275.7 examples/sec; 0.232 sec/batch)
2017-05-10 01:26:37.557520: step 335920, loss = 0.0615, acc = 0.9880 (262.7 examples/sec; 0.244 sec/batch)
2017-05-10 01:26:42.116740: step 335940, loss = 0.0838, acc = 0.9760 (284.7 examples/sec; 0.225 sec/batch)
2017-05-10 01:26:46.870112: step 335960, loss = 0.0649, acc = 0.9800 (303.0 examples/sec; 0.211 sec/batch)
2017-05-10 01:26:51.561729: step 335980, loss = 0.0920, acc = 0.9800 (281.3 examples/sec; 0.228 sec/batch)
