I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.44GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd6e14bf0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd6e18a10
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:83:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd6e1c860
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:84:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: TITAN X (Pascal), pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: TITAN X (Pascal), pci bus id: 0000:84:00.0)
The vocabulary size is: 2665791
[w2v..]
Elapsed: 36.579408884
[loading trn..]
Elapsed: 348.692080021
[loading dev..]
Elapsed: 37.9267060757
[loading tst..]
Elapsed: 25.8897759914
self.embedding Tensor("Placeholder:0", shape=(2665792, 100), dtype=float32, device=/device:CPU:0)
self.w2v Tensor("embedding/read:0", shape=(2665792, 100), dtype=float32, device=/device:CPU:0)
self.embedding_params Tensor("Assign:0", shape=(2665792, 100), dtype=float32_ref, device=/device:CPU:0)
embedded_tokens Tensor("tower_0_dev/embedding_lookup:0", shape=(?, 1200, 100), dtype=float32, device=/device:CPU:0)
embedded_tokens_expanded Tensor("tower_0_dev/ExpandDims:0", shape=(?, 1200, 100, 1), dtype=float32, device=/device:GPU:0)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:0)
h_pool Tensor("tower_0_dev/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:0)
h_pool_flat Tensor("tower_0_dev/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:0)
embedded_tokens Tensor("tower_0_tst/embedding_lookup:0", shape=(?, 1200, 100), dtype=float32, device=/device:CPU:0)
embedded_tokens_expanded Tensor("tower_0_tst/ExpandDims:0", shape=(?, 1200, 100, 1), dtype=float32, device=/device:GPU:1)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:1)
h_pool Tensor("tower_0_tst/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:1)
h_pool_flat Tensor("tower_0_tst/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:1)
embedded_tokens Tensor("tower_0/embedding_lookup:0", shape=(?, 1200, 100), dtype=float32, device=/device:CPU:0)
embedded_tokens_expanded Tensor("tower_0/ExpandDims:0", shape=(?, 1200, 100, 1), dtype=float32, device=/device:GPU:0)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:0)
h_pool Tensor("tower_0/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:0)
h_pool_flat Tensor("tower_0/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:0)
embedded_tokens Tensor("tower_1/embedding_lookup:0", shape=(?, 1200, 100), dtype=float32, device=/device:CPU:0)
embedded_tokens_expanded Tensor("tower_1/ExpandDims:0", shape=(?, 1200, 100, 1), dtype=float32, device=/device:GPU:1)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:1)
h_pool Tensor("tower_1/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:1)
h_pool_flat Tensor("tower_1/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:1)
embedded_tokens Tensor("tower_2/embedding_lookup:0", shape=(?, 1200, 100), dtype=float32, device=/device:CPU:0)
embedded_tokens_expanded Tensor("tower_2/ExpandDims:0", shape=(?, 1200, 100, 1), dtype=float32, device=/device:GPU:2)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:2)
h_pool Tensor("tower_2/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:2)
h_pool_flat Tensor("tower_2/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:2)
embedded_tokens Tensor("tower_3/embedding_lookup:0", shape=(?, 1200, 100), dtype=float32, device=/device:CPU:0)
embedded_tokens_expanded Tensor("tower_3/ExpandDims:0", shape=(?, 1200, 100, 1), dtype=float32, device=/device:GPU:3)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:3)
h_pool Tensor("tower_3/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:3)
h_pool_flat Tensor("tower_3/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:3)
2017-05-02 16:54:39.531382: step 0, loss = 1.1467, acc = 0.4900 (3.4 examples/sec; 18.818 sec/batch)
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.57GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
[Eval_batch(0)(2000,2000)] 2017-05-02 16:54:40.841848: step 0, loss = 1.1002, acc = 0.4950, f1neg = 0.5511, f1pos = 0.4229, f1 = 0.4870
[Eval_batch(1)(2000,4000)] 2017-05-02 16:54:41.308296: step 0, loss = 1.1069, acc = 0.4760, f1neg = 0.5321, f1pos = 0.4045, f1 = 0.4683
[Eval_batch(2)(2000,6000)] 2017-05-02 16:54:41.780447: step 0, loss = 1.1023, acc = 0.4845, f1neg = 0.5456, f1pos = 0.4044, f1 = 0.4750
[Eval_batch(3)(2000,8000)] 2017-05-02 16:54:42.253529: step 0, loss = 1.0996, acc = 0.4995, f1neg = 0.5691, f1pos = 0.4031, f1 = 0.4861
[Eval_batch(4)(2000,10000)] 2017-05-02 16:54:42.724063: step 0, loss = 1.1006, acc = 0.5025, f1neg = 0.5757, f1pos = 0.3988, f1 = 0.4872
[Eval_batch(5)(2000,12000)] 2017-05-02 16:54:43.168008: step 0, loss = 1.1006, acc = 0.5060, f1neg = 0.5667, f1pos = 0.4256, f1 = 0.4961
[Eval_batch(6)(2000,14000)] 2017-05-02 16:54:43.616920: step 0, loss = 1.1002, acc = 0.5080, f1neg = 0.5831, f1pos = 0.4000, f1 = 0.4915
[Eval_batch(7)(2000,16000)] 2017-05-02 16:54:44.126859: step 0, loss = 1.1044, acc = 0.4640, f1neg = 0.5206, f1pos = 0.3923, f1 = 0.4564
[Eval_batch(8)(2000,18000)] 2017-05-02 16:54:44.637074: step 0, loss = 1.1032, acc = 0.4755, f1neg = 0.5352, f1pos = 0.3982, f1 = 0.4667
[Eval_batch(9)(2000,20000)] 2017-05-02 16:54:45.131236: step 0, loss = 1.0997, acc = 0.5115, f1neg = 0.5750, f1pos = 0.4256, f1 = 0.5003
[Eval_batch(10)(2000,22000)] 2017-05-02 16:54:45.614455: step 0, loss = 1.0999, acc = 0.4940, f1neg = 0.5542, f1pos = 0.4150, f1 = 0.4846
[Eval_batch(11)(2000,24000)] 2017-05-02 16:54:46.111848: step 0, loss = 1.1040, acc = 0.4680, f1neg = 0.5271, f1pos = 0.3920, f1 = 0.4596
[Eval_batch(12)(2000,26000)] 2017-05-02 16:54:46.614843: step 0, loss = 1.0983, acc = 0.4965, f1neg = 0.5608, f1pos = 0.4101, f1 = 0.4855
[Eval_batch(13)(2000,28000)] 2017-05-02 16:54:47.092735: step 0, loss = 1.1041, acc = 0.4865, f1neg = 0.5107, f1pos = 0.4598, f1 = 0.4852
[Eval_batch(14)(2000,30000)] 2017-05-02 16:54:47.602195: step 0, loss = 1.1039, acc = 0.4845, f1neg = 0.5264, f1pos = 0.4344, f1 = 0.4804
[Eval_batch(15)(2000,32000)] 2017-05-02 16:54:48.046700: step 0, loss = 1.1011, acc = 0.4995, f1neg = 0.5588, f1pos = 0.4217, f1 = 0.4903
[Eval_batch(16)(2000,34000)] 2017-05-02 16:54:48.508681: step 0, loss = 1.1000, acc = 0.5030, f1neg = 0.5712, f1pos = 0.4090, f1 = 0.4901
[Eval_batch(17)(2000,36000)] 2017-05-02 16:54:48.976570: step 0, loss = 1.0989, acc = 0.4950, f1neg = 0.5620, f1pos = 0.4038, f1 = 0.4829
[Eval_batch(18)(2000,38000)] 2017-05-02 16:54:49.447435: step 0, loss = 1.0988, acc = 0.5100, f1neg = 0.5944, f1pos = 0.3813, f1 = 0.4878
[Eval_batch(19)(2000,40000)] 2017-05-02 16:54:49.913152: step 0, loss = 1.0994, acc = 0.5105, f1neg = 0.5519, f1pos = 0.4606, f1 = 0.5063
[Eval_batch(20)(2000,42000)] 2017-05-02 16:54:50.375598: step 0, loss = 1.0920, acc = 0.5580, f1neg = 0.6461, f1pos = 0.4115, f1 = 0.5288
[Eval_batch(21)(2000,44000)] 2017-05-02 16:54:50.890030: step 0, loss = 1.1014, acc = 0.4910, f1neg = 0.5504, f1pos = 0.4136, f1 = 0.4820
[Eval_batch(22)(2000,46000)] 2017-05-02 16:54:51.354743: step 0, loss = 1.1010, acc = 0.5040, f1neg = 0.5842, f1pos = 0.3854, f1 = 0.4848
[Eval_batch(23)(2000,48000)] 2017-05-02 16:54:51.868201: step 0, loss = 1.1023, acc = 0.4890, f1neg = 0.5346, f1pos = 0.4335, f1 = 0.4840
[Eval_batch(24)(2000,50000)] 2017-05-02 16:54:52.339326: step 0, loss = 1.1042, acc = 0.4710, f1neg = 0.5247, f1pos = 0.4036, f1 = 0.4642
[Eval_batch(25)(2000,52000)] 2017-05-02 16:54:52.794302: step 0, loss = 1.1027, acc = 0.4920, f1neg = 0.5436, f1pos = 0.4273, f1 = 0.4854
[Eval_batch(26)(2000,54000)] 2017-05-02 16:54:53.280851: step 0, loss = 1.0949, acc = 0.5325, f1neg = 0.6080, f1pos = 0.4211, f1 = 0.5145
[Eval_batch(27)(2000,56000)] 2017-05-02 16:54:54.004162: step 0, loss = 1.1002, acc = 0.5065, f1neg = 0.5777, f1pos = 0.4065, f1 = 0.4921
[Eval] 2017-05-02 16:54:54.004253: step 0, acc = 0.4969, f1 = 0.4858
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.57GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
[Test_batch(0)(2000,2000)] 2017-05-02 16:54:55.315685: step 0, loss = 1.0970, acc = 0.5200, f1neg = 0.6101, f1pos = 0.3758, f1 = 0.4929
[Test_batch(1)(2000,4000)] 2017-05-02 16:54:55.785354: step 0, loss = 1.0957, acc = 0.5470, f1neg = 0.6436, f1pos = 0.3786, f1 = 0.5111
[Test_batch(2)(2000,6000)] 2017-05-02 16:54:56.252915: step 0, loss = 1.0955, acc = 0.5305, f1neg = 0.6212, f1pos = 0.3826, f1 = 0.5019
[Test_batch(3)(2000,8000)] 2017-05-02 16:54:56.719053: step 0, loss = 1.0955, acc = 0.5375, f1neg = 0.6160, f1pos = 0.4186, f1 = 0.5173
[Test_batch(4)(2000,10000)] 2017-05-02 16:54:57.222563: step 0, loss = 1.0971, acc = 0.5140, f1neg = 0.5803, f1pos = 0.4228, f1 = 0.5016
[Test_batch(5)(2000,12000)] 2017-05-02 16:54:57.737291: step 0, loss = 1.0936, acc = 0.5415, f1neg = 0.6011, f1pos = 0.4609, f1 = 0.5310
[Test_batch(6)(2000,14000)] 2017-05-02 16:54:58.202510: step 0, loss = 1.0972, acc = 0.5320, f1neg = 0.5941, f1pos = 0.4475, f1 = 0.5208
[Test_batch(7)(2000,16000)] 2017-05-02 16:54:58.674164: step 0, loss = 1.0955, acc = 0.5350, f1neg = 0.6312, f1pos = 0.3708, f1 = 0.5010
[Test_batch(8)(2000,18000)] 2017-05-02 16:54:59.183488: step 0, loss = 1.0957, acc = 0.5390, f1neg = 0.6056, f1pos = 0.4452, f1 = 0.5254
[Test_batch(9)(2000,20000)] 2017-05-02 16:54:59.664756: step 0, loss = 1.0959, acc = 0.5185, f1neg = 0.5676, f1pos = 0.4569, f1 = 0.5122
[Test_batch(10)(2000,22000)] 2017-05-02 16:55:00.140963: step 0, loss = 1.1009, acc = 0.4985, f1neg = 0.5337, f1pos = 0.4575, f1 = 0.4956
[Test_batch(11)(2000,24000)] 2017-05-02 16:55:00.603478: step 0, loss = 1.0966, acc = 0.5165, f1neg = 0.5952, f1pos = 0.3998, f1 = 0.4975
[Test_batch(12)(2000,26000)] 2017-05-02 16:55:01.073862: step 0, loss = 1.0962, acc = 0.5210, f1neg = 0.6061, f1pos = 0.3890, f1 = 0.4976
[Test_batch(13)(2000,28000)] 2017-05-02 16:55:01.589014: step 0, loss = 1.0995, acc = 0.4980, f1neg = 0.5473, f1pos = 0.4366, f1 = 0.4920
[Test_batch(14)(2000,30000)] 2017-05-02 16:55:02.100767: step 0, loss = 1.1014, acc = 0.4945, f1neg = 0.5411, f1pos = 0.4374, f1 = 0.4892
[Test_batch(15)(2000,32000)] 2017-05-02 16:55:02.557073: step 0, loss = 1.0987, acc = 0.5245, f1neg = 0.5903, f1pos = 0.4336, f1 = 0.5119
[Test_batch(16)(2000,34000)] 2017-05-02 16:55:03.071793: step 0, loss = 1.1008, acc = 0.4895, f1neg = 0.5624, f1pos = 0.3875, f1 = 0.4749
[Test_batch(17)(2000,36000)] 2017-05-02 16:55:03.593328: step 0, loss = 1.0994, acc = 0.5145, f1neg = 0.5747, f1pos = 0.4345, f1 = 0.5046
[Test_batch(18)(2000,38000)] 2017-05-02 16:55:04.267205: step 0, loss = 1.0979, acc = 0.5120, f1neg = 0.5779, f1pos = 0.4218, f1 = 0.4998
[Test] 2017-05-02 16:55:04.267374: step 0, acc = 0.5202, f1 = 0.5041
[Status] 2017-05-02 16:55:04.267408: step 0, maxindex = 0, maxdev = 0.4969, maxtst = 0.5202
2017-05-02 16:55:19.280461: step 10, loss = 1.0251, acc = 0.5940 (215.3 examples/sec; 0.297 sec/batch)
2017-05-02 16:55:30.731942: step 20, loss = 0.9327, acc = 0.7840 (243.6 examples/sec; 0.263 sec/batch)
2017-05-02 16:55:41.784570: step 30, loss = 0.8702, acc = 0.7620 (228.5 examples/sec; 0.280 sec/batch)
2017-05-02 16:55:53.272320: step 40, loss = 0.8135, acc = 0.7740 (226.9 examples/sec; 0.282 sec/batch)
2017-05-02 16:56:04.897329: step 50, loss = 0.7422, acc = 0.8120 (221.3 examples/sec; 0.289 sec/batch)
2017-05-02 16:56:16.195761: step 60, loss = 0.7064, acc = 0.7940 (235.3 examples/sec; 0.272 sec/batch)
2017-05-02 16:56:27.470455: step 70, loss = 0.6489, acc = 0.8540 (230.1 examples/sec; 0.278 sec/batch)
2017-05-02 16:56:38.990754: step 80, loss = 0.6264, acc = 0.8120 (222.0 examples/sec; 0.288 sec/batch)
2017-05-02 16:56:50.398978: step 90, loss = 0.5705, acc = 0.8560 (224.1 examples/sec; 0.286 sec/batch)
2017-05-02 16:57:01.586924: step 100, loss = 0.5310, acc = 0.8820 (227.0 examples/sec; 0.282 sec/batch)
2017-05-02 16:57:12.834093: step 110, loss = 0.4944, acc = 0.8900 (230.5 examples/sec; 0.278 sec/batch)
2017-05-02 16:57:24.053425: step 120, loss = 0.4761, acc = 0.8760 (230.1 examples/sec; 0.278 sec/batch)
2017-05-02 16:57:35.150055: step 130, loss = 0.4753, acc = 0.8800 (227.5 examples/sec; 0.281 sec/batch)
2017-05-02 16:57:46.405531: step 140, loss = 0.4496, acc = 0.8780 (212.4 examples/sec; 0.301 sec/batch)
2017-05-02 16:57:57.455411: step 150, loss = 0.4147, acc = 0.9000 (231.1 examples/sec; 0.277 sec/batch)
2017-05-02 16:58:08.590702: step 160, loss = 0.4316, acc = 0.8820 (227.2 examples/sec; 0.282 sec/batch)
2017-05-02 16:58:19.809959: step 170, loss = 0.4238, acc = 0.8680 (228.3 examples/sec; 0.280 sec/batch)
2017-05-02 16:58:30.989683: step 180, loss = 0.3992, acc = 0.8840 (234.1 examples/sec; 0.273 sec/batch)
2017-05-02 16:58:41.871569: step 190, loss = 0.3832, acc = 0.8960 (236.4 examples/sec; 0.271 sec/batch)
2017-05-02 16:58:52.989988: step 200, loss = 0.3661, acc = 0.9020 (200.9 examples/sec; 0.319 sec/batch)
2017-05-02 16:59:03.942839: step 210, loss = 0.3584, acc = 0.8980 (233.8 examples/sec; 0.274 sec/batch)
2017-05-02 16:59:15.713773: step 220, loss = 0.3768, acc = 0.8880 (227.5 examples/sec; 0.281 sec/batch)
2017-05-02 16:59:27.532255: step 230, loss = 0.3780, acc = 0.8880 (207.6 examples/sec; 0.308 sec/batch)
2017-05-02 16:59:38.861756: step 240, loss = 0.3514, acc = 0.8980 (225.5 examples/sec; 0.284 sec/batch)
2017-05-02 16:59:50.152890: step 250, loss = 0.3375, acc = 0.9060 (226.8 examples/sec; 0.282 sec/batch)
2017-05-02 17:00:01.355315: step 260, loss = 0.3447, acc = 0.9140 (222.9 examples/sec; 0.287 sec/batch)
2017-05-02 17:00:12.553065: step 270, loss = 0.3570, acc = 0.9040 (227.9 examples/sec; 0.281 sec/batch)
2017-05-02 17:00:23.615940: step 280, loss = 0.3048, acc = 0.9280 (229.8 examples/sec; 0.278 sec/batch)
2017-05-02 17:00:35.586292: step 290, loss = 0.3331, acc = 0.9000 (222.3 examples/sec; 0.288 sec/batch)
2017-05-02 17:00:46.844207: step 300, loss = 0.3675, acc = 0.8820 (223.4 examples/sec; 0.286 sec/batch)
2017-05-02 17:00:57.996880: step 310, loss = 0.3394, acc = 0.8940 (218.0 examples/sec; 0.294 sec/batch)
2017-05-02 17:01:09.221090: step 320, loss = 0.3487, acc = 0.8860 (229.2 examples/sec; 0.279 sec/batch)
2017-05-02 17:01:20.304145: step 330, loss = 0.3136, acc = 0.9100 (234.0 examples/sec; 0.274 sec/batch)
2017-05-02 17:01:31.601083: step 340, loss = 0.3233, acc = 0.9000 (231.8 examples/sec; 0.276 sec/batch)
2017-05-02 17:01:43.035228: step 350, loss = 0.2957, acc = 0.9120 (236.9 examples/sec; 0.270 sec/batch)
2017-05-02 17:01:54.440759: step 360, loss = 0.2841, acc = 0.9280 (228.3 examples/sec; 0.280 sec/batch)
2017-05-02 17:02:05.834966: step 370, loss = 0.3083, acc = 0.9080 (222.4 examples/sec; 0.288 sec/batch)
2017-05-02 17:02:17.094193: step 380, loss = 0.3023, acc = 0.9180 (229.2 examples/sec; 0.279 sec/batch)
2017-05-02 17:02:28.150851: step 390, loss = 0.3173, acc = 0.8960 (226.6 examples/sec; 0.282 sec/batch)
2017-05-02 17:02:39.345311: step 400, loss = 0.3230, acc = 0.8940 (230.3 examples/sec; 0.278 sec/batch)
2017-05-02 17:02:50.524603: step 410, loss = 0.3235, acc = 0.9040 (233.3 examples/sec; 0.274 sec/batch)
2017-05-02 17:03:01.686914: step 420, loss = 0.3094, acc = 0.9120 (229.9 examples/sec; 0.278 sec/batch)
2017-05-02 17:03:12.677839: step 430, loss = 0.3128, acc = 0.9000 (242.7 examples/sec; 0.264 sec/batch)
2017-05-02 17:03:23.994231: step 440, loss = 0.3176, acc = 0.8840 (230.6 examples/sec; 0.278 sec/batch)
2017-05-02 17:03:35.238420: step 450, loss = 0.3198, acc = 0.8880 (221.7 examples/sec; 0.289 sec/batch)
2017-05-02 17:03:46.227865: step 460, loss = 0.3069, acc = 0.9100 (232.8 examples/sec; 0.275 sec/batch)
2017-05-02 17:03:57.475143: step 470, loss = 0.3099, acc = 0.9060 (234.1 examples/sec; 0.273 sec/batch)
2017-05-02 17:04:08.555740: step 480, loss = 0.3201, acc = 0.9000 (220.0 examples/sec; 0.291 sec/batch)
2017-05-02 17:04:19.786382: step 490, loss = 0.3087, acc = 0.9140 (233.3 examples/sec; 0.274 sec/batch)
2017-05-02 17:04:31.205738: step 500, loss = 0.2965, acc = 0.9140 (229.0 examples/sec; 0.279 sec/batch)
2017-05-02 17:04:42.498028: step 510, loss = 0.3055, acc = 0.9120 (225.3 examples/sec; 0.284 sec/batch)
2017-05-02 17:04:53.736416: step 520, loss = 0.2916, acc = 0.9160 (232.0 examples/sec; 0.276 sec/batch)
2017-05-02 17:05:04.903855: step 530, loss = 0.2921, acc = 0.9080 (222.3 examples/sec; 0.288 sec/batch)
2017-05-02 17:05:15.984879: step 540, loss = 0.3072, acc = 0.8980 (229.1 examples/sec; 0.279 sec/batch)
2017-05-02 17:05:27.024029: step 550, loss = 0.3162, acc = 0.8880 (234.2 examples/sec; 0.273 sec/batch)
2017-05-02 17:05:38.135215: step 560, loss = 0.2753, acc = 0.9160 (229.3 examples/sec; 0.279 sec/batch)
2017-05-02 17:05:49.342932: step 570, loss = 0.3019, acc = 0.9040 (236.3 examples/sec; 0.271 sec/batch)
2017-05-02 17:06:00.538072: step 580, loss = 0.2870, acc = 0.9040 (238.0 examples/sec; 0.269 sec/batch)
2017-05-02 17:06:11.886522: step 590, loss = 0.3553, acc = 0.8740 (224.9 examples/sec; 0.285 sec/batch)
2017-05-02 17:06:23.371582: step 600, loss = 0.2813, acc = 0.9240 (222.4 examples/sec; 0.288 sec/batch)
2017-05-02 17:06:35.297197: step 610, loss = 0.2816, acc = 0.9120 (224.8 examples/sec; 0.285 sec/batch)
2017-05-02 17:06:46.376610: step 620, loss = 0.2884, acc = 0.9020 (234.6 examples/sec; 0.273 sec/batch)
2017-05-02 17:06:57.874740: step 630, loss = 0.3004, acc = 0.8960 (234.7 examples/sec; 0.273 sec/batch)
2017-05-02 17:07:09.315363: step 640, loss = 0.2804, acc = 0.9160 (231.7 examples/sec; 0.276 sec/batch)
2017-05-02 17:07:20.520762: step 650, loss = 0.2785, acc = 0.9160 (229.7 examples/sec; 0.279 sec/batch)
2017-05-02 17:07:31.823643: step 660, loss = 0.2753, acc = 0.9260 (214.3 examples/sec; 0.299 sec/batch)
2017-05-02 17:07:43.089008: step 670, loss = 0.2714, acc = 0.9220 (231.3 examples/sec; 0.277 sec/batch)
2017-05-02 17:07:54.518742: step 680, loss = 0.2677, acc = 0.9180 (225.9 examples/sec; 0.283 sec/batch)
2017-05-02 17:08:05.789166: step 690, loss = 0.2722, acc = 0.9140 (230.6 examples/sec; 0.278 sec/batch)
2017-05-02 17:08:17.115283: step 700, loss = 0.3043, acc = 0.9120 (210.5 examples/sec; 0.304 sec/batch)
2017-05-02 17:08:28.528956: step 710, loss = 0.2890, acc = 0.9160 (175.7 examples/sec; 0.364 sec/batch)
2017-05-02 17:08:39.895183: step 720, loss = 0.2849, acc = 0.9040 (212.9 examples/sec; 0.301 sec/batch)
2017-05-02 17:08:50.989452: step 730, loss = 0.3274, acc = 0.8920 (232.4 examples/sec; 0.275 sec/batch)
2017-05-02 17:09:02.279053: step 740, loss = 0.2903, acc = 0.9100 (236.5 examples/sec; 0.271 sec/batch)
2017-05-02 17:09:13.646780: step 750, loss = 0.2963, acc = 0.9040 (234.2 examples/sec; 0.273 sec/batch)
2017-05-02 17:09:25.088823: step 760, loss = 0.2904, acc = 0.9000 (177.7 examples/sec; 0.360 sec/batch)
2017-05-02 17:09:36.399917: step 770, loss = 0.3082, acc = 0.8840 (230.7 examples/sec; 0.277 sec/batch)
2017-05-02 17:09:47.634618: step 780, loss = 0.2805, acc = 0.9140 (206.1 examples/sec; 0.311 sec/batch)
2017-05-02 17:09:58.918582: step 790, loss = 0.2966, acc = 0.8920 (234.8 examples/sec; 0.273 sec/batch)
2017-05-02 17:10:09.961154: step 800, loss = 0.2873, acc = 0.9100 (230.6 examples/sec; 0.278 sec/batch)
2017-05-02 17:10:21.044846: step 810, loss = 0.2839, acc = 0.9060 (230.6 examples/sec; 0.278 sec/batch)
2017-05-02 17:10:31.944730: step 820, loss = 0.2927, acc = 0.9060 (231.4 examples/sec; 0.277 sec/batch)
2017-05-02 17:10:42.993823: step 830, loss = 0.2579, acc = 0.9180 (232.3 examples/sec; 0.276 sec/batch)
2017-05-02 17:10:54.043972: step 840, loss = 0.2912, acc = 0.9060 (221.3 examples/sec; 0.289 sec/batch)
2017-05-02 17:11:05.028067: step 850, loss = 0.2696, acc = 0.9140 (219.3 examples/sec; 0.292 sec/batch)
2017-05-02 17:11:16.208417: step 860, loss = 0.2865, acc = 0.9100 (235.4 examples/sec; 0.272 sec/batch)
2017-05-02 17:11:27.322116: step 870, loss = 0.2713, acc = 0.9140 (228.4 examples/sec; 0.280 sec/batch)
2017-05-02 17:11:38.417270: step 880, loss = 0.2720, acc = 0.9060 (235.0 examples/sec; 0.272 sec/batch)
2017-05-02 17:11:49.616281: step 890, loss = 0.2953, acc = 0.9020 (231.3 examples/sec; 0.277 sec/batch)
2017-05-02 17:12:00.657479: step 900, loss = 0.2640, acc = 0.9080 (222.7 examples/sec; 0.287 sec/batch)
2017-05-02 17:12:11.603512: step 910, loss = 0.2509, acc = 0.9160 (234.5 examples/sec; 0.273 sec/batch)
2017-05-02 17:12:22.762390: step 920, loss = 0.2714, acc = 0.9080 (236.8 examples/sec; 0.270 sec/batch)
2017-05-02 17:12:33.873488: step 930, loss = 0.2604, acc = 0.9000 (209.0 examples/sec; 0.306 sec/batch)
2017-05-02 17:12:45.173816: step 940, loss = 0.2541, acc = 0.9320 (223.4 examples/sec; 0.286 sec/batch)
2017-05-02 17:12:56.314288: step 950, loss = 0.3249, acc = 0.8820 (237.6 examples/sec; 0.269 sec/batch)
2017-05-02 17:13:07.495384: step 960, loss = 0.2702, acc = 0.9120 (234.7 examples/sec; 0.273 sec/batch)
2017-05-02 17:13:18.692044: step 970, loss = 0.3044, acc = 0.8840 (236.7 examples/sec; 0.270 sec/batch)
2017-05-02 17:13:29.612003: step 980, loss = 0.2590, acc = 0.9220 (243.5 examples/sec; 0.263 sec/batch)
2017-05-02 17:13:40.690157: step 990, loss = 0.2709, acc = 0.9020 (241.9 examples/sec; 0.265 sec/batch)
2017-05-02 17:13:51.468530: step 1000, loss = 0.2764, acc = 0.8960 (243.1 examples/sec; 0.263 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 17:13:51.924712: step 1000, loss = 0.2235, acc = 0.9330, f1neg = 0.9261, f1pos = 0.9387, f1 = 0.9324
[Eval_batch(1)(2000,4000)] 2017-05-02 17:13:52.389164: step 1000, loss = 0.2309, acc = 0.9320, f1neg = 0.9211, f1pos = 0.9402, f1 = 0.9307
[Eval_batch(2)(2000,6000)] 2017-05-02 17:13:52.886655: step 1000, loss = 0.2457, acc = 0.9270, f1neg = 0.9225, f1pos = 0.9310, f1 = 0.9268
[Eval_batch(3)(2000,8000)] 2017-05-02 17:13:53.355891: step 1000, loss = 0.2554, acc = 0.9185, f1neg = 0.9192, f1pos = 0.9178, f1 = 0.9185
[Eval_batch(4)(2000,10000)] 2017-05-02 17:13:53.862005: step 1000, loss = 0.2504, acc = 0.9180, f1neg = 0.9204, f1pos = 0.9155, f1 = 0.9179
[Eval_batch(5)(2000,12000)] 2017-05-02 17:13:54.320081: step 1000, loss = 0.2399, acc = 0.9220, f1neg = 0.9170, f1pos = 0.9264, f1 = 0.9217
[Eval_batch(6)(2000,14000)] 2017-05-02 17:13:54.828235: step 1000, loss = 0.2346, acc = 0.9260, f1neg = 0.9242, f1pos = 0.9277, f1 = 0.9260
[Eval_batch(7)(2000,16000)] 2017-05-02 17:13:55.337183: step 1000, loss = 0.2309, acc = 0.9360, f1neg = 0.9244, f1pos = 0.9445, f1 = 0.9345
[Eval_batch(8)(2000,18000)] 2017-05-02 17:13:55.804705: step 1000, loss = 0.2381, acc = 0.9300, f1neg = 0.9200, f1pos = 0.9378, f1 = 0.9289
[Eval_batch(9)(2000,20000)] 2017-05-02 17:13:56.308776: step 1000, loss = 0.2351, acc = 0.9280, f1neg = 0.9230, f1pos = 0.9324, f1 = 0.9277
[Eval_batch(10)(2000,22000)] 2017-05-02 17:13:56.806655: step 1000, loss = 0.2376, acc = 0.9315, f1neg = 0.9272, f1pos = 0.9353, f1 = 0.9313
[Eval_batch(11)(2000,24000)] 2017-05-02 17:13:57.317426: step 1000, loss = 0.2501, acc = 0.9230, f1neg = 0.9107, f1pos = 0.9323, f1 = 0.9215
[Eval_batch(12)(2000,26000)] 2017-05-02 17:13:57.797153: step 1000, loss = 0.2348, acc = 0.9345, f1neg = 0.9322, f1pos = 0.9366, f1 = 0.9344
[Eval_batch(13)(2000,28000)] 2017-05-02 17:13:58.263381: step 1000, loss = 0.2285, acc = 0.9280, f1neg = 0.9101, f1pos = 0.9399, f1 = 0.9250
[Eval_batch(14)(2000,30000)] 2017-05-02 17:13:58.759620: step 1000, loss = 0.2475, acc = 0.9250, f1neg = 0.9103, f1pos = 0.9356, f1 = 0.9229
[Eval_batch(15)(2000,32000)] 2017-05-02 17:13:59.261891: step 1000, loss = 0.2390, acc = 0.9330, f1neg = 0.9276, f1pos = 0.9376, f1 = 0.9326
[Eval_batch(16)(2000,34000)] 2017-05-02 17:13:59.736485: step 1000, loss = 0.2256, acc = 0.9320, f1neg = 0.9288, f1pos = 0.9349, f1 = 0.9319
[Eval_batch(17)(2000,36000)] 2017-05-02 17:14:00.204756: step 1000, loss = 0.2440, acc = 0.9245, f1neg = 0.9245, f1pos = 0.9245, f1 = 0.9245
[Eval_batch(18)(2000,38000)] 2017-05-02 17:14:00.672154: step 1000, loss = 0.2468, acc = 0.9220, f1neg = 0.9251, f1pos = 0.9187, f1 = 0.9219
[Eval_batch(19)(2000,40000)] 2017-05-02 17:14:01.146552: step 1000, loss = 0.2292, acc = 0.9345, f1neg = 0.9240, f1pos = 0.9425, f1 = 0.9332
[Eval_batch(20)(2000,42000)] 2017-05-02 17:14:01.620050: step 1000, loss = 0.2304, acc = 0.9310, f1neg = 0.9387, f1pos = 0.9211, f1 = 0.9299
[Eval_batch(21)(2000,44000)] 2017-05-02 17:14:02.130413: step 1000, loss = 0.2295, acc = 0.9310, f1neg = 0.9249, f1pos = 0.9362, f1 = 0.9305
[Eval_batch(22)(2000,46000)] 2017-05-02 17:14:02.598591: step 1000, loss = 0.2352, acc = 0.9280, f1neg = 0.9283, f1pos = 0.9277, f1 = 0.9280
[Eval_batch(23)(2000,48000)] 2017-05-02 17:14:03.061946: step 1000, loss = 0.2440, acc = 0.9250, f1neg = 0.9146, f1pos = 0.9332, f1 = 0.9239
[Eval_batch(24)(2000,50000)] 2017-05-02 17:14:03.547670: step 1000, loss = 0.2334, acc = 0.9255, f1neg = 0.9145, f1pos = 0.9340, f1 = 0.9242
[Eval_batch(25)(2000,52000)] 2017-05-02 17:14:04.052508: step 1000, loss = 0.2272, acc = 0.9365, f1neg = 0.9303, f1pos = 0.9417, f1 = 0.9360
[Eval_batch(26)(2000,54000)] 2017-05-02 17:14:04.526051: step 1000, loss = 0.2294, acc = 0.9290, f1neg = 0.9325, f1pos = 0.9251, f1 = 0.9288
[Eval_batch(27)(2000,56000)] 2017-05-02 17:14:05.221151: step 1000, loss = 0.2164, acc = 0.9490, f1neg = 0.9463, f1pos = 0.9515, f1 = 0.9489
[Eval] 2017-05-02 17:14:05.221247: step 1000, acc = 0.9291, f1 = 0.9284
[Test_batch(0)(2000,2000)] 2017-05-02 17:14:05.684881: step 1000, loss = 0.2677, acc = 0.9170, f1neg = 0.9221, f1pos = 0.9112, f1 = 0.9166
[Test_batch(1)(2000,4000)] 2017-05-02 17:14:06.162895: step 1000, loss = 0.2570, acc = 0.9180, f1neg = 0.9272, f1pos = 0.9062, f1 = 0.9167
[Test_batch(2)(2000,6000)] 2017-05-02 17:14:06.610575: step 1000, loss = 0.2587, acc = 0.9210, f1neg = 0.9278, f1pos = 0.9128, f1 = 0.9203
[Test_batch(3)(2000,8000)] 2017-05-02 17:14:07.054855: step 1000, loss = 0.2793, acc = 0.9045, f1neg = 0.9115, f1pos = 0.8964, f1 = 0.9039
[Test_batch(4)(2000,10000)] 2017-05-02 17:14:07.572211: step 1000, loss = 0.2893, acc = 0.9065, f1neg = 0.9074, f1pos = 0.9056, f1 = 0.9065
[Test_batch(5)(2000,12000)] 2017-05-02 17:14:08.029434: step 1000, loss = 0.2955, acc = 0.8925, f1neg = 0.8938, f1pos = 0.8911, f1 = 0.8925
[Test_batch(6)(2000,14000)] 2017-05-02 17:14:08.536857: step 1000, loss = 0.2751, acc = 0.9100, f1neg = 0.9065, f1pos = 0.9132, f1 = 0.9099
[Test_batch(7)(2000,16000)] 2017-05-02 17:14:08.999667: step 1000, loss = 0.2614, acc = 0.9170, f1neg = 0.9252, f1pos = 0.9068, f1 = 0.9160
[Test_batch(8)(2000,18000)] 2017-05-02 17:14:09.510059: step 1000, loss = 0.2588, acc = 0.9295, f1neg = 0.9290, f1pos = 0.9300, f1 = 0.9295
[Test_batch(9)(2000,20000)] 2017-05-02 17:14:09.986209: step 1000, loss = 0.2871, acc = 0.9040, f1neg = 0.8985, f1pos = 0.9089, f1 = 0.9037
[Test_batch(10)(2000,22000)] 2017-05-02 17:14:10.466773: step 1000, loss = 0.2691, acc = 0.9100, f1neg = 0.8998, f1pos = 0.9183, f1 = 0.9091
[Test_batch(11)(2000,24000)] 2017-05-02 17:14:10.955890: step 1000, loss = 0.2662, acc = 0.9170, f1neg = 0.9192, f1pos = 0.9147, f1 = 0.9169
[Test_batch(12)(2000,26000)] 2017-05-02 17:14:11.458956: step 1000, loss = 0.2477, acc = 0.9245, f1neg = 0.9282, f1pos = 0.9204, f1 = 0.9243
[Test_batch(13)(2000,28000)] 2017-05-02 17:14:11.926296: step 1000, loss = 0.2859, acc = 0.9045, f1neg = 0.8945, f1pos = 0.9127, f1 = 0.9036
[Test_batch(14)(2000,30000)] 2017-05-02 17:14:12.403214: step 1000, loss = 0.2517, acc = 0.9225, f1neg = 0.9119, f1pos = 0.9308, f1 = 0.9214
[Test_batch(15)(2000,32000)] 2017-05-02 17:14:12.873919: step 1000, loss = 0.2569, acc = 0.9250, f1neg = 0.9245, f1pos = 0.9254, f1 = 0.9250
[Test_batch(16)(2000,34000)] 2017-05-02 17:14:13.362972: step 1000, loss = 0.2597, acc = 0.9240, f1neg = 0.9228, f1pos = 0.9251, f1 = 0.9240
[Test_batch(17)(2000,36000)] 2017-05-02 17:14:13.856917: step 1000, loss = 0.2389, acc = 0.9330, f1neg = 0.9268, f1pos = 0.9382, f1 = 0.9325
[Test_batch(18)(2000,38000)] 2017-05-02 17:14:14.491025: step 1000, loss = 0.2246, acc = 0.9385, f1neg = 0.9367, f1pos = 0.9402, f1 = 0.9385
[Test] 2017-05-02 17:14:14.491114: step 1000, acc = 0.9168, f1 = 0.9164
[Status] 2017-05-02 17:14:14.491140: step 1000, maxindex = 1000, maxdev = 0.9291, maxtst = 0.9168
2017-05-02 17:14:29.465946: step 1010, loss = 0.2867, acc = 0.9140 (238.4 examples/sec; 0.268 sec/batch)
2017-05-02 17:14:40.502946: step 1020, loss = 0.2784, acc = 0.9040 (232.6 examples/sec; 0.275 sec/batch)
2017-05-02 17:14:51.637573: step 1030, loss = 0.2665, acc = 0.9100 (219.8 examples/sec; 0.291 sec/batch)
2017-05-02 17:15:03.103641: step 1040, loss = 0.2607, acc = 0.9060 (232.6 examples/sec; 0.275 sec/batch)
2017-05-02 17:15:14.151803: step 1050, loss = 0.2667, acc = 0.9160 (228.5 examples/sec; 0.280 sec/batch)
2017-05-02 17:15:25.046119: step 1060, loss = 0.2715, acc = 0.9220 (230.6 examples/sec; 0.278 sec/batch)
2017-05-02 17:15:36.159096: step 1070, loss = 0.2718, acc = 0.9160 (224.8 examples/sec; 0.285 sec/batch)
2017-05-02 17:15:47.099262: step 1080, loss = 0.2672, acc = 0.9200 (238.1 examples/sec; 0.269 sec/batch)
2017-05-02 17:15:58.141228: step 1090, loss = 0.2362, acc = 0.9360 (236.9 examples/sec; 0.270 sec/batch)
2017-05-02 17:16:09.143392: step 1100, loss = 0.2506, acc = 0.9300 (237.3 examples/sec; 0.270 sec/batch)
2017-05-02 17:16:20.180619: step 1110, loss = 0.2546, acc = 0.9200 (243.3 examples/sec; 0.263 sec/batch)
2017-05-02 17:16:31.568834: step 1120, loss = 0.2533, acc = 0.9080 (190.1 examples/sec; 0.337 sec/batch)
2017-05-02 17:16:42.407683: step 1130, loss = 0.2790, acc = 0.8940 (237.5 examples/sec; 0.269 sec/batch)
2017-05-02 17:16:53.634549: step 1140, loss = 0.2512, acc = 0.9260 (228.4 examples/sec; 0.280 sec/batch)
2017-05-02 17:17:04.716112: step 1150, loss = 0.2818, acc = 0.9180 (231.3 examples/sec; 0.277 sec/batch)
2017-05-02 17:17:15.845471: step 1160, loss = 0.2418, acc = 0.9260 (229.3 examples/sec; 0.279 sec/batch)
2017-05-02 17:17:26.683944: step 1170, loss = 0.2440, acc = 0.9260 (241.5 examples/sec; 0.265 sec/batch)
2017-05-02 17:17:37.753151: step 1180, loss = 0.2704, acc = 0.9020 (228.2 examples/sec; 0.280 sec/batch)
2017-05-02 17:17:48.775951: step 1190, loss = 0.2366, acc = 0.9340 (229.7 examples/sec; 0.279 sec/batch)
2017-05-02 17:17:59.831661: step 1200, loss = 0.2693, acc = 0.9300 (238.4 examples/sec; 0.268 sec/batch)
2017-05-02 17:18:10.904885: step 1210, loss = 0.2787, acc = 0.9140 (227.7 examples/sec; 0.281 sec/batch)
2017-05-02 17:18:22.411818: step 1220, loss = 0.2600, acc = 0.9140 (219.2 examples/sec; 0.292 sec/batch)
2017-05-02 17:18:33.414187: step 1230, loss = 0.2505, acc = 0.9180 (224.3 examples/sec; 0.285 sec/batch)
2017-05-02 17:18:44.237367: step 1240, loss = 0.2410, acc = 0.9260 (248.7 examples/sec; 0.257 sec/batch)
2017-05-02 17:18:55.270345: step 1250, loss = 0.2423, acc = 0.9320 (236.9 examples/sec; 0.270 sec/batch)
2017-05-02 17:19:06.002277: step 1260, loss = 0.2308, acc = 0.9320 (236.0 examples/sec; 0.271 sec/batch)
2017-05-02 17:19:17.013217: step 1270, loss = 0.2700, acc = 0.9140 (238.2 examples/sec; 0.269 sec/batch)
2017-05-02 17:19:27.957778: step 1280, loss = 0.3150, acc = 0.8700 (237.5 examples/sec; 0.269 sec/batch)
2017-05-02 17:19:38.834071: step 1290, loss = 0.2601, acc = 0.9140 (240.8 examples/sec; 0.266 sec/batch)
2017-05-02 17:19:49.724397: step 1300, loss = 0.2604, acc = 0.9200 (227.0 examples/sec; 0.282 sec/batch)
2017-05-02 17:20:00.710713: step 1310, loss = 0.2753, acc = 0.9040 (225.7 examples/sec; 0.284 sec/batch)
2017-05-02 17:20:11.705444: step 1320, loss = 0.2957, acc = 0.8980 (236.6 examples/sec; 0.270 sec/batch)
2017-05-02 17:20:22.709595: step 1330, loss = 0.2430, acc = 0.9120 (237.4 examples/sec; 0.270 sec/batch)
2017-05-02 17:20:33.554715: step 1340, loss = 0.2721, acc = 0.9100 (240.0 examples/sec; 0.267 sec/batch)
2017-05-02 17:20:44.459208: step 1350, loss = 0.2771, acc = 0.9100 (230.5 examples/sec; 0.278 sec/batch)
2017-05-02 17:20:55.409403: step 1360, loss = 0.2768, acc = 0.8960 (235.7 examples/sec; 0.272 sec/batch)
2017-05-02 17:21:06.422178: step 1370, loss = 0.2869, acc = 0.9060 (232.9 examples/sec; 0.275 sec/batch)
2017-05-02 17:21:17.227854: step 1380, loss = 0.2442, acc = 0.9220 (242.7 examples/sec; 0.264 sec/batch)
2017-05-02 17:21:28.274903: step 1390, loss = 0.2498, acc = 0.9260 (231.1 examples/sec; 0.277 sec/batch)
2017-05-02 17:21:39.227418: step 1400, loss = 0.2451, acc = 0.9320 (219.6 examples/sec; 0.292 sec/batch)
2017-05-02 17:21:50.316308: step 1410, loss = 0.2524, acc = 0.9180 (231.6 examples/sec; 0.276 sec/batch)
2017-05-02 17:22:01.418176: step 1420, loss = 0.2503, acc = 0.9240 (238.6 examples/sec; 0.268 sec/batch)
2017-05-02 17:22:12.437603: step 1430, loss = 0.2388, acc = 0.9380 (235.6 examples/sec; 0.272 sec/batch)
2017-05-02 17:22:23.442355: step 1440, loss = 0.2615, acc = 0.9080 (239.0 examples/sec; 0.268 sec/batch)
2017-05-02 17:22:34.583295: step 1450, loss = 0.2467, acc = 0.9300 (235.4 examples/sec; 0.272 sec/batch)
2017-05-02 17:22:45.615857: step 1460, loss = 0.2425, acc = 0.9180 (235.9 examples/sec; 0.271 sec/batch)
2017-05-02 17:22:56.922741: step 1470, loss = 0.2294, acc = 0.9300 (237.6 examples/sec; 0.269 sec/batch)
2017-05-02 17:23:07.776025: step 1480, loss = 0.2629, acc = 0.9040 (229.3 examples/sec; 0.279 sec/batch)
2017-05-02 17:23:18.694817: step 1490, loss = 0.2808, acc = 0.9020 (211.9 examples/sec; 0.302 sec/batch)
2017-05-02 17:23:29.471616: step 1500, loss = 0.2516, acc = 0.9100 (224.5 examples/sec; 0.285 sec/batch)
2017-05-02 17:23:40.280239: step 1510, loss = 0.2676, acc = 0.9220 (229.7 examples/sec; 0.279 sec/batch)
2017-05-02 17:23:51.261496: step 1520, loss = 0.2609, acc = 0.9140 (238.2 examples/sec; 0.269 sec/batch)
2017-05-02 17:24:02.315598: step 1530, loss = 0.2762, acc = 0.8940 (229.5 examples/sec; 0.279 sec/batch)
2017-05-02 17:24:13.056048: step 1540, loss = 0.2243, acc = 0.9340 (235.1 examples/sec; 0.272 sec/batch)
2017-05-02 17:24:24.063489: step 1550, loss = 0.2548, acc = 0.9140 (231.9 examples/sec; 0.276 sec/batch)
2017-05-02 17:24:34.831773: step 1560, loss = 0.2470, acc = 0.9200 (237.2 examples/sec; 0.270 sec/batch)
2017-05-02 17:24:46.091167: step 1570, loss = 0.2478, acc = 0.9220 (235.9 examples/sec; 0.271 sec/batch)
2017-05-02 17:24:57.040963: step 1580, loss = 0.2494, acc = 0.9060 (238.5 examples/sec; 0.268 sec/batch)
2017-05-02 17:25:08.222996: step 1590, loss = 0.2674, acc = 0.9060 (236.5 examples/sec; 0.271 sec/batch)
2017-05-02 17:25:19.133801: step 1600, loss = 0.2469, acc = 0.9280 (229.2 examples/sec; 0.279 sec/batch)
2017-05-02 17:25:30.032329: step 1610, loss = 0.2142, acc = 0.9360 (235.8 examples/sec; 0.271 sec/batch)
2017-05-02 17:25:40.796868: step 1620, loss = 0.2255, acc = 0.9260 (232.0 examples/sec; 0.276 sec/batch)
2017-05-02 17:25:51.867336: step 1630, loss = 0.2901, acc = 0.8920 (232.9 examples/sec; 0.275 sec/batch)
2017-05-02 17:26:02.757900: step 1640, loss = 0.2517, acc = 0.9160 (231.2 examples/sec; 0.277 sec/batch)
2017-05-02 17:26:13.725166: step 1650, loss = 0.2559, acc = 0.9160 (239.4 examples/sec; 0.267 sec/batch)
2017-05-02 17:26:24.750450: step 1660, loss = 0.2345, acc = 0.9200 (225.0 examples/sec; 0.284 sec/batch)
2017-05-02 17:26:36.160076: step 1670, loss = 0.2443, acc = 0.9320 (240.2 examples/sec; 0.266 sec/batch)
2017-05-02 17:26:47.013415: step 1680, loss = 0.2772, acc = 0.8900 (237.8 examples/sec; 0.269 sec/batch)
2017-05-02 17:26:58.502788: step 1690, loss = 0.2590, acc = 0.9100 (238.0 examples/sec; 0.269 sec/batch)
2017-05-02 17:27:10.112948: step 1700, loss = 0.2422, acc = 0.9200 (225.6 examples/sec; 0.284 sec/batch)
2017-05-02 17:27:21.417817: step 1710, loss = 0.2640, acc = 0.9060 (223.6 examples/sec; 0.286 sec/batch)
2017-05-02 17:27:32.493664: step 1720, loss = 0.2195, acc = 0.9240 (232.2 examples/sec; 0.276 sec/batch)
2017-05-02 17:27:43.745670: step 1730, loss = 0.2480, acc = 0.9140 (220.6 examples/sec; 0.290 sec/batch)
2017-05-02 17:27:54.926592: step 1740, loss = 0.2310, acc = 0.9240 (229.7 examples/sec; 0.279 sec/batch)
2017-05-02 17:28:06.077721: step 1750, loss = 0.2479, acc = 0.9200 (227.1 examples/sec; 0.282 sec/batch)
2017-05-02 17:28:17.128366: step 1760, loss = 0.2527, acc = 0.9100 (234.3 examples/sec; 0.273 sec/batch)
2017-05-02 17:28:28.471345: step 1770, loss = 0.2617, acc = 0.9060 (192.4 examples/sec; 0.333 sec/batch)
2017-05-02 17:28:39.420283: step 1780, loss = 0.2475, acc = 0.9240 (234.4 examples/sec; 0.273 sec/batch)
2017-05-02 17:28:50.537244: step 1790, loss = 0.2511, acc = 0.9260 (235.2 examples/sec; 0.272 sec/batch)
2017-05-02 17:29:01.650705: step 1800, loss = 0.2387, acc = 0.9160 (231.9 examples/sec; 0.276 sec/batch)
2017-05-02 17:29:12.988054: step 1810, loss = 0.2445, acc = 0.9200 (216.0 examples/sec; 0.296 sec/batch)
2017-05-02 17:29:24.989604: step 1820, loss = 0.2289, acc = 0.9280 (234.9 examples/sec; 0.272 sec/batch)
2017-05-02 17:29:36.101647: step 1830, loss = 0.2306, acc = 0.9260 (232.9 examples/sec; 0.275 sec/batch)
2017-05-02 17:29:47.222724: step 1840, loss = 0.2660, acc = 0.9120 (234.3 examples/sec; 0.273 sec/batch)
2017-05-02 17:29:58.239738: step 1850, loss = 0.2249, acc = 0.9400 (234.1 examples/sec; 0.273 sec/batch)
2017-05-02 17:30:09.383042: step 1860, loss = 0.2651, acc = 0.8960 (236.7 examples/sec; 0.270 sec/batch)
2017-05-02 17:30:20.776465: step 1870, loss = 0.2371, acc = 0.9220 (219.8 examples/sec; 0.291 sec/batch)
2017-05-02 17:30:32.193928: step 1880, loss = 0.2194, acc = 0.9320 (227.8 examples/sec; 0.281 sec/batch)
2017-05-02 17:30:43.335798: step 1890, loss = 0.2384, acc = 0.9200 (227.3 examples/sec; 0.282 sec/batch)
2017-05-02 17:30:54.563521: step 1900, loss = 0.2378, acc = 0.9200 (231.9 examples/sec; 0.276 sec/batch)
2017-05-02 17:31:06.012882: step 1910, loss = 0.2205, acc = 0.9300 (221.8 examples/sec; 0.289 sec/batch)
2017-05-02 17:31:17.278878: step 1920, loss = 0.2405, acc = 0.9240 (232.2 examples/sec; 0.276 sec/batch)
2017-05-02 17:31:28.662653: step 1930, loss = 0.2409, acc = 0.9220 (228.5 examples/sec; 0.280 sec/batch)
2017-05-02 17:31:40.061341: step 1940, loss = 0.2365, acc = 0.9200 (224.7 examples/sec; 0.285 sec/batch)
2017-05-02 17:31:51.121826: step 1950, loss = 0.2452, acc = 0.9280 (223.8 examples/sec; 0.286 sec/batch)
2017-05-02 17:32:02.337289: step 1960, loss = 0.2471, acc = 0.9200 (227.8 examples/sec; 0.281 sec/batch)
2017-05-02 17:32:13.608017: step 1970, loss = 0.2264, acc = 0.9340 (221.8 examples/sec; 0.289 sec/batch)
2017-05-02 17:32:24.762839: step 1980, loss = 0.2443, acc = 0.9140 (230.1 examples/sec; 0.278 sec/batch)
2017-05-02 17:32:36.463029: step 1990, loss = 0.2664, acc = 0.9140 (227.3 examples/sec; 0.282 sec/batch)
2017-05-02 17:32:47.504316: step 2000, loss = 0.2389, acc = 0.9180 (233.2 examples/sec; 0.274 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 17:32:47.991332: step 2000, loss = 0.1939, acc = 0.9415, f1neg = 0.9361, f1pos = 0.9461, f1 = 0.9411
[Eval_batch(1)(2000,4000)] 2017-05-02 17:32:48.421717: step 2000, loss = 0.2030, acc = 0.9385, f1neg = 0.9292, f1pos = 0.9456, f1 = 0.9374
[Eval_batch(2)(2000,6000)] 2017-05-02 17:32:48.894369: step 2000, loss = 0.2171, acc = 0.9315, f1neg = 0.9283, f1pos = 0.9344, f1 = 0.9314
[Eval_batch(3)(2000,8000)] 2017-05-02 17:32:49.402818: step 2000, loss = 0.2240, acc = 0.9245, f1neg = 0.9260, f1pos = 0.9229, f1 = 0.9245
[Eval_batch(4)(2000,10000)] 2017-05-02 17:32:49.874767: step 2000, loss = 0.2197, acc = 0.9315, f1neg = 0.9341, f1pos = 0.9287, f1 = 0.9314
[Eval_batch(5)(2000,12000)] 2017-05-02 17:32:50.376850: step 2000, loss = 0.2110, acc = 0.9290, f1neg = 0.9252, f1pos = 0.9324, f1 = 0.9288
[Eval_batch(6)(2000,14000)] 2017-05-02 17:32:50.878044: step 2000, loss = 0.2022, acc = 0.9360, f1neg = 0.9350, f1pos = 0.9369, f1 = 0.9360
[Eval_batch(7)(2000,16000)] 2017-05-02 17:32:51.344425: step 2000, loss = 0.2024, acc = 0.9435, f1neg = 0.9340, f1pos = 0.9506, f1 = 0.9423
[Eval_batch(8)(2000,18000)] 2017-05-02 17:32:51.821091: step 2000, loss = 0.2071, acc = 0.9335, f1neg = 0.9251, f1pos = 0.9402, f1 = 0.9326
[Eval_batch(9)(2000,20000)] 2017-05-02 17:32:52.288704: step 2000, loss = 0.2063, acc = 0.9360, f1neg = 0.9325, f1pos = 0.9392, f1 = 0.9358
[Eval_batch(10)(2000,22000)] 2017-05-02 17:32:52.766476: step 2000, loss = 0.2105, acc = 0.9385, f1neg = 0.9351, f1pos = 0.9416, f1 = 0.9383
[Eval_batch(11)(2000,24000)] 2017-05-02 17:32:53.276793: step 2000, loss = 0.2199, acc = 0.9310, f1neg = 0.9211, f1pos = 0.9387, f1 = 0.9299
[Eval_batch(12)(2000,26000)] 2017-05-02 17:32:53.784145: step 2000, loss = 0.2061, acc = 0.9360, f1neg = 0.9344, f1pos = 0.9376, f1 = 0.9360
[Eval_batch(13)(2000,28000)] 2017-05-02 17:32:54.264679: step 2000, loss = 0.1990, acc = 0.9390, f1neg = 0.9244, f1pos = 0.9489, f1 = 0.9366
[Eval_batch(14)(2000,30000)] 2017-05-02 17:32:54.773228: step 2000, loss = 0.2193, acc = 0.9355, f1neg = 0.9236, f1pos = 0.9442, f1 = 0.9339
[Eval_batch(15)(2000,32000)] 2017-05-02 17:32:55.246441: step 2000, loss = 0.2087, acc = 0.9400, f1neg = 0.9356, f1pos = 0.9439, f1 = 0.9397
[Eval_batch(16)(2000,34000)] 2017-05-02 17:32:55.725785: step 2000, loss = 0.1960, acc = 0.9385, f1neg = 0.9363, f1pos = 0.9406, f1 = 0.9384
[Eval_batch(17)(2000,36000)] 2017-05-02 17:32:56.205742: step 2000, loss = 0.2149, acc = 0.9345, f1neg = 0.9350, f1pos = 0.9340, f1 = 0.9345
[Eval_batch(18)(2000,38000)] 2017-05-02 17:32:56.678410: step 2000, loss = 0.2162, acc = 0.9320, f1neg = 0.9355, f1pos = 0.9281, f1 = 0.9318
[Eval_batch(19)(2000,40000)] 2017-05-02 17:32:57.181895: step 2000, loss = 0.1977, acc = 0.9475, f1neg = 0.9401, f1pos = 0.9533, f1 = 0.9467
[Eval_batch(20)(2000,42000)] 2017-05-02 17:32:57.646999: step 2000, loss = 0.1987, acc = 0.9415, f1neg = 0.9484, f1pos = 0.9324, f1 = 0.9404
[Eval_batch(21)(2000,44000)] 2017-05-02 17:32:58.101188: step 2000, loss = 0.1987, acc = 0.9460, f1neg = 0.9419, f1pos = 0.9495, f1 = 0.9457
[Eval_batch(22)(2000,46000)] 2017-05-02 17:32:58.606626: step 2000, loss = 0.2009, acc = 0.9360, f1neg = 0.9369, f1pos = 0.9350, f1 = 0.9360
[Eval_batch(23)(2000,48000)] 2017-05-02 17:32:59.079195: step 2000, loss = 0.2135, acc = 0.9360, f1neg = 0.9281, f1pos = 0.9423, f1 = 0.9352
[Eval_batch(24)(2000,50000)] 2017-05-02 17:32:59.560857: step 2000, loss = 0.2020, acc = 0.9355, f1neg = 0.9266, f1pos = 0.9425, f1 = 0.9345
[Eval_batch(25)(2000,52000)] 2017-05-02 17:33:00.073396: step 2000, loss = 0.1943, acc = 0.9440, f1neg = 0.9392, f1pos = 0.9481, f1 = 0.9436
[Eval_batch(26)(2000,54000)] 2017-05-02 17:33:00.542527: step 2000, loss = 0.1960, acc = 0.9415, f1neg = 0.9448, f1pos = 0.9377, f1 = 0.9413
[Eval_batch(27)(2000,56000)] 2017-05-02 17:33:01.257632: step 2000, loss = 0.1856, acc = 0.9530, f1neg = 0.9509, f1pos = 0.9549, f1 = 0.9529
[Eval] 2017-05-02 17:33:01.257717: step 2000, acc = 0.9376, f1 = 0.9370
[Test_batch(0)(2000,2000)] 2017-05-02 17:33:01.758579: step 2000, loss = 0.2388, acc = 0.9230, f1neg = 0.9281, f1pos = 0.9171, f1 = 0.9226
[Test_batch(1)(2000,4000)] 2017-05-02 17:33:02.229690: step 2000, loss = 0.2265, acc = 0.9260, f1neg = 0.9350, f1pos = 0.9141, f1 = 0.9245
[Test_batch(2)(2000,6000)] 2017-05-02 17:33:02.702028: step 2000, loss = 0.2283, acc = 0.9310, f1neg = 0.9374, f1pos = 0.9231, f1 = 0.9303
[Test_batch(3)(2000,8000)] 2017-05-02 17:33:03.173180: step 2000, loss = 0.2481, acc = 0.9130, f1neg = 0.9200, f1pos = 0.9047, f1 = 0.9123
[Test_batch(4)(2000,10000)] 2017-05-02 17:33:03.652925: step 2000, loss = 0.2586, acc = 0.9155, f1neg = 0.9170, f1pos = 0.9140, f1 = 0.9155
[Test_batch(5)(2000,12000)] 2017-05-02 17:33:04.123593: step 2000, loss = 0.2634, acc = 0.9090, f1neg = 0.9108, f1pos = 0.9071, f1 = 0.9090
[Test_batch(6)(2000,14000)] 2017-05-02 17:33:04.633077: step 2000, loss = 0.2437, acc = 0.9150, f1neg = 0.9127, f1pos = 0.9172, f1 = 0.9149
[Test_batch(7)(2000,16000)] 2017-05-02 17:33:05.103677: step 2000, loss = 0.2320, acc = 0.9245, f1neg = 0.9326, f1pos = 0.9142, f1 = 0.9234
[Test_batch(8)(2000,18000)] 2017-05-02 17:33:05.616644: step 2000, loss = 0.2277, acc = 0.9335, f1neg = 0.9335, f1pos = 0.9335, f1 = 0.9335
[Test_batch(9)(2000,20000)] 2017-05-02 17:33:06.102911: step 2000, loss = 0.2592, acc = 0.9105, f1neg = 0.9069, f1pos = 0.9138, f1 = 0.9104
[Test_batch(10)(2000,22000)] 2017-05-02 17:33:06.608024: step 2000, loss = 0.2424, acc = 0.9170, f1neg = 0.9086, f1pos = 0.9240, f1 = 0.9163
[Test_batch(11)(2000,24000)] 2017-05-02 17:33:07.090007: step 2000, loss = 0.2358, acc = 0.9310, f1neg = 0.9337, f1pos = 0.9281, f1 = 0.9309
[Test_batch(12)(2000,26000)] 2017-05-02 17:33:07.567086: step 2000, loss = 0.2189, acc = 0.9325, f1neg = 0.9362, f1pos = 0.9284, f1 = 0.9323
[Test_batch(13)(2000,28000)] 2017-05-02 17:33:08.044871: step 2000, loss = 0.2567, acc = 0.9090, f1neg = 0.9007, f1pos = 0.9161, f1 = 0.9084
[Test_batch(14)(2000,30000)] 2017-05-02 17:33:08.536158: step 2000, loss = 0.2230, acc = 0.9260, f1neg = 0.9167, f1pos = 0.9335, f1 = 0.9251
[Test_batch(15)(2000,32000)] 2017-05-02 17:33:09.036993: step 2000, loss = 0.2289, acc = 0.9305, f1neg = 0.9308, f1pos = 0.9302, f1 = 0.9305
[Test_batch(16)(2000,34000)] 2017-05-02 17:33:09.505723: step 2000, loss = 0.2261, acc = 0.9290, f1neg = 0.9286, f1pos = 0.9294, f1 = 0.9290
[Test_batch(17)(2000,36000)] 2017-05-02 17:33:09.977642: step 2000, loss = 0.2070, acc = 0.9430, f1neg = 0.9381, f1pos = 0.9472, f1 = 0.9426
[Test_batch(18)(2000,38000)] 2017-05-02 17:33:10.604779: step 2000, loss = 0.1919, acc = 0.9485, f1neg = 0.9478, f1pos = 0.9491, f1 = 0.9485
[Test] 2017-05-02 17:33:10.604880: step 2000, acc = 0.9246, f1 = 0.9242
[Status] 2017-05-02 17:33:10.604907: step 2000, maxindex = 2000, maxdev = 0.9376, maxtst = 0.9246
2017-05-02 17:33:24.419086: step 2010, loss = 0.2438, acc = 0.9260 (236.8 examples/sec; 0.270 sec/batch)
2017-05-02 17:33:36.459852: step 2020, loss = 0.2408, acc = 0.9160 (223.0 examples/sec; 0.287 sec/batch)
2017-05-02 17:33:47.458380: step 2030, loss = 0.2496, acc = 0.9180 (231.7 examples/sec; 0.276 sec/batch)
2017-05-02 17:33:58.503344: step 2040, loss = 0.2245, acc = 0.9460 (236.6 examples/sec; 0.271 sec/batch)
2017-05-02 17:34:09.490870: step 2050, loss = 0.2131, acc = 0.9400 (228.8 examples/sec; 0.280 sec/batch)
2017-05-02 17:34:20.624790: step 2060, loss = 0.2519, acc = 0.8900 (225.5 examples/sec; 0.284 sec/batch)
2017-05-02 17:34:31.734264: step 2070, loss = 0.2533, acc = 0.9200 (235.8 examples/sec; 0.271 sec/batch)
2017-05-02 17:34:42.717074: step 2080, loss = 0.2499, acc = 0.9240 (237.8 examples/sec; 0.269 sec/batch)
2017-05-02 17:34:53.677972: step 2090, loss = 0.2111, acc = 0.9380 (231.0 examples/sec; 0.277 sec/batch)
2017-05-02 17:35:04.539653: step 2100, loss = 0.2136, acc = 0.9300 (232.8 examples/sec; 0.275 sec/batch)
2017-05-02 17:35:15.496307: step 2110, loss = 0.2241, acc = 0.9360 (226.9 examples/sec; 0.282 sec/batch)
2017-05-02 17:35:26.435334: step 2120, loss = 0.2575, acc = 0.9080 (223.3 examples/sec; 0.287 sec/batch)
2017-05-02 17:35:37.527034: step 2130, loss = 0.2122, acc = 0.9280 (232.8 examples/sec; 0.275 sec/batch)
2017-05-02 17:35:48.769658: step 2140, loss = 0.2541, acc = 0.9200 (195.7 examples/sec; 0.327 sec/batch)
2017-05-02 17:35:59.745191: step 2150, loss = 0.2390, acc = 0.9180 (238.9 examples/sec; 0.268 sec/batch)
2017-05-02 17:36:10.979462: step 2160, loss = 0.2183, acc = 0.9280 (231.5 examples/sec; 0.276 sec/batch)
2017-05-02 17:36:21.797754: step 2170, loss = 0.2419, acc = 0.9140 (238.2 examples/sec; 0.269 sec/batch)
2017-05-02 17:36:32.862656: step 2180, loss = 0.2549, acc = 0.9180 (238.3 examples/sec; 0.269 sec/batch)
2017-05-02 17:36:43.881724: step 2190, loss = 0.2242, acc = 0.9140 (226.0 examples/sec; 0.283 sec/batch)
2017-05-02 17:36:55.472433: step 2200, loss = 0.2083, acc = 0.9360 (245.1 examples/sec; 0.261 sec/batch)
2017-05-02 17:37:06.406544: step 2210, loss = 0.2817, acc = 0.9020 (229.2 examples/sec; 0.279 sec/batch)
2017-05-02 17:37:17.553276: step 2220, loss = 0.2271, acc = 0.9280 (220.6 examples/sec; 0.290 sec/batch)
2017-05-02 17:37:28.729348: step 2230, loss = 0.2224, acc = 0.9280 (235.1 examples/sec; 0.272 sec/batch)
2017-05-02 17:37:39.828381: step 2240, loss = 0.2417, acc = 0.9000 (242.2 examples/sec; 0.264 sec/batch)
2017-05-02 17:37:50.878018: step 2250, loss = 0.2045, acc = 0.9440 (225.5 examples/sec; 0.284 sec/batch)
2017-05-02 17:38:02.578057: step 2260, loss = 0.2602, acc = 0.9000 (228.5 examples/sec; 0.280 sec/batch)
2017-05-02 17:38:13.662129: step 2270, loss = 0.2470, acc = 0.9180 (236.3 examples/sec; 0.271 sec/batch)
2017-05-02 17:38:24.719884: step 2280, loss = 0.2256, acc = 0.9320 (237.2 examples/sec; 0.270 sec/batch)
2017-05-02 17:38:35.800860: step 2290, loss = 0.2401, acc = 0.9280 (223.3 examples/sec; 0.287 sec/batch)
2017-05-02 17:38:46.869869: step 2300, loss = 0.2305, acc = 0.9180 (227.6 examples/sec; 0.281 sec/batch)
2017-05-02 17:38:58.023081: step 2310, loss = 0.2160, acc = 0.9380 (233.2 examples/sec; 0.274 sec/batch)
2017-05-02 17:39:09.183328: step 2320, loss = 0.2210, acc = 0.9220 (227.4 examples/sec; 0.281 sec/batch)
2017-05-02 17:39:20.129062: step 2330, loss = 0.2245, acc = 0.9240 (241.7 examples/sec; 0.265 sec/batch)
2017-05-02 17:39:31.250576: step 2340, loss = 0.2198, acc = 0.9300 (233.1 examples/sec; 0.275 sec/batch)
2017-05-02 17:39:42.296056: step 2350, loss = 0.2316, acc = 0.9240 (219.8 examples/sec; 0.291 sec/batch)
2017-05-02 17:39:53.271137: step 2360, loss = 0.2831, acc = 0.9040 (231.0 examples/sec; 0.277 sec/batch)
2017-05-02 17:40:04.288355: step 2370, loss = 0.2151, acc = 0.9320 (241.0 examples/sec; 0.266 sec/batch)
2017-05-02 17:40:15.414336: step 2380, loss = 0.2016, acc = 0.9380 (239.5 examples/sec; 0.267 sec/batch)
2017-05-02 17:40:26.453257: step 2390, loss = 0.2415, acc = 0.9100 (235.5 examples/sec; 0.272 sec/batch)
2017-05-02 17:40:37.744782: step 2400, loss = 0.2491, acc = 0.9060 (233.8 examples/sec; 0.274 sec/batch)
2017-05-02 17:40:49.010085: step 2410, loss = 0.2072, acc = 0.9360 (228.1 examples/sec; 0.281 sec/batch)
2017-05-02 17:41:00.430582: step 2420, loss = 0.2203, acc = 0.9340 (200.8 examples/sec; 0.319 sec/batch)
2017-05-02 17:41:11.507663: step 2430, loss = 0.2376, acc = 0.9260 (226.3 examples/sec; 0.283 sec/batch)
2017-05-02 17:41:22.530694: step 2440, loss = 0.2647, acc = 0.9040 (250.2 examples/sec; 0.256 sec/batch)
2017-05-02 17:41:33.575048: step 2450, loss = 0.2079, acc = 0.9320 (217.2 examples/sec; 0.295 sec/batch)
2017-05-02 17:41:44.693538: step 2460, loss = 0.2250, acc = 0.9460 (240.0 examples/sec; 0.267 sec/batch)
2017-05-02 17:41:55.797264: step 2470, loss = 0.2292, acc = 0.9220 (236.7 examples/sec; 0.270 sec/batch)
2017-05-02 17:42:07.000375: step 2480, loss = 0.2218, acc = 0.9300 (223.9 examples/sec; 0.286 sec/batch)
2017-05-02 17:42:18.157269: step 2490, loss = 0.2438, acc = 0.9020 (235.8 examples/sec; 0.271 sec/batch)
2017-05-02 17:42:29.258800: step 2500, loss = 0.2453, acc = 0.9180 (240.0 examples/sec; 0.267 sec/batch)
2017-05-02 17:42:40.398847: step 2510, loss = 0.2344, acc = 0.9140 (228.9 examples/sec; 0.280 sec/batch)
2017-05-02 17:42:51.559813: step 2520, loss = 0.2318, acc = 0.9180 (247.8 examples/sec; 0.258 sec/batch)
2017-05-02 17:43:02.484017: step 2530, loss = 0.2055, acc = 0.9340 (238.8 examples/sec; 0.268 sec/batch)
2017-05-02 17:43:13.604844: step 2540, loss = 0.2229, acc = 0.9360 (228.6 examples/sec; 0.280 sec/batch)
2017-05-02 17:43:24.587135: step 2550, loss = 0.1909, acc = 0.9460 (192.5 examples/sec; 0.333 sec/batch)
2017-05-02 17:43:35.355941: step 2560, loss = 0.2211, acc = 0.9240 (235.6 examples/sec; 0.272 sec/batch)
2017-05-02 17:43:46.127316: step 2570, loss = 0.2319, acc = 0.9280 (238.2 examples/sec; 0.269 sec/batch)
2017-05-02 17:43:56.889274: step 2580, loss = 0.2316, acc = 0.9200 (235.3 examples/sec; 0.272 sec/batch)
2017-05-02 17:44:07.572539: step 2590, loss = 0.2008, acc = 0.9360 (243.9 examples/sec; 0.262 sec/batch)
2017-05-02 17:44:18.481721: step 2600, loss = 0.2336, acc = 0.9200 (232.5 examples/sec; 0.275 sec/batch)
2017-05-02 17:44:29.362000: step 2610, loss = 0.2316, acc = 0.9320 (236.0 examples/sec; 0.271 sec/batch)
2017-05-02 17:44:40.088064: step 2620, loss = 0.2057, acc = 0.9340 (236.0 examples/sec; 0.271 sec/batch)
2017-05-02 17:44:51.113995: step 2630, loss = 0.2336, acc = 0.9120 (227.8 examples/sec; 0.281 sec/batch)
2017-05-02 17:45:01.754990: step 2640, loss = 0.2227, acc = 0.9280 (235.2 examples/sec; 0.272 sec/batch)
2017-05-02 17:45:12.546289: step 2650, loss = 0.2349, acc = 0.9260 (230.9 examples/sec; 0.277 sec/batch)
2017-05-02 17:45:23.274369: step 2660, loss = 0.2066, acc = 0.9340 (232.6 examples/sec; 0.275 sec/batch)
2017-05-02 17:45:33.820837: step 2670, loss = 0.2342, acc = 0.9200 (237.9 examples/sec; 0.269 sec/batch)
2017-05-02 17:45:44.481035: step 2680, loss = 0.2426, acc = 0.9260 (235.7 examples/sec; 0.272 sec/batch)
2017-05-02 17:45:55.237614: step 2690, loss = 0.2290, acc = 0.9360 (238.5 examples/sec; 0.268 sec/batch)
2017-05-02 17:46:05.978860: step 2700, loss = 0.2333, acc = 0.9360 (235.1 examples/sec; 0.272 sec/batch)
2017-05-02 17:46:16.571867: step 2710, loss = 0.2216, acc = 0.9300 (256.2 examples/sec; 0.250 sec/batch)
2017-05-02 17:46:27.534791: step 2720, loss = 0.2492, acc = 0.9040 (246.0 examples/sec; 0.260 sec/batch)
2017-05-02 17:46:38.105093: step 2730, loss = 0.2191, acc = 0.9320 (231.2 examples/sec; 0.277 sec/batch)
2017-05-02 17:46:48.918319: step 2740, loss = 0.2269, acc = 0.9280 (247.3 examples/sec; 0.259 sec/batch)
2017-05-02 17:46:59.432111: step 2750, loss = 0.2100, acc = 0.9480 (245.5 examples/sec; 0.261 sec/batch)
2017-05-02 17:47:10.193218: step 2760, loss = 0.2125, acc = 0.9360 (230.3 examples/sec; 0.278 sec/batch)
2017-05-02 17:47:20.613110: step 2770, loss = 0.2358, acc = 0.9280 (259.1 examples/sec; 0.247 sec/batch)
2017-05-02 17:47:31.205550: step 2780, loss = 0.2244, acc = 0.9380 (231.4 examples/sec; 0.277 sec/batch)
2017-05-02 17:47:41.684301: step 2790, loss = 0.2158, acc = 0.9320 (243.0 examples/sec; 0.263 sec/batch)
2017-05-02 17:47:52.360612: step 2800, loss = 0.2263, acc = 0.9320 (236.9 examples/sec; 0.270 sec/batch)
2017-05-02 17:48:03.026718: step 2810, loss = 0.2068, acc = 0.9260 (230.2 examples/sec; 0.278 sec/batch)
2017-05-02 17:48:13.677977: step 2820, loss = 0.1983, acc = 0.9360 (243.4 examples/sec; 0.263 sec/batch)
2017-05-02 17:48:24.566841: step 2830, loss = 0.2204, acc = 0.9300 (248.1 examples/sec; 0.258 sec/batch)
2017-05-02 17:48:35.310955: step 2840, loss = 0.2341, acc = 0.9260 (243.6 examples/sec; 0.263 sec/batch)
2017-05-02 17:48:45.774723: step 2850, loss = 0.2215, acc = 0.9240 (250.7 examples/sec; 0.255 sec/batch)
2017-05-02 17:48:56.264904: step 2860, loss = 0.2155, acc = 0.9240 (250.0 examples/sec; 0.256 sec/batch)
2017-05-02 17:49:06.753367: step 2870, loss = 0.2386, acc = 0.9280 (243.6 examples/sec; 0.263 sec/batch)
2017-05-02 17:49:17.178236: step 2880, loss = 0.2576, acc = 0.9040 (249.6 examples/sec; 0.256 sec/batch)
2017-05-02 17:49:27.581005: step 2890, loss = 0.2061, acc = 0.9340 (250.1 examples/sec; 0.256 sec/batch)
2017-05-02 17:49:38.016777: step 2900, loss = 0.2204, acc = 0.9340 (239.4 examples/sec; 0.267 sec/batch)
2017-05-02 17:49:48.463545: step 2910, loss = 0.2046, acc = 0.9300 (249.9 examples/sec; 0.256 sec/batch)
2017-05-02 17:49:58.875093: step 2920, loss = 0.2072, acc = 0.9340 (227.0 examples/sec; 0.282 sec/batch)
2017-05-02 17:50:09.304866: step 2930, loss = 0.2408, acc = 0.9160 (243.6 examples/sec; 0.263 sec/batch)
2017-05-02 17:50:19.700472: step 2940, loss = 0.2409, acc = 0.9240 (240.9 examples/sec; 0.266 sec/batch)
2017-05-02 17:50:30.338565: step 2950, loss = 0.2624, acc = 0.9100 (245.3 examples/sec; 0.261 sec/batch)
2017-05-02 17:50:41.181282: step 2960, loss = 0.2111, acc = 0.9480 (252.1 examples/sec; 0.254 sec/batch)
2017-05-02 17:50:51.844575: step 2970, loss = 0.2351, acc = 0.9280 (239.7 examples/sec; 0.267 sec/batch)
2017-05-02 17:51:02.367166: step 2980, loss = 0.2096, acc = 0.9400 (247.4 examples/sec; 0.259 sec/batch)
2017-05-02 17:51:13.088940: step 2990, loss = 0.2283, acc = 0.9240 (222.1 examples/sec; 0.288 sec/batch)
2017-05-02 17:51:23.705138: step 3000, loss = 0.2256, acc = 0.9240 (239.4 examples/sec; 0.267 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 17:51:24.212311: step 3000, loss = 0.1790, acc = 0.9435, f1neg = 0.9377, f1pos = 0.9483, f1 = 0.9430
[Eval_batch(1)(2000,4000)] 2017-05-02 17:51:24.687583: step 3000, loss = 0.1827, acc = 0.9460, f1neg = 0.9371, f1pos = 0.9527, f1 = 0.9449
[Eval_batch(2)(2000,6000)] 2017-05-02 17:51:25.196420: step 3000, loss = 0.1998, acc = 0.9340, f1neg = 0.9300, f1pos = 0.9376, f1 = 0.9338
[Eval_batch(3)(2000,8000)] 2017-05-02 17:51:25.675531: step 3000, loss = 0.2069, acc = 0.9320, f1neg = 0.9319, f1pos = 0.9321, f1 = 0.9320
[Eval_batch(4)(2000,10000)] 2017-05-02 17:51:26.153111: step 3000, loss = 0.2054, acc = 0.9330, f1neg = 0.9346, f1pos = 0.9313, f1 = 0.9330
[Eval_batch(5)(2000,12000)] 2017-05-02 17:51:26.631260: step 3000, loss = 0.1937, acc = 0.9325, f1neg = 0.9279, f1pos = 0.9365, f1 = 0.9322
[Eval_batch(6)(2000,14000)] 2017-05-02 17:51:27.144533: step 3000, loss = 0.1890, acc = 0.9410, f1neg = 0.9394, f1pos = 0.9425, f1 = 0.9410
[Eval_batch(7)(2000,16000)] 2017-05-02 17:51:27.624534: step 3000, loss = 0.1835, acc = 0.9475, f1neg = 0.9378, f1pos = 0.9546, f1 = 0.9462
[Eval_batch(8)(2000,18000)] 2017-05-02 17:51:28.117321: step 3000, loss = 0.1896, acc = 0.9395, f1neg = 0.9306, f1pos = 0.9464, f1 = 0.9385
[Eval_batch(9)(2000,20000)] 2017-05-02 17:51:28.621256: step 3000, loss = 0.1883, acc = 0.9425, f1neg = 0.9385, f1pos = 0.9460, f1 = 0.9423
[Eval_batch(10)(2000,22000)] 2017-05-02 17:51:29.123141: step 3000, loss = 0.1941, acc = 0.9390, f1neg = 0.9348, f1pos = 0.9427, f1 = 0.9387
[Eval_batch(11)(2000,24000)] 2017-05-02 17:51:29.589668: step 3000, loss = 0.1985, acc = 0.9370, f1neg = 0.9269, f1pos = 0.9446, f1 = 0.9358
[Eval_batch(12)(2000,26000)] 2017-05-02 17:51:30.057918: step 3000, loss = 0.1907, acc = 0.9430, f1neg = 0.9407, f1pos = 0.9451, f1 = 0.9429
[Eval_batch(13)(2000,28000)] 2017-05-02 17:51:30.564453: step 3000, loss = 0.1797, acc = 0.9435, f1neg = 0.9288, f1pos = 0.9532, f1 = 0.9410
[Eval_batch(14)(2000,30000)] 2017-05-02 17:51:31.040930: step 3000, loss = 0.2021, acc = 0.9425, f1neg = 0.9310, f1pos = 0.9507, f1 = 0.9409
[Eval_batch(15)(2000,32000)] 2017-05-02 17:51:31.527021: step 3000, loss = 0.1911, acc = 0.9415, f1neg = 0.9364, f1pos = 0.9458, f1 = 0.9411
[Eval_batch(16)(2000,34000)] 2017-05-02 17:51:31.994528: step 3000, loss = 0.1805, acc = 0.9405, f1neg = 0.9377, f1pos = 0.9430, f1 = 0.9404
[Eval_batch(17)(2000,36000)] 2017-05-02 17:51:32.467129: step 3000, loss = 0.2009, acc = 0.9380, f1neg = 0.9377, f1pos = 0.9383, f1 = 0.9380
[Eval_batch(18)(2000,38000)] 2017-05-02 17:51:32.969062: step 3000, loss = 0.2031, acc = 0.9300, f1neg = 0.9324, f1pos = 0.9275, f1 = 0.9299
[Eval_batch(19)(2000,40000)] 2017-05-02 17:51:33.434907: step 3000, loss = 0.1802, acc = 0.9525, f1neg = 0.9449, f1pos = 0.9583, f1 = 0.9516
[Eval_batch(20)(2000,42000)] 2017-05-02 17:51:33.906799: step 3000, loss = 0.1863, acc = 0.9425, f1neg = 0.9489, f1pos = 0.9343, f1 = 0.9416
[Eval_batch(21)(2000,44000)] 2017-05-02 17:51:34.421948: step 3000, loss = 0.1776, acc = 0.9485, f1neg = 0.9439, f1pos = 0.9524, f1 = 0.9481
[Eval_batch(22)(2000,46000)] 2017-05-02 17:51:34.940731: step 3000, loss = 0.1850, acc = 0.9420, f1neg = 0.9422, f1pos = 0.9418, f1 = 0.9420
[Eval_batch(23)(2000,48000)] 2017-05-02 17:51:35.455142: step 3000, loss = 0.1962, acc = 0.9410, f1neg = 0.9328, f1pos = 0.9474, f1 = 0.9401
[Eval_batch(24)(2000,50000)] 2017-05-02 17:51:35.962323: step 3000, loss = 0.1848, acc = 0.9400, f1neg = 0.9306, f1pos = 0.9472, f1 = 0.9389
[Eval_batch(25)(2000,52000)] 2017-05-02 17:51:36.476658: step 3000, loss = 0.1752, acc = 0.9520, f1neg = 0.9471, f1pos = 0.9560, f1 = 0.9516
[Eval_batch(26)(2000,54000)] 2017-05-02 17:51:36.977023: step 3000, loss = 0.1822, acc = 0.9440, f1neg = 0.9469, f1pos = 0.9408, f1 = 0.9438
[Eval_batch(27)(2000,56000)] 2017-05-02 17:51:37.693186: step 3000, loss = 0.1707, acc = 0.9555, f1neg = 0.9532, f1pos = 0.9576, f1 = 0.9554
[Eval] 2017-05-02 17:51:37.693290: step 3000, acc = 0.9416, f1 = 0.9410
[Test_batch(0)(2000,2000)] 2017-05-02 17:51:38.174961: step 3000, loss = 0.2206, acc = 0.9305, f1neg = 0.9342, f1pos = 0.9263, f1 = 0.9303
[Test_batch(1)(2000,4000)] 2017-05-02 17:51:38.690115: step 3000, loss = 0.2122, acc = 0.9305, f1neg = 0.9381, f1pos = 0.9207, f1 = 0.9294
[Test_batch(2)(2000,6000)] 2017-05-02 17:51:39.169332: step 3000, loss = 0.2126, acc = 0.9380, f1neg = 0.9430, f1pos = 0.9321, f1 = 0.9375
[Test_batch(3)(2000,8000)] 2017-05-02 17:51:39.681578: step 3000, loss = 0.2311, acc = 0.9195, f1neg = 0.9247, f1pos = 0.9135, f1 = 0.9191
[Test_batch(4)(2000,10000)] 2017-05-02 17:51:40.191595: step 3000, loss = 0.2389, acc = 0.9180, f1neg = 0.9178, f1pos = 0.9182, f1 = 0.9180
[Test_batch(5)(2000,12000)] 2017-05-02 17:51:40.706510: step 3000, loss = 0.2450, acc = 0.9125, f1neg = 0.9131, f1pos = 0.9119, f1 = 0.9125
[Test_batch(6)(2000,14000)] 2017-05-02 17:51:41.211808: step 3000, loss = 0.2271, acc = 0.9200, f1neg = 0.9166, f1pos = 0.9232, f1 = 0.9199
[Test_batch(7)(2000,16000)] 2017-05-02 17:51:41.717362: step 3000, loss = 0.2142, acc = 0.9305, f1neg = 0.9370, f1pos = 0.9226, f1 = 0.9298
[Test_batch(8)(2000,18000)] 2017-05-02 17:51:42.223187: step 3000, loss = 0.2099, acc = 0.9370, f1neg = 0.9362, f1pos = 0.9377, f1 = 0.9370
[Test_batch(9)(2000,20000)] 2017-05-02 17:51:42.735864: step 3000, loss = 0.2407, acc = 0.9190, f1neg = 0.9142, f1pos = 0.9233, f1 = 0.9187
[Test_batch(10)(2000,22000)] 2017-05-02 17:51:43.254546: step 3000, loss = 0.2197, acc = 0.9275, f1neg = 0.9183, f1pos = 0.9348, f1 = 0.9266
[Test_batch(11)(2000,24000)] 2017-05-02 17:51:43.760626: step 3000, loss = 0.2198, acc = 0.9335, f1neg = 0.9351, f1pos = 0.9318, f1 = 0.9335
[Test_batch(12)(2000,26000)] 2017-05-02 17:51:44.267736: step 3000, loss = 0.1992, acc = 0.9405, f1neg = 0.9430, f1pos = 0.9377, f1 = 0.9404
[Test_batch(13)(2000,28000)] 2017-05-02 17:51:44.774823: step 3000, loss = 0.2353, acc = 0.9155, f1neg = 0.9057, f1pos = 0.9234, f1 = 0.9146
[Test_batch(14)(2000,30000)] 2017-05-02 17:51:45.282406: step 3000, loss = 0.2011, acc = 0.9355, f1neg = 0.9262, f1pos = 0.9427, f1 = 0.9345
[Test_batch(15)(2000,32000)] 2017-05-02 17:51:45.795328: step 3000, loss = 0.2110, acc = 0.9380, f1neg = 0.9371, f1pos = 0.9389, f1 = 0.9380
[Test_batch(16)(2000,34000)] 2017-05-02 17:51:46.304347: step 3000, loss = 0.2059, acc = 0.9360, f1neg = 0.9346, f1pos = 0.9373, f1 = 0.9360
[Test_batch(17)(2000,36000)] 2017-05-02 17:51:46.817242: step 3000, loss = 0.1897, acc = 0.9445, f1neg = 0.9391, f1pos = 0.9490, f1 = 0.9441
[Test_batch(18)(2000,38000)] 2017-05-02 17:51:47.468619: step 3000, loss = 0.1781, acc = 0.9500, f1neg = 0.9486, f1pos = 0.9513, f1 = 0.9500
[Test] 2017-05-02 17:51:47.468696: step 3000, acc = 0.9303, f1 = 0.9300
[Status] 2017-05-02 17:51:47.468719: step 3000, maxindex = 3000, maxdev = 0.9416, maxtst = 0.9303
2017-05-02 17:52:00.782274: step 3010, loss = 0.2074, acc = 0.9280 (245.3 examples/sec; 0.261 sec/batch)
2017-05-02 17:52:11.319074: step 3020, loss = 0.2295, acc = 0.9320 (234.1 examples/sec; 0.273 sec/batch)
2017-05-02 17:52:22.674071: step 3030, loss = 0.2123, acc = 0.9240 (249.2 examples/sec; 0.257 sec/batch)
2017-05-02 17:52:33.234290: step 3040, loss = 0.1988, acc = 0.9400 (230.1 examples/sec; 0.278 sec/batch)
2017-05-02 17:52:43.601607: step 3050, loss = 0.1983, acc = 0.9460 (251.7 examples/sec; 0.254 sec/batch)
2017-05-02 17:52:54.559818: step 3060, loss = 0.2300, acc = 0.9180 (235.0 examples/sec; 0.272 sec/batch)
2017-05-02 17:53:04.973740: step 3070, loss = 0.2134, acc = 0.9240 (244.9 examples/sec; 0.261 sec/batch)
2017-05-02 17:53:15.417910: step 3080, loss = 0.2152, acc = 0.9260 (241.1 examples/sec; 0.265 sec/batch)
2017-05-02 17:53:25.914416: step 3090, loss = 0.1945, acc = 0.9460 (251.2 examples/sec; 0.255 sec/batch)
2017-05-02 17:53:36.404627: step 3100, loss = 0.1844, acc = 0.9440 (242.3 examples/sec; 0.264 sec/batch)
2017-05-02 17:53:46.960311: step 3110, loss = 0.2117, acc = 0.9300 (254.4 examples/sec; 0.252 sec/batch)
2017-05-02 17:53:57.523561: step 3120, loss = 0.1555, acc = 0.9620 (240.4 examples/sec; 0.266 sec/batch)
2017-05-02 17:54:07.838988: step 3130, loss = 0.2266, acc = 0.9120 (251.5 examples/sec; 0.254 sec/batch)
2017-05-02 17:54:18.253248: step 3140, loss = 0.2004, acc = 0.9360 (250.1 examples/sec; 0.256 sec/batch)
2017-05-02 17:54:28.688721: step 3150, loss = 0.2197, acc = 0.9320 (245.8 examples/sec; 0.260 sec/batch)
2017-05-02 17:54:39.311578: step 3160, loss = 0.2099, acc = 0.9220 (238.7 examples/sec; 0.268 sec/batch)
2017-05-02 17:54:49.666115: step 3170, loss = 0.1751, acc = 0.9400 (248.1 examples/sec; 0.258 sec/batch)
2017-05-02 17:54:59.984521: step 3180, loss = 0.2207, acc = 0.9120 (244.7 examples/sec; 0.262 sec/batch)
2017-05-02 17:55:10.476164: step 3190, loss = 0.2219, acc = 0.9280 (249.1 examples/sec; 0.257 sec/batch)
2017-05-02 17:55:21.088896: step 3200, loss = 0.2288, acc = 0.9220 (240.8 examples/sec; 0.266 sec/batch)
2017-05-02 17:55:31.452740: step 3210, loss = 0.2176, acc = 0.9300 (243.1 examples/sec; 0.263 sec/batch)
2017-05-02 17:55:41.851399: step 3220, loss = 0.2452, acc = 0.9100 (250.4 examples/sec; 0.256 sec/batch)
2017-05-02 17:55:52.225892: step 3230, loss = 0.2228, acc = 0.9300 (247.4 examples/sec; 0.259 sec/batch)
2017-05-02 17:56:02.568079: step 3240, loss = 0.2593, acc = 0.9060 (236.2 examples/sec; 0.271 sec/batch)
2017-05-02 17:56:12.403579: step 3250, loss = 0.1992, acc = 0.9360 (252.6 examples/sec; 0.253 sec/batch)
2017-05-02 17:56:22.427994: step 3260, loss = 0.2255, acc = 0.9220 (247.9 examples/sec; 0.258 sec/batch)
2017-05-02 17:56:32.487579: step 3270, loss = 0.1804, acc = 0.9580 (254.2 examples/sec; 0.252 sec/batch)
2017-05-02 17:56:42.568690: step 3280, loss = 0.2429, acc = 0.9220 (253.5 examples/sec; 0.252 sec/batch)
2017-05-02 17:56:52.468576: step 3290, loss = 0.1890, acc = 0.9500 (259.9 examples/sec; 0.246 sec/batch)
2017-05-02 17:57:03.187722: step 3300, loss = 0.2387, acc = 0.9120 (241.0 examples/sec; 0.266 sec/batch)
2017-05-02 17:57:13.361058: step 3310, loss = 0.2309, acc = 0.9280 (251.2 examples/sec; 0.255 sec/batch)
2017-05-02 17:57:23.225477: step 3320, loss = 0.2090, acc = 0.9200 (263.3 examples/sec; 0.243 sec/batch)
2017-05-02 17:57:33.179990: step 3330, loss = 0.2204, acc = 0.9160 (258.7 examples/sec; 0.247 sec/batch)
2017-05-02 17:57:43.322916: step 3340, loss = 0.2055, acc = 0.9260 (264.5 examples/sec; 0.242 sec/batch)
2017-05-02 17:57:53.289238: step 3350, loss = 0.1976, acc = 0.9380 (252.3 examples/sec; 0.254 sec/batch)
2017-05-02 17:58:03.199615: step 3360, loss = 0.2085, acc = 0.9260 (255.1 examples/sec; 0.251 sec/batch)
2017-05-02 17:58:13.198915: step 3370, loss = 0.2071, acc = 0.9380 (262.3 examples/sec; 0.244 sec/batch)
2017-05-02 17:58:23.324228: step 3380, loss = 0.2161, acc = 0.9260 (255.3 examples/sec; 0.251 sec/batch)
2017-05-02 17:58:33.323005: step 3390, loss = 0.2125, acc = 0.9320 (262.8 examples/sec; 0.244 sec/batch)
2017-05-02 17:58:43.696331: step 3400, loss = 0.1833, acc = 0.9400 (261.6 examples/sec; 0.245 sec/batch)
2017-05-02 17:58:53.520198: step 3410, loss = 0.2310, acc = 0.9160 (265.9 examples/sec; 0.241 sec/batch)
2017-05-02 17:59:03.274731: step 3420, loss = 0.1950, acc = 0.9360 (257.1 examples/sec; 0.249 sec/batch)
2017-05-02 17:59:13.125180: step 3430, loss = 0.1882, acc = 0.9360 (261.8 examples/sec; 0.244 sec/batch)
2017-05-02 17:59:22.954248: step 3440, loss = 0.2048, acc = 0.9220 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 17:59:32.672462: step 3450, loss = 0.2005, acc = 0.9380 (256.5 examples/sec; 0.249 sec/batch)
2017-05-02 17:59:42.566516: step 3460, loss = 0.2337, acc = 0.9240 (265.1 examples/sec; 0.241 sec/batch)
2017-05-02 17:59:52.611612: step 3470, loss = 0.2256, acc = 0.9280 (263.7 examples/sec; 0.243 sec/batch)
2017-05-02 18:00:02.374523: step 3480, loss = 0.2060, acc = 0.9280 (260.5 examples/sec; 0.246 sec/batch)
2017-05-02 18:00:12.323093: step 3490, loss = 0.2090, acc = 0.9220 (264.9 examples/sec; 0.242 sec/batch)
2017-05-02 18:00:22.114853: step 3500, loss = 0.2295, acc = 0.9240 (270.8 examples/sec; 0.236 sec/batch)
2017-05-02 18:00:31.566231: step 3510, loss = 0.1868, acc = 0.9440 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 18:00:41.201245: step 3520, loss = 0.2055, acc = 0.9380 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 18:00:50.908548: step 3530, loss = 0.2392, acc = 0.9120 (264.4 examples/sec; 0.242 sec/batch)
2017-05-02 18:01:00.640816: step 3540, loss = 0.2187, acc = 0.9180 (270.2 examples/sec; 0.237 sec/batch)
2017-05-02 18:01:10.262530: step 3550, loss = 0.1857, acc = 0.9360 (268.6 examples/sec; 0.238 sec/batch)
2017-05-02 18:01:20.220288: step 3560, loss = 0.2096, acc = 0.9420 (180.4 examples/sec; 0.355 sec/batch)
2017-05-02 18:01:29.733745: step 3570, loss = 0.2233, acc = 0.9320 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 18:01:39.195382: step 3580, loss = 0.1985, acc = 0.9300 (263.4 examples/sec; 0.243 sec/batch)
2017-05-02 18:01:48.923262: step 3590, loss = 0.2107, acc = 0.9220 (267.7 examples/sec; 0.239 sec/batch)
2017-05-02 18:01:58.396559: step 3600, loss = 0.2227, acc = 0.9320 (289.3 examples/sec; 0.221 sec/batch)
2017-05-02 18:02:07.848368: step 3610, loss = 0.2264, acc = 0.9200 (293.6 examples/sec; 0.218 sec/batch)
2017-05-02 18:02:17.400780: step 3620, loss = 0.1927, acc = 0.9460 (251.5 examples/sec; 0.255 sec/batch)
2017-05-02 18:02:26.943454: step 3630, loss = 0.2251, acc = 0.9260 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 18:02:36.575337: step 3640, loss = 0.2227, acc = 0.9240 (258.8 examples/sec; 0.247 sec/batch)
2017-05-02 18:02:45.944138: step 3650, loss = 0.2200, acc = 0.9100 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 18:02:55.193769: step 3660, loss = 0.2509, acc = 0.9040 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 18:03:04.701498: step 3670, loss = 0.2286, acc = 0.9220 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 18:03:14.036914: step 3680, loss = 0.1896, acc = 0.9440 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 18:03:23.555168: step 3690, loss = 0.2268, acc = 0.9240 (264.7 examples/sec; 0.242 sec/batch)
2017-05-02 18:03:33.067540: step 3700, loss = 0.2306, acc = 0.9240 (260.9 examples/sec; 0.245 sec/batch)
2017-05-02 18:03:42.536153: step 3710, loss = 0.2107, acc = 0.9300 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 18:03:52.127617: step 3720, loss = 0.2293, acc = 0.9260 (259.5 examples/sec; 0.247 sec/batch)
2017-05-02 18:04:01.657836: step 3730, loss = 0.2127, acc = 0.9320 (262.8 examples/sec; 0.244 sec/batch)
2017-05-02 18:04:11.317684: step 3740, loss = 0.2243, acc = 0.9260 (266.7 examples/sec; 0.240 sec/batch)
2017-05-02 18:04:20.873170: step 3750, loss = 0.2096, acc = 0.9260 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 18:04:30.216738: step 3760, loss = 0.2083, acc = 0.9380 (268.6 examples/sec; 0.238 sec/batch)
2017-05-02 18:04:39.720460: step 3770, loss = 0.2307, acc = 0.9140 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 18:04:49.388978: step 3780, loss = 0.2063, acc = 0.9340 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 18:04:58.789206: step 3790, loss = 0.2520, acc = 0.9080 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 18:05:08.608375: step 3800, loss = 0.1988, acc = 0.9360 (274.3 examples/sec; 0.233 sec/batch)
2017-05-02 18:05:18.058952: step 3810, loss = 0.2169, acc = 0.9140 (266.2 examples/sec; 0.240 sec/batch)
2017-05-02 18:05:27.599092: step 3820, loss = 0.1980, acc = 0.9440 (257.5 examples/sec; 0.249 sec/batch)
2017-05-02 18:05:36.996177: step 3830, loss = 0.2413, acc = 0.9220 (265.0 examples/sec; 0.242 sec/batch)
2017-05-02 18:05:46.585724: step 3840, loss = 0.2106, acc = 0.9360 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 18:05:56.246172: step 3850, loss = 0.2010, acc = 0.9300 (264.1 examples/sec; 0.242 sec/batch)
2017-05-02 18:06:05.844299: step 3860, loss = 0.2004, acc = 0.9380 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 18:06:15.372588: step 3870, loss = 0.2119, acc = 0.9320 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 18:06:24.839641: step 3880, loss = 0.1866, acc = 0.9440 (264.1 examples/sec; 0.242 sec/batch)
2017-05-02 18:06:34.467026: step 3890, loss = 0.2155, acc = 0.9380 (249.2 examples/sec; 0.257 sec/batch)
2017-05-02 18:06:43.822761: step 3900, loss = 0.2252, acc = 0.9140 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 18:06:53.487121: step 3910, loss = 0.1932, acc = 0.9360 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 18:07:03.076972: step 3920, loss = 0.1870, acc = 0.9360 (269.1 examples/sec; 0.238 sec/batch)
2017-05-02 18:07:12.619971: step 3930, loss = 0.1889, acc = 0.9420 (262.4 examples/sec; 0.244 sec/batch)
2017-05-02 18:07:22.234442: step 3940, loss = 0.2046, acc = 0.9360 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 18:07:31.937110: step 3950, loss = 0.2244, acc = 0.9120 (266.6 examples/sec; 0.240 sec/batch)
2017-05-02 18:07:41.644731: step 3960, loss = 0.1996, acc = 0.9340 (263.6 examples/sec; 0.243 sec/batch)
2017-05-02 18:07:51.467464: step 3970, loss = 0.2121, acc = 0.9340 (265.6 examples/sec; 0.241 sec/batch)
2017-05-02 18:08:00.996117: step 3980, loss = 0.1796, acc = 0.9560 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:08:10.547809: step 3990, loss = 0.2099, acc = 0.9240 (268.1 examples/sec; 0.239 sec/batch)
2017-05-02 18:08:20.066620: step 4000, loss = 0.1782, acc = 0.9520 (272.0 examples/sec; 0.235 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 18:08:20.520971: step 4000, loss = 0.1683, acc = 0.9470, f1neg = 0.9417, f1pos = 0.9514, f1 = 0.9466
[Eval_batch(1)(2000,4000)] 2017-05-02 18:08:20.981034: step 4000, loss = 0.1709, acc = 0.9490, f1neg = 0.9405, f1pos = 0.9554, f1 = 0.9479
[Eval_batch(2)(2000,6000)] 2017-05-02 18:08:21.478846: step 4000, loss = 0.1887, acc = 0.9420, f1neg = 0.9385, f1pos = 0.9451, f1 = 0.9418
[Eval_batch(3)(2000,8000)] 2017-05-02 18:08:21.977142: step 4000, loss = 0.1951, acc = 0.9385, f1neg = 0.9385, f1pos = 0.9385, f1 = 0.9385
[Eval_batch(4)(2000,10000)] 2017-05-02 18:08:22.481068: step 4000, loss = 0.1943, acc = 0.9385, f1neg = 0.9400, f1pos = 0.9370, f1 = 0.9385
[Eval_batch(5)(2000,12000)] 2017-05-02 18:08:22.992826: step 4000, loss = 0.1834, acc = 0.9380, f1neg = 0.9335, f1pos = 0.9419, f1 = 0.9377
[Eval_batch(6)(2000,14000)] 2017-05-02 18:08:23.469308: step 4000, loss = 0.1782, acc = 0.9450, f1neg = 0.9435, f1pos = 0.9464, f1 = 0.9450
[Eval_batch(7)(2000,16000)] 2017-05-02 18:08:23.975782: step 4000, loss = 0.1736, acc = 0.9480, f1neg = 0.9385, f1pos = 0.9550, f1 = 0.9467
[Eval_batch(8)(2000,18000)] 2017-05-02 18:08:24.438293: step 4000, loss = 0.1776, acc = 0.9420, f1neg = 0.9336, f1pos = 0.9485, f1 = 0.9410
[Eval_batch(9)(2000,20000)] 2017-05-02 18:08:24.950170: step 4000, loss = 0.1765, acc = 0.9450, f1neg = 0.9411, f1pos = 0.9484, f1 = 0.9448
[Eval_batch(10)(2000,22000)] 2017-05-02 18:08:25.422349: step 4000, loss = 0.1849, acc = 0.9435, f1neg = 0.9397, f1pos = 0.9469, f1 = 0.9433
[Eval_batch(11)(2000,24000)] 2017-05-02 18:08:25.891000: step 4000, loss = 0.1868, acc = 0.9440, f1neg = 0.9350, f1pos = 0.9508, f1 = 0.9429
[Eval_batch(12)(2000,26000)] 2017-05-02 18:08:26.357929: step 4000, loss = 0.1811, acc = 0.9460, f1neg = 0.9439, f1pos = 0.9479, f1 = 0.9459
[Eval_batch(13)(2000,28000)] 2017-05-02 18:08:26.827471: step 4000, loss = 0.1689, acc = 0.9480, f1neg = 0.9346, f1pos = 0.9568, f1 = 0.9457
[Eval_batch(14)(2000,30000)] 2017-05-02 18:08:27.334000: step 4000, loss = 0.1918, acc = 0.9460, f1neg = 0.9351, f1pos = 0.9538, f1 = 0.9444
[Eval_batch(15)(2000,32000)] 2017-05-02 18:08:27.810408: step 4000, loss = 0.1791, acc = 0.9440, f1neg = 0.9393, f1pos = 0.9481, f1 = 0.9437
[Eval_batch(16)(2000,34000)] 2017-05-02 18:08:28.322549: step 4000, loss = 0.1699, acc = 0.9430, f1neg = 0.9402, f1pos = 0.9456, f1 = 0.9429
[Eval_batch(17)(2000,36000)] 2017-05-02 18:08:28.838245: step 4000, loss = 0.1904, acc = 0.9385, f1neg = 0.9383, f1pos = 0.9387, f1 = 0.9385
[Eval_batch(18)(2000,38000)] 2017-05-02 18:08:29.313015: step 4000, loss = 0.1923, acc = 0.9330, f1neg = 0.9354, f1pos = 0.9304, f1 = 0.9329
[Eval_batch(19)(2000,40000)] 2017-05-02 18:08:29.779574: step 4000, loss = 0.1694, acc = 0.9580, f1neg = 0.9514, f1pos = 0.9630, f1 = 0.9572
[Eval_batch(20)(2000,42000)] 2017-05-02 18:08:30.275586: step 4000, loss = 0.1755, acc = 0.9475, f1neg = 0.9534, f1pos = 0.9399, f1 = 0.9466
[Eval_batch(21)(2000,44000)] 2017-05-02 18:08:30.782808: step 4000, loss = 0.1657, acc = 0.9525, f1neg = 0.9483, f1pos = 0.9560, f1 = 0.9522
[Eval_batch(22)(2000,46000)] 2017-05-02 18:08:31.292295: step 4000, loss = 0.1728, acc = 0.9470, f1neg = 0.9473, f1pos = 0.9467, f1 = 0.9470
[Eval_batch(23)(2000,48000)] 2017-05-02 18:08:31.774266: step 4000, loss = 0.1842, acc = 0.9420, f1neg = 0.9339, f1pos = 0.9483, f1 = 0.9411
[Eval_batch(24)(2000,50000)] 2017-05-02 18:08:32.279903: step 4000, loss = 0.1737, acc = 0.9450, f1neg = 0.9365, f1pos = 0.9515, f1 = 0.9440
[Eval_batch(25)(2000,52000)] 2017-05-02 18:08:32.760178: step 4000, loss = 0.1631, acc = 0.9550, f1neg = 0.9504, f1pos = 0.9588, f1 = 0.9546
[Eval_batch(26)(2000,54000)] 2017-05-02 18:08:33.219650: step 4000, loss = 0.1705, acc = 0.9485, f1neg = 0.9512, f1pos = 0.9455, f1 = 0.9483
[Eval_batch(27)(2000,56000)] 2017-05-02 18:08:33.814835: step 4000, loss = 0.1602, acc = 0.9575, f1neg = 0.9553, f1pos = 0.9595, f1 = 0.9574
[Eval] 2017-05-02 18:08:33.814899: step 4000, acc = 0.9454, f1 = 0.9449
[Test_batch(0)(2000,2000)] 2017-05-02 18:08:34.323781: step 4000, loss = 0.2099, acc = 0.9360, f1neg = 0.9392, f1pos = 0.9324, f1 = 0.9358
[Test_batch(1)(2000,4000)] 2017-05-02 18:08:34.796880: step 4000, loss = 0.2012, acc = 0.9365, f1neg = 0.9436, f1pos = 0.9274, f1 = 0.9355
[Test_batch(2)(2000,6000)] 2017-05-02 18:08:35.272897: step 4000, loss = 0.2026, acc = 0.9415, f1neg = 0.9461, f1pos = 0.9360, f1 = 0.9411
[Test_batch(3)(2000,8000)] 2017-05-02 18:08:35.744593: step 4000, loss = 0.2188, acc = 0.9225, f1neg = 0.9275, f1pos = 0.9168, f1 = 0.9221
[Test_batch(4)(2000,10000)] 2017-05-02 18:08:36.219922: step 4000, loss = 0.2259, acc = 0.9220, f1neg = 0.9218, f1pos = 0.9222, f1 = 0.9220
[Test_batch(5)(2000,12000)] 2017-05-02 18:08:36.693706: step 4000, loss = 0.2326, acc = 0.9185, f1neg = 0.9188, f1pos = 0.9182, f1 = 0.9185
[Test_batch(6)(2000,14000)] 2017-05-02 18:08:37.170696: step 4000, loss = 0.2145, acc = 0.9215, f1neg = 0.9181, f1pos = 0.9246, f1 = 0.9214
[Test_batch(7)(2000,16000)] 2017-05-02 18:08:37.651749: step 4000, loss = 0.2025, acc = 0.9345, f1neg = 0.9405, f1pos = 0.9271, f1 = 0.9338
[Test_batch(8)(2000,18000)] 2017-05-02 18:08:38.118925: step 4000, loss = 0.1984, acc = 0.9410, f1neg = 0.9402, f1pos = 0.9418, f1 = 0.9410
[Test_batch(9)(2000,20000)] 2017-05-02 18:08:38.597261: step 4000, loss = 0.2298, acc = 0.9210, f1neg = 0.9164, f1pos = 0.9251, f1 = 0.9208
[Test_batch(10)(2000,22000)] 2017-05-02 18:08:39.068633: step 4000, loss = 0.2090, acc = 0.9325, f1neg = 0.9244, f1pos = 0.9391, f1 = 0.9317
[Test_batch(11)(2000,24000)] 2017-05-02 18:08:39.507928: step 4000, loss = 0.2084, acc = 0.9370, f1neg = 0.9386, f1pos = 0.9353, f1 = 0.9370
[Test_batch(12)(2000,26000)] 2017-05-02 18:08:39.977087: step 4000, loss = 0.1877, acc = 0.9430, f1neg = 0.9454, f1pos = 0.9404, f1 = 0.9429
[Test_batch(13)(2000,28000)] 2017-05-02 18:08:40.442263: step 4000, loss = 0.2218, acc = 0.9190, f1neg = 0.9093, f1pos = 0.9268, f1 = 0.9181
[Test_batch(14)(2000,30000)] 2017-05-02 18:08:40.904399: step 4000, loss = 0.1891, acc = 0.9400, f1neg = 0.9314, f1pos = 0.9467, f1 = 0.9390
[Test_batch(15)(2000,32000)] 2017-05-02 18:08:41.374123: step 4000, loss = 0.2006, acc = 0.9405, f1neg = 0.9398, f1pos = 0.9412, f1 = 0.9405
[Test_batch(16)(2000,34000)] 2017-05-02 18:08:41.835450: step 4000, loss = 0.1934, acc = 0.9425, f1neg = 0.9414, f1pos = 0.9435, f1 = 0.9425
[Test_batch(17)(2000,36000)] 2017-05-02 18:08:42.303411: step 4000, loss = 0.1783, acc = 0.9465, f1neg = 0.9414, f1pos = 0.9508, f1 = 0.9461
[Test_batch(18)(2000,38000)] 2017-05-02 18:08:42.798171: step 4000, loss = 0.1673, acc = 0.9545, f1neg = 0.9533, f1pos = 0.9556, f1 = 0.9545
[Test] 2017-05-02 18:08:42.798248: step 4000, acc = 0.9342, f1 = 0.9339
[Status] 2017-05-02 18:08:42.798272: step 4000, maxindex = 4000, maxdev = 0.9454, maxtst = 0.9342
2017-05-02 18:08:55.198587: step 4010, loss = 0.1936, acc = 0.9380 (271.2 examples/sec; 0.236 sec/batch)
2017-05-02 18:09:04.802119: step 4020, loss = 0.2030, acc = 0.9220 (261.5 examples/sec; 0.245 sec/batch)
2017-05-02 18:09:14.246376: step 4030, loss = 0.1977, acc = 0.9320 (263.0 examples/sec; 0.243 sec/batch)
2017-05-02 18:09:24.549005: step 4040, loss = 0.2038, acc = 0.9220 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 18:09:33.918130: step 4050, loss = 0.1719, acc = 0.9520 (267.7 examples/sec; 0.239 sec/batch)
2017-05-02 18:09:43.377022: step 4060, loss = 0.2187, acc = 0.9180 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 18:09:52.892952: step 4070, loss = 0.2163, acc = 0.9300 (267.1 examples/sec; 0.240 sec/batch)
2017-05-02 18:10:02.458614: step 4080, loss = 0.2099, acc = 0.9320 (259.7 examples/sec; 0.246 sec/batch)
2017-05-02 18:10:12.109769: step 4090, loss = 0.2252, acc = 0.9200 (275.5 examples/sec; 0.232 sec/batch)
2017-05-02 18:10:21.561849: step 4100, loss = 0.1944, acc = 0.9400 (264.6 examples/sec; 0.242 sec/batch)
2017-05-02 18:10:31.103279: step 4110, loss = 0.2184, acc = 0.9280 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:10:40.484411: step 4120, loss = 0.1865, acc = 0.9460 (270.4 examples/sec; 0.237 sec/batch)
2017-05-02 18:10:49.950648: step 4130, loss = 0.2172, acc = 0.9260 (258.5 examples/sec; 0.248 sec/batch)
2017-05-02 18:10:59.441521: step 4140, loss = 0.2263, acc = 0.9300 (267.2 examples/sec; 0.240 sec/batch)
2017-05-02 18:11:08.958214: step 4150, loss = 0.2129, acc = 0.9300 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 18:11:18.752763: step 4160, loss = 0.1703, acc = 0.9640 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 18:11:28.199330: step 4170, loss = 0.2056, acc = 0.9200 (275.5 examples/sec; 0.232 sec/batch)
2017-05-02 18:11:37.874412: step 4180, loss = 0.1963, acc = 0.9440 (245.1 examples/sec; 0.261 sec/batch)
2017-05-02 18:11:47.526654: step 4190, loss = 0.2119, acc = 0.9300 (274.3 examples/sec; 0.233 sec/batch)
2017-05-02 18:11:57.066733: step 4200, loss = 0.1937, acc = 0.9360 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 18:12:06.697511: step 4210, loss = 0.2010, acc = 0.9260 (263.8 examples/sec; 0.243 sec/batch)
2017-05-02 18:12:16.116477: step 4220, loss = 0.2064, acc = 0.9300 (270.7 examples/sec; 0.236 sec/batch)
2017-05-02 18:12:26.012934: step 4230, loss = 0.2278, acc = 0.9240 (262.1 examples/sec; 0.244 sec/batch)
2017-05-02 18:12:35.687130: step 4240, loss = 0.1815, acc = 0.9500 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 18:12:45.226540: step 4250, loss = 0.1836, acc = 0.9480 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 18:12:54.759519: step 4260, loss = 0.2144, acc = 0.9260 (260.0 examples/sec; 0.246 sec/batch)
2017-05-02 18:13:04.149090: step 4270, loss = 0.1883, acc = 0.9420 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 18:13:13.447493: step 4280, loss = 0.2450, acc = 0.9240 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 18:13:23.010921: step 4290, loss = 0.2213, acc = 0.9200 (266.9 examples/sec; 0.240 sec/batch)
2017-05-02 18:13:32.517644: step 4300, loss = 0.1907, acc = 0.9340 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 18:13:42.061934: step 4310, loss = 0.1769, acc = 0.9380 (266.6 examples/sec; 0.240 sec/batch)
2017-05-02 18:13:51.579605: step 4320, loss = 0.2096, acc = 0.9300 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 18:14:00.845500: step 4330, loss = 0.1715, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 18:14:10.240949: step 4340, loss = 0.1916, acc = 0.9360 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 18:14:19.545665: step 4350, loss = 0.2097, acc = 0.9360 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 18:14:28.830558: step 4360, loss = 0.2437, acc = 0.9020 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 18:14:38.267919: step 4370, loss = 0.1890, acc = 0.9380 (260.0 examples/sec; 0.246 sec/batch)
2017-05-02 18:14:47.674622: step 4380, loss = 0.1947, acc = 0.9340 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 18:14:57.116427: step 4390, loss = 0.2056, acc = 0.9380 (258.3 examples/sec; 0.248 sec/batch)
2017-05-02 18:15:06.498807: step 4400, loss = 0.2290, acc = 0.9380 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 18:15:15.913370: step 4410, loss = 0.2004, acc = 0.9340 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 18:15:25.484596: step 4420, loss = 0.2263, acc = 0.9240 (265.6 examples/sec; 0.241 sec/batch)
2017-05-02 18:15:34.927647: step 4430, loss = 0.2121, acc = 0.9320 (264.4 examples/sec; 0.242 sec/batch)
2017-05-02 18:15:44.303741: step 4440, loss = 0.2049, acc = 0.9280 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 18:15:53.771063: step 4450, loss = 0.2093, acc = 0.9300 (267.6 examples/sec; 0.239 sec/batch)
2017-05-02 18:16:03.180836: step 4460, loss = 0.2220, acc = 0.9160 (266.3 examples/sec; 0.240 sec/batch)
2017-05-02 18:16:12.509187: step 4470, loss = 0.1944, acc = 0.9420 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 18:16:21.838610: step 4480, loss = 0.2152, acc = 0.9240 (264.8 examples/sec; 0.242 sec/batch)
2017-05-02 18:16:31.125965: step 4490, loss = 0.1981, acc = 0.9260 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 18:16:40.474798: step 4500, loss = 0.1947, acc = 0.9320 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 18:16:49.673475: step 4510, loss = 0.2318, acc = 0.9220 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 18:16:59.148198: step 4520, loss = 0.1793, acc = 0.9460 (265.1 examples/sec; 0.241 sec/batch)
2017-05-02 18:17:08.643339: step 4530, loss = 0.2098, acc = 0.9160 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 18:17:18.035395: step 4540, loss = 0.1862, acc = 0.9480 (267.1 examples/sec; 0.240 sec/batch)
2017-05-02 18:17:27.440595: step 4550, loss = 0.2024, acc = 0.9320 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 18:17:36.884533: step 4560, loss = 0.1985, acc = 0.9380 (260.4 examples/sec; 0.246 sec/batch)
2017-05-02 18:17:46.265336: step 4570, loss = 0.2146, acc = 0.9180 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 18:17:55.686304: step 4580, loss = 0.1847, acc = 0.9500 (269.2 examples/sec; 0.238 sec/batch)
2017-05-02 18:18:05.018229: step 4590, loss = 0.1860, acc = 0.9380 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 18:18:14.375863: step 4600, loss = 0.1877, acc = 0.9360 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 18:18:23.621726: step 4610, loss = 0.1992, acc = 0.9360 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 18:18:32.995181: step 4620, loss = 0.1671, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 18:18:42.347756: step 4630, loss = 0.1803, acc = 0.9500 (270.5 examples/sec; 0.237 sec/batch)
2017-05-02 18:18:51.831571: step 4640, loss = 0.1801, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 18:19:01.198837: step 4650, loss = 0.2028, acc = 0.9340 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 18:19:10.855592: step 4660, loss = 0.1800, acc = 0.9480 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 18:19:20.138006: step 4670, loss = 0.2157, acc = 0.9340 (266.0 examples/sec; 0.241 sec/batch)
2017-05-02 18:19:29.453604: step 4680, loss = 0.1906, acc = 0.9400 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 18:19:38.846567: step 4690, loss = 0.1741, acc = 0.9520 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 18:19:48.169952: step 4700, loss = 0.1947, acc = 0.9400 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 18:19:57.629813: step 4710, loss = 0.1949, acc = 0.9340 (258.2 examples/sec; 0.248 sec/batch)
2017-05-02 18:20:06.849398: step 4720, loss = 0.2315, acc = 0.9180 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 18:20:16.293200: step 4730, loss = 0.1947, acc = 0.9400 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 18:20:25.650636: step 4740, loss = 0.2343, acc = 0.9220 (263.1 examples/sec; 0.243 sec/batch)
2017-05-02 18:20:35.351759: step 4750, loss = 0.2036, acc = 0.9160 (244.1 examples/sec; 0.262 sec/batch)
2017-05-02 18:20:44.698025: step 4760, loss = 0.2228, acc = 0.9280 (264.5 examples/sec; 0.242 sec/batch)
2017-05-02 18:20:53.963756: step 4770, loss = 0.2019, acc = 0.9300 (266.0 examples/sec; 0.241 sec/batch)
2017-05-02 18:21:03.171693: step 4780, loss = 0.2139, acc = 0.9260 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 18:21:12.569370: step 4790, loss = 0.1856, acc = 0.9440 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 18:21:21.878519: step 4800, loss = 0.1793, acc = 0.9420 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:21:31.501902: step 4810, loss = 0.1915, acc = 0.9400 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 18:21:40.877677: step 4820, loss = 0.1970, acc = 0.9220 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:21:50.267330: step 4830, loss = 0.2222, acc = 0.9240 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 18:21:59.534914: step 4840, loss = 0.2384, acc = 0.9120 (288.4 examples/sec; 0.222 sec/batch)
2017-05-02 18:22:09.014819: step 4850, loss = 0.2079, acc = 0.9340 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 18:22:18.566720: step 4860, loss = 0.1866, acc = 0.9380 (257.5 examples/sec; 0.249 sec/batch)
2017-05-02 18:22:27.844445: step 4870, loss = 0.1975, acc = 0.9380 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 18:22:37.184820: step 4880, loss = 0.2109, acc = 0.9400 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 18:22:46.506223: step 4890, loss = 0.2106, acc = 0.9340 (270.6 examples/sec; 0.237 sec/batch)
2017-05-02 18:22:56.018763: step 4900, loss = 0.2055, acc = 0.9360 (266.1 examples/sec; 0.241 sec/batch)
2017-05-02 18:23:05.379816: step 4910, loss = 0.2020, acc = 0.9360 (284.6 examples/sec; 0.225 sec/batch)
2017-05-02 18:23:14.670892: step 4920, loss = 0.1981, acc = 0.9340 (261.8 examples/sec; 0.244 sec/batch)
2017-05-02 18:23:24.119116: step 4930, loss = 0.1735, acc = 0.9460 (265.1 examples/sec; 0.241 sec/batch)
2017-05-02 18:23:33.540429: step 4940, loss = 0.1991, acc = 0.9320 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 18:23:42.976915: step 4950, loss = 0.2277, acc = 0.9260 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 18:23:52.378249: step 4960, loss = 0.1931, acc = 0.9360 (269.8 examples/sec; 0.237 sec/batch)
2017-05-02 18:24:01.764737: step 4970, loss = 0.1854, acc = 0.9420 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 18:24:11.582235: step 4980, loss = 0.1866, acc = 0.9340 (268.5 examples/sec; 0.238 sec/batch)
2017-05-02 18:24:21.160472: step 4990, loss = 0.1950, acc = 0.9460 (277.7 examples/sec; 0.231 sec/batch)
2017-05-02 18:24:30.327223: step 5000, loss = 0.2041, acc = 0.9340 (281.1 examples/sec; 0.228 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 18:24:30.832782: step 5000, loss = 0.1619, acc = 0.9480, f1neg = 0.9430, f1pos = 0.9522, f1 = 0.9476
[Eval_batch(1)(2000,4000)] 2017-05-02 18:24:31.335522: step 5000, loss = 0.1629, acc = 0.9515, f1neg = 0.9432, f1pos = 0.9577, f1 = 0.9505
[Eval_batch(2)(2000,6000)] 2017-05-02 18:24:31.845347: step 5000, loss = 0.1812, acc = 0.9430, f1neg = 0.9395, f1pos = 0.9461, f1 = 0.9428
[Eval_batch(3)(2000,8000)] 2017-05-02 18:24:32.360172: step 5000, loss = 0.1870, acc = 0.9405, f1neg = 0.9405, f1pos = 0.9405, f1 = 0.9405
[Eval_batch(4)(2000,10000)] 2017-05-02 18:24:32.861958: step 5000, loss = 0.1856, acc = 0.9405, f1neg = 0.9419, f1pos = 0.9390, f1 = 0.9405
[Eval_batch(5)(2000,12000)] 2017-05-02 18:24:33.364848: step 5000, loss = 0.1761, acc = 0.9395, f1neg = 0.9351, f1pos = 0.9433, f1 = 0.9392
[Eval_batch(6)(2000,14000)] 2017-05-02 18:24:33.862205: step 5000, loss = 0.1705, acc = 0.9485, f1neg = 0.9470, f1pos = 0.9499, f1 = 0.9485
[Eval_batch(7)(2000,16000)] 2017-05-02 18:24:34.364361: step 5000, loss = 0.1663, acc = 0.9485, f1neg = 0.9388, f1pos = 0.9555, f1 = 0.9472
[Eval_batch(8)(2000,18000)] 2017-05-02 18:24:34.875421: step 5000, loss = 0.1698, acc = 0.9450, f1neg = 0.9371, f1pos = 0.9512, f1 = 0.9441
[Eval_batch(9)(2000,20000)] 2017-05-02 18:24:35.380459: step 5000, loss = 0.1680, acc = 0.9460, f1neg = 0.9421, f1pos = 0.9494, f1 = 0.9458
[Eval_batch(10)(2000,22000)] 2017-05-02 18:24:35.890413: step 5000, loss = 0.1781, acc = 0.9460, f1neg = 0.9421, f1pos = 0.9494, f1 = 0.9457
[Eval_batch(11)(2000,24000)] 2017-05-02 18:24:36.403166: step 5000, loss = 0.1785, acc = 0.9445, f1neg = 0.9356, f1pos = 0.9513, f1 = 0.9434
[Eval_batch(12)(2000,26000)] 2017-05-02 18:24:36.909490: step 5000, loss = 0.1742, acc = 0.9460, f1neg = 0.9439, f1pos = 0.9480, f1 = 0.9459
[Eval_batch(13)(2000,28000)] 2017-05-02 18:24:37.414970: step 5000, loss = 0.1612, acc = 0.9510, f1neg = 0.9383, f1pos = 0.9594, f1 = 0.9488
[Eval_batch(14)(2000,30000)] 2017-05-02 18:24:37.922385: step 5000, loss = 0.1853, acc = 0.9460, f1neg = 0.9351, f1pos = 0.9538, f1 = 0.9444
[Eval_batch(15)(2000,32000)] 2017-05-02 18:24:38.417878: step 5000, loss = 0.1715, acc = 0.9480, f1neg = 0.9436, f1pos = 0.9518, f1 = 0.9477
[Eval_batch(16)(2000,34000)] 2017-05-02 18:24:38.918157: step 5000, loss = 0.1626, acc = 0.9485, f1neg = 0.9461, f1pos = 0.9507, f1 = 0.9484
[Eval_batch(17)(2000,36000)] 2017-05-02 18:24:39.426533: step 5000, loss = 0.1832, acc = 0.9395, f1neg = 0.9392, f1pos = 0.9398, f1 = 0.9395
[Eval_batch(18)(2000,38000)] 2017-05-02 18:24:39.865929: step 5000, loss = 0.1846, acc = 0.9360, f1neg = 0.9381, f1pos = 0.9337, f1 = 0.9359
[Eval_batch(19)(2000,40000)] 2017-05-02 18:24:40.363825: step 5000, loss = 0.1615, acc = 0.9580, f1neg = 0.9514, f1pos = 0.9630, f1 = 0.9572
[Eval_batch(20)(2000,42000)] 2017-05-02 18:24:40.823723: step 5000, loss = 0.1681, acc = 0.9510, f1neg = 0.9564, f1pos = 0.9440, f1 = 0.9502
[Eval_batch(21)(2000,44000)] 2017-05-02 18:24:41.300168: step 5000, loss = 0.1576, acc = 0.9555, f1neg = 0.9516, f1pos = 0.9588, f1 = 0.9552
[Eval_batch(22)(2000,46000)] 2017-05-02 18:24:41.770603: step 5000, loss = 0.1652, acc = 0.9480, f1neg = 0.9482, f1pos = 0.9478, f1 = 0.9480
[Eval_batch(23)(2000,48000)] 2017-05-02 18:24:42.273242: step 5000, loss = 0.1760, acc = 0.9435, f1neg = 0.9355, f1pos = 0.9497, f1 = 0.9426
[Eval_batch(24)(2000,50000)] 2017-05-02 18:24:42.739108: step 5000, loss = 0.1654, acc = 0.9450, f1neg = 0.9366, f1pos = 0.9515, f1 = 0.9440
[Eval_batch(25)(2000,52000)] 2017-05-02 18:24:43.180957: step 5000, loss = 0.1553, acc = 0.9575, f1neg = 0.9532, f1pos = 0.9611, f1 = 0.9571
[Eval_batch(26)(2000,54000)] 2017-05-02 18:24:43.649847: step 5000, loss = 0.1631, acc = 0.9510, f1neg = 0.9536, f1pos = 0.9480, f1 = 0.9508
[Eval_batch(27)(2000,56000)] 2017-05-02 18:24:44.238323: step 5000, loss = 0.1526, acc = 0.9560, f1neg = 0.9536, f1pos = 0.9581, f1 = 0.9559
[Eval] 2017-05-02 18:24:44.238394: step 5000, acc = 0.9472, f1 = 0.9467
[Test_batch(0)(2000,2000)] 2017-05-02 18:24:44.739241: step 5000, loss = 0.2014, acc = 0.9390, f1neg = 0.9418, f1pos = 0.9359, f1 = 0.9389
[Test_batch(1)(2000,4000)] 2017-05-02 18:24:45.240818: step 5000, loss = 0.1934, acc = 0.9350, f1neg = 0.9422, f1pos = 0.9258, f1 = 0.9340
[Test_batch(2)(2000,6000)] 2017-05-02 18:24:45.719355: step 5000, loss = 0.1959, acc = 0.9425, f1neg = 0.9470, f1pos = 0.9372, f1 = 0.9421
[Test_batch(3)(2000,8000)] 2017-05-02 18:24:46.202395: step 5000, loss = 0.2100, acc = 0.9255, f1neg = 0.9302, f1pos = 0.9201, f1 = 0.9252
[Test_batch(4)(2000,10000)] 2017-05-02 18:24:46.706707: step 5000, loss = 0.2165, acc = 0.9255, f1neg = 0.9252, f1pos = 0.9258, f1 = 0.9255
[Test_batch(5)(2000,12000)] 2017-05-02 18:24:47.179763: step 5000, loss = 0.2219, acc = 0.9245, f1neg = 0.9248, f1pos = 0.9242, f1 = 0.9245
[Test_batch(6)(2000,14000)] 2017-05-02 18:24:47.677925: step 5000, loss = 0.2057, acc = 0.9260, f1neg = 0.9230, f1pos = 0.9288, f1 = 0.9259
[Test_batch(7)(2000,16000)] 2017-05-02 18:24:48.182821: step 5000, loss = 0.1942, acc = 0.9355, f1neg = 0.9413, f1pos = 0.9284, f1 = 0.9349
[Test_batch(8)(2000,18000)] 2017-05-02 18:24:48.644502: step 5000, loss = 0.1910, acc = 0.9425, f1neg = 0.9417, f1pos = 0.9433, f1 = 0.9425
[Test_batch(9)(2000,20000)] 2017-05-02 18:24:49.157424: step 5000, loss = 0.2223, acc = 0.9225, f1neg = 0.9179, f1pos = 0.9266, f1 = 0.9223
[Test_batch(10)(2000,22000)] 2017-05-02 18:24:49.652962: step 5000, loss = 0.1999, acc = 0.9325, f1neg = 0.9241, f1pos = 0.9392, f1 = 0.9317
[Test_batch(11)(2000,24000)] 2017-05-02 18:24:50.117130: step 5000, loss = 0.1998, acc = 0.9395, f1neg = 0.9409, f1pos = 0.9380, f1 = 0.9395
[Test_batch(12)(2000,26000)] 2017-05-02 18:24:50.600443: step 5000, loss = 0.1801, acc = 0.9435, f1neg = 0.9459, f1pos = 0.9409, f1 = 0.9434
[Test_batch(13)(2000,28000)] 2017-05-02 18:24:51.098811: step 5000, loss = 0.2129, acc = 0.9230, f1neg = 0.9138, f1pos = 0.9304, f1 = 0.9221
[Test_batch(14)(2000,30000)] 2017-05-02 18:24:51.577155: step 5000, loss = 0.1801, acc = 0.9435, f1neg = 0.9352, f1pos = 0.9499, f1 = 0.9426
[Test_batch(15)(2000,32000)] 2017-05-02 18:24:52.048558: step 5000, loss = 0.1933, acc = 0.9425, f1neg = 0.9416, f1pos = 0.9434, f1 = 0.9425
[Test_batch(16)(2000,34000)] 2017-05-02 18:24:52.519005: step 5000, loss = 0.1841, acc = 0.9445, f1neg = 0.9433, f1pos = 0.9456, f1 = 0.9445
[Test_batch(17)(2000,36000)] 2017-05-02 18:24:53.021659: step 5000, loss = 0.1710, acc = 0.9510, f1neg = 0.9462, f1pos = 0.9550, f1 = 0.9506
[Test_batch(18)(2000,38000)] 2017-05-02 18:24:53.545081: step 5000, loss = 0.1604, acc = 0.9540, f1neg = 0.9528, f1pos = 0.9552, f1 = 0.9540
[Test] 2017-05-02 18:24:53.545197: step 5000, acc = 0.9364, f1 = 0.9361
[Status] 2017-05-02 18:24:53.545228: step 5000, maxindex = 5000, maxdev = 0.9472, maxtst = 0.9364
2017-05-02 18:25:06.022526: step 5010, loss = 0.1983, acc = 0.9540 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 18:25:15.463186: step 5020, loss = 0.2074, acc = 0.9400 (266.2 examples/sec; 0.240 sec/batch)
2017-05-02 18:25:24.815961: step 5030, loss = 0.1772, acc = 0.9380 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 18:25:35.022012: step 5040, loss = 0.1791, acc = 0.9420 (266.4 examples/sec; 0.240 sec/batch)
2017-05-02 18:25:44.230963: step 5050, loss = 0.1879, acc = 0.9540 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 18:25:53.531695: step 5060, loss = 0.1838, acc = 0.9400 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 18:26:02.836291: step 5070, loss = 0.2002, acc = 0.9420 (267.2 examples/sec; 0.239 sec/batch)
2017-05-02 18:26:12.145755: step 5080, loss = 0.1926, acc = 0.9380 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 18:26:21.535407: step 5090, loss = 0.2014, acc = 0.9360 (260.2 examples/sec; 0.246 sec/batch)
2017-05-02 18:26:30.747775: step 5100, loss = 0.1688, acc = 0.9400 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 18:26:40.147085: step 5110, loss = 0.1895, acc = 0.9460 (266.8 examples/sec; 0.240 sec/batch)
2017-05-02 18:26:49.765847: step 5120, loss = 0.2065, acc = 0.9240 (265.8 examples/sec; 0.241 sec/batch)
2017-05-02 18:26:58.870129: step 5130, loss = 0.2269, acc = 0.9120 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 18:27:08.202083: step 5140, loss = 0.2014, acc = 0.9460 (264.2 examples/sec; 0.242 sec/batch)
2017-05-02 18:27:17.434595: step 5150, loss = 0.1913, acc = 0.9300 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 18:27:26.896807: step 5160, loss = 0.1633, acc = 0.9500 (222.9 examples/sec; 0.287 sec/batch)
2017-05-02 18:27:36.272066: step 5170, loss = 0.2040, acc = 0.9320 (266.3 examples/sec; 0.240 sec/batch)
2017-05-02 18:27:45.630776: step 5180, loss = 0.2127, acc = 0.9360 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 18:27:55.056296: step 5190, loss = 0.2042, acc = 0.9260 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 18:28:04.553666: step 5200, loss = 0.1952, acc = 0.9120 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:28:14.277756: step 5210, loss = 0.2066, acc = 0.9280 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 18:28:23.545638: step 5220, loss = 0.2285, acc = 0.9200 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 18:28:32.856502: step 5230, loss = 0.1825, acc = 0.9460 (259.1 examples/sec; 0.247 sec/batch)
2017-05-02 18:28:42.142733: step 5240, loss = 0.2334, acc = 0.9040 (266.5 examples/sec; 0.240 sec/batch)
2017-05-02 18:28:51.493479: step 5250, loss = 0.1904, acc = 0.9440 (268.2 examples/sec; 0.239 sec/batch)
2017-05-02 18:29:00.746413: step 5260, loss = 0.1674, acc = 0.9520 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 18:29:10.128178: step 5270, loss = 0.1751, acc = 0.9480 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 18:29:19.610112: step 5280, loss = 0.2044, acc = 0.9260 (274.3 examples/sec; 0.233 sec/batch)
2017-05-02 18:29:28.955198: step 5290, loss = 0.1664, acc = 0.9460 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 18:29:38.322068: step 5300, loss = 0.1967, acc = 0.9220 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:29:47.822823: step 5310, loss = 0.1910, acc = 0.9300 (269.4 examples/sec; 0.238 sec/batch)
2017-05-02 18:29:57.161687: step 5320, loss = 0.1920, acc = 0.9340 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 18:30:06.533876: step 5330, loss = 0.2038, acc = 0.9300 (274.4 examples/sec; 0.233 sec/batch)
2017-05-02 18:30:16.212482: step 5340, loss = 0.1995, acc = 0.9420 (258.3 examples/sec; 0.248 sec/batch)
2017-05-02 18:30:25.643851: step 5350, loss = 0.1816, acc = 0.9440 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 18:30:35.034563: step 5360, loss = 0.1883, acc = 0.9520 (257.5 examples/sec; 0.249 sec/batch)
2017-05-02 18:30:44.519582: step 5370, loss = 0.1826, acc = 0.9400 (256.7 examples/sec; 0.249 sec/batch)
2017-05-02 18:30:53.922909: step 5380, loss = 0.2091, acc = 0.9220 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 18:31:03.355196: step 5390, loss = 0.1978, acc = 0.9360 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 18:31:12.594833: step 5400, loss = 0.1902, acc = 0.9500 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 18:31:21.856176: step 5410, loss = 0.2077, acc = 0.9220 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:31:31.236492: step 5420, loss = 0.2005, acc = 0.9300 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 18:31:40.579304: step 5430, loss = 0.2481, acc = 0.9120 (279.9 examples/sec; 0.229 sec/batch)
2017-05-02 18:31:49.985294: step 5440, loss = 0.2073, acc = 0.9280 (265.2 examples/sec; 0.241 sec/batch)
2017-05-02 18:31:59.209725: step 5450, loss = 0.1944, acc = 0.9420 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 18:32:08.636681: step 5460, loss = 0.2342, acc = 0.9220 (261.2 examples/sec; 0.245 sec/batch)
2017-05-02 18:32:17.822362: step 5470, loss = 0.1822, acc = 0.9480 (264.2 examples/sec; 0.242 sec/batch)
2017-05-02 18:32:26.967443: step 5480, loss = 0.1825, acc = 0.9440 (286.9 examples/sec; 0.223 sec/batch)
2017-05-02 18:32:36.571623: step 5490, loss = 0.2071, acc = 0.9240 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 18:32:45.760074: step 5500, loss = 0.2367, acc = 0.9120 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 18:32:55.016198: step 5510, loss = 0.1908, acc = 0.9300 (247.8 examples/sec; 0.258 sec/batch)
2017-05-02 18:33:04.354913: step 5520, loss = 0.1955, acc = 0.9280 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 18:33:13.599766: step 5530, loss = 0.2056, acc = 0.9420 (282.8 examples/sec; 0.226 sec/batch)
2017-05-02 18:33:23.079205: step 5540, loss = 0.2174, acc = 0.9300 (282.6 examples/sec; 0.227 sec/batch)
2017-05-02 18:33:32.340602: step 5550, loss = 0.2123, acc = 0.9280 (263.8 examples/sec; 0.243 sec/batch)
2017-05-02 18:33:41.656745: step 5560, loss = 0.2161, acc = 0.9260 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 18:33:51.044571: step 5570, loss = 0.1789, acc = 0.9420 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 18:34:00.455161: step 5580, loss = 0.1830, acc = 0.9520 (268.6 examples/sec; 0.238 sec/batch)
2017-05-02 18:34:09.825994: step 5590, loss = 0.2079, acc = 0.9240 (265.2 examples/sec; 0.241 sec/batch)
2017-05-02 18:34:19.074838: step 5600, loss = 0.2161, acc = 0.9180 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 18:34:28.520190: step 5610, loss = 0.2067, acc = 0.9240 (268.3 examples/sec; 0.238 sec/batch)
2017-05-02 18:34:37.868182: step 5620, loss = 0.1938, acc = 0.9440 (260.7 examples/sec; 0.245 sec/batch)
2017-05-02 18:34:47.259484: step 5630, loss = 0.1942, acc = 0.9280 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 18:34:56.512363: step 5640, loss = 0.1889, acc = 0.9280 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 18:35:05.778875: step 5650, loss = 0.1954, acc = 0.9340 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:35:15.017880: step 5660, loss = 0.1886, acc = 0.9480 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 18:35:24.218221: step 5670, loss = 0.2028, acc = 0.9240 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 18:35:33.579159: step 5680, loss = 0.2185, acc = 0.9320 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 18:35:42.930022: step 5690, loss = 0.1990, acc = 0.9340 (265.3 examples/sec; 0.241 sec/batch)
2017-05-02 18:35:52.147059: step 5700, loss = 0.1833, acc = 0.9460 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 18:36:01.374037: step 5710, loss = 0.1634, acc = 0.9540 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 18:36:10.856913: step 5720, loss = 0.1810, acc = 0.9400 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 18:36:20.159897: step 5730, loss = 0.2100, acc = 0.9240 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 18:36:29.315067: step 5740, loss = 0.1973, acc = 0.9320 (292.9 examples/sec; 0.218 sec/batch)
2017-05-02 18:36:38.458878: step 5750, loss = 0.1955, acc = 0.9240 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 18:36:47.791490: step 5760, loss = 0.1960, acc = 0.9360 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 18:36:57.022843: step 5770, loss = 0.1959, acc = 0.9400 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 18:37:06.189449: step 5780, loss = 0.1792, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 18:37:15.477178: step 5790, loss = 0.1807, acc = 0.9480 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 18:37:24.806550: step 5800, loss = 0.1847, acc = 0.9380 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 18:37:34.193315: step 5810, loss = 0.1822, acc = 0.9500 (258.8 examples/sec; 0.247 sec/batch)
2017-05-02 18:37:43.408528: step 5820, loss = 0.1944, acc = 0.9520 (274.1 examples/sec; 0.233 sec/batch)
2017-05-02 18:37:52.717327: step 5830, loss = 0.2095, acc = 0.9380 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 18:38:01.979156: step 5840, loss = 0.2037, acc = 0.9300 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 18:38:11.198866: step 5850, loss = 0.1654, acc = 0.9440 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 18:38:20.515180: step 5860, loss = 0.2180, acc = 0.9320 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 18:38:29.853018: step 5870, loss = 0.1753, acc = 0.9480 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 18:38:39.238051: step 5880, loss = 0.1929, acc = 0.9400 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 18:38:48.472763: step 5890, loss = 0.2028, acc = 0.9360 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 18:38:57.758403: step 5900, loss = 0.2391, acc = 0.9260 (266.3 examples/sec; 0.240 sec/batch)
2017-05-02 18:39:07.067092: step 5910, loss = 0.1965, acc = 0.9320 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 18:39:16.331911: step 5920, loss = 0.1713, acc = 0.9420 (268.2 examples/sec; 0.239 sec/batch)
2017-05-02 18:39:25.666720: step 5930, loss = 0.1977, acc = 0.9360 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 18:39:35.084324: step 5940, loss = 0.2019, acc = 0.9320 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 18:39:44.519200: step 5950, loss = 0.2032, acc = 0.9360 (278.6 examples/sec; 0.230 sec/batch)
2017-05-02 18:39:53.840675: step 5960, loss = 0.1840, acc = 0.9380 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 18:40:03.354711: step 5970, loss = 0.2132, acc = 0.9320 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 18:40:12.604404: step 5980, loss = 0.1934, acc = 0.9400 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 18:40:22.087972: step 5990, loss = 0.1924, acc = 0.9440 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:40:31.423160: step 6000, loss = 0.1990, acc = 0.9380 (278.9 examples/sec; 0.229 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 18:40:31.880927: step 6000, loss = 0.1541, acc = 0.9515, f1neg = 0.9472, f1pos = 0.9552, f1 = 0.9512
[Eval_batch(1)(2000,4000)] 2017-05-02 18:40:32.340709: step 6000, loss = 0.1596, acc = 0.9530, f1neg = 0.9459, f1pos = 0.9585, f1 = 0.9522
[Eval_batch(2)(2000,6000)] 2017-05-02 18:40:32.856331: step 6000, loss = 0.1755, acc = 0.9455, f1neg = 0.9428, f1pos = 0.9480, f1 = 0.9454
[Eval_batch(3)(2000,8000)] 2017-05-02 18:40:33.356816: step 6000, loss = 0.1795, acc = 0.9400, f1neg = 0.9408, f1pos = 0.9392, f1 = 0.9400
[Eval_batch(4)(2000,10000)] 2017-05-02 18:40:33.859466: step 6000, loss = 0.1772, acc = 0.9430, f1neg = 0.9448, f1pos = 0.9411, f1 = 0.9429
[Eval_batch(5)(2000,12000)] 2017-05-02 18:40:34.367804: step 6000, loss = 0.1710, acc = 0.9415, f1neg = 0.9380, f1pos = 0.9446, f1 = 0.9413
[Eval_batch(6)(2000,14000)] 2017-05-02 18:40:34.839631: step 6000, loss = 0.1615, acc = 0.9495, f1neg = 0.9487, f1pos = 0.9503, f1 = 0.9495
[Eval_batch(7)(2000,16000)] 2017-05-02 18:40:35.334399: step 6000, loss = 0.1620, acc = 0.9485, f1neg = 0.9394, f1pos = 0.9552, f1 = 0.9473
[Eval_batch(8)(2000,18000)] 2017-05-02 18:40:35.797146: step 6000, loss = 0.1629, acc = 0.9490, f1neg = 0.9422, f1pos = 0.9543, f1 = 0.9483
[Eval_batch(9)(2000,20000)] 2017-05-02 18:40:36.303201: step 6000, loss = 0.1632, acc = 0.9510, f1neg = 0.9481, f1pos = 0.9536, f1 = 0.9508
[Eval_batch(10)(2000,22000)] 2017-05-02 18:40:36.808912: step 6000, loss = 0.1722, acc = 0.9470, f1neg = 0.9440, f1pos = 0.9497, f1 = 0.9468
[Eval_batch(11)(2000,24000)] 2017-05-02 18:40:37.286357: step 6000, loss = 0.1738, acc = 0.9445, f1neg = 0.9364, f1pos = 0.9508, f1 = 0.9436
[Eval_batch(12)(2000,26000)] 2017-05-02 18:40:37.755025: step 6000, loss = 0.1673, acc = 0.9455, f1neg = 0.9442, f1pos = 0.9468, f1 = 0.9455
[Eval_batch(13)(2000,28000)] 2017-05-02 18:40:38.220921: step 6000, loss = 0.1558, acc = 0.9540, f1neg = 0.9428, f1pos = 0.9615, f1 = 0.9522
[Eval_batch(14)(2000,30000)] 2017-05-02 18:40:38.727357: step 6000, loss = 0.1794, acc = 0.9435, f1neg = 0.9327, f1pos = 0.9513, f1 = 0.9420
[Eval_batch(15)(2000,32000)] 2017-05-02 18:40:39.229285: step 6000, loss = 0.1658, acc = 0.9515, f1neg = 0.9482, f1pos = 0.9544, f1 = 0.9513
[Eval_batch(16)(2000,34000)] 2017-05-02 18:40:39.674363: step 6000, loss = 0.1573, acc = 0.9480, f1neg = 0.9462, f1pos = 0.9497, f1 = 0.9479
[Eval_batch(17)(2000,36000)] 2017-05-02 18:40:40.144460: step 6000, loss = 0.1765, acc = 0.9420, f1neg = 0.9427, f1pos = 0.9413, f1 = 0.9420
[Eval_batch(18)(2000,38000)] 2017-05-02 18:40:40.606019: step 6000, loss = 0.1754, acc = 0.9390, f1neg = 0.9418, f1pos = 0.9359, f1 = 0.9389
[Eval_batch(19)(2000,40000)] 2017-05-02 18:40:41.072671: step 6000, loss = 0.1554, acc = 0.9595, f1neg = 0.9535, f1pos = 0.9641, f1 = 0.9588
[Eval_batch(20)(2000,42000)] 2017-05-02 18:40:41.555535: step 6000, loss = 0.1578, acc = 0.9525, f1neg = 0.9582, f1pos = 0.9451, f1 = 0.9516
[Eval_batch(21)(2000,44000)] 2017-05-02 18:40:42.058222: step 6000, loss = 0.1545, acc = 0.9550, f1neg = 0.9515, f1pos = 0.9580, f1 = 0.9548
[Eval_batch(22)(2000,46000)] 2017-05-02 18:40:42.530899: step 6000, loss = 0.1585, acc = 0.9495, f1neg = 0.9502, f1pos = 0.9488, f1 = 0.9495
[Eval_batch(23)(2000,48000)] 2017-05-02 18:40:42.971309: step 6000, loss = 0.1693, acc = 0.9445, f1neg = 0.9377, f1pos = 0.9499, f1 = 0.9438
[Eval_batch(24)(2000,50000)] 2017-05-02 18:40:43.472712: step 6000, loss = 0.1584, acc = 0.9450, f1neg = 0.9374, f1pos = 0.9510, f1 = 0.9442
[Eval_batch(25)(2000,52000)] 2017-05-02 18:40:43.977931: step 6000, loss = 0.1486, acc = 0.9570, f1neg = 0.9531, f1pos = 0.9603, f1 = 0.9567
[Eval_batch(26)(2000,54000)] 2017-05-02 18:40:44.450638: step 6000, loss = 0.1534, acc = 0.9540, f1neg = 0.9568, f1pos = 0.9508, f1 = 0.9538
[Eval_batch(27)(2000,56000)] 2017-05-02 18:40:45.058675: step 6000, loss = 0.1444, acc = 0.9585, f1neg = 0.9566, f1pos = 0.9602, f1 = 0.9584
[Eval] 2017-05-02 18:40:45.058754: step 6000, acc = 0.9487, f1 = 0.9482
[Test_batch(0)(2000,2000)] 2017-05-02 18:40:45.557863: step 6000, loss = 0.1949, acc = 0.9405, f1neg = 0.9441, f1pos = 0.9364, f1 = 0.9403
[Test_batch(1)(2000,4000)] 2017-05-02 18:40:46.028380: step 6000, loss = 0.1839, acc = 0.9390, f1neg = 0.9463, f1pos = 0.9295, f1 = 0.9379
[Test_batch(2)(2000,6000)] 2017-05-02 18:40:46.507853: step 6000, loss = 0.1889, acc = 0.9405, f1neg = 0.9458, f1pos = 0.9341, f1 = 0.9399
[Test_batch(3)(2000,8000)] 2017-05-02 18:40:46.962616: step 6000, loss = 0.2030, acc = 0.9225, f1neg = 0.9285, f1pos = 0.9153, f1 = 0.9219
[Test_batch(4)(2000,10000)] 2017-05-02 18:40:47.429807: step 6000, loss = 0.2098, acc = 0.9270, f1neg = 0.9279, f1pos = 0.9260, f1 = 0.9270
[Test_batch(5)(2000,12000)] 2017-05-02 18:40:47.904125: step 6000, loss = 0.2152, acc = 0.9240, f1neg = 0.9256, f1pos = 0.9223, f1 = 0.9240
[Test_batch(6)(2000,14000)] 2017-05-02 18:40:48.381194: step 6000, loss = 0.1975, acc = 0.9340, f1neg = 0.9323, f1pos = 0.9356, f1 = 0.9340
[Test_batch(7)(2000,16000)] 2017-05-02 18:40:48.844928: step 6000, loss = 0.1877, acc = 0.9380, f1neg = 0.9445, f1pos = 0.9297, f1 = 0.9371
[Test_batch(8)(2000,18000)] 2017-05-02 18:40:49.317883: step 6000, loss = 0.1853, acc = 0.9455, f1neg = 0.9455, f1pos = 0.9455, f1 = 0.9455
[Test_batch(9)(2000,20000)] 2017-05-02 18:40:49.784021: step 6000, loss = 0.2181, acc = 0.9210, f1neg = 0.9180, f1pos = 0.9238, f1 = 0.9209
[Test_batch(10)(2000,22000)] 2017-05-02 18:40:50.264503: step 6000, loss = 0.1984, acc = 0.9355, f1neg = 0.9288, f1pos = 0.9410, f1 = 0.9349
[Test_batch(11)(2000,24000)] 2017-05-02 18:40:50.739574: step 6000, loss = 0.1921, acc = 0.9390, f1neg = 0.9413, f1pos = 0.9365, f1 = 0.9389
[Test_batch(12)(2000,26000)] 2017-05-02 18:40:51.217088: step 6000, loss = 0.1755, acc = 0.9425, f1neg = 0.9454, f1pos = 0.9392, f1 = 0.9423
[Test_batch(13)(2000,28000)] 2017-05-02 18:40:51.686344: step 6000, loss = 0.2076, acc = 0.9260, f1neg = 0.9187, f1pos = 0.9321, f1 = 0.9254
[Test_batch(14)(2000,30000)] 2017-05-02 18:40:52.153347: step 6000, loss = 0.1770, acc = 0.9470, f1neg = 0.9402, f1pos = 0.9524, f1 = 0.9463
[Test_batch(15)(2000,32000)] 2017-05-02 18:40:52.623669: step 6000, loss = 0.1884, acc = 0.9380, f1neg = 0.9379, f1pos = 0.9381, f1 = 0.9380
[Test_batch(16)(2000,34000)] 2017-05-02 18:40:53.080553: step 6000, loss = 0.1776, acc = 0.9430, f1neg = 0.9424, f1pos = 0.9436, f1 = 0.9430
[Test_batch(17)(2000,36000)] 2017-05-02 18:40:53.547345: step 6000, loss = 0.1643, acc = 0.9515, f1neg = 0.9474, f1pos = 0.9550, f1 = 0.9512
[Test_batch(18)(2000,38000)] 2017-05-02 18:40:54.060761: step 6000, loss = 0.1510, acc = 0.9585, f1neg = 0.9578, f1pos = 0.9592, f1 = 0.9585
[Test] 2017-05-02 18:40:54.060851: step 6000, acc = 0.9375, f1 = 0.9372
[Status] 2017-05-02 18:40:54.060878: step 6000, maxindex = 6000, maxdev = 0.9487, maxtst = 0.9375
2017-05-02 18:41:06.574043: step 6010, loss = 0.2039, acc = 0.9300 (267.9 examples/sec; 0.239 sec/batch)
2017-05-02 18:41:15.948713: step 6020, loss = 0.1658, acc = 0.9480 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 18:41:25.296200: step 6030, loss = 0.2013, acc = 0.9360 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 18:41:34.615453: step 6040, loss = 0.1941, acc = 0.9360 (250.4 examples/sec; 0.256 sec/batch)
2017-05-02 18:41:44.595305: step 6050, loss = 0.1926, acc = 0.9380 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 18:41:53.877987: step 6060, loss = 0.1801, acc = 0.9340 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 18:42:03.183915: step 6070, loss = 0.2189, acc = 0.9300 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 18:42:12.511959: step 6080, loss = 0.1884, acc = 0.9300 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 18:42:21.956361: step 6090, loss = 0.1707, acc = 0.9520 (270.6 examples/sec; 0.236 sec/batch)
2017-05-02 18:42:31.370953: step 6100, loss = 0.2024, acc = 0.9240 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 18:42:40.796121: step 6110, loss = 0.1774, acc = 0.9480 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 18:42:50.001328: step 6120, loss = 0.2170, acc = 0.9240 (281.3 examples/sec; 0.228 sec/batch)
2017-05-02 18:42:59.256507: step 6130, loss = 0.2117, acc = 0.9220 (267.5 examples/sec; 0.239 sec/batch)
2017-05-02 18:43:08.599258: step 6140, loss = 0.1920, acc = 0.9360 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 18:43:18.021454: step 6150, loss = 0.1814, acc = 0.9400 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 18:43:27.374164: step 6160, loss = 0.1799, acc = 0.9360 (285.1 examples/sec; 0.224 sec/batch)
2017-05-02 18:43:36.860592: step 6170, loss = 0.1867, acc = 0.9420 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:43:46.170404: step 6180, loss = 0.1863, acc = 0.9340 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 18:43:55.475598: step 6190, loss = 0.2354, acc = 0.9100 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 18:44:04.940678: step 6200, loss = 0.1861, acc = 0.9420 (289.7 examples/sec; 0.221 sec/batch)
2017-05-02 18:44:14.271750: step 6210, loss = 0.2025, acc = 0.9280 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 18:44:23.778197: step 6220, loss = 0.1726, acc = 0.9400 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 18:44:33.338778: step 6230, loss = 0.1765, acc = 0.9580 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 18:44:42.533039: step 6240, loss = 0.2295, acc = 0.9180 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 18:44:51.725100: step 6250, loss = 0.2352, acc = 0.9120 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 18:45:00.953903: step 6260, loss = 0.1585, acc = 0.9600 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 18:45:10.303706: step 6270, loss = 0.1951, acc = 0.9320 (270.4 examples/sec; 0.237 sec/batch)
2017-05-02 18:45:19.433283: step 6280, loss = 0.1789, acc = 0.9460 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 18:45:28.661530: step 6290, loss = 0.2385, acc = 0.9180 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 18:45:38.072082: step 6300, loss = 0.1765, acc = 0.9440 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 18:45:47.591238: step 6310, loss = 0.1765, acc = 0.9420 (234.8 examples/sec; 0.273 sec/batch)
2017-05-02 18:45:56.943355: step 6320, loss = 0.2185, acc = 0.9300 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 18:46:06.190983: step 6330, loss = 0.1816, acc = 0.9400 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 18:46:15.493470: step 6340, loss = 0.2045, acc = 0.9260 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 18:46:24.812056: step 6350, loss = 0.2014, acc = 0.9340 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 18:46:34.217074: step 6360, loss = 0.1808, acc = 0.9420 (264.9 examples/sec; 0.242 sec/batch)
2017-05-02 18:46:43.487897: step 6370, loss = 0.1867, acc = 0.9440 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 18:46:52.985436: step 6380, loss = 0.1939, acc = 0.9380 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 18:47:02.254633: step 6390, loss = 0.1766, acc = 0.9440 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 18:47:11.507274: step 6400, loss = 0.1987, acc = 0.9300 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 18:47:20.774472: step 6410, loss = 0.2227, acc = 0.9180 (257.5 examples/sec; 0.249 sec/batch)
2017-05-02 18:47:30.128227: step 6420, loss = 0.1857, acc = 0.9420 (265.7 examples/sec; 0.241 sec/batch)
2017-05-02 18:47:39.366953: step 6430, loss = 0.2104, acc = 0.9360 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 18:47:48.593771: step 6440, loss = 0.1592, acc = 0.9540 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 18:47:57.849302: step 6450, loss = 0.2283, acc = 0.9220 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 18:48:07.126964: step 6460, loss = 0.1948, acc = 0.9420 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 18:48:16.522184: step 6470, loss = 0.1668, acc = 0.9500 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:48:25.821462: step 6480, loss = 0.2048, acc = 0.9300 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 18:48:35.187997: step 6490, loss = 0.2038, acc = 0.9300 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 18:48:44.414597: step 6500, loss = 0.1852, acc = 0.9300 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 18:48:54.020218: step 6510, loss = 0.1755, acc = 0.9460 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 18:49:03.331282: step 6520, loss = 0.1924, acc = 0.9460 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 18:49:12.718577: step 6530, loss = 0.1778, acc = 0.9460 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 18:49:21.912516: step 6540, loss = 0.1823, acc = 0.9460 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:49:31.261833: step 6550, loss = 0.1667, acc = 0.9480 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 18:49:40.679535: step 6560, loss = 0.1616, acc = 0.9500 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 18:49:49.966515: step 6570, loss = 0.1759, acc = 0.9340 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 18:49:59.366447: step 6580, loss = 0.1873, acc = 0.9460 (268.2 examples/sec; 0.239 sec/batch)
2017-05-02 18:50:08.773713: step 6590, loss = 0.1741, acc = 0.9460 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 18:50:18.154599: step 6600, loss = 0.2037, acc = 0.9280 (266.9 examples/sec; 0.240 sec/batch)
2017-05-02 18:50:27.549196: step 6610, loss = 0.1919, acc = 0.9400 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 18:50:36.939795: step 6620, loss = 0.1926, acc = 0.9340 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 18:50:46.134157: step 6630, loss = 0.1968, acc = 0.9300 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 18:50:55.469608: step 6640, loss = 0.1814, acc = 0.9380 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 18:51:04.747746: step 6650, loss = 0.2073, acc = 0.9380 (266.7 examples/sec; 0.240 sec/batch)
2017-05-02 18:51:14.289495: step 6660, loss = 0.1800, acc = 0.9400 (265.2 examples/sec; 0.241 sec/batch)
2017-05-02 18:51:23.948117: step 6670, loss = 0.1662, acc = 0.9460 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 18:51:33.258913: step 6680, loss = 0.2044, acc = 0.9320 (269.5 examples/sec; 0.237 sec/batch)
2017-05-02 18:51:42.652667: step 6690, loss = 0.2351, acc = 0.9080 (267.5 examples/sec; 0.239 sec/batch)
2017-05-02 18:51:52.021211: step 6700, loss = 0.2050, acc = 0.9260 (260.5 examples/sec; 0.246 sec/batch)
2017-05-02 18:52:01.235844: step 6710, loss = 0.1962, acc = 0.9280 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 18:52:10.697619: step 6720, loss = 0.1755, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 18:52:19.900852: step 6730, loss = 0.1784, acc = 0.9440 (262.4 examples/sec; 0.244 sec/batch)
2017-05-02 18:52:28.921956: step 6740, loss = 0.1820, acc = 0.9300 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 18:52:38.386472: step 6750, loss = 0.1962, acc = 0.9280 (283.5 examples/sec; 0.226 sec/batch)
2017-05-02 18:52:48.286189: step 6760, loss = 0.1524, acc = 0.9520 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 18:52:57.588299: step 6770, loss = 0.2244, acc = 0.9160 (276.5 examples/sec; 0.232 sec/batch)
2017-05-02 18:53:07.295761: step 6780, loss = 0.1925, acc = 0.9380 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 18:53:16.470342: step 6790, loss = 0.1918, acc = 0.9240 (292.2 examples/sec; 0.219 sec/batch)
2017-05-02 18:53:25.777997: step 6800, loss = 0.1806, acc = 0.9440 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 18:53:35.292660: step 6810, loss = 0.1549, acc = 0.9560 (253.0 examples/sec; 0.253 sec/batch)
2017-05-02 18:53:44.519024: step 6820, loss = 0.1769, acc = 0.9420 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 18:53:53.757549: step 6830, loss = 0.1660, acc = 0.9440 (270.7 examples/sec; 0.236 sec/batch)
2017-05-02 18:54:02.847255: step 6840, loss = 0.1709, acc = 0.9480 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 18:54:12.716922: step 6850, loss = 0.1894, acc = 0.9340 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 18:54:22.027514: step 6860, loss = 0.1638, acc = 0.9540 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 18:54:31.244062: step 6870, loss = 0.2053, acc = 0.9280 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 18:54:40.440608: step 6880, loss = 0.1681, acc = 0.9480 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 18:54:49.765422: step 6890, loss = 0.1648, acc = 0.9480 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 18:54:59.111499: step 6900, loss = 0.1828, acc = 0.9300 (264.7 examples/sec; 0.242 sec/batch)
2017-05-02 18:55:08.469906: step 6910, loss = 0.1484, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 18:55:17.859811: step 6920, loss = 0.1601, acc = 0.9420 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 18:55:27.254738: step 6930, loss = 0.1593, acc = 0.9520 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 18:55:36.494362: step 6940, loss = 0.1615, acc = 0.9420 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 18:55:45.953374: step 6950, loss = 0.1757, acc = 0.9380 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 18:55:55.431516: step 6960, loss = 0.1974, acc = 0.9320 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 18:56:04.762456: step 6970, loss = 0.1980, acc = 0.9200 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 18:56:14.264874: step 6980, loss = 0.1994, acc = 0.9260 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 18:56:23.525855: step 6990, loss = 0.1861, acc = 0.9400 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 18:56:32.829457: step 7000, loss = 0.1731, acc = 0.9540 (270.3 examples/sec; 0.237 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 18:56:33.337199: step 7000, loss = 0.1502, acc = 0.9510, f1neg = 0.9464, f1pos = 0.9549, f1 = 0.9506
[Eval_batch(1)(2000,4000)] 2017-05-02 18:56:33.842785: step 7000, loss = 0.1532, acc = 0.9545, f1neg = 0.9472, f1pos = 0.9600, f1 = 0.9536
[Eval_batch(2)(2000,6000)] 2017-05-02 18:56:34.339164: step 7000, loss = 0.1694, acc = 0.9465, f1neg = 0.9435, f1pos = 0.9492, f1 = 0.9464
[Eval_batch(3)(2000,8000)] 2017-05-02 18:56:34.854196: step 7000, loss = 0.1742, acc = 0.9455, f1neg = 0.9459, f1pos = 0.9451, f1 = 0.9455
[Eval_batch(4)(2000,10000)] 2017-05-02 18:56:35.325687: step 7000, loss = 0.1735, acc = 0.9410, f1neg = 0.9425, f1pos = 0.9394, f1 = 0.9410
[Eval_batch(5)(2000,12000)] 2017-05-02 18:56:35.801994: step 7000, loss = 0.1652, acc = 0.9430, f1neg = 0.9390, f1pos = 0.9465, f1 = 0.9428
[Eval_batch(6)(2000,14000)] 2017-05-02 18:56:36.257327: step 7000, loss = 0.1580, acc = 0.9515, f1neg = 0.9504, f1pos = 0.9526, f1 = 0.9515
[Eval_batch(7)(2000,16000)] 2017-05-02 18:56:36.749400: step 7000, loss = 0.1561, acc = 0.9500, f1neg = 0.9408, f1pos = 0.9567, f1 = 0.9488
[Eval_batch(8)(2000,18000)] 2017-05-02 18:56:37.190207: step 7000, loss = 0.1573, acc = 0.9485, f1neg = 0.9414, f1pos = 0.9541, f1 = 0.9477
[Eval_batch(9)(2000,20000)] 2017-05-02 18:56:37.662883: step 7000, loss = 0.1567, acc = 0.9495, f1neg = 0.9462, f1pos = 0.9524, f1 = 0.9493
[Eval_batch(10)(2000,22000)] 2017-05-02 18:56:38.128303: step 7000, loss = 0.1668, acc = 0.9495, f1neg = 0.9461, f1pos = 0.9525, f1 = 0.9493
[Eval_batch(11)(2000,24000)] 2017-05-02 18:56:38.636598: step 7000, loss = 0.1676, acc = 0.9465, f1neg = 0.9383, f1pos = 0.9528, f1 = 0.9455
[Eval_batch(12)(2000,26000)] 2017-05-02 18:56:39.109310: step 7000, loss = 0.1625, acc = 0.9495, f1neg = 0.9479, f1pos = 0.9510, f1 = 0.9495
[Eval_batch(13)(2000,28000)] 2017-05-02 18:56:39.576752: step 7000, loss = 0.1498, acc = 0.9545, f1neg = 0.9432, f1pos = 0.9621, f1 = 0.9526
[Eval_batch(14)(2000,30000)] 2017-05-02 18:56:40.083135: step 7000, loss = 0.1750, acc = 0.9455, f1neg = 0.9348, f1pos = 0.9532, f1 = 0.9440
[Eval_batch(15)(2000,32000)] 2017-05-02 18:56:40.543761: step 7000, loss = 0.1592, acc = 0.9540, f1neg = 0.9505, f1pos = 0.9570, f1 = 0.9538
[Eval_batch(16)(2000,34000)] 2017-05-02 18:56:40.994015: step 7000, loss = 0.1523, acc = 0.9510, f1neg = 0.9490, f1pos = 0.9528, f1 = 0.9509
[Eval_batch(17)(2000,36000)] 2017-05-02 18:56:41.461922: step 7000, loss = 0.1723, acc = 0.9440, f1neg = 0.9443, f1pos = 0.9437, f1 = 0.9440
[Eval_batch(18)(2000,38000)] 2017-05-02 18:56:41.952933: step 7000, loss = 0.1713, acc = 0.9405, f1neg = 0.9428, f1pos = 0.9381, f1 = 0.9404
[Eval_batch(19)(2000,40000)] 2017-05-02 18:56:42.422545: step 7000, loss = 0.1502, acc = 0.9605, f1neg = 0.9544, f1pos = 0.9652, f1 = 0.9598
[Eval_batch(20)(2000,42000)] 2017-05-02 18:56:42.895009: step 7000, loss = 0.1548, acc = 0.9540, f1neg = 0.9594, f1pos = 0.9469, f1 = 0.9532
[Eval_batch(21)(2000,44000)] 2017-05-02 18:56:43.377965: step 7000, loss = 0.1472, acc = 0.9550, f1neg = 0.9513, f1pos = 0.9582, f1 = 0.9547
[Eval_batch(22)(2000,46000)] 2017-05-02 18:56:43.822808: step 7000, loss = 0.1540, acc = 0.9520, f1neg = 0.9525, f1pos = 0.9515, f1 = 0.9520
[Eval_batch(23)(2000,48000)] 2017-05-02 18:56:44.288426: step 7000, loss = 0.1638, acc = 0.9475, f1neg = 0.9405, f1pos = 0.9530, f1 = 0.9468
[Eval_batch(24)(2000,50000)] 2017-05-02 18:56:44.779375: step 7000, loss = 0.1524, acc = 0.9490, f1neg = 0.9416, f1pos = 0.9547, f1 = 0.9482
[Eval_batch(25)(2000,52000)] 2017-05-02 18:56:45.244522: step 7000, loss = 0.1421, acc = 0.9630, f1neg = 0.9593, f1pos = 0.9661, f1 = 0.9627
[Eval_batch(26)(2000,54000)] 2017-05-02 18:56:45.723510: step 7000, loss = 0.1497, acc = 0.9520, f1neg = 0.9548, f1pos = 0.9489, f1 = 0.9518
[Eval_batch(27)(2000,56000)] 2017-05-02 18:56:46.296472: step 7000, loss = 0.1398, acc = 0.9595, f1neg = 0.9575, f1pos = 0.9613, f1 = 0.9594
[Eval] 2017-05-02 18:56:46.296579: step 7000, acc = 0.9503, f1 = 0.9498
[Test_batch(0)(2000,2000)] 2017-05-02 18:56:46.783437: step 7000, loss = 0.1885, acc = 0.9435, f1neg = 0.9465, f1pos = 0.9401, f1 = 0.9433
[Test_batch(1)(2000,4000)] 2017-05-02 18:56:47.224658: step 7000, loss = 0.1790, acc = 0.9400, f1neg = 0.9469, f1pos = 0.9310, f1 = 0.9390
[Test_batch(2)(2000,6000)] 2017-05-02 18:56:47.689278: step 7000, loss = 0.1840, acc = 0.9450, f1neg = 0.9496, f1pos = 0.9394, f1 = 0.9445
[Test_batch(3)(2000,8000)] 2017-05-02 18:56:48.164755: step 7000, loss = 0.1968, acc = 0.9260, f1neg = 0.9312, f1pos = 0.9200, f1 = 0.9256
[Test_batch(4)(2000,10000)] 2017-05-02 18:56:48.607018: step 7000, loss = 0.2029, acc = 0.9290, f1neg = 0.9292, f1pos = 0.9288, f1 = 0.9290
[Test_batch(5)(2000,12000)] 2017-05-02 18:56:49.084095: step 7000, loss = 0.2085, acc = 0.9270, f1neg = 0.9281, f1pos = 0.9258, f1 = 0.9270
[Test_batch(6)(2000,14000)] 2017-05-02 18:56:49.551634: step 7000, loss = 0.1925, acc = 0.9345, f1neg = 0.9323, f1pos = 0.9366, f1 = 0.9344
[Test_batch(7)(2000,16000)] 2017-05-02 18:56:50.024367: step 7000, loss = 0.1817, acc = 0.9405, f1neg = 0.9464, f1pos = 0.9331, f1 = 0.9398
[Test_batch(8)(2000,18000)] 2017-05-02 18:56:50.500625: step 7000, loss = 0.1795, acc = 0.9445, f1neg = 0.9441, f1pos = 0.9449, f1 = 0.9445
[Test_batch(9)(2000,20000)] 2017-05-02 18:56:50.970056: step 7000, loss = 0.2123, acc = 0.9230, f1neg = 0.9189, f1pos = 0.9267, f1 = 0.9228
[Test_batch(10)(2000,22000)] 2017-05-02 18:56:51.431955: step 7000, loss = 0.1898, acc = 0.9365, f1neg = 0.9292, f1pos = 0.9424, f1 = 0.9358
[Test_batch(11)(2000,24000)] 2017-05-02 18:56:51.912470: step 7000, loss = 0.1872, acc = 0.9440, f1neg = 0.9458, f1pos = 0.9421, f1 = 0.9439
[Test_batch(12)(2000,26000)] 2017-05-02 18:56:52.378772: step 7000, loss = 0.1686, acc = 0.9480, f1neg = 0.9503, f1pos = 0.9454, f1 = 0.9479
[Test_batch(13)(2000,28000)] 2017-05-02 18:56:52.848404: step 7000, loss = 0.1998, acc = 0.9265, f1neg = 0.9185, f1pos = 0.9331, f1 = 0.9258
[Test_batch(14)(2000,30000)] 2017-05-02 18:56:53.316743: step 7000, loss = 0.1684, acc = 0.9460, f1neg = 0.9385, f1pos = 0.9519, f1 = 0.9452
[Test_batch(15)(2000,32000)] 2017-05-02 18:56:53.784857: step 7000, loss = 0.1833, acc = 0.9410, f1neg = 0.9406, f1pos = 0.9414, f1 = 0.9410
[Test_batch(16)(2000,34000)] 2017-05-02 18:56:54.266244: step 7000, loss = 0.1711, acc = 0.9475, f1neg = 0.9465, f1pos = 0.9485, f1 = 0.9475
[Test_batch(17)(2000,36000)] 2017-05-02 18:56:54.776062: step 7000, loss = 0.1593, acc = 0.9530, f1neg = 0.9489, f1pos = 0.9565, f1 = 0.9527
[Test_batch(18)(2000,38000)] 2017-05-02 18:56:55.361959: step 7000, loss = 0.1470, acc = 0.9600, f1neg = 0.9592, f1pos = 0.9607, f1 = 0.9600
[Test] 2017-05-02 18:56:55.362022: step 7000, acc = 0.9398, f1 = 0.9395
[Status] 2017-05-02 18:56:55.362048: step 7000, maxindex = 7000, maxdev = 0.9503, maxtst = 0.9398
2017-05-02 18:57:07.532205: step 7010, loss = 0.1878, acc = 0.9400 (283.5 examples/sec; 0.226 sec/batch)
2017-05-02 18:57:16.780947: step 7020, loss = 0.2186, acc = 0.9260 (268.5 examples/sec; 0.238 sec/batch)
2017-05-02 18:57:26.621256: step 7030, loss = 0.1760, acc = 0.9520 (246.7 examples/sec; 0.259 sec/batch)
2017-05-02 18:57:35.976867: step 7040, loss = 0.1835, acc = 0.9420 (253.9 examples/sec; 0.252 sec/batch)
2017-05-02 18:57:45.176144: step 7050, loss = 0.1814, acc = 0.9380 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 18:57:55.404194: step 7060, loss = 0.1641, acc = 0.9520 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 18:58:04.547875: step 7070, loss = 0.1831, acc = 0.9400 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 18:58:13.606943: step 7080, loss = 0.1665, acc = 0.9380 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 18:58:22.817371: step 7090, loss = 0.1926, acc = 0.9340 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 18:58:32.559898: step 7100, loss = 0.1868, acc = 0.9420 (180.3 examples/sec; 0.355 sec/batch)
2017-05-02 18:58:41.737242: step 7110, loss = 0.1805, acc = 0.9460 (292.3 examples/sec; 0.219 sec/batch)
2017-05-02 18:58:51.029595: step 7120, loss = 0.1774, acc = 0.9500 (264.3 examples/sec; 0.242 sec/batch)
2017-05-02 18:59:00.374270: step 7130, loss = 0.2028, acc = 0.9320 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 18:59:09.699022: step 7140, loss = 0.1885, acc = 0.9380 (267.8 examples/sec; 0.239 sec/batch)
2017-05-02 18:59:19.545034: step 7150, loss = 0.1698, acc = 0.9480 (269.2 examples/sec; 0.238 sec/batch)
2017-05-02 18:59:28.644205: step 7160, loss = 0.1627, acc = 0.9500 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 18:59:37.871617: step 7170, loss = 0.1886, acc = 0.9380 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 18:59:47.309100: step 7180, loss = 0.1548, acc = 0.9500 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 18:59:56.585865: step 7190, loss = 0.1834, acc = 0.9400 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 19:00:05.860750: step 7200, loss = 0.2021, acc = 0.9200 (261.9 examples/sec; 0.244 sec/batch)
2017-05-02 19:00:15.016690: step 7210, loss = 0.1895, acc = 0.9340 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 19:00:24.197374: step 7220, loss = 0.1969, acc = 0.9400 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 19:00:33.447937: step 7230, loss = 0.1705, acc = 0.9480 (271.2 examples/sec; 0.236 sec/batch)
2017-05-02 19:00:42.738909: step 7240, loss = 0.1958, acc = 0.9480 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 19:00:52.022653: step 7250, loss = 0.1626, acc = 0.9520 (266.8 examples/sec; 0.240 sec/batch)
2017-05-02 19:01:01.164040: step 7260, loss = 0.2067, acc = 0.9320 (261.8 examples/sec; 0.244 sec/batch)
2017-05-02 19:01:10.466094: step 7270, loss = 0.1716, acc = 0.9460 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 19:01:19.538383: step 7280, loss = 0.1822, acc = 0.9320 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 19:01:28.993992: step 7290, loss = 0.1596, acc = 0.9500 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 19:01:38.204115: step 7300, loss = 0.2098, acc = 0.9240 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 19:01:47.446731: step 7310, loss = 0.2117, acc = 0.9280 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 19:01:56.824216: step 7320, loss = 0.2152, acc = 0.9220 (271.3 examples/sec; 0.236 sec/batch)
2017-05-02 19:02:06.130705: step 7330, loss = 0.1569, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 19:02:15.504195: step 7340, loss = 0.1658, acc = 0.9500 (275.1 examples/sec; 0.233 sec/batch)
2017-05-02 19:02:24.918041: step 7350, loss = 0.1880, acc = 0.9460 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 19:02:34.214984: step 7360, loss = 0.1926, acc = 0.9340 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 19:02:43.603624: step 7370, loss = 0.1801, acc = 0.9380 (270.8 examples/sec; 0.236 sec/batch)
2017-05-02 19:02:53.091916: step 7380, loss = 0.2169, acc = 0.9220 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 19:03:02.397438: step 7390, loss = 0.1972, acc = 0.9280 (257.1 examples/sec; 0.249 sec/batch)
2017-05-02 19:03:11.801494: step 7400, loss = 0.1930, acc = 0.9320 (251.0 examples/sec; 0.255 sec/batch)
2017-05-02 19:03:21.134968: step 7410, loss = 0.1932, acc = 0.9320 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 19:03:30.403410: step 7420, loss = 0.1828, acc = 0.9360 (287.9 examples/sec; 0.222 sec/batch)
2017-05-02 19:03:39.559081: step 7430, loss = 0.1679, acc = 0.9440 (269.8 examples/sec; 0.237 sec/batch)
2017-05-02 19:03:48.886308: step 7440, loss = 0.1965, acc = 0.9340 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 19:03:58.130889: step 7450, loss = 0.2030, acc = 0.9320 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 19:04:07.480549: step 7460, loss = 0.1965, acc = 0.9320 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 19:04:16.986866: step 7470, loss = 0.2078, acc = 0.9320 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 19:04:26.396366: step 7480, loss = 0.1812, acc = 0.9420 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 19:04:35.660603: step 7490, loss = 0.1764, acc = 0.9440 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 19:04:45.013198: step 7500, loss = 0.1820, acc = 0.9420 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 19:04:54.280163: step 7510, loss = 0.1522, acc = 0.9560 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 19:05:03.817599: step 7520, loss = 0.1744, acc = 0.9440 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 19:05:13.118979: step 7530, loss = 0.1843, acc = 0.9380 (286.1 examples/sec; 0.224 sec/batch)
2017-05-02 19:05:22.473539: step 7540, loss = 0.1774, acc = 0.9400 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 19:05:31.973949: step 7550, loss = 0.1651, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 19:05:41.479566: step 7560, loss = 0.2006, acc = 0.9360 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 19:05:50.796251: step 7570, loss = 0.1777, acc = 0.9340 (269.4 examples/sec; 0.238 sec/batch)
2017-05-02 19:06:00.033661: step 7580, loss = 0.1544, acc = 0.9540 (275.5 examples/sec; 0.232 sec/batch)
2017-05-02 19:06:09.235870: step 7590, loss = 0.1803, acc = 0.9420 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 19:06:18.455363: step 7600, loss = 0.1830, acc = 0.9420 (289.6 examples/sec; 0.221 sec/batch)
2017-05-02 19:06:27.587898: step 7610, loss = 0.1953, acc = 0.9360 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 19:06:36.974167: step 7620, loss = 0.2028, acc = 0.9260 (260.3 examples/sec; 0.246 sec/batch)
2017-05-02 19:06:46.381168: step 7630, loss = 0.2024, acc = 0.9180 (263.6 examples/sec; 0.243 sec/batch)
2017-05-02 19:06:55.667070: step 7640, loss = 0.1685, acc = 0.9540 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 19:07:05.147626: step 7650, loss = 0.1913, acc = 0.9340 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 19:07:14.446304: step 7660, loss = 0.1852, acc = 0.9400 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 19:07:23.807815: step 7670, loss = 0.1935, acc = 0.9420 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 19:07:32.974858: step 7680, loss = 0.1772, acc = 0.9360 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 19:07:42.233340: step 7690, loss = 0.1857, acc = 0.9380 (267.7 examples/sec; 0.239 sec/batch)
2017-05-02 19:07:51.606871: step 7700, loss = 0.1484, acc = 0.9640 (270.4 examples/sec; 0.237 sec/batch)
2017-05-02 19:08:00.797022: step 7710, loss = 0.1867, acc = 0.9460 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 19:08:10.029593: step 7720, loss = 0.1898, acc = 0.9400 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 19:08:19.360004: step 7730, loss = 0.1755, acc = 0.9500 (266.8 examples/sec; 0.240 sec/batch)
2017-05-02 19:08:28.941397: step 7740, loss = 0.2168, acc = 0.9180 (273.5 examples/sec; 0.234 sec/batch)
2017-05-02 19:08:38.279866: step 7750, loss = 0.2001, acc = 0.9280 (268.3 examples/sec; 0.238 sec/batch)
2017-05-02 19:08:47.778521: step 7760, loss = 0.2382, acc = 0.9200 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 19:08:57.283452: step 7770, loss = 0.1890, acc = 0.9420 (258.0 examples/sec; 0.248 sec/batch)
2017-05-02 19:09:06.588872: step 7780, loss = 0.1707, acc = 0.9580 (263.8 examples/sec; 0.243 sec/batch)
2017-05-02 19:09:16.056582: step 7790, loss = 0.1811, acc = 0.9340 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 19:09:25.405756: step 7800, loss = 0.1774, acc = 0.9360 (287.5 examples/sec; 0.223 sec/batch)
2017-05-02 19:09:34.738507: step 7810, loss = 0.1828, acc = 0.9280 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 19:09:44.065196: step 7820, loss = 0.1603, acc = 0.9520 (283.5 examples/sec; 0.226 sec/batch)
2017-05-02 19:09:53.794081: step 7830, loss = 0.1797, acc = 0.9480 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 19:10:03.035642: step 7840, loss = 0.1933, acc = 0.9420 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 19:10:12.270141: step 7850, loss = 0.1652, acc = 0.9560 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 19:10:21.556775: step 7860, loss = 0.1834, acc = 0.9460 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 19:10:30.808618: step 7870, loss = 0.2136, acc = 0.9360 (288.3 examples/sec; 0.222 sec/batch)
2017-05-02 19:10:39.876381: step 7880, loss = 0.1498, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-02 19:10:49.064141: step 7890, loss = 0.1729, acc = 0.9440 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 19:10:58.482875: step 7900, loss = 0.1803, acc = 0.9400 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 19:11:07.886837: step 7910, loss = 0.1776, acc = 0.9320 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 19:11:17.174160: step 7920, loss = 0.1953, acc = 0.9380 (267.6 examples/sec; 0.239 sec/batch)
2017-05-02 19:11:26.418855: step 7930, loss = 0.1731, acc = 0.9460 (284.6 examples/sec; 0.225 sec/batch)
2017-05-02 19:11:35.674737: step 7940, loss = 0.2360, acc = 0.9260 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 19:11:45.036369: step 7950, loss = 0.1779, acc = 0.9400 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 19:11:54.242932: step 7960, loss = 0.2017, acc = 0.9220 (272.9 examples/sec; 0.234 sec/batch)
2017-05-02 19:12:03.633825: step 7970, loss = 0.1818, acc = 0.9420 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 19:12:12.926999: step 7980, loss = 0.1662, acc = 0.9520 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 19:12:22.106717: step 7990, loss = 0.2020, acc = 0.9300 (285.1 examples/sec; 0.224 sec/batch)
2017-05-02 19:12:31.435370: step 8000, loss = 0.1836, acc = 0.9380 (268.6 examples/sec; 0.238 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 19:12:31.935131: step 8000, loss = 0.1462, acc = 0.9535, f1neg = 0.9493, f1pos = 0.9571, f1 = 0.9532
[Eval_batch(1)(2000,4000)] 2017-05-02 19:12:32.410625: step 8000, loss = 0.1493, acc = 0.9540, f1neg = 0.9467, f1pos = 0.9595, f1 = 0.9531
[Eval_batch(2)(2000,6000)] 2017-05-02 19:12:32.877883: step 8000, loss = 0.1645, acc = 0.9490, f1neg = 0.9461, f1pos = 0.9516, f1 = 0.9489
[Eval_batch(3)(2000,8000)] 2017-05-02 19:12:33.349327: step 8000, loss = 0.1699, acc = 0.9450, f1neg = 0.9453, f1pos = 0.9447, f1 = 0.9450
[Eval_batch(4)(2000,10000)] 2017-05-02 19:12:33.814480: step 8000, loss = 0.1692, acc = 0.9430, f1neg = 0.9444, f1pos = 0.9415, f1 = 0.9430
[Eval_batch(5)(2000,12000)] 2017-05-02 19:12:34.277664: step 8000, loss = 0.1619, acc = 0.9435, f1neg = 0.9395, f1pos = 0.9470, f1 = 0.9432
[Eval_batch(6)(2000,14000)] 2017-05-02 19:12:34.781595: step 8000, loss = 0.1541, acc = 0.9545, f1neg = 0.9536, f1pos = 0.9554, f1 = 0.9545
[Eval_batch(7)(2000,16000)] 2017-05-02 19:12:35.254313: step 8000, loss = 0.1522, acc = 0.9490, f1neg = 0.9397, f1pos = 0.9558, f1 = 0.9478
[Eval_batch(8)(2000,18000)] 2017-05-02 19:12:35.728251: step 8000, loss = 0.1533, acc = 0.9510, f1neg = 0.9444, f1pos = 0.9562, f1 = 0.9503
[Eval_batch(9)(2000,20000)] 2017-05-02 19:12:36.199438: step 8000, loss = 0.1528, acc = 0.9525, f1neg = 0.9495, f1pos = 0.9552, f1 = 0.9523
[Eval_batch(10)(2000,22000)] 2017-05-02 19:12:36.690318: step 8000, loss = 0.1638, acc = 0.9490, f1neg = 0.9456, f1pos = 0.9520, f1 = 0.9488
[Eval_batch(11)(2000,24000)] 2017-05-02 19:12:37.144937: step 8000, loss = 0.1634, acc = 0.9495, f1neg = 0.9419, f1pos = 0.9553, f1 = 0.9486
[Eval_batch(12)(2000,26000)] 2017-05-02 19:12:37.617569: step 8000, loss = 0.1587, acc = 0.9480, f1neg = 0.9464, f1pos = 0.9495, f1 = 0.9480
[Eval_batch(13)(2000,28000)] 2017-05-02 19:12:38.117872: step 8000, loss = 0.1461, acc = 0.9560, f1neg = 0.9450, f1pos = 0.9633, f1 = 0.9542
[Eval_batch(14)(2000,30000)] 2017-05-02 19:12:38.588550: step 8000, loss = 0.1711, acc = 0.9470, f1neg = 0.9365, f1pos = 0.9545, f1 = 0.9455
[Eval_batch(15)(2000,32000)] 2017-05-02 19:12:39.062843: step 8000, loss = 0.1559, acc = 0.9555, f1neg = 0.9522, f1pos = 0.9584, f1 = 0.9553
[Eval_batch(16)(2000,34000)] 2017-05-02 19:12:39.558637: step 8000, loss = 0.1489, acc = 0.9515, f1neg = 0.9497, f1pos = 0.9532, f1 = 0.9514
[Eval_batch(17)(2000,36000)] 2017-05-02 19:12:40.064474: step 8000, loss = 0.1687, acc = 0.9455, f1neg = 0.9458, f1pos = 0.9452, f1 = 0.9455
[Eval_batch(18)(2000,38000)] 2017-05-02 19:12:40.547883: step 8000, loss = 0.1670, acc = 0.9440, f1neg = 0.9463, f1pos = 0.9415, f1 = 0.9439
[Eval_batch(19)(2000,40000)] 2017-05-02 19:12:41.053577: step 8000, loss = 0.1465, acc = 0.9625, f1neg = 0.9567, f1pos = 0.9669, f1 = 0.9618
[Eval_batch(20)(2000,42000)] 2017-05-02 19:12:41.537070: step 8000, loss = 0.1509, acc = 0.9550, f1neg = 0.9603, f1pos = 0.9480, f1 = 0.9542
[Eval_batch(21)(2000,44000)] 2017-05-02 19:12:42.000613: step 8000, loss = 0.1439, acc = 0.9570, f1neg = 0.9534, f1pos = 0.9601, f1 = 0.9567
[Eval_batch(22)(2000,46000)] 2017-05-02 19:12:42.493501: step 8000, loss = 0.1499, acc = 0.9525, f1neg = 0.9531, f1pos = 0.9519, f1 = 0.9525
[Eval_batch(23)(2000,48000)] 2017-05-02 19:12:42.997050: step 8000, loss = 0.1596, acc = 0.9480, f1neg = 0.9413, f1pos = 0.9533, f1 = 0.9473
[Eval_batch(24)(2000,50000)] 2017-05-02 19:12:43.498555: step 8000, loss = 0.1488, acc = 0.9510, f1neg = 0.9440, f1pos = 0.9564, f1 = 0.9502
[Eval_batch(25)(2000,52000)] 2017-05-02 19:12:43.980892: step 8000, loss = 0.1383, acc = 0.9630, f1neg = 0.9594, f1pos = 0.9660, f1 = 0.9627
[Eval_batch(26)(2000,54000)] 2017-05-02 19:12:44.457982: step 8000, loss = 0.1457, acc = 0.9505, f1neg = 0.9534, f1pos = 0.9473, f1 = 0.9503
[Eval_batch(27)(2000,56000)] 2017-05-02 19:12:45.032090: step 8000, loss = 0.1358, acc = 0.9600, f1neg = 0.9581, f1pos = 0.9617, f1 = 0.9599
[Eval] 2017-05-02 19:12:45.032181: step 8000, acc = 0.9514, f1 = 0.9510
[Test_batch(0)(2000,2000)] 2017-05-02 19:12:45.535996: step 8000, loss = 0.1842, acc = 0.9445, f1neg = 0.9473, f1pos = 0.9414, f1 = 0.9443
[Test_batch(1)(2000,4000)] 2017-05-02 19:12:46.008831: step 8000, loss = 0.1743, acc = 0.9420, f1neg = 0.9487, f1pos = 0.9333, f1 = 0.9410
[Test_batch(2)(2000,6000)] 2017-05-02 19:12:46.512750: step 8000, loss = 0.1804, acc = 0.9460, f1neg = 0.9505, f1pos = 0.9406, f1 = 0.9455
[Test_batch(3)(2000,8000)] 2017-05-02 19:12:46.969155: step 8000, loss = 0.1929, acc = 0.9295, f1neg = 0.9343, f1pos = 0.9240, f1 = 0.9291
[Test_batch(4)(2000,10000)] 2017-05-02 19:12:47.450761: step 8000, loss = 0.1979, acc = 0.9295, f1neg = 0.9297, f1pos = 0.9293, f1 = 0.9295
[Test_batch(5)(2000,12000)] 2017-05-02 19:12:47.922670: step 8000, loss = 0.2049, acc = 0.9280, f1neg = 0.9291, f1pos = 0.9269, f1 = 0.9280
[Test_batch(6)(2000,14000)] 2017-05-02 19:12:48.386774: step 8000, loss = 0.1876, acc = 0.9360, f1neg = 0.9339, f1pos = 0.9380, f1 = 0.9359
[Test_batch(7)(2000,16000)] 2017-05-02 19:12:48.867804: step 8000, loss = 0.1772, acc = 0.9405, f1neg = 0.9463, f1pos = 0.9333, f1 = 0.9398
[Test_batch(8)(2000,18000)] 2017-05-02 19:12:49.333097: step 8000, loss = 0.1750, acc = 0.9475, f1neg = 0.9472, f1pos = 0.9478, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-02 19:12:49.812910: step 8000, loss = 0.2083, acc = 0.9250, f1neg = 0.9213, f1pos = 0.9284, f1 = 0.9248
[Test_batch(10)(2000,22000)] 2017-05-02 19:12:50.321498: step 8000, loss = 0.1858, acc = 0.9395, f1neg = 0.9328, f1pos = 0.9450, f1 = 0.9389
[Test_batch(11)(2000,24000)] 2017-05-02 19:12:50.803200: step 8000, loss = 0.1829, acc = 0.9445, f1neg = 0.9463, f1pos = 0.9426, f1 = 0.9444
[Test_batch(12)(2000,26000)] 2017-05-02 19:12:51.285524: step 8000, loss = 0.1641, acc = 0.9480, f1neg = 0.9503, f1pos = 0.9455, f1 = 0.9479
[Test_batch(13)(2000,28000)] 2017-05-02 19:12:51.794607: step 8000, loss = 0.1949, acc = 0.9280, f1neg = 0.9201, f1pos = 0.9345, f1 = 0.9273
[Test_batch(14)(2000,30000)] 2017-05-02 19:12:52.293724: step 8000, loss = 0.1646, acc = 0.9475, f1neg = 0.9404, f1pos = 0.9531, f1 = 0.9468
[Test_batch(15)(2000,32000)] 2017-05-02 19:12:52.799023: step 8000, loss = 0.1790, acc = 0.9415, f1neg = 0.9411, f1pos = 0.9419, f1 = 0.9415
[Test_batch(16)(2000,34000)] 2017-05-02 19:12:53.260271: step 8000, loss = 0.1668, acc = 0.9470, f1neg = 0.9460, f1pos = 0.9480, f1 = 0.9470
[Test_batch(17)(2000,36000)] 2017-05-02 19:12:53.707410: step 8000, loss = 0.1559, acc = 0.9530, f1neg = 0.9489, f1pos = 0.9565, f1 = 0.9527
[Test_batch(18)(2000,38000)] 2017-05-02 19:12:54.289400: step 8000, loss = 0.1436, acc = 0.9600, f1neg = 0.9593, f1pos = 0.9607, f1 = 0.9600
[Test] 2017-05-02 19:12:54.289492: step 8000, acc = 0.9409, f1 = 0.9406
[Status] 2017-05-02 19:12:54.289519: step 8000, maxindex = 8000, maxdev = 0.9514, maxtst = 0.9409
2017-05-02 19:13:06.798936: step 8010, loss = 0.1862, acc = 0.9400 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 19:13:15.999872: step 8020, loss = 0.1928, acc = 0.9380 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 19:13:25.260798: step 8030, loss = 0.1851, acc = 0.9360 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 19:13:34.462837: step 8040, loss = 0.1783, acc = 0.9380 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 19:13:43.538669: step 8050, loss = 0.2022, acc = 0.9220 (288.9 examples/sec; 0.222 sec/batch)
2017-05-02 19:13:52.878873: step 8060, loss = 0.1786, acc = 0.9480 (266.0 examples/sec; 0.241 sec/batch)
2017-05-02 19:14:02.735129: step 8070, loss = 0.1719, acc = 0.9440 (272.9 examples/sec; 0.234 sec/batch)
2017-05-02 19:14:11.957255: step 8080, loss = 0.1751, acc = 0.9400 (288.2 examples/sec; 0.222 sec/batch)
2017-05-02 19:14:21.148586: step 8090, loss = 0.1942, acc = 0.9360 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 19:14:30.591310: step 8100, loss = 0.1789, acc = 0.9340 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 19:14:39.765294: step 8110, loss = 0.1708, acc = 0.9440 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 19:14:49.026508: step 8120, loss = 0.1610, acc = 0.9460 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 19:14:58.256682: step 8130, loss = 0.2023, acc = 0.9240 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 19:15:07.572779: step 8140, loss = 0.1718, acc = 0.9460 (287.9 examples/sec; 0.222 sec/batch)
2017-05-02 19:15:16.802908: step 8150, loss = 0.1261, acc = 0.9700 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 19:15:26.062912: step 8160, loss = 0.1949, acc = 0.9340 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 19:15:35.201755: step 8170, loss = 0.1991, acc = 0.9380 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 19:15:44.377953: step 8180, loss = 0.1750, acc = 0.9380 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 19:15:53.650219: step 8190, loss = 0.1564, acc = 0.9560 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 19:16:02.922283: step 8200, loss = 0.1559, acc = 0.9520 (262.8 examples/sec; 0.244 sec/batch)
2017-05-02 19:16:12.051345: step 8210, loss = 0.1681, acc = 0.9480 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 19:16:21.407700: step 8220, loss = 0.2069, acc = 0.9240 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 19:16:30.859507: step 8230, loss = 0.1916, acc = 0.9320 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 19:16:40.036052: step 8240, loss = 0.1854, acc = 0.9440 (264.6 examples/sec; 0.242 sec/batch)
2017-05-02 19:16:49.100963: step 8250, loss = 0.1785, acc = 0.9460 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 19:16:58.312121: step 8260, loss = 0.1953, acc = 0.9380 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 19:17:07.611187: step 8270, loss = 0.1848, acc = 0.9460 (260.1 examples/sec; 0.246 sec/batch)
2017-05-02 19:17:16.801680: step 8280, loss = 0.1866, acc = 0.9400 (267.5 examples/sec; 0.239 sec/batch)
2017-05-02 19:17:26.025386: step 8290, loss = 0.2183, acc = 0.9140 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 19:17:36.076904: step 8300, loss = 0.1822, acc = 0.9420 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 19:17:45.188925: step 8310, loss = 0.1978, acc = 0.9260 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 19:17:54.479762: step 8320, loss = 0.2037, acc = 0.9280 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 19:18:03.695640: step 8330, loss = 0.1671, acc = 0.9440 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 19:18:12.911950: step 8340, loss = 0.1715, acc = 0.9380 (262.8 examples/sec; 0.244 sec/batch)
2017-05-02 19:18:22.300948: step 8350, loss = 0.2026, acc = 0.9340 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 19:18:31.676245: step 8360, loss = 0.1453, acc = 0.9600 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 19:18:40.958673: step 8370, loss = 0.1651, acc = 0.9500 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 19:18:50.219122: step 8380, loss = 0.1461, acc = 0.9580 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 19:18:59.459587: step 8390, loss = 0.1685, acc = 0.9540 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 19:19:09.063266: step 8400, loss = 0.1793, acc = 0.9540 (266.4 examples/sec; 0.240 sec/batch)
2017-05-02 19:19:18.287896: step 8410, loss = 0.1606, acc = 0.9600 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 19:19:27.520541: step 8420, loss = 0.2117, acc = 0.9200 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 19:19:36.619114: step 8430, loss = 0.1680, acc = 0.9480 (289.7 examples/sec; 0.221 sec/batch)
2017-05-02 19:19:46.280002: step 8440, loss = 0.1781, acc = 0.9380 (191.9 examples/sec; 0.333 sec/batch)
2017-05-02 19:19:55.659653: step 8450, loss = 0.1502, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 19:20:05.001282: step 8460, loss = 0.1931, acc = 0.9220 (281.0 examples/sec; 0.228 sec/batch)
2017-05-02 19:20:14.210554: step 8470, loss = 0.1754, acc = 0.9380 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 19:20:23.492845: step 8480, loss = 0.1371, acc = 0.9640 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 19:20:32.822135: step 8490, loss = 0.1517, acc = 0.9500 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 19:20:42.003080: step 8500, loss = 0.1807, acc = 0.9480 (292.7 examples/sec; 0.219 sec/batch)
2017-05-02 19:20:51.471429: step 8510, loss = 0.1707, acc = 0.9320 (263.4 examples/sec; 0.243 sec/batch)
2017-05-02 19:21:00.916075: step 8520, loss = 0.1920, acc = 0.9440 (268.1 examples/sec; 0.239 sec/batch)
2017-05-02 19:21:10.082786: step 8530, loss = 0.1959, acc = 0.9240 (264.7 examples/sec; 0.242 sec/batch)
2017-05-02 19:21:19.536795: step 8540, loss = 0.1722, acc = 0.9480 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 19:21:28.850322: step 8550, loss = 0.1827, acc = 0.9360 (268.0 examples/sec; 0.239 sec/batch)
2017-05-02 19:21:38.071940: step 8560, loss = 0.1823, acc = 0.9380 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 19:21:47.404409: step 8570, loss = 0.1868, acc = 0.9380 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 19:21:56.564906: step 8580, loss = 0.2066, acc = 0.9320 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 19:22:05.844066: step 8590, loss = 0.1713, acc = 0.9380 (273.5 examples/sec; 0.234 sec/batch)
2017-05-02 19:22:15.177151: step 8600, loss = 0.1439, acc = 0.9580 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 19:22:24.442387: step 8610, loss = 0.1502, acc = 0.9660 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 19:22:33.713392: step 8620, loss = 0.1681, acc = 0.9500 (267.7 examples/sec; 0.239 sec/batch)
2017-05-02 19:22:42.999042: step 8630, loss = 0.2034, acc = 0.9320 (270.2 examples/sec; 0.237 sec/batch)
2017-05-02 19:22:52.260717: step 8640, loss = 0.1618, acc = 0.9460 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 19:23:01.674048: step 8650, loss = 0.1950, acc = 0.9300 (270.5 examples/sec; 0.237 sec/batch)
2017-05-02 19:23:11.031067: step 8660, loss = 0.2004, acc = 0.9380 (267.5 examples/sec; 0.239 sec/batch)
2017-05-02 19:23:20.528777: step 8670, loss = 0.2047, acc = 0.9440 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 19:23:29.630586: step 8680, loss = 0.1831, acc = 0.9380 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 19:23:38.786986: step 8690, loss = 0.1728, acc = 0.9380 (259.1 examples/sec; 0.247 sec/batch)
2017-05-02 19:23:48.246659: step 8700, loss = 0.2027, acc = 0.9320 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 19:23:57.542669: step 8710, loss = 0.1663, acc = 0.9440 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 19:24:06.788811: step 8720, loss = 0.1635, acc = 0.9480 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 19:24:16.294103: step 8730, loss = 0.1495, acc = 0.9560 (270.4 examples/sec; 0.237 sec/batch)
2017-05-02 19:24:25.600712: step 8740, loss = 0.2342, acc = 0.9120 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 19:24:35.355068: step 8750, loss = 0.1870, acc = 0.9360 (261.4 examples/sec; 0.245 sec/batch)
2017-05-02 19:24:44.626771: step 8760, loss = 0.1586, acc = 0.9460 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 19:24:53.922490: step 8770, loss = 0.1839, acc = 0.9420 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 19:25:03.370324: step 8780, loss = 0.1836, acc = 0.9440 (267.0 examples/sec; 0.240 sec/batch)
2017-05-02 19:25:12.633372: step 8790, loss = 0.1806, acc = 0.9460 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 19:25:21.827125: step 8800, loss = 0.1908, acc = 0.9520 (263.4 examples/sec; 0.243 sec/batch)
2017-05-02 19:25:31.368872: step 8810, loss = 0.1819, acc = 0.9320 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 19:25:40.586333: step 8820, loss = 0.2045, acc = 0.9340 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 19:25:49.769665: step 8830, loss = 0.1679, acc = 0.9360 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 19:25:59.030135: step 8840, loss = 0.2130, acc = 0.9340 (263.6 examples/sec; 0.243 sec/batch)
2017-05-02 19:26:08.089521: step 8850, loss = 0.1680, acc = 0.9540 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 19:26:17.453011: step 8860, loss = 0.2027, acc = 0.9340 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 19:26:26.664843: step 8870, loss = 0.1617, acc = 0.9400 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 19:26:36.269660: step 8880, loss = 0.2096, acc = 0.9080 (233.9 examples/sec; 0.274 sec/batch)
2017-05-02 19:26:45.486161: step 8890, loss = 0.1900, acc = 0.9340 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 19:26:54.723317: step 8900, loss = 0.2224, acc = 0.9120 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 19:27:04.393728: step 8910, loss = 0.1849, acc = 0.9340 (266.0 examples/sec; 0.241 sec/batch)
2017-05-02 19:27:13.672656: step 8920, loss = 0.1874, acc = 0.9460 (267.4 examples/sec; 0.239 sec/batch)
2017-05-02 19:27:23.067678: step 8930, loss = 0.2046, acc = 0.9280 (281.3 examples/sec; 0.228 sec/batch)
2017-05-02 19:27:32.375525: step 8940, loss = 0.2009, acc = 0.9180 (257.9 examples/sec; 0.248 sec/batch)
2017-05-02 19:27:41.785814: step 8950, loss = 0.1929, acc = 0.9240 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 19:27:50.946399: step 8960, loss = 0.1791, acc = 0.9440 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 19:28:00.241433: step 8970, loss = 0.1772, acc = 0.9440 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 19:28:09.608126: step 8980, loss = 0.1744, acc = 0.9420 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 19:28:18.837059: step 8990, loss = 0.1533, acc = 0.9540 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 19:28:28.026821: step 9000, loss = 0.2071, acc = 0.9280 (277.4 examples/sec; 0.231 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 19:28:28.536793: step 9000, loss = 0.1580, acc = 0.9475, f1neg = 0.9411, f1pos = 0.9526, f1 = 0.9469
[Eval_batch(1)(2000,4000)] 2017-05-02 19:28:29.039957: step 9000, loss = 0.1499, acc = 0.9495, f1neg = 0.9396, f1pos = 0.9566, f1 = 0.9481
[Eval_batch(2)(2000,6000)] 2017-05-02 19:28:29.543451: step 9000, loss = 0.1731, acc = 0.9445, f1neg = 0.9395, f1pos = 0.9487, f1 = 0.9441
[Eval_batch(3)(2000,8000)] 2017-05-02 19:28:30.037636: step 9000, loss = 0.1821, acc = 0.9395, f1neg = 0.9379, f1pos = 0.9410, f1 = 0.9395
[Eval_batch(4)(2000,10000)] 2017-05-02 19:28:30.545150: step 9000, loss = 0.1854, acc = 0.9360, f1neg = 0.9363, f1pos = 0.9357, f1 = 0.9360
[Eval_batch(5)(2000,12000)] 2017-05-02 19:28:31.054091: step 9000, loss = 0.1698, acc = 0.9440, f1neg = 0.9385, f1pos = 0.9486, f1 = 0.9436
[Eval_batch(6)(2000,14000)] 2017-05-02 19:28:31.516785: step 9000, loss = 0.1700, acc = 0.9445, f1neg = 0.9420, f1pos = 0.9468, f1 = 0.9444
[Eval_batch(7)(2000,16000)] 2017-05-02 19:28:32.024879: step 9000, loss = 0.1573, acc = 0.9525, f1neg = 0.9422, f1pos = 0.9597, f1 = 0.9509
[Eval_batch(8)(2000,18000)] 2017-05-02 19:28:32.507567: step 9000, loss = 0.1627, acc = 0.9440, f1neg = 0.9340, f1pos = 0.9513, f1 = 0.9427
[Eval_batch(9)(2000,20000)] 2017-05-02 19:28:33.013967: step 9000, loss = 0.1584, acc = 0.9500, f1neg = 0.9451, f1pos = 0.9541, f1 = 0.9496
[Eval_batch(10)(2000,22000)] 2017-05-02 19:28:33.513497: step 9000, loss = 0.1714, acc = 0.9435, f1neg = 0.9380, f1pos = 0.9481, f1 = 0.9431
[Eval_batch(11)(2000,24000)] 2017-05-02 19:28:33.987815: step 9000, loss = 0.1681, acc = 0.9435, f1neg = 0.9329, f1pos = 0.9512, f1 = 0.9421
[Eval_batch(12)(2000,26000)] 2017-05-02 19:28:34.493374: step 9000, loss = 0.1677, acc = 0.9450, f1neg = 0.9419, f1pos = 0.9478, f1 = 0.9448
[Eval_batch(13)(2000,28000)] 2017-05-02 19:28:35.000271: step 9000, loss = 0.1515, acc = 0.9520, f1neg = 0.9383, f1pos = 0.9607, f1 = 0.9495
[Eval_batch(14)(2000,30000)] 2017-05-02 19:28:35.500508: step 9000, loss = 0.1792, acc = 0.9435, f1neg = 0.9304, f1pos = 0.9525, f1 = 0.9414
[Eval_batch(15)(2000,32000)] 2017-05-02 19:28:35.937082: step 9000, loss = 0.1621, acc = 0.9445, f1neg = 0.9385, f1pos = 0.9494, f1 = 0.9440
[Eval_batch(16)(2000,34000)] 2017-05-02 19:28:36.446346: step 9000, loss = 0.1551, acc = 0.9460, f1neg = 0.9424, f1pos = 0.9492, f1 = 0.9458
[Eval_batch(17)(2000,36000)] 2017-05-02 19:28:36.922660: step 9000, loss = 0.1797, acc = 0.9380, f1neg = 0.9366, f1pos = 0.9393, f1 = 0.9380
[Eval_batch(18)(2000,38000)] 2017-05-02 19:28:37.410175: step 9000, loss = 0.1828, acc = 0.9355, f1neg = 0.9362, f1pos = 0.9347, f1 = 0.9355
[Eval_batch(19)(2000,40000)] 2017-05-02 19:28:37.918554: step 9000, loss = 0.1556, acc = 0.9575, f1neg = 0.9499, f1pos = 0.9631, f1 = 0.9565
[Eval_batch(20)(2000,42000)] 2017-05-02 19:28:38.425380: step 9000, loss = 0.1677, acc = 0.9500, f1neg = 0.9549, f1pos = 0.9439, f1 = 0.9494
[Eval_batch(21)(2000,44000)] 2017-05-02 19:28:38.931144: step 9000, loss = 0.1446, acc = 0.9560, f1neg = 0.9512, f1pos = 0.9599, f1 = 0.9556
[Eval_batch(22)(2000,46000)] 2017-05-02 19:28:39.439246: step 9000, loss = 0.1604, acc = 0.9475, f1neg = 0.9468, f1pos = 0.9482, f1 = 0.9475
[Eval_batch(23)(2000,48000)] 2017-05-02 19:28:39.948508: step 9000, loss = 0.1670, acc = 0.9450, f1neg = 0.9361, f1pos = 0.9517, f1 = 0.9439
[Eval_batch(24)(2000,50000)] 2017-05-02 19:28:40.422857: step 9000, loss = 0.1568, acc = 0.9490, f1neg = 0.9395, f1pos = 0.9559, f1 = 0.9477
[Eval_batch(25)(2000,52000)] 2017-05-02 19:28:40.887333: step 9000, loss = 0.1454, acc = 0.9550, f1neg = 0.9494, f1pos = 0.9595, f1 = 0.9544
[Eval_batch(26)(2000,54000)] 2017-05-02 19:28:41.391594: step 9000, loss = 0.1601, acc = 0.9480, f1neg = 0.9499, f1pos = 0.9460, f1 = 0.9479
[Eval_batch(27)(2000,56000)] 2017-05-02 19:28:41.972309: step 9000, loss = 0.1472, acc = 0.9555, f1neg = 0.9523, f1pos = 0.9583, f1 = 0.9553
[Eval] 2017-05-02 19:28:41.972408: step 9000, acc = 0.9467, f1 = 0.9460
[Test_batch(0)(2000,2000)] 2017-05-02 19:28:42.439026: step 9000, loss = 0.1956, acc = 0.9370, f1neg = 0.9386, f1pos = 0.9353, f1 = 0.9370
[Test_batch(1)(2000,4000)] 2017-05-02 19:28:42.953297: step 9000, loss = 0.1874, acc = 0.9405, f1neg = 0.9459, f1pos = 0.9339, f1 = 0.9399
[Test_batch(2)(2000,6000)] 2017-05-02 19:28:43.465668: step 9000, loss = 0.1942, acc = 0.9360, f1neg = 0.9397, f1pos = 0.9318, f1 = 0.9358
[Test_batch(3)(2000,8000)] 2017-05-02 19:28:43.975370: step 9000, loss = 0.2041, acc = 0.9320, f1neg = 0.9348, f1pos = 0.9289, f1 = 0.9319
[Test_batch(4)(2000,10000)] 2017-05-02 19:28:44.488906: step 9000, loss = 0.2073, acc = 0.9250, f1neg = 0.9228, f1pos = 0.9271, f1 = 0.9249
[Test_batch(5)(2000,12000)] 2017-05-02 19:28:44.992871: step 9000, loss = 0.2165, acc = 0.9205, f1neg = 0.9186, f1pos = 0.9223, f1 = 0.9205
[Test_batch(6)(2000,14000)] 2017-05-02 19:28:45.483973: step 9000, loss = 0.2016, acc = 0.9230, f1neg = 0.9180, f1pos = 0.9274, f1 = 0.9227
[Test_batch(7)(2000,16000)] 2017-05-02 19:28:45.984069: step 9000, loss = 0.1865, acc = 0.9345, f1neg = 0.9390, f1pos = 0.9292, f1 = 0.9341
[Test_batch(8)(2000,18000)] 2017-05-02 19:28:46.500119: step 9000, loss = 0.1843, acc = 0.9380, f1neg = 0.9358, f1pos = 0.9400, f1 = 0.9379
[Test_batch(9)(2000,20000)] 2017-05-02 19:28:47.014670: step 9000, loss = 0.2131, acc = 0.9235, f1neg = 0.9167, f1pos = 0.9293, f1 = 0.9230
[Test_batch(10)(2000,22000)] 2017-05-02 19:28:47.517618: step 9000, loss = 0.1853, acc = 0.9320, f1neg = 0.9211, f1pos = 0.9402, f1 = 0.9307
[Test_batch(11)(2000,24000)] 2017-05-02 19:28:48.020142: step 9000, loss = 0.1947, acc = 0.9375, f1neg = 0.9377, f1pos = 0.9373, f1 = 0.9375
[Test_batch(12)(2000,26000)] 2017-05-02 19:28:48.517460: step 9000, loss = 0.1675, acc = 0.9470, f1neg = 0.9479, f1pos = 0.9461, f1 = 0.9470
[Test_batch(13)(2000,28000)] 2017-05-02 19:28:49.006457: step 9000, loss = 0.1998, acc = 0.9225, f1neg = 0.9110, f1pos = 0.9314, f1 = 0.9212
[Test_batch(14)(2000,30000)] 2017-05-02 19:28:49.524530: step 9000, loss = 0.1642, acc = 0.9450, f1neg = 0.9354, f1pos = 0.9521, f1 = 0.9438
[Test_batch(15)(2000,32000)] 2017-05-02 19:28:50.038583: step 9000, loss = 0.1856, acc = 0.9435, f1neg = 0.9414, f1pos = 0.9454, f1 = 0.9434
[Test_batch(16)(2000,34000)] 2017-05-02 19:28:50.543285: step 9000, loss = 0.1740, acc = 0.9420, f1neg = 0.9389, f1pos = 0.9448, f1 = 0.9418
[Test_batch(17)(2000,36000)] 2017-05-02 19:28:51.053459: step 9000, loss = 0.1651, acc = 0.9510, f1neg = 0.9451, f1pos = 0.9557, f1 = 0.9504
[Test_batch(18)(2000,38000)] 2017-05-02 19:28:51.639834: step 9000, loss = 0.1582, acc = 0.9455, f1neg = 0.9431, f1pos = 0.9477, f1 = 0.9454
[Test] 2017-05-02 19:28:51.639931: step 9000, acc = 0.9356, f1 = 0.9352
[Status] 2017-05-02 19:28:51.639957: step 9000, maxindex = 8000, maxdev = 0.9514, maxtst = 0.9409
2017-05-02 19:29:01.211500: step 9010, loss = 0.2188, acc = 0.9120 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 19:29:10.689573: step 9020, loss = 0.1635, acc = 0.9560 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 19:29:19.906758: step 9030, loss = 0.1771, acc = 0.9340 (262.4 examples/sec; 0.244 sec/batch)
2017-05-02 19:29:29.303548: step 9040, loss = 0.1649, acc = 0.9440 (279.9 examples/sec; 0.229 sec/batch)
2017-05-02 19:29:38.404313: step 9050, loss = 0.1712, acc = 0.9460 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 19:29:47.648505: step 9060, loss = 0.2011, acc = 0.9220 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 19:29:56.734214: step 9070, loss = 0.1603, acc = 0.9440 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 19:30:07.105613: step 9080, loss = 0.1663, acc = 0.9380 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 19:30:16.390331: step 9090, loss = 0.1886, acc = 0.9440 (269.8 examples/sec; 0.237 sec/batch)
2017-05-02 19:30:25.583923: step 9100, loss = 0.1821, acc = 0.9520 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 19:30:34.929349: step 9110, loss = 0.1951, acc = 0.9280 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 19:30:44.264421: step 9120, loss = 0.1664, acc = 0.9540 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 19:30:53.481473: step 9130, loss = 0.1724, acc = 0.9520 (282.8 examples/sec; 0.226 sec/batch)
2017-05-02 19:31:02.748871: step 9140, loss = 0.1466, acc = 0.9600 (294.7 examples/sec; 0.217 sec/batch)
2017-05-02 19:31:11.873908: step 9150, loss = 0.1946, acc = 0.9360 (292.8 examples/sec; 0.219 sec/batch)
2017-05-02 19:31:21.135754: step 9160, loss = 0.1790, acc = 0.9440 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 19:31:30.315982: step 9170, loss = 0.1774, acc = 0.9400 (270.8 examples/sec; 0.236 sec/batch)
2017-05-02 19:31:39.617683: step 9180, loss = 0.1569, acc = 0.9400 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 19:31:48.838580: step 9190, loss = 0.1730, acc = 0.9380 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 19:31:58.393929: step 9200, loss = 0.1770, acc = 0.9420 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 19:32:07.805051: step 9210, loss = 0.2027, acc = 0.9320 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 19:32:16.900870: step 9220, loss = 0.1580, acc = 0.9560 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 19:32:26.070448: step 9230, loss = 0.1640, acc = 0.9580 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 19:32:35.162905: step 9240, loss = 0.1545, acc = 0.9560 (272.1 examples/sec; 0.235 sec/batch)
2017-05-02 19:32:44.331125: step 9250, loss = 0.1839, acc = 0.9400 (285.1 examples/sec; 0.225 sec/batch)
2017-05-02 19:32:53.651222: step 9260, loss = 0.1773, acc = 0.9460 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 19:33:02.803911: step 9270, loss = 0.1887, acc = 0.9340 (270.5 examples/sec; 0.237 sec/batch)
2017-05-02 19:33:12.004347: step 9280, loss = 0.1476, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 19:33:21.164196: step 9290, loss = 0.1783, acc = 0.9420 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 19:33:30.373010: step 9300, loss = 0.1794, acc = 0.9440 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 19:33:39.458124: step 9310, loss = 0.1789, acc = 0.9380 (269.8 examples/sec; 0.237 sec/batch)
2017-05-02 19:33:49.124599: step 9320, loss = 0.1668, acc = 0.9460 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 19:33:58.490887: step 9330, loss = 0.2000, acc = 0.9360 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 19:34:07.784382: step 9340, loss = 0.2082, acc = 0.9180 (270.6 examples/sec; 0.236 sec/batch)
2017-05-02 19:34:17.003395: step 9350, loss = 0.1803, acc = 0.9280 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 19:34:26.255956: step 9360, loss = 0.1479, acc = 0.9560 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 19:34:35.432457: step 9370, loss = 0.1867, acc = 0.9380 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 19:34:44.720319: step 9380, loss = 0.1473, acc = 0.9520 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 19:34:53.958444: step 9390, loss = 0.1942, acc = 0.9460 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 19:35:03.064756: step 9400, loss = 0.1928, acc = 0.9300 (291.7 examples/sec; 0.219 sec/batch)
2017-05-02 19:35:12.207977: step 9410, loss = 0.1785, acc = 0.9420 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 19:35:21.472221: step 9420, loss = 0.1698, acc = 0.9440 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 19:35:30.843108: step 9430, loss = 0.1756, acc = 0.9420 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 19:35:40.210664: step 9440, loss = 0.1828, acc = 0.9280 (285.1 examples/sec; 0.224 sec/batch)
2017-05-02 19:35:49.620929: step 9450, loss = 0.1546, acc = 0.9460 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 19:35:58.888248: step 9460, loss = 0.1668, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 19:36:08.148510: step 9470, loss = 0.1813, acc = 0.9420 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 19:36:17.418019: step 9480, loss = 0.1860, acc = 0.9400 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 19:36:26.721647: step 9490, loss = 0.1966, acc = 0.9320 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 19:36:36.003498: step 9500, loss = 0.1867, acc = 0.9280 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 19:36:45.165209: step 9510, loss = 0.1797, acc = 0.9400 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 19:36:54.731023: step 9520, loss = 0.1521, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 19:37:04.056271: step 9530, loss = 0.1964, acc = 0.9220 (268.9 examples/sec; 0.238 sec/batch)
2017-05-02 19:37:13.273955: step 9540, loss = 0.1731, acc = 0.9380 (292.0 examples/sec; 0.219 sec/batch)
2017-05-02 19:37:22.575086: step 9550, loss = 0.1638, acc = 0.9500 (267.9 examples/sec; 0.239 sec/batch)
2017-05-02 19:37:31.838805: step 9560, loss = 0.1670, acc = 0.9420 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 19:37:41.182033: step 9570, loss = 0.1466, acc = 0.9540 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 19:37:50.619966: step 9580, loss = 0.1797, acc = 0.9340 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 19:37:59.799531: step 9590, loss = 0.1599, acc = 0.9440 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 19:38:09.283454: step 9600, loss = 0.1595, acc = 0.9460 (262.6 examples/sec; 0.244 sec/batch)
2017-05-02 19:38:18.599622: step 9610, loss = 0.1823, acc = 0.9400 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 19:38:27.895630: step 9620, loss = 0.1667, acc = 0.9460 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 19:38:37.173210: step 9630, loss = 0.1862, acc = 0.9300 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 19:38:46.467235: step 9640, loss = 0.1839, acc = 0.9340 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 19:38:55.812513: step 9650, loss = 0.1781, acc = 0.9320 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 19:39:05.191085: step 9660, loss = 0.1886, acc = 0.9360 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 19:39:14.487267: step 9670, loss = 0.2016, acc = 0.9260 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 19:39:23.784421: step 9680, loss = 0.1779, acc = 0.9420 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 19:39:32.943274: step 9690, loss = 0.1723, acc = 0.9380 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 19:39:42.282482: step 9700, loss = 0.1648, acc = 0.9440 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 19:39:51.568789: step 9710, loss = 0.1420, acc = 0.9600 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 19:40:00.691511: step 9720, loss = 0.1982, acc = 0.9300 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 19:40:10.009315: step 9730, loss = 0.1554, acc = 0.9500 (269.1 examples/sec; 0.238 sec/batch)
2017-05-02 19:40:19.321668: step 9740, loss = 0.1789, acc = 0.9360 (266.0 examples/sec; 0.241 sec/batch)
2017-05-02 19:40:28.593568: step 9750, loss = 0.1748, acc = 0.9460 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 19:40:37.792509: step 9760, loss = 0.2025, acc = 0.9360 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 19:40:46.953152: step 9770, loss = 0.1471, acc = 0.9560 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 19:40:56.368718: step 9780, loss = 0.1636, acc = 0.9400 (224.8 examples/sec; 0.285 sec/batch)
2017-05-02 19:41:06.362833: step 9790, loss = 0.1755, acc = 0.9480 (199.2 examples/sec; 0.321 sec/batch)
2017-05-02 19:41:15.716826: step 9800, loss = 0.1676, acc = 0.9420 (265.4 examples/sec; 0.241 sec/batch)
2017-05-02 19:41:25.020360: step 9810, loss = 0.1586, acc = 0.9480 (293.6 examples/sec; 0.218 sec/batch)
2017-05-02 19:41:34.199644: step 9820, loss = 0.1723, acc = 0.9500 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 19:41:43.460171: step 9830, loss = 0.1683, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 19:41:52.714785: step 9840, loss = 0.1875, acc = 0.9460 (262.4 examples/sec; 0.244 sec/batch)
2017-05-02 19:42:02.352848: step 9850, loss = 0.1414, acc = 0.9600 (274.3 examples/sec; 0.233 sec/batch)
2017-05-02 19:42:11.492739: step 9860, loss = 0.2090, acc = 0.9260 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 19:42:20.708845: step 9870, loss = 0.1855, acc = 0.9440 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 19:42:29.956754: step 9880, loss = 0.1631, acc = 0.9580 (278.6 examples/sec; 0.230 sec/batch)
2017-05-02 19:42:39.371286: step 9890, loss = 0.1819, acc = 0.9320 (259.3 examples/sec; 0.247 sec/batch)
2017-05-02 19:42:48.706542: step 9900, loss = 0.1744, acc = 0.9520 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 19:42:57.972521: step 9910, loss = 0.1430, acc = 0.9640 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 19:43:07.140300: step 9920, loss = 0.1934, acc = 0.9380 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 19:43:16.328169: step 9930, loss = 0.1597, acc = 0.9560 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 19:43:25.696579: step 9940, loss = 0.1694, acc = 0.9380 (262.5 examples/sec; 0.244 sec/batch)
2017-05-02 19:43:34.964938: step 9950, loss = 0.1657, acc = 0.9480 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 19:43:44.136958: step 9960, loss = 0.1841, acc = 0.9420 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 19:43:53.345676: step 9970, loss = 0.1764, acc = 0.9420 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 19:44:02.640381: step 9980, loss = 0.1764, acc = 0.9400 (269.0 examples/sec; 0.238 sec/batch)
2017-05-02 19:44:11.772274: step 9990, loss = 0.1716, acc = 0.9520 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 19:44:21.098074: step 10000, loss = 0.1575, acc = 0.9480 (289.1 examples/sec; 0.221 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 19:44:21.565235: step 10000, loss = 0.1405, acc = 0.9525, f1neg = 0.9481, f1pos = 0.9562, f1 = 0.9522
[Eval_batch(1)(2000,4000)] 2017-05-02 19:44:22.066567: step 10000, loss = 0.1429, acc = 0.9550, f1neg = 0.9478, f1pos = 0.9605, f1 = 0.9541
[Eval_batch(2)(2000,6000)] 2017-05-02 19:44:22.565498: step 10000, loss = 0.1574, acc = 0.9495, f1neg = 0.9466, f1pos = 0.9521, f1 = 0.9493
[Eval_batch(3)(2000,8000)] 2017-05-02 19:44:23.069779: step 10000, loss = 0.1634, acc = 0.9480, f1neg = 0.9483, f1pos = 0.9477, f1 = 0.9480
[Eval_batch(4)(2000,10000)] 2017-05-02 19:44:23.576473: step 10000, loss = 0.1624, acc = 0.9430, f1neg = 0.9446, f1pos = 0.9414, f1 = 0.9430
[Eval_batch(5)(2000,12000)] 2017-05-02 19:44:24.058117: step 10000, loss = 0.1569, acc = 0.9450, f1neg = 0.9411, f1pos = 0.9485, f1 = 0.9448
[Eval_batch(6)(2000,14000)] 2017-05-02 19:44:24.557134: step 10000, loss = 0.1474, acc = 0.9555, f1neg = 0.9544, f1pos = 0.9566, f1 = 0.9555
[Eval_batch(7)(2000,16000)] 2017-05-02 19:44:25.055951: step 10000, loss = 0.1464, acc = 0.9530, f1neg = 0.9442, f1pos = 0.9594, f1 = 0.9518
[Eval_batch(8)(2000,18000)] 2017-05-02 19:44:25.567544: step 10000, loss = 0.1459, acc = 0.9540, f1neg = 0.9477, f1pos = 0.9590, f1 = 0.9533
[Eval_batch(9)(2000,20000)] 2017-05-02 19:44:26.072367: step 10000, loss = 0.1462, acc = 0.9555, f1neg = 0.9526, f1pos = 0.9581, f1 = 0.9553
[Eval_batch(10)(2000,22000)] 2017-05-02 19:44:26.572936: step 10000, loss = 0.1584, acc = 0.9505, f1neg = 0.9471, f1pos = 0.9535, f1 = 0.9503
[Eval_batch(11)(2000,24000)] 2017-05-02 19:44:27.063126: step 10000, loss = 0.1573, acc = 0.9520, f1neg = 0.9448, f1pos = 0.9576, f1 = 0.9512
[Eval_batch(12)(2000,26000)] 2017-05-02 19:44:27.573737: step 10000, loss = 0.1533, acc = 0.9505, f1neg = 0.9492, f1pos = 0.9518, f1 = 0.9505
[Eval_batch(13)(2000,28000)] 2017-05-02 19:44:28.083809: step 10000, loss = 0.1401, acc = 0.9585, f1neg = 0.9481, f1pos = 0.9654, f1 = 0.9568
[Eval_batch(14)(2000,30000)] 2017-05-02 19:44:28.569006: step 10000, loss = 0.1650, acc = 0.9470, f1neg = 0.9365, f1pos = 0.9545, f1 = 0.9455
[Eval_batch(15)(2000,32000)] 2017-05-02 19:44:29.080514: step 10000, loss = 0.1490, acc = 0.9575, f1neg = 0.9543, f1pos = 0.9603, f1 = 0.9573
[Eval_batch(16)(2000,34000)] 2017-05-02 19:44:29.565301: step 10000, loss = 0.1432, acc = 0.9525, f1neg = 0.9505, f1pos = 0.9543, f1 = 0.9524
[Eval_batch(17)(2000,36000)] 2017-05-02 19:44:30.060608: step 10000, loss = 0.1626, acc = 0.9465, f1neg = 0.9467, f1pos = 0.9463, f1 = 0.9465
[Eval_batch(18)(2000,38000)] 2017-05-02 19:44:30.561119: step 10000, loss = 0.1617, acc = 0.9450, f1neg = 0.9471, f1pos = 0.9427, f1 = 0.9449
[Eval_batch(19)(2000,40000)] 2017-05-02 19:44:31.060689: step 10000, loss = 0.1413, acc = 0.9635, f1neg = 0.9577, f1pos = 0.9679, f1 = 0.9628
[Eval_batch(20)(2000,42000)] 2017-05-02 19:44:31.573310: step 10000, loss = 0.1450, acc = 0.9560, f1neg = 0.9611, f1pos = 0.9494, f1 = 0.9552
[Eval_batch(21)(2000,44000)] 2017-05-02 19:44:32.079955: step 10000, loss = 0.1383, acc = 0.9580, f1neg = 0.9544, f1pos = 0.9610, f1 = 0.9577
[Eval_batch(22)(2000,46000)] 2017-05-02 19:44:32.584228: step 10000, loss = 0.1443, acc = 0.9545, f1neg = 0.9549, f1pos = 0.9541, f1 = 0.9545
[Eval_batch(23)(2000,48000)] 2017-05-02 19:44:33.083982: step 10000, loss = 0.1533, acc = 0.9510, f1neg = 0.9447, f1pos = 0.9560, f1 = 0.9504
[Eval_batch(24)(2000,50000)] 2017-05-02 19:44:33.596586: step 10000, loss = 0.1424, acc = 0.9530, f1neg = 0.9462, f1pos = 0.9583, f1 = 0.9522
[Eval_batch(25)(2000,52000)] 2017-05-02 19:44:34.100967: step 10000, loss = 0.1317, acc = 0.9630, f1neg = 0.9594, f1pos = 0.9660, f1 = 0.9627
[Eval_batch(26)(2000,54000)] 2017-05-02 19:44:34.599509: step 10000, loss = 0.1407, acc = 0.9545, f1neg = 0.9570, f1pos = 0.9517, f1 = 0.9543
[Eval_batch(27)(2000,56000)] 2017-05-02 19:44:35.234571: step 10000, loss = 0.1292, acc = 0.9620, f1neg = 0.9601, f1pos = 0.9637, f1 = 0.9619
[Eval] 2017-05-02 19:44:35.234656: step 10000, acc = 0.9531, f1 = 0.9527
[Test_batch(0)(2000,2000)] 2017-05-02 19:44:35.740952: step 10000, loss = 0.1780, acc = 0.9475, f1neg = 0.9502, f1pos = 0.9445, f1 = 0.9473
[Test_batch(1)(2000,4000)] 2017-05-02 19:44:36.202984: step 10000, loss = 0.1670, acc = 0.9425, f1neg = 0.9491, f1pos = 0.9339, f1 = 0.9415
[Test_batch(2)(2000,6000)] 2017-05-02 19:44:36.705027: step 10000, loss = 0.1747, acc = 0.9465, f1neg = 0.9508, f1pos = 0.9414, f1 = 0.9461
[Test_batch(3)(2000,8000)] 2017-05-02 19:44:37.205313: step 10000, loss = 0.1851, acc = 0.9320, f1neg = 0.9366, f1pos = 0.9267, f1 = 0.9316
[Test_batch(4)(2000,10000)] 2017-05-02 19:44:37.714489: step 10000, loss = 0.1905, acc = 0.9335, f1neg = 0.9337, f1pos = 0.9333, f1 = 0.9335
[Test_batch(5)(2000,12000)] 2017-05-02 19:44:38.223569: step 10000, loss = 0.1974, acc = 0.9320, f1neg = 0.9329, f1pos = 0.9311, f1 = 0.9320
[Test_batch(6)(2000,14000)] 2017-05-02 19:44:38.732141: step 10000, loss = 0.1816, acc = 0.9385, f1neg = 0.9364, f1pos = 0.9404, f1 = 0.9384
[Test_batch(7)(2000,16000)] 2017-05-02 19:44:39.239923: step 10000, loss = 0.1698, acc = 0.9450, f1neg = 0.9503, f1pos = 0.9384, f1 = 0.9444
[Test_batch(8)(2000,18000)] 2017-05-02 19:44:39.741366: step 10000, loss = 0.1697, acc = 0.9460, f1neg = 0.9456, f1pos = 0.9464, f1 = 0.9460
[Test_batch(9)(2000,20000)] 2017-05-02 19:44:40.244448: step 10000, loss = 0.2023, acc = 0.9255, f1neg = 0.9219, f1pos = 0.9288, f1 = 0.9253
[Test_batch(10)(2000,22000)] 2017-05-02 19:44:40.752149: step 10000, loss = 0.1793, acc = 0.9400, f1neg = 0.9330, f1pos = 0.9457, f1 = 0.9393
[Test_batch(11)(2000,24000)] 2017-05-02 19:44:41.266162: step 10000, loss = 0.1764, acc = 0.9450, f1neg = 0.9467, f1pos = 0.9432, f1 = 0.9449
[Test_batch(12)(2000,26000)] 2017-05-02 19:44:41.759121: step 10000, loss = 0.1571, acc = 0.9510, f1neg = 0.9531, f1pos = 0.9487, f1 = 0.9509
[Test_batch(13)(2000,28000)] 2017-05-02 19:44:42.274050: step 10000, loss = 0.1875, acc = 0.9330, f1neg = 0.9256, f1pos = 0.9391, f1 = 0.9323
[Test_batch(14)(2000,30000)] 2017-05-02 19:44:42.781418: step 10000, loss = 0.1570, acc = 0.9505, f1neg = 0.9437, f1pos = 0.9558, f1 = 0.9498
[Test_batch(15)(2000,32000)] 2017-05-02 19:44:43.279581: step 10000, loss = 0.1739, acc = 0.9430, f1neg = 0.9424, f1pos = 0.9436, f1 = 0.9430
[Test_batch(16)(2000,34000)] 2017-05-02 19:44:43.779558: step 10000, loss = 0.1599, acc = 0.9515, f1neg = 0.9506, f1pos = 0.9524, f1 = 0.9515
[Test_batch(17)(2000,36000)] 2017-05-02 19:44:44.269497: step 10000, loss = 0.1496, acc = 0.9550, f1neg = 0.9510, f1pos = 0.9584, f1 = 0.9547
[Test_batch(18)(2000,38000)] 2017-05-02 19:44:44.859509: step 10000, loss = 0.1381, acc = 0.9610, f1neg = 0.9602, f1pos = 0.9618, f1 = 0.9610
[Test] 2017-05-02 19:44:44.859597: step 10000, acc = 0.9431, f1 = 0.9428
[Status] 2017-05-02 19:44:44.859624: step 10000, maxindex = 10000, maxdev = 0.9531, maxtst = 0.9431
2017-05-02 19:44:57.343913: step 10010, loss = 0.1582, acc = 0.9440 (285.8 examples/sec; 0.224 sec/batch)
2017-05-02 19:45:06.709317: step 10020, loss = 0.1674, acc = 0.9480 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 19:45:15.966407: step 10030, loss = 0.1704, acc = 0.9420 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 19:45:25.195298: step 10040, loss = 0.1556, acc = 0.9500 (287.5 examples/sec; 0.223 sec/batch)
2017-05-02 19:45:34.470287: step 10050, loss = 0.1639, acc = 0.9440 (263.7 examples/sec; 0.243 sec/batch)
2017-05-02 19:45:43.677110: step 10060, loss = 0.1762, acc = 0.9400 (292.6 examples/sec; 0.219 sec/batch)
2017-05-02 19:45:52.946146: step 10070, loss = 0.1591, acc = 0.9460 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 19:46:03.064293: step 10080, loss = 0.1808, acc = 0.9320 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 19:46:12.778696: step 10090, loss = 0.1647, acc = 0.9460 (176.2 examples/sec; 0.363 sec/batch)
2017-05-02 19:46:22.039132: step 10100, loss = 0.1748, acc = 0.9420 (265.8 examples/sec; 0.241 sec/batch)
2017-05-02 19:46:31.180533: step 10110, loss = 0.1790, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 19:46:40.484355: step 10120, loss = 0.1486, acc = 0.9540 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 19:46:49.669833: step 10130, loss = 0.1752, acc = 0.9400 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 19:46:58.834367: step 10140, loss = 0.1691, acc = 0.9440 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 19:47:08.005189: step 10150, loss = 0.1596, acc = 0.9460 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 19:47:17.221655: step 10160, loss = 0.1758, acc = 0.9440 (266.7 examples/sec; 0.240 sec/batch)
2017-05-02 19:47:26.573930: step 10170, loss = 0.1923, acc = 0.9320 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 19:47:35.787593: step 10180, loss = 0.1765, acc = 0.9460 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 19:47:44.895943: step 10190, loss = 0.1751, acc = 0.9480 (298.2 examples/sec; 0.215 sec/batch)
2017-05-02 19:47:54.253349: step 10200, loss = 0.1752, acc = 0.9400 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 19:48:03.405905: step 10210, loss = 0.1307, acc = 0.9540 (276.4 examples/sec; 0.232 sec/batch)
2017-05-02 19:48:12.667447: step 10220, loss = 0.1879, acc = 0.9260 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 19:48:21.990130: step 10230, loss = 0.1673, acc = 0.9560 (256.7 examples/sec; 0.249 sec/batch)
2017-05-02 19:48:31.427279: step 10240, loss = 0.1607, acc = 0.9540 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 19:48:40.840703: step 10250, loss = 0.1606, acc = 0.9520 (255.0 examples/sec; 0.251 sec/batch)
2017-05-02 19:48:50.474502: step 10260, loss = 0.1753, acc = 0.9320 (228.2 examples/sec; 0.280 sec/batch)
2017-05-02 19:48:59.673332: step 10270, loss = 0.1777, acc = 0.9400 (288.8 examples/sec; 0.222 sec/batch)
2017-05-02 19:49:09.027436: step 10280, loss = 0.1591, acc = 0.9560 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 19:49:18.266270: step 10290, loss = 0.2186, acc = 0.9300 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 19:49:27.647052: step 10300, loss = 0.1837, acc = 0.9460 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 19:49:37.123633: step 10310, loss = 0.1778, acc = 0.9360 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 19:49:46.331585: step 10320, loss = 0.1446, acc = 0.9720 (285.0 examples/sec; 0.225 sec/batch)
2017-05-02 19:49:55.695968: step 10330, loss = 0.1754, acc = 0.9360 (270.7 examples/sec; 0.236 sec/batch)
2017-05-02 19:50:04.829976: step 10340, loss = 0.1511, acc = 0.9560 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 19:50:14.136162: step 10350, loss = 0.1559, acc = 0.9620 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 19:50:23.378209: step 10360, loss = 0.1787, acc = 0.9460 (265.6 examples/sec; 0.241 sec/batch)
2017-05-02 19:50:32.522003: step 10370, loss = 0.1952, acc = 0.9380 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 19:50:41.774557: step 10380, loss = 0.1665, acc = 0.9500 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 19:50:50.904064: step 10390, loss = 0.1793, acc = 0.9400 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 19:51:00.114583: step 10400, loss = 0.1528, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 19:51:09.433934: step 10410, loss = 0.1806, acc = 0.9540 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 19:51:18.555279: step 10420, loss = 0.2026, acc = 0.9260 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 19:51:28.170068: step 10430, loss = 0.1932, acc = 0.9360 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 19:51:37.479366: step 10440, loss = 0.1550, acc = 0.9480 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 19:51:46.795376: step 10450, loss = 0.1616, acc = 0.9620 (257.0 examples/sec; 0.249 sec/batch)
2017-05-02 19:51:55.925816: step 10460, loss = 0.1919, acc = 0.9380 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 19:52:05.053412: step 10470, loss = 0.1878, acc = 0.9280 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 19:52:14.650627: step 10480, loss = 0.1919, acc = 0.9320 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 19:52:23.836925: step 10490, loss = 0.1622, acc = 0.9520 (292.3 examples/sec; 0.219 sec/batch)
2017-05-02 19:52:33.029525: step 10500, loss = 0.1950, acc = 0.9300 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 19:52:42.328601: step 10510, loss = 0.1865, acc = 0.9360 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 19:52:51.423829: step 10520, loss = 0.1582, acc = 0.9380 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 19:53:00.666912: step 10530, loss = 0.1858, acc = 0.9420 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 19:53:09.998290: step 10540, loss = 0.1748, acc = 0.9480 (266.1 examples/sec; 0.240 sec/batch)
2017-05-02 19:53:19.244680: step 10550, loss = 0.1691, acc = 0.9480 (287.8 examples/sec; 0.222 sec/batch)
2017-05-02 19:53:28.473415: step 10560, loss = 0.1551, acc = 0.9500 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 19:53:37.743184: step 10570, loss = 0.1400, acc = 0.9580 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 19:53:46.997370: step 10580, loss = 0.1620, acc = 0.9600 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 19:53:56.238760: step 10590, loss = 0.1505, acc = 0.9560 (260.6 examples/sec; 0.246 sec/batch)
2017-05-02 19:54:05.449610: step 10600, loss = 0.1727, acc = 0.9420 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 19:54:14.861841: step 10610, loss = 0.1704, acc = 0.9360 (271.3 examples/sec; 0.236 sec/batch)
2017-05-02 19:54:24.082255: step 10620, loss = 0.1535, acc = 0.9540 (278.6 examples/sec; 0.230 sec/batch)
2017-05-02 19:54:33.322870: step 10630, loss = 0.1905, acc = 0.9440 (292.8 examples/sec; 0.219 sec/batch)
2017-05-02 19:54:42.476385: step 10640, loss = 0.1782, acc = 0.9340 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 19:54:51.630118: step 10650, loss = 0.1670, acc = 0.9540 (293.0 examples/sec; 0.218 sec/batch)
2017-05-02 19:55:00.882950: step 10660, loss = 0.2030, acc = 0.9280 (274.1 examples/sec; 0.233 sec/batch)
2017-05-02 19:55:10.173170: step 10670, loss = 0.1627, acc = 0.9460 (263.5 examples/sec; 0.243 sec/batch)
2017-05-02 19:55:19.278066: step 10680, loss = 0.1665, acc = 0.9520 (287.6 examples/sec; 0.223 sec/batch)
2017-05-02 19:55:28.488662: step 10690, loss = 0.1495, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 19:55:37.701770: step 10700, loss = 0.1696, acc = 0.9420 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 19:55:47.665655: step 10710, loss = 0.1578, acc = 0.9440 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 19:55:56.873595: step 10720, loss = 0.1669, acc = 0.9460 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 19:56:06.094533: step 10730, loss = 0.1770, acc = 0.9380 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 19:56:15.315241: step 10740, loss = 0.1746, acc = 0.9420 (285.6 examples/sec; 0.224 sec/batch)
2017-05-02 19:56:24.548933: step 10750, loss = 0.1869, acc = 0.9440 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 19:56:33.888777: step 10760, loss = 0.1427, acc = 0.9620 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 19:56:43.130084: step 10770, loss = 0.1751, acc = 0.9500 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 19:56:52.522856: step 10780, loss = 0.1730, acc = 0.9480 (222.8 examples/sec; 0.287 sec/batch)
2017-05-02 19:57:01.760414: step 10790, loss = 0.1662, acc = 0.9460 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 19:57:10.965239: step 10800, loss = 0.1580, acc = 0.9480 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 19:57:20.158477: step 10810, loss = 0.1732, acc = 0.9460 (291.0 examples/sec; 0.220 sec/batch)
2017-05-02 19:57:29.402795: step 10820, loss = 0.1548, acc = 0.9480 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 19:57:39.321634: step 10830, loss = 0.1653, acc = 0.9420 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 19:57:48.606504: step 10840, loss = 0.2132, acc = 0.9300 (266.1 examples/sec; 0.240 sec/batch)
2017-05-02 19:57:57.793412: step 10850, loss = 0.2153, acc = 0.9240 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 19:58:06.923362: step 10860, loss = 0.1472, acc = 0.9560 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 19:58:16.154127: step 10870, loss = 0.1442, acc = 0.9560 (289.0 examples/sec; 0.221 sec/batch)
2017-05-02 19:58:25.736305: step 10880, loss = 0.1600, acc = 0.9480 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 19:58:34.997096: step 10890, loss = 0.1754, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 19:58:44.246069: step 10900, loss = 0.1781, acc = 0.9360 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 19:58:53.277644: step 10910, loss = 0.1669, acc = 0.9440 (276.5 examples/sec; 0.232 sec/batch)
2017-05-02 19:59:02.617784: step 10920, loss = 0.1910, acc = 0.9360 (263.1 examples/sec; 0.243 sec/batch)
2017-05-02 19:59:11.906745: step 10930, loss = 0.2132, acc = 0.9200 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 19:59:21.418182: step 10940, loss = 0.1507, acc = 0.9560 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 19:59:30.829765: step 10950, loss = 0.1632, acc = 0.9560 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 19:59:39.987272: step 10960, loss = 0.1659, acc = 0.9500 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 19:59:49.266768: step 10970, loss = 0.1893, acc = 0.9380 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 19:59:58.476902: step 10980, loss = 0.1678, acc = 0.9420 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 20:00:07.712045: step 10990, loss = 0.1543, acc = 0.9460 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 20:00:17.196290: step 11000, loss = 0.1471, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 20:00:17.696530: step 11000, loss = 0.1429, acc = 0.9500, f1neg = 0.9445, f1pos = 0.9545, f1 = 0.9495
[Eval_batch(1)(2000,4000)] 2017-05-02 20:00:18.203733: step 11000, loss = 0.1403, acc = 0.9535, f1neg = 0.9453, f1pos = 0.9595, f1 = 0.9524
[Eval_batch(2)(2000,6000)] 2017-05-02 20:00:18.711670: step 11000, loss = 0.1578, acc = 0.9505, f1neg = 0.9469, f1pos = 0.9537, f1 = 0.9503
[Eval_batch(3)(2000,8000)] 2017-05-02 20:00:19.216209: step 11000, loss = 0.1656, acc = 0.9410, f1neg = 0.9403, f1pos = 0.9417, f1 = 0.9410
[Eval_batch(4)(2000,10000)] 2017-05-02 20:00:19.690625: step 11000, loss = 0.1663, acc = 0.9415, f1neg = 0.9424, f1pos = 0.9406, f1 = 0.9415
[Eval_batch(5)(2000,12000)] 2017-05-02 20:00:20.148822: step 11000, loss = 0.1574, acc = 0.9470, f1neg = 0.9425, f1pos = 0.9509, f1 = 0.9467
[Eval_batch(6)(2000,14000)] 2017-05-02 20:00:20.649663: step 11000, loss = 0.1518, acc = 0.9540, f1neg = 0.9525, f1pos = 0.9554, f1 = 0.9540
[Eval_batch(7)(2000,16000)] 2017-05-02 20:00:21.157065: step 11000, loss = 0.1450, acc = 0.9550, f1neg = 0.9458, f1pos = 0.9615, f1 = 0.9537
[Eval_batch(8)(2000,18000)] 2017-05-02 20:00:21.630800: step 11000, loss = 0.1473, acc = 0.9510, f1neg = 0.9434, f1pos = 0.9568, f1 = 0.9501
[Eval_batch(9)(2000,20000)] 2017-05-02 20:00:22.132327: step 11000, loss = 0.1456, acc = 0.9565, f1neg = 0.9530, f1pos = 0.9595, f1 = 0.9563
[Eval_batch(10)(2000,22000)] 2017-05-02 20:00:22.594505: step 11000, loss = 0.1591, acc = 0.9510, f1neg = 0.9470, f1pos = 0.9545, f1 = 0.9507
[Eval_batch(11)(2000,24000)] 2017-05-02 20:00:23.073318: step 11000, loss = 0.1559, acc = 0.9540, f1neg = 0.9464, f1pos = 0.9597, f1 = 0.9531
[Eval_batch(12)(2000,26000)] 2017-05-02 20:00:23.586186: step 11000, loss = 0.1541, acc = 0.9470, f1neg = 0.9448, f1pos = 0.9490, f1 = 0.9469
[Eval_batch(13)(2000,28000)] 2017-05-02 20:00:24.047897: step 11000, loss = 0.1393, acc = 0.9585, f1neg = 0.9474, f1pos = 0.9657, f1 = 0.9566
[Eval_batch(14)(2000,30000)] 2017-05-02 20:00:24.516558: step 11000, loss = 0.1659, acc = 0.9460, f1neg = 0.9343, f1pos = 0.9542, f1 = 0.9442
[Eval_batch(15)(2000,32000)] 2017-05-02 20:00:24.981760: step 11000, loss = 0.1491, acc = 0.9575, f1neg = 0.9538, f1pos = 0.9606, f1 = 0.9572
[Eval_batch(16)(2000,34000)] 2017-05-02 20:00:25.488559: step 11000, loss = 0.1435, acc = 0.9530, f1neg = 0.9505, f1pos = 0.9552, f1 = 0.9529
[Eval_batch(17)(2000,36000)] 2017-05-02 20:00:25.947033: step 11000, loss = 0.1641, acc = 0.9455, f1neg = 0.9451, f1pos = 0.9459, f1 = 0.9455
[Eval_batch(18)(2000,38000)] 2017-05-02 20:00:26.451372: step 11000, loss = 0.1655, acc = 0.9455, f1neg = 0.9469, f1pos = 0.9441, f1 = 0.9455
[Eval_batch(19)(2000,40000)] 2017-05-02 20:00:26.956209: step 11000, loss = 0.1419, acc = 0.9650, f1neg = 0.9592, f1pos = 0.9694, f1 = 0.9643
[Eval_batch(20)(2000,42000)] 2017-05-02 20:00:27.430336: step 11000, loss = 0.1492, acc = 0.9550, f1neg = 0.9598, f1pos = 0.9489, f1 = 0.9543
[Eval_batch(21)(2000,44000)] 2017-05-02 20:00:27.930753: step 11000, loss = 0.1350, acc = 0.9575, f1neg = 0.9535, f1pos = 0.9609, f1 = 0.9572
[Eval_batch(22)(2000,46000)] 2017-05-02 20:00:28.443603: step 11000, loss = 0.1457, acc = 0.9565, f1neg = 0.9564, f1pos = 0.9566, f1 = 0.9565
[Eval_batch(23)(2000,48000)] 2017-05-02 20:00:28.920741: step 11000, loss = 0.1524, acc = 0.9515, f1neg = 0.9446, f1pos = 0.9569, f1 = 0.9507
[Eval_batch(24)(2000,50000)] 2017-05-02 20:00:29.433315: step 11000, loss = 0.1423, acc = 0.9535, f1neg = 0.9460, f1pos = 0.9592, f1 = 0.9526
[Eval_batch(25)(2000,52000)] 2017-05-02 20:00:29.950139: step 11000, loss = 0.1322, acc = 0.9605, f1neg = 0.9561, f1pos = 0.9641, f1 = 0.9601
[Eval_batch(26)(2000,54000)] 2017-05-02 20:00:30.425271: step 11000, loss = 0.1436, acc = 0.9510, f1neg = 0.9533, f1pos = 0.9484, f1 = 0.9509
[Eval_batch(27)(2000,56000)] 2017-05-02 20:00:31.003800: step 11000, loss = 0.1318, acc = 0.9600, f1neg = 0.9577, f1pos = 0.9621, f1 = 0.9599
[Eval] 2017-05-02 20:00:31.003888: step 11000, acc = 0.9524, f1 = 0.9519
[Test_batch(0)(2000,2000)] 2017-05-02 20:00:31.486145: step 11000, loss = 0.1792, acc = 0.9440, f1neg = 0.9463, f1pos = 0.9415, f1 = 0.9439
[Test_batch(1)(2000,4000)] 2017-05-02 20:00:31.995013: step 11000, loss = 0.1692, acc = 0.9440, f1neg = 0.9497, f1pos = 0.9369, f1 = 0.9433
[Test_batch(2)(2000,6000)] 2017-05-02 20:00:32.503318: step 11000, loss = 0.1775, acc = 0.9455, f1neg = 0.9494, f1pos = 0.9409, f1 = 0.9452
[Test_batch(3)(2000,8000)] 2017-05-02 20:00:33.005212: step 11000, loss = 0.1876, acc = 0.9325, f1neg = 0.9360, f1pos = 0.9286, f1 = 0.9323
[Test_batch(4)(2000,10000)] 2017-05-02 20:00:33.519186: step 11000, loss = 0.1905, acc = 0.9295, f1neg = 0.9286, f1pos = 0.9304, f1 = 0.9295
[Test_batch(5)(2000,12000)] 2017-05-02 20:00:34.025650: step 11000, loss = 0.1986, acc = 0.9295, f1neg = 0.9295, f1pos = 0.9295, f1 = 0.9295
[Test_batch(6)(2000,14000)] 2017-05-02 20:00:34.535077: step 11000, loss = 0.1840, acc = 0.9350, f1neg = 0.9321, f1pos = 0.9377, f1 = 0.9349
[Test_batch(7)(2000,16000)] 2017-05-02 20:00:35.044112: step 11000, loss = 0.1708, acc = 0.9425, f1neg = 0.9472, f1pos = 0.9368, f1 = 0.9420
[Test_batch(8)(2000,18000)] 2017-05-02 20:00:35.550136: step 11000, loss = 0.1698, acc = 0.9450, f1neg = 0.9439, f1pos = 0.9460, f1 = 0.9450
[Test_batch(9)(2000,20000)] 2017-05-02 20:00:36.064641: step 11000, loss = 0.2003, acc = 0.9300, f1neg = 0.9254, f1pos = 0.9341, f1 = 0.9297
[Test_batch(10)(2000,22000)] 2017-05-02 20:00:36.574002: step 11000, loss = 0.1745, acc = 0.9390, f1neg = 0.9306, f1pos = 0.9456, f1 = 0.9381
[Test_batch(11)(2000,24000)] 2017-05-02 20:00:37.086031: step 11000, loss = 0.1782, acc = 0.9430, f1neg = 0.9442, f1pos = 0.9418, f1 = 0.9430
[Test_batch(12)(2000,26000)] 2017-05-02 20:00:37.607133: step 11000, loss = 0.1554, acc = 0.9520, f1neg = 0.9534, f1pos = 0.9505, f1 = 0.9520
[Test_batch(13)(2000,28000)] 2017-05-02 20:00:38.122941: step 11000, loss = 0.1847, acc = 0.9320, f1neg = 0.9233, f1pos = 0.9390, f1 = 0.9311
[Test_batch(14)(2000,30000)] 2017-05-02 20:00:38.625423: step 11000, loss = 0.1536, acc = 0.9485, f1neg = 0.9404, f1pos = 0.9546, f1 = 0.9475
[Test_batch(15)(2000,32000)] 2017-05-02 20:00:39.125460: step 11000, loss = 0.1730, acc = 0.9445, f1neg = 0.9431, f1pos = 0.9458, f1 = 0.9445
[Test_batch(16)(2000,34000)] 2017-05-02 20:00:39.637421: step 11000, loss = 0.1599, acc = 0.9495, f1neg = 0.9477, f1pos = 0.9511, f1 = 0.9494
[Test_batch(17)(2000,36000)] 2017-05-02 20:00:40.144478: step 11000, loss = 0.1512, acc = 0.9560, f1neg = 0.9515, f1pos = 0.9597, f1 = 0.9556
[Test_batch(18)(2000,38000)] 2017-05-02 20:00:40.722552: step 11000, loss = 0.1416, acc = 0.9565, f1neg = 0.9553, f1pos = 0.9577, f1 = 0.9565
[Test] 2017-05-02 20:00:40.722638: step 11000, acc = 0.9420, f1 = 0.9417
[Status] 2017-05-02 20:00:40.722664: step 11000, maxindex = 10000, maxdev = 0.9531, maxtst = 0.9431
2017-05-02 20:00:50.105934: step 11010, loss = 0.1634, acc = 0.9480 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 20:00:59.318954: step 11020, loss = 0.1673, acc = 0.9480 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 20:01:08.400602: step 11030, loss = 0.1392, acc = 0.9540 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 20:01:17.777747: step 11040, loss = 0.1675, acc = 0.9540 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:01:27.050825: step 11050, loss = 0.1484, acc = 0.9500 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 20:01:36.254672: step 11060, loss = 0.1610, acc = 0.9540 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 20:01:45.397663: step 11070, loss = 0.1484, acc = 0.9540 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 20:01:54.574050: step 11080, loss = 0.1685, acc = 0.9360 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 20:02:05.004085: step 11090, loss = 0.1483, acc = 0.9580 (219.6 examples/sec; 0.291 sec/batch)
2017-05-02 20:02:14.107749: step 11100, loss = 0.1762, acc = 0.9400 (275.5 examples/sec; 0.232 sec/batch)
2017-05-02 20:02:23.351310: step 11110, loss = 0.1460, acc = 0.9500 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 20:02:32.692950: step 11120, loss = 0.2084, acc = 0.9200 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 20:02:41.950309: step 11130, loss = 0.1798, acc = 0.9460 (284.6 examples/sec; 0.225 sec/batch)
2017-05-02 20:02:50.983119: step 11140, loss = 0.1806, acc = 0.9420 (281.0 examples/sec; 0.228 sec/batch)
2017-05-02 20:03:00.214049: step 11150, loss = 0.1568, acc = 0.9420 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 20:03:09.444530: step 11160, loss = 0.1861, acc = 0.9280 (274.4 examples/sec; 0.233 sec/batch)
2017-05-02 20:03:18.705416: step 11170, loss = 0.1904, acc = 0.9400 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 20:03:27.994706: step 11180, loss = 0.1757, acc = 0.9460 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 20:03:37.129628: step 11190, loss = 0.1602, acc = 0.9540 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 20:03:46.243902: step 11200, loss = 0.1522, acc = 0.9660 (293.2 examples/sec; 0.218 sec/batch)
2017-05-02 20:03:55.461386: step 11210, loss = 0.1797, acc = 0.9360 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 20:04:04.783358: step 11220, loss = 0.1662, acc = 0.9300 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 20:04:13.991293: step 11230, loss = 0.1628, acc = 0.9540 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 20:04:23.232646: step 11240, loss = 0.1783, acc = 0.9280 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 20:04:32.541041: step 11250, loss = 0.1815, acc = 0.9520 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 20:04:41.985391: step 11260, loss = 0.1505, acc = 0.9580 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 20:04:51.307048: step 11270, loss = 0.2026, acc = 0.9320 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 20:05:00.633136: step 11280, loss = 0.1776, acc = 0.9360 (260.1 examples/sec; 0.246 sec/batch)
2017-05-02 20:05:10.237050: step 11290, loss = 0.1673, acc = 0.9400 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 20:05:19.386085: step 11300, loss = 0.1480, acc = 0.9540 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 20:05:28.449695: step 11310, loss = 0.1787, acc = 0.9420 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 20:05:37.888166: step 11320, loss = 0.1779, acc = 0.9360 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 20:05:47.354038: step 11330, loss = 0.1762, acc = 0.9360 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 20:05:56.503365: step 11340, loss = 0.1437, acc = 0.9580 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 20:06:05.795032: step 11350, loss = 0.1665, acc = 0.9460 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 20:06:15.082374: step 11360, loss = 0.1866, acc = 0.9360 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 20:06:24.467422: step 11370, loss = 0.1728, acc = 0.9380 (270.8 examples/sec; 0.236 sec/batch)
2017-05-02 20:06:33.708515: step 11380, loss = 0.1517, acc = 0.9560 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 20:06:43.090755: step 11390, loss = 0.1567, acc = 0.9500 (281.3 examples/sec; 0.227 sec/batch)
2017-05-02 20:06:52.298934: step 11400, loss = 0.1634, acc = 0.9380 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 20:07:01.654884: step 11410, loss = 0.1753, acc = 0.9440 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 20:07:11.025232: step 11420, loss = 0.1864, acc = 0.9360 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 20:07:20.202152: step 11430, loss = 0.1949, acc = 0.9320 (268.7 examples/sec; 0.238 sec/batch)
2017-05-02 20:07:29.414121: step 11440, loss = 0.1578, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 20:07:38.729071: step 11450, loss = 0.1665, acc = 0.9440 (255.6 examples/sec; 0.250 sec/batch)
2017-05-02 20:07:47.976644: step 11460, loss = 0.1561, acc = 0.9420 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 20:07:57.208574: step 11470, loss = 0.1712, acc = 0.9440 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 20:08:06.514356: step 11480, loss = 0.2027, acc = 0.9320 (260.4 examples/sec; 0.246 sec/batch)
2017-05-02 20:08:15.690675: step 11490, loss = 0.1603, acc = 0.9440 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 20:08:24.852288: step 11500, loss = 0.1375, acc = 0.9620 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 20:08:34.099866: step 11510, loss = 0.1859, acc = 0.9360 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 20:08:43.203106: step 11520, loss = 0.1609, acc = 0.9500 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 20:08:52.498936: step 11530, loss = 0.1712, acc = 0.9580 (259.3 examples/sec; 0.247 sec/batch)
2017-05-02 20:09:01.963369: step 11540, loss = 0.1502, acc = 0.9540 (269.4 examples/sec; 0.238 sec/batch)
2017-05-02 20:09:11.130936: step 11550, loss = 0.1562, acc = 0.9480 (281.0 examples/sec; 0.228 sec/batch)
2017-05-02 20:09:20.379287: step 11560, loss = 0.1652, acc = 0.9520 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 20:09:29.729381: step 11570, loss = 0.1604, acc = 0.9440 (268.0 examples/sec; 0.239 sec/batch)
2017-05-02 20:09:39.013073: step 11580, loss = 0.1432, acc = 0.9640 (261.4 examples/sec; 0.245 sec/batch)
2017-05-02 20:09:48.475594: step 11590, loss = 0.1648, acc = 0.9380 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 20:09:57.907770: step 11600, loss = 0.1724, acc = 0.9420 (252.4 examples/sec; 0.254 sec/batch)
2017-05-02 20:10:07.148550: step 11610, loss = 0.1882, acc = 0.9420 (258.7 examples/sec; 0.247 sec/batch)
2017-05-02 20:10:16.369832: step 11620, loss = 0.1943, acc = 0.9400 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 20:10:25.582821: step 11630, loss = 0.1670, acc = 0.9440 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 20:10:34.877407: step 11640, loss = 0.1631, acc = 0.9420 (292.9 examples/sec; 0.219 sec/batch)
2017-05-02 20:10:44.194658: step 11650, loss = 0.1808, acc = 0.9360 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 20:10:53.177073: step 11660, loss = 0.1521, acc = 0.9560 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 20:11:02.257846: step 11670, loss = 0.1634, acc = 0.9460 (273.5 examples/sec; 0.234 sec/batch)
2017-05-02 20:11:11.409832: step 11680, loss = 0.1672, acc = 0.9520 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 20:11:20.789427: step 11690, loss = 0.1872, acc = 0.9380 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 20:11:29.937391: step 11700, loss = 0.1583, acc = 0.9400 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 20:11:39.078216: step 11710, loss = 0.1797, acc = 0.9320 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 20:11:48.376423: step 11720, loss = 0.1538, acc = 0.9440 (288.0 examples/sec; 0.222 sec/batch)
2017-05-02 20:11:57.519740: step 11730, loss = 0.1665, acc = 0.9500 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 20:12:06.772804: step 11740, loss = 0.1596, acc = 0.9440 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 20:12:15.940126: step 11750, loss = 0.1595, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 20:12:25.124243: step 11760, loss = 0.1810, acc = 0.9440 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 20:12:34.575236: step 11770, loss = 0.1666, acc = 0.9440 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 20:12:43.786395: step 11780, loss = 0.1700, acc = 0.9380 (265.3 examples/sec; 0.241 sec/batch)
2017-05-02 20:12:53.088212: step 11790, loss = 0.1725, acc = 0.9260 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 20:13:02.263857: step 11800, loss = 0.1821, acc = 0.9320 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 20:13:11.432973: step 11810, loss = 0.1512, acc = 0.9580 (291.0 examples/sec; 0.220 sec/batch)
2017-05-02 20:13:20.973482: step 11820, loss = 0.1811, acc = 0.9440 (284.6 examples/sec; 0.225 sec/batch)
2017-05-02 20:13:30.127459: step 11830, loss = 0.1688, acc = 0.9460 (290.1 examples/sec; 0.221 sec/batch)
2017-05-02 20:13:39.320681: step 11840, loss = 0.1829, acc = 0.9380 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 20:13:48.524690: step 11850, loss = 0.1902, acc = 0.9340 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 20:13:57.689129: step 11860, loss = 0.1959, acc = 0.9320 (286.1 examples/sec; 0.224 sec/batch)
2017-05-02 20:14:06.965279: step 11870, loss = 0.1518, acc = 0.9560 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 20:14:16.037161: step 11880, loss = 0.1541, acc = 0.9440 (290.7 examples/sec; 0.220 sec/batch)
2017-05-02 20:14:25.270155: step 11890, loss = 0.1766, acc = 0.9480 (269.3 examples/sec; 0.238 sec/batch)
2017-05-02 20:14:34.553937: step 11900, loss = 0.1684, acc = 0.9380 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 20:14:43.815373: step 11910, loss = 0.2150, acc = 0.9220 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 20:14:52.860002: step 11920, loss = 0.1686, acc = 0.9480 (292.0 examples/sec; 0.219 sec/batch)
2017-05-02 20:15:02.160983: step 11930, loss = 0.1648, acc = 0.9440 (287.6 examples/sec; 0.223 sec/batch)
2017-05-02 20:15:11.402961: step 11940, loss = 0.1603, acc = 0.9560 (262.2 examples/sec; 0.244 sec/batch)
2017-05-02 20:15:20.683076: step 11950, loss = 0.1748, acc = 0.9520 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 20:15:30.012996: step 11960, loss = 0.1857, acc = 0.9300 (254.5 examples/sec; 0.252 sec/batch)
2017-05-02 20:15:39.263100: step 11970, loss = 0.1841, acc = 0.9400 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 20:15:48.553451: step 11980, loss = 0.1600, acc = 0.9380 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 20:15:57.819089: step 11990, loss = 0.1326, acc = 0.9680 (290.4 examples/sec; 0.220 sec/batch)
2017-05-02 20:16:06.949488: step 12000, loss = 0.1818, acc = 0.9540 (283.8 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 20:16:07.422443: step 12000, loss = 0.1365, acc = 0.9545, f1neg = 0.9505, f1pos = 0.9579, f1 = 0.9542
[Eval_batch(1)(2000,4000)] 2017-05-02 20:16:07.884477: step 12000, loss = 0.1400, acc = 0.9585, f1neg = 0.9522, f1pos = 0.9633, f1 = 0.9578
[Eval_batch(2)(2000,6000)] 2017-05-02 20:16:08.355925: step 12000, loss = 0.1523, acc = 0.9495, f1neg = 0.9468, f1pos = 0.9520, f1 = 0.9494
[Eval_batch(3)(2000,8000)] 2017-05-02 20:16:08.828092: step 12000, loss = 0.1581, acc = 0.9500, f1neg = 0.9505, f1pos = 0.9495, f1 = 0.9500
[Eval_batch(4)(2000,10000)] 2017-05-02 20:16:09.307532: step 12000, loss = 0.1577, acc = 0.9465, f1neg = 0.9482, f1pos = 0.9446, f1 = 0.9464
[Eval_batch(5)(2000,12000)] 2017-05-02 20:16:09.780636: step 12000, loss = 0.1523, acc = 0.9475, f1neg = 0.9440, f1pos = 0.9506, f1 = 0.9473
[Eval_batch(6)(2000,14000)] 2017-05-02 20:16:10.248654: step 12000, loss = 0.1433, acc = 0.9585, f1neg = 0.9579, f1pos = 0.9591, f1 = 0.9585
[Eval_batch(7)(2000,16000)] 2017-05-02 20:16:10.719319: step 12000, loss = 0.1432, acc = 0.9520, f1neg = 0.9434, f1pos = 0.9583, f1 = 0.9509
[Eval_batch(8)(2000,18000)] 2017-05-02 20:16:11.162673: step 12000, loss = 0.1416, acc = 0.9535, f1neg = 0.9475, f1pos = 0.9582, f1 = 0.9529
[Eval_batch(9)(2000,20000)] 2017-05-02 20:16:11.608895: step 12000, loss = 0.1422, acc = 0.9565, f1neg = 0.9539, f1pos = 0.9588, f1 = 0.9564
[Eval_batch(10)(2000,22000)] 2017-05-02 20:16:12.056259: step 12000, loss = 0.1530, acc = 0.9520, f1neg = 0.9489, f1pos = 0.9548, f1 = 0.9518
[Eval_batch(11)(2000,24000)] 2017-05-02 20:16:12.525835: step 12000, loss = 0.1536, acc = 0.9530, f1neg = 0.9461, f1pos = 0.9583, f1 = 0.9522
[Eval_batch(12)(2000,26000)] 2017-05-02 20:16:13.001383: step 12000, loss = 0.1487, acc = 0.9515, f1neg = 0.9505, f1pos = 0.9524, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-02 20:16:13.468191: step 12000, loss = 0.1360, acc = 0.9580, f1neg = 0.9477, f1pos = 0.9649, f1 = 0.9563
[Eval_batch(14)(2000,30000)] 2017-05-02 20:16:13.931433: step 12000, loss = 0.1614, acc = 0.9490, f1neg = 0.9391, f1pos = 0.9561, f1 = 0.9476
[Eval_batch(15)(2000,32000)] 2017-05-02 20:16:14.396138: step 12000, loss = 0.1454, acc = 0.9570, f1neg = 0.9541, f1pos = 0.9595, f1 = 0.9568
[Eval_batch(16)(2000,34000)] 2017-05-02 20:16:14.858937: step 12000, loss = 0.1388, acc = 0.9560, f1neg = 0.9544, f1pos = 0.9575, f1 = 0.9559
[Eval_batch(17)(2000,36000)] 2017-05-02 20:16:15.306686: step 12000, loss = 0.1583, acc = 0.9485, f1neg = 0.9489, f1pos = 0.9481, f1 = 0.9485
[Eval_batch(18)(2000,38000)] 2017-05-02 20:16:15.765318: step 12000, loss = 0.1561, acc = 0.9455, f1neg = 0.9480, f1pos = 0.9428, f1 = 0.9454
[Eval_batch(19)(2000,40000)] 2017-05-02 20:16:16.214545: step 12000, loss = 0.1377, acc = 0.9630, f1neg = 0.9574, f1pos = 0.9673, f1 = 0.9624
[Eval_batch(20)(2000,42000)] 2017-05-02 20:16:16.673921: step 12000, loss = 0.1384, acc = 0.9570, f1neg = 0.9621, f1pos = 0.9502, f1 = 0.9562
[Eval_batch(21)(2000,44000)] 2017-05-02 20:16:17.122485: step 12000, loss = 0.1349, acc = 0.9560, f1neg = 0.9525, f1pos = 0.9590, f1 = 0.9558
[Eval_batch(22)(2000,46000)] 2017-05-02 20:16:17.583666: step 12000, loss = 0.1401, acc = 0.9545, f1neg = 0.9552, f1pos = 0.9538, f1 = 0.9545
[Eval_batch(23)(2000,48000)] 2017-05-02 20:16:18.056759: step 12000, loss = 0.1484, acc = 0.9520, f1neg = 0.9461, f1pos = 0.9567, f1 = 0.9514
[Eval_batch(24)(2000,50000)] 2017-05-02 20:16:18.521501: step 12000, loss = 0.1372, acc = 0.9550, f1neg = 0.9488, f1pos = 0.9599, f1 = 0.9543
[Eval_batch(25)(2000,52000)] 2017-05-02 20:16:18.969747: step 12000, loss = 0.1270, acc = 0.9640, f1neg = 0.9607, f1pos = 0.9668, f1 = 0.9637
[Eval_batch(26)(2000,54000)] 2017-05-02 20:16:19.444323: step 12000, loss = 0.1354, acc = 0.9560, f1neg = 0.9587, f1pos = 0.9529, f1 = 0.9558
[Eval_batch(27)(2000,56000)] 2017-05-02 20:16:20.042659: step 12000, loss = 0.1240, acc = 0.9635, f1neg = 0.9618, f1pos = 0.9651, f1 = 0.9634
[Eval] 2017-05-02 20:16:20.042761: step 12000, acc = 0.9542, f1 = 0.9538
[Test_batch(0)(2000,2000)] 2017-05-02 20:16:20.555959: step 12000, loss = 0.1739, acc = 0.9495, f1neg = 0.9523, f1pos = 0.9463, f1 = 0.9493
[Test_batch(1)(2000,4000)] 2017-05-02 20:16:21.065444: step 12000, loss = 0.1608, acc = 0.9460, f1neg = 0.9525, f1pos = 0.9375, f1 = 0.9450
[Test_batch(2)(2000,6000)] 2017-05-02 20:16:21.544096: step 12000, loss = 0.1691, acc = 0.9500, f1neg = 0.9545, f1pos = 0.9446, f1 = 0.9495
[Test_batch(3)(2000,8000)] 2017-05-02 20:16:22.021701: step 12000, loss = 0.1800, acc = 0.9345, f1neg = 0.9393, f1pos = 0.9289, f1 = 0.9341
[Test_batch(4)(2000,10000)] 2017-05-02 20:16:22.501914: step 12000, loss = 0.1844, acc = 0.9405, f1neg = 0.9410, f1pos = 0.9400, f1 = 0.9405
[Test_batch(5)(2000,12000)] 2017-05-02 20:16:23.011382: step 12000, loss = 0.1917, acc = 0.9345, f1neg = 0.9357, f1pos = 0.9333, f1 = 0.9345
[Test_batch(6)(2000,14000)] 2017-05-02 20:16:23.481735: step 12000, loss = 0.1757, acc = 0.9435, f1neg = 0.9420, f1pos = 0.9450, f1 = 0.9435
[Test_batch(7)(2000,16000)] 2017-05-02 20:16:23.956620: step 12000, loss = 0.1657, acc = 0.9465, f1neg = 0.9521, f1pos = 0.9394, f1 = 0.9458
[Test_batch(8)(2000,18000)] 2017-05-02 20:16:24.405874: step 12000, loss = 0.1654, acc = 0.9495, f1neg = 0.9494, f1pos = 0.9496, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-02 20:16:24.896539: step 12000, loss = 0.1989, acc = 0.9245, f1neg = 0.9215, f1pos = 0.9273, f1 = 0.9244
[Test_batch(10)(2000,22000)] 2017-05-02 20:16:25.375485: step 12000, loss = 0.1761, acc = 0.9425, f1neg = 0.9361, f1pos = 0.9477, f1 = 0.9419
[Test_batch(11)(2000,24000)] 2017-05-02 20:16:25.874584: step 12000, loss = 0.1713, acc = 0.9460, f1neg = 0.9478, f1pos = 0.9440, f1 = 0.9459
[Test_batch(12)(2000,26000)] 2017-05-02 20:16:26.315321: step 12000, loss = 0.1534, acc = 0.9490, f1neg = 0.9513, f1pos = 0.9464, f1 = 0.9489
[Test_batch(13)(2000,28000)] 2017-05-02 20:16:26.755460: step 12000, loss = 0.1829, acc = 0.9355, f1neg = 0.9289, f1pos = 0.9410, f1 = 0.9349
[Test_batch(14)(2000,30000)] 2017-05-02 20:16:27.262930: step 12000, loss = 0.1535, acc = 0.9525, f1neg = 0.9463, f1pos = 0.9574, f1 = 0.9519
[Test_batch(15)(2000,32000)] 2017-05-02 20:16:27.739337: step 12000, loss = 0.1700, acc = 0.9445, f1neg = 0.9442, f1pos = 0.9447, f1 = 0.9445
[Test_batch(16)(2000,34000)] 2017-05-02 20:16:28.212807: step 12000, loss = 0.1555, acc = 0.9515, f1neg = 0.9508, f1pos = 0.9522, f1 = 0.9515
[Test_batch(17)(2000,36000)] 2017-05-02 20:16:28.690906: step 12000, loss = 0.1450, acc = 0.9555, f1neg = 0.9517, f1pos = 0.9587, f1 = 0.9552
[Test_batch(18)(2000,38000)] 2017-05-02 20:16:29.241981: step 12000, loss = 0.1330, acc = 0.9630, f1neg = 0.9624, f1pos = 0.9636, f1 = 0.9630
[Test] 2017-05-02 20:16:29.242069: step 12000, acc = 0.9452, f1 = 0.9449
[Status] 2017-05-02 20:16:29.242096: step 12000, maxindex = 12000, maxdev = 0.9542, maxtst = 0.9452
2017-05-02 20:16:41.570405: step 12010, loss = 0.1781, acc = 0.9460 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 20:16:50.879653: step 12020, loss = 0.1802, acc = 0.9340 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 20:17:00.050754: step 12030, loss = 0.1763, acc = 0.9380 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:17:09.284741: step 12040, loss = 0.2037, acc = 0.9160 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:17:18.538694: step 12050, loss = 0.1624, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 20:17:27.696763: step 12060, loss = 0.1673, acc = 0.9400 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:17:36.862935: step 12070, loss = 0.1780, acc = 0.9440 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 20:17:46.184502: step 12080, loss = 0.1755, acc = 0.9440 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 20:17:55.754984: step 12090, loss = 0.1557, acc = 0.9400 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 20:18:05.774566: step 12100, loss = 0.1738, acc = 0.9400 (268.9 examples/sec; 0.238 sec/batch)
2017-05-02 20:18:15.028922: step 12110, loss = 0.1647, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:18:24.452397: step 12120, loss = 0.1943, acc = 0.9300 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 20:18:33.590453: step 12130, loss = 0.1583, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-02 20:18:42.766223: step 12140, loss = 0.1566, acc = 0.9440 (286.9 examples/sec; 0.223 sec/batch)
2017-05-02 20:18:51.840422: step 12150, loss = 0.1843, acc = 0.9400 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 20:19:00.942975: step 12160, loss = 0.1473, acc = 0.9540 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 20:19:10.165791: step 12170, loss = 0.1948, acc = 0.9300 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 20:19:19.333421: step 12180, loss = 0.2093, acc = 0.9160 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 20:19:28.469551: step 12190, loss = 0.1593, acc = 0.9580 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 20:19:37.513639: step 12200, loss = 0.1701, acc = 0.9400 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 20:19:46.694083: step 12210, loss = 0.1800, acc = 0.9320 (283.8 examples/sec; 0.225 sec/batch)
2017-05-02 20:19:55.794222: step 12220, loss = 0.1308, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 20:20:04.955551: step 12230, loss = 0.1644, acc = 0.9420 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 20:20:14.165420: step 12240, loss = 0.1404, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 20:20:23.324718: step 12250, loss = 0.1850, acc = 0.9420 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 20:20:32.638158: step 12260, loss = 0.1838, acc = 0.9360 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 20:20:42.106143: step 12270, loss = 0.1782, acc = 0.9380 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 20:20:51.654992: step 12280, loss = 0.1589, acc = 0.9520 (257.6 examples/sec; 0.248 sec/batch)
2017-05-02 20:21:01.092240: step 12290, loss = 0.1601, acc = 0.9540 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 20:21:10.405635: step 12300, loss = 0.1683, acc = 0.9380 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 20:21:20.308796: step 12310, loss = 0.1802, acc = 0.9520 (250.2 examples/sec; 0.256 sec/batch)
2017-05-02 20:21:29.764485: step 12320, loss = 0.1641, acc = 0.9500 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 20:21:39.104293: step 12330, loss = 0.1567, acc = 0.9500 (284.6 examples/sec; 0.225 sec/batch)
2017-05-02 20:21:48.441855: step 12340, loss = 0.1287, acc = 0.9680 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 20:21:57.708183: step 12350, loss = 0.1557, acc = 0.9580 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 20:22:06.956312: step 12360, loss = 0.1600, acc = 0.9500 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 20:22:16.279546: step 12370, loss = 0.1560, acc = 0.9440 (288.2 examples/sec; 0.222 sec/batch)
2017-05-02 20:22:25.918783: step 12380, loss = 0.1669, acc = 0.9460 (250.8 examples/sec; 0.255 sec/batch)
2017-05-02 20:22:35.455228: step 12390, loss = 0.1634, acc = 0.9420 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 20:22:44.784927: step 12400, loss = 0.1424, acc = 0.9600 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 20:22:54.087760: step 12410, loss = 0.1731, acc = 0.9360 (264.6 examples/sec; 0.242 sec/batch)
2017-05-02 20:23:03.382211: step 12420, loss = 0.1688, acc = 0.9480 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 20:23:12.714293: step 12430, loss = 0.1573, acc = 0.9400 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 20:23:21.990797: step 12440, loss = 0.1505, acc = 0.9580 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 20:23:31.317351: step 12450, loss = 0.1575, acc = 0.9580 (264.9 examples/sec; 0.242 sec/batch)
2017-05-02 20:23:40.672068: step 12460, loss = 0.1563, acc = 0.9480 (266.7 examples/sec; 0.240 sec/batch)
2017-05-02 20:23:49.944234: step 12470, loss = 0.1562, acc = 0.9560 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 20:23:59.221649: step 12480, loss = 0.1770, acc = 0.9480 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 20:24:08.479036: step 12490, loss = 0.1940, acc = 0.9180 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 20:24:17.641264: step 12500, loss = 0.1496, acc = 0.9580 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 20:24:26.807784: step 12510, loss = 0.1467, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 20:24:35.973894: step 12520, loss = 0.1336, acc = 0.9600 (295.0 examples/sec; 0.217 sec/batch)
2017-05-02 20:24:45.087371: step 12530, loss = 0.1692, acc = 0.9460 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 20:24:54.321937: step 12540, loss = 0.1786, acc = 0.9300 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 20:25:03.511521: step 12550, loss = 0.1637, acc = 0.9460 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 20:25:12.746626: step 12560, loss = 0.1464, acc = 0.9500 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 20:25:21.961631: step 12570, loss = 0.1614, acc = 0.9400 (290.6 examples/sec; 0.220 sec/batch)
2017-05-02 20:25:31.179357: step 12580, loss = 0.1787, acc = 0.9400 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 20:25:40.416624: step 12590, loss = 0.1644, acc = 0.9540 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 20:25:49.692743: step 12600, loss = 0.1694, acc = 0.9400 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 20:25:58.986290: step 12610, loss = 0.1671, acc = 0.9400 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 20:26:08.201585: step 12620, loss = 0.1747, acc = 0.9340 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 20:26:17.451070: step 12630, loss = 0.1831, acc = 0.9380 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 20:26:26.506858: step 12640, loss = 0.1654, acc = 0.9540 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 20:26:35.627769: step 12650, loss = 0.1721, acc = 0.9420 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 20:26:44.858128: step 12660, loss = 0.1716, acc = 0.9440 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 20:26:54.134197: step 12670, loss = 0.1923, acc = 0.9280 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 20:27:03.237871: step 12680, loss = 0.1528, acc = 0.9500 (290.3 examples/sec; 0.220 sec/batch)
2017-05-02 20:27:12.534938: step 12690, loss = 0.1593, acc = 0.9500 (248.7 examples/sec; 0.257 sec/batch)
2017-05-02 20:27:21.914095: step 12700, loss = 0.1639, acc = 0.9540 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 20:27:31.173415: step 12710, loss = 0.1591, acc = 0.9500 (268.6 examples/sec; 0.238 sec/batch)
2017-05-02 20:27:40.234937: step 12720, loss = 0.1709, acc = 0.9540 (289.1 examples/sec; 0.221 sec/batch)
2017-05-02 20:27:49.362148: step 12730, loss = 0.1363, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 20:27:58.587620: step 12740, loss = 0.1910, acc = 0.9300 (262.9 examples/sec; 0.243 sec/batch)
2017-05-02 20:28:07.879441: step 12750, loss = 0.1859, acc = 0.9300 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 20:28:17.155742: step 12760, loss = 0.1741, acc = 0.9500 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 20:28:26.523108: step 12770, loss = 0.1742, acc = 0.9540 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 20:28:35.758514: step 12780, loss = 0.1743, acc = 0.9440 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 20:28:45.097233: step 12790, loss = 0.1494, acc = 0.9580 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 20:28:54.253371: step 12800, loss = 0.1500, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 20:29:03.411802: step 12810, loss = 0.1690, acc = 0.9420 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 20:29:12.607618: step 12820, loss = 0.1630, acc = 0.9460 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 20:29:21.828758: step 12830, loss = 0.1749, acc = 0.9340 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 20:29:31.012242: step 12840, loss = 0.1937, acc = 0.9480 (249.8 examples/sec; 0.256 sec/batch)
2017-05-02 20:29:40.281295: step 12850, loss = 0.1875, acc = 0.9320 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 20:29:49.661647: step 12860, loss = 0.1716, acc = 0.9400 (248.8 examples/sec; 0.257 sec/batch)
2017-05-02 20:29:58.750500: step 12870, loss = 0.1777, acc = 0.9380 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 20:30:08.004427: step 12880, loss = 0.1575, acc = 0.9520 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 20:30:17.249766: step 12890, loss = 0.1784, acc = 0.9320 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 20:30:26.489805: step 12900, loss = 0.1646, acc = 0.9460 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 20:30:35.800080: step 12910, loss = 0.1963, acc = 0.9280 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 20:30:45.038327: step 12920, loss = 0.1883, acc = 0.9240 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 20:30:54.366237: step 12930, loss = 0.2236, acc = 0.9200 (269.1 examples/sec; 0.238 sec/batch)
2017-05-02 20:31:03.839839: step 12940, loss = 0.1310, acc = 0.9640 (269.5 examples/sec; 0.237 sec/batch)
2017-05-02 20:31:13.101627: step 12950, loss = 0.1562, acc = 0.9480 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 20:31:22.383413: step 12960, loss = 0.1818, acc = 0.9320 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 20:31:31.571393: step 12970, loss = 0.1204, acc = 0.9720 (290.6 examples/sec; 0.220 sec/batch)
2017-05-02 20:31:40.777346: step 12980, loss = 0.1884, acc = 0.9380 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 20:31:50.295685: step 12990, loss = 0.1551, acc = 0.9460 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 20:31:59.715725: step 13000, loss = 0.1603, acc = 0.9420 (290.9 examples/sec; 0.220 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 20:32:00.218494: step 13000, loss = 0.1354, acc = 0.9560, f1neg = 0.9527, f1pos = 0.9589, f1 = 0.9558
[Eval_batch(1)(2000,4000)] 2017-05-02 20:32:00.680661: step 13000, loss = 0.1440, acc = 0.9570, f1neg = 0.9509, f1pos = 0.9618, f1 = 0.9563
[Eval_batch(2)(2000,6000)] 2017-05-02 20:32:01.139483: step 13000, loss = 0.1537, acc = 0.9490, f1neg = 0.9469, f1pos = 0.9510, f1 = 0.9489
[Eval_batch(3)(2000,8000)] 2017-05-02 20:32:01.608313: step 13000, loss = 0.1589, acc = 0.9475, f1neg = 0.9486, f1pos = 0.9463, f1 = 0.9475
[Eval_batch(4)(2000,10000)] 2017-05-02 20:32:02.118806: step 13000, loss = 0.1564, acc = 0.9480, f1neg = 0.9503, f1pos = 0.9454, f1 = 0.9479
[Eval_batch(5)(2000,12000)] 2017-05-02 20:32:02.574383: step 13000, loss = 0.1533, acc = 0.9450, f1neg = 0.9424, f1pos = 0.9474, f1 = 0.9449
[Eval_batch(6)(2000,14000)] 2017-05-02 20:32:03.050999: step 13000, loss = 0.1411, acc = 0.9585, f1neg = 0.9581, f1pos = 0.9588, f1 = 0.9585
[Eval_batch(7)(2000,16000)] 2017-05-02 20:32:03.511675: step 13000, loss = 0.1449, acc = 0.9520, f1neg = 0.9439, f1pos = 0.9580, f1 = 0.9510
[Eval_batch(8)(2000,18000)] 2017-05-02 20:32:03.987128: step 13000, loss = 0.1415, acc = 0.9550, f1neg = 0.9498, f1pos = 0.9592, f1 = 0.9545
[Eval_batch(9)(2000,20000)] 2017-05-02 20:32:04.424876: step 13000, loss = 0.1450, acc = 0.9535, f1neg = 0.9511, f1pos = 0.9557, f1 = 0.9534
[Eval_batch(10)(2000,22000)] 2017-05-02 20:32:04.892911: step 13000, loss = 0.1541, acc = 0.9520, f1neg = 0.9495, f1pos = 0.9542, f1 = 0.9519
[Eval_batch(11)(2000,24000)] 2017-05-02 20:32:05.365069: step 13000, loss = 0.1553, acc = 0.9490, f1neg = 0.9421, f1pos = 0.9544, f1 = 0.9483
[Eval_batch(12)(2000,26000)] 2017-05-02 20:32:05.867668: step 13000, loss = 0.1487, acc = 0.9475, f1neg = 0.9469, f1pos = 0.9481, f1 = 0.9475
[Eval_batch(13)(2000,28000)] 2017-05-02 20:32:06.332522: step 13000, loss = 0.1379, acc = 0.9590, f1neg = 0.9494, f1pos = 0.9655, f1 = 0.9575
[Eval_batch(14)(2000,30000)] 2017-05-02 20:32:06.822012: step 13000, loss = 0.1621, acc = 0.9465, f1neg = 0.9370, f1pos = 0.9535, f1 = 0.9453
[Eval_batch(15)(2000,32000)] 2017-05-02 20:32:07.325256: step 13000, loss = 0.1467, acc = 0.9570, f1neg = 0.9544, f1pos = 0.9593, f1 = 0.9569
[Eval_batch(16)(2000,34000)] 2017-05-02 20:32:07.795244: step 13000, loss = 0.1406, acc = 0.9560, f1neg = 0.9549, f1pos = 0.9570, f1 = 0.9560
[Eval_batch(17)(2000,36000)] 2017-05-02 20:32:08.257750: step 13000, loss = 0.1593, acc = 0.9510, f1neg = 0.9520, f1pos = 0.9500, f1 = 0.9510
[Eval_batch(18)(2000,38000)] 2017-05-02 20:32:08.722901: step 13000, loss = 0.1550, acc = 0.9475, f1neg = 0.9504, f1pos = 0.9442, f1 = 0.9473
[Eval_batch(19)(2000,40000)] 2017-05-02 20:32:09.225790: step 13000, loss = 0.1383, acc = 0.9600, f1neg = 0.9546, f1pos = 0.9642, f1 = 0.9594
[Eval_batch(20)(2000,42000)] 2017-05-02 20:32:09.698827: step 13000, loss = 0.1360, acc = 0.9595, f1neg = 0.9644, f1pos = 0.9530, f1 = 0.9587
[Eval_batch(21)(2000,44000)] 2017-05-02 20:32:10.197825: step 13000, loss = 0.1388, acc = 0.9540, f1neg = 0.9509, f1pos = 0.9568, f1 = 0.9538
[Eval_batch(22)(2000,46000)] 2017-05-02 20:32:10.673324: step 13000, loss = 0.1411, acc = 0.9530, f1neg = 0.9540, f1pos = 0.9519, f1 = 0.9530
[Eval_batch(23)(2000,48000)] 2017-05-02 20:32:11.147430: step 13000, loss = 0.1499, acc = 0.9495, f1neg = 0.9438, f1pos = 0.9542, f1 = 0.9490
[Eval_batch(24)(2000,50000)] 2017-05-02 20:32:11.610018: step 13000, loss = 0.1378, acc = 0.9530, f1neg = 0.9471, f1pos = 0.9577, f1 = 0.9524
[Eval_batch(25)(2000,52000)] 2017-05-02 20:32:12.118052: step 13000, loss = 0.1270, acc = 0.9635, f1neg = 0.9604, f1pos = 0.9661, f1 = 0.9633
[Eval_batch(26)(2000,54000)] 2017-05-02 20:32:12.614197: step 13000, loss = 0.1332, acc = 0.9580, f1neg = 0.9609, f1pos = 0.9546, f1 = 0.9578
[Eval_batch(27)(2000,56000)] 2017-05-02 20:32:13.201489: step 13000, loss = 0.1229, acc = 0.9630, f1neg = 0.9615, f1pos = 0.9644, f1 = 0.9629
[Eval] 2017-05-02 20:32:13.201606: step 13000, acc = 0.9536, f1 = 0.9532
[Test_batch(0)(2000,2000)] 2017-05-02 20:32:13.684316: step 13000, loss = 0.1746, acc = 0.9485, f1neg = 0.9521, f1pos = 0.9444, f1 = 0.9482
[Test_batch(1)(2000,4000)] 2017-05-02 20:32:14.163918: step 13000, loss = 0.1597, acc = 0.9480, f1neg = 0.9545, f1pos = 0.9393, f1 = 0.9469
[Test_batch(2)(2000,6000)] 2017-05-02 20:32:14.617065: step 13000, loss = 0.1691, acc = 0.9485, f1neg = 0.9535, f1pos = 0.9423, f1 = 0.9479
[Test_batch(3)(2000,8000)] 2017-05-02 20:32:15.104867: step 13000, loss = 0.1816, acc = 0.9325, f1neg = 0.9383, f1pos = 0.9255, f1 = 0.9319
[Test_batch(4)(2000,10000)] 2017-05-02 20:32:15.620939: step 13000, loss = 0.1871, acc = 0.9350, f1neg = 0.9365, f1pos = 0.9334, f1 = 0.9350
[Test_batch(5)(2000,12000)] 2017-05-02 20:32:16.126593: step 13000, loss = 0.1942, acc = 0.9315, f1neg = 0.9339, f1pos = 0.9289, f1 = 0.9314
[Test_batch(6)(2000,14000)] 2017-05-02 20:32:16.607248: step 13000, loss = 0.1755, acc = 0.9445, f1neg = 0.9436, f1pos = 0.9453, f1 = 0.9445
[Test_batch(7)(2000,16000)] 2017-05-02 20:32:17.079476: step 13000, loss = 0.1660, acc = 0.9445, f1neg = 0.9508, f1pos = 0.9363, f1 = 0.9436
[Test_batch(8)(2000,18000)] 2017-05-02 20:32:17.575449: step 13000, loss = 0.1672, acc = 0.9490, f1neg = 0.9493, f1pos = 0.9487, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-02 20:32:18.087938: step 13000, loss = 0.2029, acc = 0.9235, f1neg = 0.9214, f1pos = 0.9255, f1 = 0.9234
[Test_batch(10)(2000,22000)] 2017-05-02 20:32:18.596695: step 13000, loss = 0.1815, acc = 0.9375, f1neg = 0.9317, f1pos = 0.9424, f1 = 0.9370
[Test_batch(11)(2000,24000)] 2017-05-02 20:32:19.073315: step 13000, loss = 0.1722, acc = 0.9450, f1neg = 0.9475, f1pos = 0.9423, f1 = 0.9449
[Test_batch(12)(2000,26000)] 2017-05-02 20:32:19.560470: step 13000, loss = 0.1560, acc = 0.9500, f1neg = 0.9526, f1pos = 0.9471, f1 = 0.9498
[Test_batch(13)(2000,28000)] 2017-05-02 20:32:20.006382: step 13000, loss = 0.1866, acc = 0.9385, f1neg = 0.9335, f1pos = 0.9428, f1 = 0.9381
[Test_batch(14)(2000,30000)] 2017-05-02 20:32:20.512591: step 13000, loss = 0.1587, acc = 0.9495, f1neg = 0.9435, f1pos = 0.9544, f1 = 0.9489
[Test_batch(15)(2000,32000)] 2017-05-02 20:32:20.988450: step 13000, loss = 0.1728, acc = 0.9445, f1neg = 0.9450, f1pos = 0.9440, f1 = 0.9445
[Test_batch(16)(2000,34000)] 2017-05-02 20:32:21.464193: step 13000, loss = 0.1569, acc = 0.9535, f1neg = 0.9533, f1pos = 0.9537, f1 = 0.9535
[Test_batch(17)(2000,36000)] 2017-05-02 20:32:21.961000: step 13000, loss = 0.1456, acc = 0.9555, f1neg = 0.9520, f1pos = 0.9585, f1 = 0.9553
[Test_batch(18)(2000,38000)] 2017-05-02 20:32:22.537156: step 13000, loss = 0.1316, acc = 0.9630, f1neg = 0.9627, f1pos = 0.9633, f1 = 0.9630
[Test] 2017-05-02 20:32:22.537226: step 13000, acc = 0.9443, f1 = 0.9440
[Status] 2017-05-02 20:32:22.537241: step 13000, maxindex = 12000, maxdev = 0.9542, maxtst = 0.9452
2017-05-02 20:32:31.861496: step 13010, loss = 0.1688, acc = 0.9420 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 20:32:41.159976: step 13020, loss = 0.1780, acc = 0.9340 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 20:32:50.389581: step 13030, loss = 0.2166, acc = 0.9300 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 20:32:59.468859: step 13040, loss = 0.1525, acc = 0.9500 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 20:33:08.502802: step 13050, loss = 0.1372, acc = 0.9620 (289.1 examples/sec; 0.221 sec/batch)
2017-05-02 20:33:17.664778: step 13060, loss = 0.1653, acc = 0.9500 (288.2 examples/sec; 0.222 sec/batch)
2017-05-02 20:33:26.991240: step 13070, loss = 0.1632, acc = 0.9460 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 20:33:36.179335: step 13080, loss = 0.1448, acc = 0.9620 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 20:33:45.396407: step 13090, loss = 0.1593, acc = 0.9540 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 20:33:54.582746: step 13100, loss = 0.1696, acc = 0.9460 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:34:04.686734: step 13110, loss = 0.1481, acc = 0.9480 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 20:34:13.843036: step 13120, loss = 0.2172, acc = 0.9320 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 20:34:22.970944: step 13130, loss = 0.1519, acc = 0.9480 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 20:34:32.327844: step 13140, loss = 0.1800, acc = 0.9340 (286.1 examples/sec; 0.224 sec/batch)
2017-05-02 20:34:41.583515: step 13150, loss = 0.1608, acc = 0.9600 (288.7 examples/sec; 0.222 sec/batch)
2017-05-02 20:34:50.793497: step 13160, loss = 0.1817, acc = 0.9260 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 20:35:00.319407: step 13170, loss = 0.1710, acc = 0.9480 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 20:35:09.558785: step 13180, loss = 0.1479, acc = 0.9600 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 20:35:18.600331: step 13190, loss = 0.1424, acc = 0.9520 (286.1 examples/sec; 0.224 sec/batch)
2017-05-02 20:35:27.779539: step 13200, loss = 0.1726, acc = 0.9380 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 20:35:36.992747: step 13210, loss = 0.1677, acc = 0.9360 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 20:35:46.317642: step 13220, loss = 0.1596, acc = 0.9460 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 20:35:55.634900: step 13230, loss = 0.1672, acc = 0.9540 (261.9 examples/sec; 0.244 sec/batch)
2017-05-02 20:36:04.950509: step 13240, loss = 0.1906, acc = 0.9280 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 20:36:14.076378: step 13250, loss = 0.1447, acc = 0.9640 (294.0 examples/sec; 0.218 sec/batch)
2017-05-02 20:36:23.502602: step 13260, loss = 0.1909, acc = 0.9340 (262.3 examples/sec; 0.244 sec/batch)
2017-05-02 20:36:32.786684: step 13270, loss = 0.1544, acc = 0.9580 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 20:36:42.021704: step 13280, loss = 0.1388, acc = 0.9560 (270.1 examples/sec; 0.237 sec/batch)
2017-05-02 20:36:51.231583: step 13290, loss = 0.1350, acc = 0.9660 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 20:37:00.392589: step 13300, loss = 0.1728, acc = 0.9480 (291.0 examples/sec; 0.220 sec/batch)
2017-05-02 20:37:09.569769: step 13310, loss = 0.1778, acc = 0.9260 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 20:37:18.721433: step 13320, loss = 0.1598, acc = 0.9540 (292.7 examples/sec; 0.219 sec/batch)
2017-05-02 20:37:27.832005: step 13330, loss = 0.1491, acc = 0.9620 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 20:37:37.175042: step 13340, loss = 0.1622, acc = 0.9400 (274.1 examples/sec; 0.234 sec/batch)
2017-05-02 20:37:46.269857: step 13350, loss = 0.1487, acc = 0.9560 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 20:37:55.363806: step 13360, loss = 0.1666, acc = 0.9480 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 20:38:04.536383: step 13370, loss = 0.1626, acc = 0.9440 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 20:38:13.758884: step 13380, loss = 0.1353, acc = 0.9620 (261.9 examples/sec; 0.244 sec/batch)
2017-05-02 20:38:22.842047: step 13390, loss = 0.1475, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 20:38:32.040628: step 13400, loss = 0.1632, acc = 0.9540 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 20:38:41.342119: step 13410, loss = 0.1648, acc = 0.9400 (264.3 examples/sec; 0.242 sec/batch)
2017-05-02 20:38:50.729267: step 13420, loss = 0.1732, acc = 0.9440 (249.9 examples/sec; 0.256 sec/batch)
2017-05-02 20:39:00.020110: step 13430, loss = 0.1671, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 20:39:09.113520: step 13440, loss = 0.1573, acc = 0.9520 (291.1 examples/sec; 0.220 sec/batch)
2017-05-02 20:39:18.322693: step 13450, loss = 0.1604, acc = 0.9480 (272.1 examples/sec; 0.235 sec/batch)
2017-05-02 20:39:27.493160: step 13460, loss = 0.1512, acc = 0.9580 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 20:39:36.716411: step 13470, loss = 0.2153, acc = 0.9460 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 20:39:45.947111: step 13480, loss = 0.1708, acc = 0.9320 (267.4 examples/sec; 0.239 sec/batch)
2017-05-02 20:39:54.971153: step 13490, loss = 0.1771, acc = 0.9400 (269.0 examples/sec; 0.238 sec/batch)
2017-05-02 20:40:04.018338: step 13500, loss = 0.1565, acc = 0.9420 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 20:40:13.210265: step 13510, loss = 0.1682, acc = 0.9420 (289.2 examples/sec; 0.221 sec/batch)
2017-05-02 20:40:22.451477: step 13520, loss = 0.1488, acc = 0.9500 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 20:40:31.834917: step 13530, loss = 0.1643, acc = 0.9500 (259.0 examples/sec; 0.247 sec/batch)
2017-05-02 20:40:41.283240: step 13540, loss = 0.1767, acc = 0.9360 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 20:40:50.559920: step 13550, loss = 0.1412, acc = 0.9480 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 20:40:59.648141: step 13560, loss = 0.1913, acc = 0.9200 (283.8 examples/sec; 0.226 sec/batch)
2017-05-02 20:41:08.829329: step 13570, loss = 0.1625, acc = 0.9480 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 20:41:18.349116: step 13580, loss = 0.1633, acc = 0.9360 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 20:41:27.510902: step 13590, loss = 0.1996, acc = 0.9380 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 20:41:36.671931: step 13600, loss = 0.1586, acc = 0.9500 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 20:41:45.890416: step 13610, loss = 0.1744, acc = 0.9460 (261.5 examples/sec; 0.245 sec/batch)
2017-05-02 20:41:55.237721: step 13620, loss = 0.1668, acc = 0.9500 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 20:42:04.900074: step 13630, loss = 0.1629, acc = 0.9480 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 20:42:14.173648: step 13640, loss = 0.1484, acc = 0.9500 (259.8 examples/sec; 0.246 sec/batch)
2017-05-02 20:42:23.284696: step 13650, loss = 0.1456, acc = 0.9580 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 20:42:32.541774: step 13660, loss = 0.2021, acc = 0.9360 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 20:42:41.829854: step 13670, loss = 0.1570, acc = 0.9500 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 20:42:51.071688: step 13680, loss = 0.1715, acc = 0.9300 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 20:43:00.421119: step 13690, loss = 0.1748, acc = 0.9360 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 20:43:09.701849: step 13700, loss = 0.1774, acc = 0.9400 (288.0 examples/sec; 0.222 sec/batch)
2017-05-02 20:43:18.886013: step 13710, loss = 0.1562, acc = 0.9480 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 20:43:28.081988: step 13720, loss = 0.1526, acc = 0.9540 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 20:43:37.344787: step 13730, loss = 0.1699, acc = 0.9420 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 20:43:46.524139: step 13740, loss = 0.2100, acc = 0.9380 (264.8 examples/sec; 0.242 sec/batch)
2017-05-02 20:43:55.902541: step 13750, loss = 0.1644, acc = 0.9400 (280.1 examples/sec; 0.228 sec/batch)
2017-05-02 20:44:05.441635: step 13760, loss = 0.1909, acc = 0.9420 (224.0 examples/sec; 0.286 sec/batch)
2017-05-02 20:44:14.786586: step 13770, loss = 0.1510, acc = 0.9460 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 20:44:24.072258: step 13780, loss = 0.1924, acc = 0.9260 (263.9 examples/sec; 0.242 sec/batch)
2017-05-02 20:44:33.272125: step 13790, loss = 0.1729, acc = 0.9460 (268.5 examples/sec; 0.238 sec/batch)
2017-05-02 20:44:42.535075: step 13800, loss = 0.1879, acc = 0.9380 (283.8 examples/sec; 0.225 sec/batch)
2017-05-02 20:44:51.833779: step 13810, loss = 0.1486, acc = 0.9600 (290.7 examples/sec; 0.220 sec/batch)
2017-05-02 20:45:01.021512: step 13820, loss = 0.1458, acc = 0.9600 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 20:45:10.366716: step 13830, loss = 0.1748, acc = 0.9380 (255.1 examples/sec; 0.251 sec/batch)
2017-05-02 20:45:19.624759: step 13840, loss = 0.1827, acc = 0.9400 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 20:45:29.177492: step 13850, loss = 0.1741, acc = 0.9500 (273.5 examples/sec; 0.234 sec/batch)
2017-05-02 20:45:38.344054: step 13860, loss = 0.1447, acc = 0.9560 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 20:45:47.505312: step 13870, loss = 0.2023, acc = 0.9260 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 20:45:56.621681: step 13880, loss = 0.1515, acc = 0.9440 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 20:46:05.779253: step 13890, loss = 0.1847, acc = 0.9420 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 20:46:14.960995: step 13900, loss = 0.1748, acc = 0.9340 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 20:46:24.153191: step 13910, loss = 0.1698, acc = 0.9360 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 20:46:33.304596: step 13920, loss = 0.1692, acc = 0.9420 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 20:46:42.444063: step 13930, loss = 0.1600, acc = 0.9460 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 20:46:51.679744: step 13940, loss = 0.1618, acc = 0.9420 (282.8 examples/sec; 0.226 sec/batch)
2017-05-02 20:47:01.166090: step 13950, loss = 0.1485, acc = 0.9560 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 20:47:10.405910: step 13960, loss = 0.1836, acc = 0.9420 (269.8 examples/sec; 0.237 sec/batch)
2017-05-02 20:47:19.630412: step 13970, loss = 0.1316, acc = 0.9660 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 20:47:28.909989: step 13980, loss = 0.1779, acc = 0.9340 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 20:47:38.274405: step 13990, loss = 0.1647, acc = 0.9460 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 20:47:47.471266: step 14000, loss = 0.1615, acc = 0.9360 (278.5 examples/sec; 0.230 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 20:47:47.955705: step 14000, loss = 0.1325, acc = 0.9540, f1neg = 0.9497, f1pos = 0.9576, f1 = 0.9537
[Eval_batch(1)(2000,4000)] 2017-05-02 20:47:48.414613: step 14000, loss = 0.1355, acc = 0.9595, f1neg = 0.9532, f1pos = 0.9643, f1 = 0.9588
[Eval_batch(2)(2000,6000)] 2017-05-02 20:47:48.883787: step 14000, loss = 0.1483, acc = 0.9515, f1neg = 0.9487, f1pos = 0.9541, f1 = 0.9514
[Eval_batch(3)(2000,8000)] 2017-05-02 20:47:49.355662: step 14000, loss = 0.1551, acc = 0.9490, f1neg = 0.9494, f1pos = 0.9486, f1 = 0.9490
[Eval_batch(4)(2000,10000)] 2017-05-02 20:47:49.834083: step 14000, loss = 0.1541, acc = 0.9475, f1neg = 0.9491, f1pos = 0.9458, f1 = 0.9475
[Eval_batch(5)(2000,12000)] 2017-05-02 20:47:50.304729: step 14000, loss = 0.1487, acc = 0.9485, f1neg = 0.9451, f1pos = 0.9515, f1 = 0.9483
[Eval_batch(6)(2000,14000)] 2017-05-02 20:47:50.771028: step 14000, loss = 0.1395, acc = 0.9610, f1neg = 0.9604, f1pos = 0.9616, f1 = 0.9610
[Eval_batch(7)(2000,16000)] 2017-05-02 20:47:51.247117: step 14000, loss = 0.1397, acc = 0.9550, f1neg = 0.9466, f1pos = 0.9611, f1 = 0.9539
[Eval_batch(8)(2000,18000)] 2017-05-02 20:47:51.751129: step 14000, loss = 0.1364, acc = 0.9545, f1neg = 0.9484, f1pos = 0.9593, f1 = 0.9539
[Eval_batch(9)(2000,20000)] 2017-05-02 20:47:52.252706: step 14000, loss = 0.1382, acc = 0.9595, f1neg = 0.9569, f1pos = 0.9618, f1 = 0.9594
[Eval_batch(10)(2000,22000)] 2017-05-02 20:47:52.762441: step 14000, loss = 0.1491, acc = 0.9525, f1neg = 0.9493, f1pos = 0.9553, f1 = 0.9523
[Eval_batch(11)(2000,24000)] 2017-05-02 20:47:53.266788: step 14000, loss = 0.1490, acc = 0.9535, f1neg = 0.9466, f1pos = 0.9588, f1 = 0.9527
[Eval_batch(12)(2000,26000)] 2017-05-02 20:47:53.763915: step 14000, loss = 0.1449, acc = 0.9525, f1neg = 0.9513, f1pos = 0.9537, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-02 20:47:54.210017: step 14000, loss = 0.1322, acc = 0.9600, f1neg = 0.9501, f1pos = 0.9666, f1 = 0.9584
[Eval_batch(14)(2000,30000)] 2017-05-02 20:47:54.721090: step 14000, loss = 0.1578, acc = 0.9490, f1neg = 0.9389, f1pos = 0.9562, f1 = 0.9476
[Eval_batch(15)(2000,32000)] 2017-05-02 20:47:55.218935: step 14000, loss = 0.1404, acc = 0.9590, f1neg = 0.9562, f1pos = 0.9615, f1 = 0.9588
[Eval_batch(16)(2000,34000)] 2017-05-02 20:47:55.724943: step 14000, loss = 0.1351, acc = 0.9570, f1neg = 0.9553, f1pos = 0.9586, f1 = 0.9569
[Eval_batch(17)(2000,36000)] 2017-05-02 20:47:56.237576: step 14000, loss = 0.1552, acc = 0.9510, f1neg = 0.9514, f1pos = 0.9506, f1 = 0.9510
[Eval_batch(18)(2000,38000)] 2017-05-02 20:47:56.720875: step 14000, loss = 0.1523, acc = 0.9485, f1neg = 0.9507, f1pos = 0.9460, f1 = 0.9484
[Eval_batch(19)(2000,40000)] 2017-05-02 20:47:57.200880: step 14000, loss = 0.1343, acc = 0.9635, f1neg = 0.9579, f1pos = 0.9678, f1 = 0.9628
[Eval_batch(20)(2000,42000)] 2017-05-02 20:47:57.674249: step 14000, loss = 0.1361, acc = 0.9585, f1neg = 0.9634, f1pos = 0.9522, f1 = 0.9578
[Eval_batch(21)(2000,44000)] 2017-05-02 20:47:58.153600: step 14000, loss = 0.1306, acc = 0.9565, f1neg = 0.9529, f1pos = 0.9596, f1 = 0.9562
[Eval_batch(22)(2000,46000)] 2017-05-02 20:47:58.620976: step 14000, loss = 0.1368, acc = 0.9550, f1neg = 0.9556, f1pos = 0.9544, f1 = 0.9550
[Eval_batch(23)(2000,48000)] 2017-05-02 20:47:59.089535: step 14000, loss = 0.1442, acc = 0.9535, f1neg = 0.9477, f1pos = 0.9581, f1 = 0.9529
[Eval_batch(24)(2000,50000)] 2017-05-02 20:47:59.591672: step 14000, loss = 0.1327, acc = 0.9580, f1neg = 0.9521, f1pos = 0.9626, f1 = 0.9573
[Eval_batch(25)(2000,52000)] 2017-05-02 20:48:00.096450: step 14000, loss = 0.1226, acc = 0.9640, f1neg = 0.9606, f1pos = 0.9669, f1 = 0.9637
[Eval_batch(26)(2000,54000)] 2017-05-02 20:48:00.595338: step 14000, loss = 0.1318, acc = 0.9540, f1neg = 0.9566, f1pos = 0.9510, f1 = 0.9538
[Eval_batch(27)(2000,56000)] 2017-05-02 20:48:01.250085: step 14000, loss = 0.1207, acc = 0.9640, f1neg = 0.9623, f1pos = 0.9656, f1 = 0.9639
[Eval] 2017-05-02 20:48:01.250172: step 14000, acc = 0.9554, f1 = 0.9550
[Test_batch(0)(2000,2000)] 2017-05-02 20:48:01.760789: step 14000, loss = 0.1708, acc = 0.9505, f1neg = 0.9531, f1pos = 0.9476, f1 = 0.9504
[Test_batch(1)(2000,4000)] 2017-05-02 20:48:02.262102: step 14000, loss = 0.1568, acc = 0.9490, f1neg = 0.9549, f1pos = 0.9412, f1 = 0.9481
[Test_batch(2)(2000,6000)] 2017-05-02 20:48:02.771829: step 14000, loss = 0.1666, acc = 0.9505, f1neg = 0.9547, f1pos = 0.9454, f1 = 0.9501
[Test_batch(3)(2000,8000)] 2017-05-02 20:48:03.277479: step 14000, loss = 0.1762, acc = 0.9380, f1neg = 0.9423, f1pos = 0.9330, f1 = 0.9377
[Test_batch(4)(2000,10000)] 2017-05-02 20:48:03.777222: step 14000, loss = 0.1791, acc = 0.9390, f1neg = 0.9394, f1pos = 0.9386, f1 = 0.9390
[Test_batch(5)(2000,12000)] 2017-05-02 20:48:04.253818: step 14000, loss = 0.1881, acc = 0.9335, f1neg = 0.9346, f1pos = 0.9323, f1 = 0.9335
[Test_batch(6)(2000,14000)] 2017-05-02 20:48:04.757235: step 14000, loss = 0.1716, acc = 0.9435, f1neg = 0.9418, f1pos = 0.9451, f1 = 0.9435
[Test_batch(7)(2000,16000)] 2017-05-02 20:48:05.231410: step 14000, loss = 0.1610, acc = 0.9465, f1neg = 0.9518, f1pos = 0.9399, f1 = 0.9459
[Test_batch(8)(2000,18000)] 2017-05-02 20:48:05.731469: step 14000, loss = 0.1614, acc = 0.9490, f1neg = 0.9487, f1pos = 0.9493, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-02 20:48:06.214646: step 14000, loss = 0.1940, acc = 0.9290, f1neg = 0.9256, f1pos = 0.9321, f1 = 0.9288
[Test_batch(10)(2000,22000)] 2017-05-02 20:48:06.716777: step 14000, loss = 0.1709, acc = 0.9460, f1neg = 0.9397, f1pos = 0.9511, f1 = 0.9454
[Test_batch(11)(2000,24000)] 2017-05-02 20:48:07.170188: step 14000, loss = 0.1676, acc = 0.9455, f1neg = 0.9472, f1pos = 0.9437, f1 = 0.9454
[Test_batch(12)(2000,26000)] 2017-05-02 20:48:07.637714: step 14000, loss = 0.1482, acc = 0.9545, f1neg = 0.9564, f1pos = 0.9525, f1 = 0.9544
[Test_batch(13)(2000,28000)] 2017-05-02 20:48:08.146179: step 14000, loss = 0.1775, acc = 0.9365, f1neg = 0.9296, f1pos = 0.9422, f1 = 0.9359
[Test_batch(14)(2000,30000)] 2017-05-02 20:48:08.625629: step 14000, loss = 0.1478, acc = 0.9560, f1neg = 0.9502, f1pos = 0.9606, f1 = 0.9554
[Test_batch(15)(2000,32000)] 2017-05-02 20:48:09.109824: step 14000, loss = 0.1658, acc = 0.9460, f1neg = 0.9457, f1pos = 0.9463, f1 = 0.9460
[Test_batch(16)(2000,34000)] 2017-05-02 20:48:09.587039: step 14000, loss = 0.1505, acc = 0.9535, f1neg = 0.9527, f1pos = 0.9543, f1 = 0.9535
[Test_batch(17)(2000,36000)] 2017-05-02 20:48:10.067286: step 14000, loss = 0.1413, acc = 0.9580, f1neg = 0.9543, f1pos = 0.9611, f1 = 0.9577
[Test_batch(18)(2000,38000)] 2017-05-02 20:48:10.665943: step 14000, loss = 0.1303, acc = 0.9615, f1neg = 0.9608, f1pos = 0.9622, f1 = 0.9615
[Test] 2017-05-02 20:48:10.666025: step 14000, acc = 0.9466, f1 = 0.9464
[Status] 2017-05-02 20:48:10.666049: step 14000, maxindex = 14000, maxdev = 0.9554, maxtst = 0.9466
2017-05-02 20:48:23.163887: step 14010, loss = 0.1683, acc = 0.9360 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 20:48:32.430675: step 14020, loss = 0.1387, acc = 0.9580 (266.3 examples/sec; 0.240 sec/batch)
2017-05-02 20:48:41.725615: step 14030, loss = 0.1676, acc = 0.9500 (269.0 examples/sec; 0.238 sec/batch)
2017-05-02 20:48:51.014472: step 14040, loss = 0.1549, acc = 0.9560 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 20:49:00.473413: step 14050, loss = 0.1534, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 20:49:09.814798: step 14060, loss = 0.1550, acc = 0.9520 (258.4 examples/sec; 0.248 sec/batch)
2017-05-02 20:49:19.451090: step 14070, loss = 0.1900, acc = 0.9300 (267.8 examples/sec; 0.239 sec/batch)
2017-05-02 20:49:28.737139: step 14080, loss = 0.1567, acc = 0.9520 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:49:37.703343: step 14090, loss = 0.1649, acc = 0.9380 (292.1 examples/sec; 0.219 sec/batch)
2017-05-02 20:49:46.851795: step 14100, loss = 0.1748, acc = 0.9360 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 20:49:56.174858: step 14110, loss = 0.1596, acc = 0.9400 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 20:50:06.413802: step 14120, loss = 0.1529, acc = 0.9560 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 20:50:15.571284: step 14130, loss = 0.1847, acc = 0.9320 (301.3 examples/sec; 0.212 sec/batch)
2017-05-02 20:50:24.917576: step 14140, loss = 0.2003, acc = 0.9340 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 20:50:34.563881: step 14150, loss = 0.1590, acc = 0.9560 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 20:50:44.001588: step 14160, loss = 0.1705, acc = 0.9480 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 20:50:53.285750: step 14170, loss = 0.1616, acc = 0.9380 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 20:51:02.595549: step 14180, loss = 0.1709, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 20:51:11.844524: step 14190, loss = 0.1953, acc = 0.9400 (271.7 examples/sec; 0.236 sec/batch)
2017-05-02 20:51:21.092272: step 14200, loss = 0.1587, acc = 0.9440 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 20:51:30.281099: step 14210, loss = 0.1673, acc = 0.9360 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 20:51:39.542551: step 14220, loss = 0.1692, acc = 0.9460 (272.1 examples/sec; 0.235 sec/batch)
2017-05-02 20:51:48.688953: step 14230, loss = 0.1689, acc = 0.9320 (287.1 examples/sec; 0.223 sec/batch)
2017-05-02 20:51:57.847026: step 14240, loss = 0.1898, acc = 0.9340 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:52:07.008685: step 14250, loss = 0.1866, acc = 0.9400 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 20:52:16.120175: step 14260, loss = 0.1673, acc = 0.9420 (282.8 examples/sec; 0.226 sec/batch)
2017-05-02 20:52:25.242446: step 14270, loss = 0.1545, acc = 0.9460 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 20:52:34.500590: step 14280, loss = 0.1675, acc = 0.9480 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 20:52:43.688447: step 14290, loss = 0.1565, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 20:52:52.966794: step 14300, loss = 0.1468, acc = 0.9440 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 20:53:02.222953: step 14310, loss = 0.1633, acc = 0.9420 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 20:53:11.490498: step 14320, loss = 0.1722, acc = 0.9420 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 20:53:20.803759: step 14330, loss = 0.1484, acc = 0.9580 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 20:53:29.950827: step 14340, loss = 0.1674, acc = 0.9500 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 20:53:39.383966: step 14350, loss = 0.1605, acc = 0.9480 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 20:53:48.698129: step 14360, loss = 0.1425, acc = 0.9420 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 20:53:57.994195: step 14370, loss = 0.2056, acc = 0.9120 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 20:54:07.278640: step 14380, loss = 0.1390, acc = 0.9580 (273.5 examples/sec; 0.234 sec/batch)
2017-05-02 20:54:16.405878: step 14390, loss = 0.1620, acc = 0.9460 (286.4 examples/sec; 0.223 sec/batch)
2017-05-02 20:54:25.548179: step 14400, loss = 0.1599, acc = 0.9360 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 20:54:34.823755: step 14410, loss = 0.1638, acc = 0.9360 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:54:44.188032: step 14420, loss = 0.1558, acc = 0.9520 (279.9 examples/sec; 0.229 sec/batch)
2017-05-02 20:54:53.460921: step 14430, loss = 0.1749, acc = 0.9400 (263.5 examples/sec; 0.243 sec/batch)
2017-05-02 20:55:02.731876: step 14440, loss = 0.1704, acc = 0.9520 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 20:55:12.029102: step 14450, loss = 0.1575, acc = 0.9540 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 20:55:21.410822: step 14460, loss = 0.1836, acc = 0.9340 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 20:55:30.850103: step 14470, loss = 0.1599, acc = 0.9380 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 20:55:40.090295: step 14480, loss = 0.1893, acc = 0.9300 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 20:55:49.369950: step 14490, loss = 0.1679, acc = 0.9520 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 20:55:58.635766: step 14500, loss = 0.1605, acc = 0.9520 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 20:56:07.835518: step 14510, loss = 0.1415, acc = 0.9600 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 20:56:17.204599: step 14520, loss = 0.1597, acc = 0.9560 (260.2 examples/sec; 0.246 sec/batch)
2017-05-02 20:56:26.423072: step 14530, loss = 0.1788, acc = 0.9380 (262.4 examples/sec; 0.244 sec/batch)
2017-05-02 20:56:35.635647: step 14540, loss = 0.1419, acc = 0.9500 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 20:56:45.023850: step 14550, loss = 0.1544, acc = 0.9500 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 20:56:54.259871: step 14560, loss = 0.1864, acc = 0.9380 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 20:57:03.353191: step 14570, loss = 0.1718, acc = 0.9420 (287.6 examples/sec; 0.222 sec/batch)
2017-05-02 20:57:12.531492: step 14580, loss = 0.1980, acc = 0.9400 (287.8 examples/sec; 0.222 sec/batch)
2017-05-02 20:57:21.777271: step 14590, loss = 0.1560, acc = 0.9540 (272.1 examples/sec; 0.235 sec/batch)
2017-05-02 20:57:31.026425: step 14600, loss = 0.1262, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 20:57:40.281534: step 14610, loss = 0.1697, acc = 0.9380 (257.8 examples/sec; 0.248 sec/batch)
2017-05-02 20:57:49.507122: step 14620, loss = 0.1876, acc = 0.9380 (267.9 examples/sec; 0.239 sec/batch)
2017-05-02 20:57:58.623594: step 14630, loss = 0.1480, acc = 0.9460 (289.1 examples/sec; 0.221 sec/batch)
2017-05-02 20:58:07.869054: step 14640, loss = 0.1383, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 20:58:17.062798: step 14650, loss = 0.1945, acc = 0.9440 (266.7 examples/sec; 0.240 sec/batch)
2017-05-02 20:58:26.503218: step 14660, loss = 0.1254, acc = 0.9700 (265.1 examples/sec; 0.241 sec/batch)
2017-05-02 20:58:35.600738: step 14670, loss = 0.1654, acc = 0.9380 (298.5 examples/sec; 0.214 sec/batch)
2017-05-02 20:58:44.796907: step 14680, loss = 0.1730, acc = 0.9420 (276.4 examples/sec; 0.232 sec/batch)
2017-05-02 20:58:53.906277: step 14690, loss = 0.1546, acc = 0.9520 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 20:59:03.024534: step 14700, loss = 0.1535, acc = 0.9480 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 20:59:12.351657: step 14710, loss = 0.1262, acc = 0.9760 (284.6 examples/sec; 0.225 sec/batch)
2017-05-02 20:59:21.665245: step 14720, loss = 0.1356, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 20:59:30.827031: step 14730, loss = 0.1594, acc = 0.9520 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 20:59:39.953666: step 14740, loss = 0.1710, acc = 0.9400 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 20:59:49.145902: step 14750, loss = 0.1514, acc = 0.9460 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 20:59:58.453614: step 14760, loss = 0.1505, acc = 0.9480 (275.1 examples/sec; 0.233 sec/batch)
2017-05-02 21:00:07.627207: step 14770, loss = 0.1269, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 21:00:16.817061: step 14780, loss = 0.1964, acc = 0.9300 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 21:00:26.123878: step 14790, loss = 0.1550, acc = 0.9520 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 21:00:35.353796: step 14800, loss = 0.1788, acc = 0.9440 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 21:00:44.461669: step 14810, loss = 0.1447, acc = 0.9580 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 21:00:53.817854: step 14820, loss = 0.1659, acc = 0.9500 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 21:01:03.151712: step 14830, loss = 0.1909, acc = 0.9340 (274.4 examples/sec; 0.233 sec/batch)
2017-05-02 21:01:12.457278: step 14840, loss = 0.1669, acc = 0.9360 (259.0 examples/sec; 0.247 sec/batch)
2017-05-02 21:01:21.599765: step 14850, loss = 0.1732, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 21:01:30.714060: step 14860, loss = 0.1563, acc = 0.9540 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 21:01:39.841523: step 14870, loss = 0.1681, acc = 0.9360 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 21:01:49.076003: step 14880, loss = 0.1819, acc = 0.9420 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 21:01:58.344878: step 14890, loss = 0.1567, acc = 0.9500 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 21:02:07.637316: step 14900, loss = 0.1510, acc = 0.9440 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 21:02:16.763477: step 14910, loss = 0.1559, acc = 0.9460 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 21:02:25.885552: step 14920, loss = 0.1656, acc = 0.9460 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 21:02:35.281776: step 14930, loss = 0.1388, acc = 0.9600 (262.0 examples/sec; 0.244 sec/batch)
2017-05-02 21:02:44.465738: step 14940, loss = 0.1636, acc = 0.9440 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 21:02:53.712699: step 14950, loss = 0.1549, acc = 0.9460 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 21:03:03.393983: step 14960, loss = 0.1958, acc = 0.9320 (294.2 examples/sec; 0.218 sec/batch)
2017-05-02 21:03:12.844290: step 14970, loss = 0.1714, acc = 0.9460 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 21:03:22.205504: step 14980, loss = 0.1574, acc = 0.9460 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 21:03:31.456018: step 14990, loss = 0.1653, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 21:03:40.743091: step 15000, loss = 0.1695, acc = 0.9360 (282.6 examples/sec; 0.227 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 21:03:41.258773: step 15000, loss = 0.1317, acc = 0.9530, f1neg = 0.9486, f1pos = 0.9567, f1 = 0.9526
[Eval_batch(1)(2000,4000)] 2017-05-02 21:03:41.765158: step 15000, loss = 0.1339, acc = 0.9590, f1neg = 0.9523, f1pos = 0.9640, f1 = 0.9582
[Eval_batch(2)(2000,6000)] 2017-05-02 21:03:42.227613: step 15000, loss = 0.1470, acc = 0.9540, f1neg = 0.9511, f1pos = 0.9566, f1 = 0.9538
[Eval_batch(3)(2000,8000)] 2017-05-02 21:03:42.710947: step 15000, loss = 0.1531, acc = 0.9500, f1neg = 0.9501, f1pos = 0.9498, f1 = 0.9500
[Eval_batch(4)(2000,10000)] 2017-05-02 21:03:43.228346: step 15000, loss = 0.1528, acc = 0.9465, f1neg = 0.9479, f1pos = 0.9450, f1 = 0.9465
[Eval_batch(5)(2000,12000)] 2017-05-02 21:03:43.707107: step 15000, loss = 0.1475, acc = 0.9495, f1neg = 0.9458, f1pos = 0.9527, f1 = 0.9493
[Eval_batch(6)(2000,14000)] 2017-05-02 21:03:44.171332: step 15000, loss = 0.1395, acc = 0.9605, f1neg = 0.9598, f1pos = 0.9612, f1 = 0.9605
[Eval_batch(7)(2000,16000)] 2017-05-02 21:03:44.678831: step 15000, loss = 0.1368, acc = 0.9550, f1neg = 0.9464, f1pos = 0.9612, f1 = 0.9538
[Eval_batch(8)(2000,18000)] 2017-05-02 21:03:45.179985: step 15000, loss = 0.1351, acc = 0.9560, f1neg = 0.9500, f1pos = 0.9607, f1 = 0.9554
[Eval_batch(9)(2000,20000)] 2017-05-02 21:03:45.695439: step 15000, loss = 0.1366, acc = 0.9605, f1neg = 0.9578, f1pos = 0.9629, f1 = 0.9603
[Eval_batch(10)(2000,22000)] 2017-05-02 21:03:46.197274: step 15000, loss = 0.1478, acc = 0.9510, f1neg = 0.9475, f1pos = 0.9540, f1 = 0.9508
[Eval_batch(11)(2000,24000)] 2017-05-02 21:03:46.693163: step 15000, loss = 0.1474, acc = 0.9550, f1neg = 0.9482, f1pos = 0.9602, f1 = 0.9542
[Eval_batch(12)(2000,26000)] 2017-05-02 21:03:47.177195: step 15000, loss = 0.1433, acc = 0.9530, f1neg = 0.9517, f1pos = 0.9542, f1 = 0.9530
[Eval_batch(13)(2000,28000)] 2017-05-02 21:03:47.689393: step 15000, loss = 0.1306, acc = 0.9600, f1neg = 0.9500, f1pos = 0.9667, f1 = 0.9583
[Eval_batch(14)(2000,30000)] 2017-05-02 21:03:48.157459: step 15000, loss = 0.1570, acc = 0.9495, f1neg = 0.9393, f1pos = 0.9568, f1 = 0.9480
[Eval_batch(15)(2000,32000)] 2017-05-02 21:03:48.633171: step 15000, loss = 0.1387, acc = 0.9605, f1neg = 0.9575, f1pos = 0.9631, f1 = 0.9603
[Eval_batch(16)(2000,34000)] 2017-05-02 21:03:49.127181: step 15000, loss = 0.1343, acc = 0.9565, f1neg = 0.9546, f1pos = 0.9582, f1 = 0.9564
[Eval_batch(17)(2000,36000)] 2017-05-02 21:03:49.632838: step 15000, loss = 0.1547, acc = 0.9505, f1neg = 0.9508, f1pos = 0.9502, f1 = 0.9505
[Eval_batch(18)(2000,38000)] 2017-05-02 21:03:50.146248: step 15000, loss = 0.1524, acc = 0.9500, f1neg = 0.9519, f1pos = 0.9479, f1 = 0.9499
[Eval_batch(19)(2000,40000)] 2017-05-02 21:03:50.650469: step 15000, loss = 0.1329, acc = 0.9635, f1neg = 0.9578, f1pos = 0.9678, f1 = 0.9628
[Eval_batch(20)(2000,42000)] 2017-05-02 21:03:51.126167: step 15000, loss = 0.1354, acc = 0.9590, f1neg = 0.9638, f1pos = 0.9528, f1 = 0.9583
[Eval_batch(21)(2000,44000)] 2017-05-02 21:03:51.629733: step 15000, loss = 0.1286, acc = 0.9590, f1neg = 0.9554, f1pos = 0.9621, f1 = 0.9587
[Eval_batch(22)(2000,46000)] 2017-05-02 21:03:52.132284: step 15000, loss = 0.1358, acc = 0.9565, f1neg = 0.9570, f1pos = 0.9560, f1 = 0.9565
[Eval_batch(23)(2000,48000)] 2017-05-02 21:03:52.623704: step 15000, loss = 0.1428, acc = 0.9535, f1neg = 0.9475, f1pos = 0.9583, f1 = 0.9529
[Eval_batch(24)(2000,50000)] 2017-05-02 21:03:53.096217: step 15000, loss = 0.1306, acc = 0.9600, f1neg = 0.9541, f1pos = 0.9645, f1 = 0.9593
[Eval_batch(25)(2000,52000)] 2017-05-02 21:03:53.576353: step 15000, loss = 0.1213, acc = 0.9635, f1neg = 0.9600, f1pos = 0.9665, f1 = 0.9632
[Eval_batch(26)(2000,54000)] 2017-05-02 21:03:54.035192: step 15000, loss = 0.1310, acc = 0.9540, f1neg = 0.9565, f1pos = 0.9512, f1 = 0.9538
[Eval_batch(27)(2000,56000)] 2017-05-02 21:03:54.657790: step 15000, loss = 0.1204, acc = 0.9640, f1neg = 0.9623, f1pos = 0.9656, f1 = 0.9639
[Eval] 2017-05-02 21:03:54.657878: step 15000, acc = 0.9558, f1 = 0.9554
[Test_batch(0)(2000,2000)] 2017-05-02 21:03:55.155642: step 15000, loss = 0.1681, acc = 0.9505, f1neg = 0.9530, f1pos = 0.9477, f1 = 0.9504
[Test_batch(1)(2000,4000)] 2017-05-02 21:03:55.671430: step 15000, loss = 0.1550, acc = 0.9480, f1neg = 0.9539, f1pos = 0.9403, f1 = 0.9471
[Test_batch(2)(2000,6000)] 2017-05-02 21:03:56.179533: step 15000, loss = 0.1654, acc = 0.9490, f1neg = 0.9531, f1pos = 0.9441, f1 = 0.9486
[Test_batch(3)(2000,8000)] 2017-05-02 21:03:56.692079: step 15000, loss = 0.1753, acc = 0.9380, f1neg = 0.9421, f1pos = 0.9333, f1 = 0.9377
[Test_batch(4)(2000,10000)] 2017-05-02 21:03:57.200856: step 15000, loss = 0.1777, acc = 0.9370, f1neg = 0.9371, f1pos = 0.9369, f1 = 0.9370
[Test_batch(5)(2000,12000)] 2017-05-02 21:03:57.704043: step 15000, loss = 0.1860, acc = 0.9350, f1neg = 0.9357, f1pos = 0.9343, f1 = 0.9350
[Test_batch(6)(2000,14000)] 2017-05-02 21:03:58.210503: step 15000, loss = 0.1703, acc = 0.9420, f1neg = 0.9401, f1pos = 0.9438, f1 = 0.9419
[Test_batch(7)(2000,16000)] 2017-05-02 21:03:58.721584: step 15000, loss = 0.1598, acc = 0.9455, f1neg = 0.9507, f1pos = 0.9391, f1 = 0.9449
[Test_batch(8)(2000,18000)] 2017-05-02 21:03:59.238029: step 15000, loss = 0.1601, acc = 0.9480, f1neg = 0.9476, f1pos = 0.9484, f1 = 0.9480
[Test_batch(9)(2000,20000)] 2017-05-02 21:03:59.755023: step 15000, loss = 0.1924, acc = 0.9295, f1neg = 0.9259, f1pos = 0.9328, f1 = 0.9293
[Test_batch(10)(2000,22000)] 2017-05-02 21:04:00.230502: step 15000, loss = 0.1683, acc = 0.9440, f1neg = 0.9372, f1pos = 0.9495, f1 = 0.9433
[Test_batch(11)(2000,24000)] 2017-05-02 21:04:00.675756: step 15000, loss = 0.1661, acc = 0.9455, f1neg = 0.9471, f1pos = 0.9438, f1 = 0.9455
[Test_batch(12)(2000,26000)] 2017-05-02 21:04:01.141286: step 15000, loss = 0.1456, acc = 0.9550, f1neg = 0.9567, f1pos = 0.9531, f1 = 0.9549
[Test_batch(13)(2000,28000)] 2017-05-02 21:04:01.615681: step 15000, loss = 0.1753, acc = 0.9370, f1neg = 0.9301, f1pos = 0.9427, f1 = 0.9364
[Test_batch(14)(2000,30000)] 2017-05-02 21:04:02.136556: step 15000, loss = 0.1459, acc = 0.9535, f1neg = 0.9471, f1pos = 0.9585, f1 = 0.9528
[Test_batch(15)(2000,32000)] 2017-05-02 21:04:02.595841: step 15000, loss = 0.1643, acc = 0.9450, f1neg = 0.9444, f1pos = 0.9455, f1 = 0.9450
[Test_batch(16)(2000,34000)] 2017-05-02 21:04:03.082921: step 15000, loss = 0.1490, acc = 0.9525, f1neg = 0.9516, f1pos = 0.9534, f1 = 0.9525
[Test_batch(17)(2000,36000)] 2017-05-02 21:04:03.593796: step 15000, loss = 0.1401, acc = 0.9585, f1neg = 0.9548, f1pos = 0.9616, f1 = 0.9582
[Test_batch(18)(2000,38000)] 2017-05-02 21:04:04.112310: step 15000, loss = 0.1296, acc = 0.9610, f1neg = 0.9602, f1pos = 0.9618, f1 = 0.9610
[Test] 2017-05-02 21:04:04.112400: step 15000, acc = 0.9460, f1 = 0.9458
[Status] 2017-05-02 21:04:04.112426: step 15000, maxindex = 15000, maxdev = 0.9558, maxtst = 0.9460
2017-05-02 21:04:16.550818: step 15010, loss = 0.1586, acc = 0.9400 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 21:04:25.845633: step 15020, loss = 0.1540, acc = 0.9520 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 21:04:35.033086: step 15030, loss = 0.1702, acc = 0.9540 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 21:04:44.275317: step 15040, loss = 0.1851, acc = 0.9340 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 21:04:53.424309: step 15050, loss = 0.1932, acc = 0.9300 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 21:05:02.492484: step 15060, loss = 0.1414, acc = 0.9620 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 21:05:11.721905: step 15070, loss = 0.1549, acc = 0.9560 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 21:05:20.892852: step 15080, loss = 0.1612, acc = 0.9540 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 21:05:30.101364: step 15090, loss = 0.1729, acc = 0.9580 (274.1 examples/sec; 0.234 sec/batch)
2017-05-02 21:05:39.324095: step 15100, loss = 0.1588, acc = 0.9520 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 21:05:48.514756: step 15110, loss = 0.1405, acc = 0.9520 (274.1 examples/sec; 0.233 sec/batch)
2017-05-02 21:05:58.682274: step 15120, loss = 0.1873, acc = 0.9300 (259.2 examples/sec; 0.247 sec/batch)
2017-05-02 21:06:07.878944: step 15130, loss = 0.1623, acc = 0.9420 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 21:06:17.203736: step 15140, loss = 0.1561, acc = 0.9460 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 21:06:26.409577: step 15150, loss = 0.1878, acc = 0.9360 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 21:06:35.588318: step 15160, loss = 0.1285, acc = 0.9740 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 21:06:44.750406: step 15170, loss = 0.1660, acc = 0.9400 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 21:06:53.846196: step 15180, loss = 0.1927, acc = 0.9340 (289.2 examples/sec; 0.221 sec/batch)
2017-05-02 21:07:03.048036: step 15190, loss = 0.1626, acc = 0.9500 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 21:07:12.317929: step 15200, loss = 0.1495, acc = 0.9500 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 21:07:21.381024: step 15210, loss = 0.1953, acc = 0.9360 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 21:07:30.605360: step 15220, loss = 0.1680, acc = 0.9540 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 21:07:39.831529: step 15230, loss = 0.1534, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 21:07:49.005706: step 15240, loss = 0.1783, acc = 0.9400 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 21:07:58.139629: step 15250, loss = 0.1525, acc = 0.9560 (269.5 examples/sec; 0.237 sec/batch)
2017-05-02 21:08:07.247260: step 15260, loss = 0.1569, acc = 0.9460 (306.0 examples/sec; 0.209 sec/batch)
2017-05-02 21:08:16.534581: step 15270, loss = 0.1527, acc = 0.9500 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 21:08:26.221178: step 15280, loss = 0.1730, acc = 0.9520 (253.4 examples/sec; 0.253 sec/batch)
2017-05-02 21:08:35.413587: step 15290, loss = 0.1462, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 21:08:44.882737: step 15300, loss = 0.1622, acc = 0.9460 (263.7 examples/sec; 0.243 sec/batch)
2017-05-02 21:08:54.073706: step 15310, loss = 0.1704, acc = 0.9400 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 21:09:03.421470: step 15320, loss = 0.1811, acc = 0.9320 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 21:09:12.801717: step 15330, loss = 0.1420, acc = 0.9560 (274.3 examples/sec; 0.233 sec/batch)
2017-05-02 21:09:22.123780: step 15340, loss = 0.1349, acc = 0.9640 (293.1 examples/sec; 0.218 sec/batch)
2017-05-02 21:09:31.251051: step 15350, loss = 0.1759, acc = 0.9420 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 21:09:40.484781: step 15360, loss = 0.1550, acc = 0.9520 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 21:09:49.721528: step 15370, loss = 0.1547, acc = 0.9520 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 21:09:58.892385: step 15380, loss = 0.1539, acc = 0.9540 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 21:10:08.098703: step 15390, loss = 0.1746, acc = 0.9400 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 21:10:17.411800: step 15400, loss = 0.1717, acc = 0.9520 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 21:10:26.607670: step 15410, loss = 0.1619, acc = 0.9480 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 21:10:35.836180: step 15420, loss = 0.1550, acc = 0.9600 (265.9 examples/sec; 0.241 sec/batch)
2017-05-02 21:10:45.067902: step 15430, loss = 0.1604, acc = 0.9500 (259.4 examples/sec; 0.247 sec/batch)
2017-05-02 21:10:54.412489: step 15440, loss = 0.1565, acc = 0.9520 (265.2 examples/sec; 0.241 sec/batch)
2017-05-02 21:11:03.558378: step 15450, loss = 0.1960, acc = 0.9280 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 21:11:12.655572: step 15460, loss = 0.1352, acc = 0.9640 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 21:11:21.783822: step 15470, loss = 0.1598, acc = 0.9460 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 21:11:31.260547: step 15480, loss = 0.1527, acc = 0.9600 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 21:11:40.368958: step 15490, loss = 0.1483, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 21:11:49.594836: step 15500, loss = 0.1744, acc = 0.9540 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 21:11:59.082062: step 15510, loss = 0.1544, acc = 0.9380 (274.1 examples/sec; 0.233 sec/batch)
2017-05-02 21:12:08.280522: step 15520, loss = 0.1934, acc = 0.9380 (258.7 examples/sec; 0.247 sec/batch)
2017-05-02 21:12:17.537499: step 15530, loss = 0.2011, acc = 0.9160 (265.7 examples/sec; 0.241 sec/batch)
2017-05-02 21:12:26.761078: step 15540, loss = 0.1543, acc = 0.9580 (270.7 examples/sec; 0.236 sec/batch)
2017-05-02 21:12:35.857424: step 15550, loss = 0.1630, acc = 0.9460 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 21:12:45.063314: step 15560, loss = 0.1977, acc = 0.9240 (288.8 examples/sec; 0.222 sec/batch)
2017-05-02 21:12:54.068032: step 15570, loss = 0.1902, acc = 0.9400 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 21:13:03.201190: step 15580, loss = 0.1604, acc = 0.9500 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 21:13:12.475323: step 15590, loss = 0.1554, acc = 0.9500 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 21:13:21.637030: step 15600, loss = 0.1565, acc = 0.9560 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 21:13:30.624048: step 15610, loss = 0.1315, acc = 0.9660 (293.9 examples/sec; 0.218 sec/batch)
2017-05-02 21:13:39.989703: step 15620, loss = 0.1346, acc = 0.9480 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 21:13:49.537708: step 15630, loss = 0.1418, acc = 0.9520 (286.4 examples/sec; 0.223 sec/batch)
2017-05-02 21:13:59.053798: step 15640, loss = 0.1736, acc = 0.9280 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 21:14:08.481057: step 15650, loss = 0.1458, acc = 0.9640 (270.2 examples/sec; 0.237 sec/batch)
2017-05-02 21:14:17.792986: step 15660, loss = 0.1527, acc = 0.9460 (271.2 examples/sec; 0.236 sec/batch)
2017-05-02 21:14:27.049510: step 15670, loss = 0.1340, acc = 0.9660 (285.0 examples/sec; 0.225 sec/batch)
2017-05-02 21:14:36.284176: step 15680, loss = 0.1519, acc = 0.9480 (285.0 examples/sec; 0.225 sec/batch)
2017-05-02 21:14:45.480948: step 15690, loss = 0.1490, acc = 0.9520 (287.6 examples/sec; 0.223 sec/batch)
2017-05-02 21:14:54.793413: step 15700, loss = 0.1575, acc = 0.9540 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 21:15:04.110216: step 15710, loss = 0.1575, acc = 0.9460 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 21:15:13.401543: step 15720, loss = 0.1631, acc = 0.9360 (265.3 examples/sec; 0.241 sec/batch)
2017-05-02 21:15:22.756987: step 15730, loss = 0.1741, acc = 0.9480 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 21:15:31.955871: step 15740, loss = 0.1429, acc = 0.9520 (261.9 examples/sec; 0.244 sec/batch)
2017-05-02 21:15:41.102707: step 15750, loss = 0.1700, acc = 0.9340 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 21:15:50.366505: step 15760, loss = 0.1951, acc = 0.9240 (281.3 examples/sec; 0.227 sec/batch)
2017-05-02 21:15:59.577860: step 15770, loss = 0.1704, acc = 0.9260 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 21:16:08.786119: step 15780, loss = 0.1497, acc = 0.9540 (262.5 examples/sec; 0.244 sec/batch)
2017-05-02 21:16:17.845430: step 15790, loss = 0.1528, acc = 0.9500 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 21:16:27.017520: step 15800, loss = 0.1643, acc = 0.9440 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 21:16:36.238871: step 15810, loss = 0.1278, acc = 0.9580 (290.9 examples/sec; 0.220 sec/batch)
2017-05-02 21:16:45.460770: step 15820, loss = 0.1616, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 21:16:54.697744: step 15830, loss = 0.1692, acc = 0.9480 (269.3 examples/sec; 0.238 sec/batch)
2017-05-02 21:17:04.031432: step 15840, loss = 0.1742, acc = 0.9380 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 21:17:13.446625: step 15850, loss = 0.1600, acc = 0.9600 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 21:17:22.838897: step 15860, loss = 0.1506, acc = 0.9560 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 21:17:32.427024: step 15870, loss = 0.1452, acc = 0.9540 (266.1 examples/sec; 0.241 sec/batch)
2017-05-02 21:17:41.676313: step 15880, loss = 0.1671, acc = 0.9460 (290.7 examples/sec; 0.220 sec/batch)
2017-05-02 21:17:50.927621: step 15890, loss = 0.1409, acc = 0.9500 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 21:18:00.148908: step 15900, loss = 0.1604, acc = 0.9520 (279.9 examples/sec; 0.229 sec/batch)
2017-05-02 21:18:09.673797: step 15910, loss = 0.1581, acc = 0.9540 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 21:18:18.864624: step 15920, loss = 0.1632, acc = 0.9480 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 21:18:28.171311: step 15930, loss = 0.1623, acc = 0.9480 (266.8 examples/sec; 0.240 sec/batch)
2017-05-02 21:18:37.528072: step 15940, loss = 0.1623, acc = 0.9440 (268.7 examples/sec; 0.238 sec/batch)
2017-05-02 21:18:46.994639: step 15950, loss = 0.1353, acc = 0.9540 (259.2 examples/sec; 0.247 sec/batch)
2017-05-02 21:18:56.326077: step 15960, loss = 0.1425, acc = 0.9620 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 21:19:05.412229: step 15970, loss = 0.1963, acc = 0.9440 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 21:19:14.563799: step 15980, loss = 0.1550, acc = 0.9500 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 21:19:23.795148: step 15990, loss = 0.1381, acc = 0.9560 (262.2 examples/sec; 0.244 sec/batch)
2017-05-02 21:19:33.136882: step 16000, loss = 0.1444, acc = 0.9560 (276.1 examples/sec; 0.232 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 21:19:33.648158: step 16000, loss = 0.1298, acc = 0.9545, f1neg = 0.9505, f1pos = 0.9579, f1 = 0.9542
[Eval_batch(1)(2000,4000)] 2017-05-02 21:19:34.150576: step 16000, loss = 0.1337, acc = 0.9590, f1neg = 0.9527, f1pos = 0.9638, f1 = 0.9583
[Eval_batch(2)(2000,6000)] 2017-05-02 21:19:34.657333: step 16000, loss = 0.1450, acc = 0.9515, f1neg = 0.9488, f1pos = 0.9540, f1 = 0.9514
[Eval_batch(3)(2000,8000)] 2017-05-02 21:19:35.165995: step 16000, loss = 0.1511, acc = 0.9495, f1neg = 0.9498, f1pos = 0.9492, f1 = 0.9495
[Eval_batch(4)(2000,10000)] 2017-05-02 21:19:35.670011: step 16000, loss = 0.1499, acc = 0.9490, f1neg = 0.9506, f1pos = 0.9473, f1 = 0.9489
[Eval_batch(5)(2000,12000)] 2017-05-02 21:19:36.174566: step 16000, loss = 0.1460, acc = 0.9490, f1neg = 0.9457, f1pos = 0.9519, f1 = 0.9488
[Eval_batch(6)(2000,14000)] 2017-05-02 21:19:36.685342: step 16000, loss = 0.1359, acc = 0.9605, f1neg = 0.9599, f1pos = 0.9611, f1 = 0.9605
[Eval_batch(7)(2000,16000)] 2017-05-02 21:19:37.190080: step 16000, loss = 0.1363, acc = 0.9540, f1neg = 0.9456, f1pos = 0.9602, f1 = 0.9529
[Eval_batch(8)(2000,18000)] 2017-05-02 21:19:37.696768: step 16000, loss = 0.1329, acc = 0.9555, f1neg = 0.9496, f1pos = 0.9601, f1 = 0.9549
[Eval_batch(9)(2000,20000)] 2017-05-02 21:19:38.202692: step 16000, loss = 0.1353, acc = 0.9600, f1neg = 0.9575, f1pos = 0.9622, f1 = 0.9599
[Eval_batch(10)(2000,22000)] 2017-05-02 21:19:38.690372: step 16000, loss = 0.1461, acc = 0.9510, f1neg = 0.9478, f1pos = 0.9538, f1 = 0.9508
[Eval_batch(11)(2000,24000)] 2017-05-02 21:19:39.193239: step 16000, loss = 0.1461, acc = 0.9545, f1neg = 0.9479, f1pos = 0.9596, f1 = 0.9538
[Eval_batch(12)(2000,26000)] 2017-05-02 21:19:39.698299: step 16000, loss = 0.1421, acc = 0.9515, f1neg = 0.9505, f1pos = 0.9524, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-02 21:19:40.206631: step 16000, loss = 0.1293, acc = 0.9615, f1neg = 0.9520, f1pos = 0.9678, f1 = 0.9599
[Eval_batch(14)(2000,30000)] 2017-05-02 21:19:40.718632: step 16000, loss = 0.1549, acc = 0.9510, f1neg = 0.9415, f1pos = 0.9578, f1 = 0.9497
[Eval_batch(15)(2000,32000)] 2017-05-02 21:19:41.227895: step 16000, loss = 0.1382, acc = 0.9590, f1neg = 0.9562, f1pos = 0.9615, f1 = 0.9588
[Eval_batch(16)(2000,34000)] 2017-05-02 21:19:41.734630: step 16000, loss = 0.1329, acc = 0.9585, f1neg = 0.9569, f1pos = 0.9600, f1 = 0.9584
[Eval_batch(17)(2000,36000)] 2017-05-02 21:19:42.234884: step 16000, loss = 0.1522, acc = 0.9515, f1neg = 0.9520, f1pos = 0.9510, f1 = 0.9515
[Eval_batch(18)(2000,38000)] 2017-05-02 21:19:42.747759: step 16000, loss = 0.1494, acc = 0.9500, f1neg = 0.9522, f1pos = 0.9476, f1 = 0.9499
[Eval_batch(19)(2000,40000)] 2017-05-02 21:19:43.257835: step 16000, loss = 0.1312, acc = 0.9655, f1neg = 0.9603, f1pos = 0.9695, f1 = 0.9649
[Eval_batch(20)(2000,42000)] 2017-05-02 21:19:43.777301: step 16000, loss = 0.1317, acc = 0.9585, f1neg = 0.9634, f1pos = 0.9521, f1 = 0.9577
[Eval_batch(21)(2000,44000)] 2017-05-02 21:19:44.277160: step 16000, loss = 0.1284, acc = 0.9560, f1neg = 0.9524, f1pos = 0.9591, f1 = 0.9557
[Eval_batch(22)(2000,46000)] 2017-05-02 21:19:44.778686: step 16000, loss = 0.1337, acc = 0.9560, f1neg = 0.9567, f1pos = 0.9553, f1 = 0.9560
[Eval_batch(23)(2000,48000)] 2017-05-02 21:19:45.284866: step 16000, loss = 0.1414, acc = 0.9520, f1neg = 0.9461, f1pos = 0.9567, f1 = 0.9514
[Eval_batch(24)(2000,50000)] 2017-05-02 21:19:45.803655: step 16000, loss = 0.1298, acc = 0.9575, f1neg = 0.9517, f1pos = 0.9621, f1 = 0.9569
[Eval_batch(25)(2000,52000)] 2017-05-02 21:19:46.312157: step 16000, loss = 0.1196, acc = 0.9640, f1neg = 0.9607, f1pos = 0.9668, f1 = 0.9637
[Eval_batch(26)(2000,54000)] 2017-05-02 21:19:46.823131: step 16000, loss = 0.1284, acc = 0.9570, f1neg = 0.9595, f1pos = 0.9541, f1 = 0.9568
[Eval_batch(27)(2000,56000)] 2017-05-02 21:19:47.484249: step 16000, loss = 0.1175, acc = 0.9655, f1neg = 0.9639, f1pos = 0.9669, f1 = 0.9654
[Eval] 2017-05-02 21:19:47.484338: step 16000, acc = 0.9558, f1 = 0.9554
[Test_batch(0)(2000,2000)] 2017-05-02 21:19:47.991992: step 16000, loss = 0.1673, acc = 0.9510, f1neg = 0.9537, f1pos = 0.9480, f1 = 0.9508
[Test_batch(1)(2000,4000)] 2017-05-02 21:19:48.499095: step 16000, loss = 0.1521, acc = 0.9515, f1neg = 0.9572, f1pos = 0.9440, f1 = 0.9506
[Test_batch(2)(2000,6000)] 2017-05-02 21:19:49.003886: step 16000, loss = 0.1634, acc = 0.9505, f1neg = 0.9549, f1pos = 0.9452, f1 = 0.9500
[Test_batch(3)(2000,8000)] 2017-05-02 21:19:49.506448: step 16000, loss = 0.1734, acc = 0.9385, f1neg = 0.9430, f1pos = 0.9333, f1 = 0.9381
[Test_batch(4)(2000,10000)] 2017-05-02 21:19:50.018386: step 16000, loss = 0.1757, acc = 0.9420, f1neg = 0.9425, f1pos = 0.9415, f1 = 0.9420
[Test_batch(5)(2000,12000)] 2017-05-02 21:19:50.529138: step 16000, loss = 0.1841, acc = 0.9380, f1neg = 0.9392, f1pos = 0.9367, f1 = 0.9380
[Test_batch(6)(2000,14000)] 2017-05-02 21:19:51.035264: step 16000, loss = 0.1686, acc = 0.9420, f1neg = 0.9402, f1pos = 0.9437, f1 = 0.9419
[Test_batch(7)(2000,16000)] 2017-05-02 21:19:51.531358: step 16000, loss = 0.1577, acc = 0.9480, f1neg = 0.9532, f1pos = 0.9414, f1 = 0.9473
[Test_batch(8)(2000,18000)] 2017-05-02 21:19:52.043295: step 16000, loss = 0.1588, acc = 0.9475, f1neg = 0.9473, f1pos = 0.9477, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-02 21:19:52.545719: step 16000, loss = 0.1912, acc = 0.9310, f1neg = 0.9279, f1pos = 0.9338, f1 = 0.9309
[Test_batch(10)(2000,22000)] 2017-05-02 21:19:53.062127: step 16000, loss = 0.1683, acc = 0.9465, f1neg = 0.9404, f1pos = 0.9515, f1 = 0.9459
[Test_batch(11)(2000,24000)] 2017-05-02 21:19:53.567482: step 16000, loss = 0.1652, acc = 0.9470, f1neg = 0.9488, f1pos = 0.9451, f1 = 0.9469
[Test_batch(12)(2000,26000)] 2017-05-02 21:19:54.069008: step 16000, loss = 0.1461, acc = 0.9545, f1neg = 0.9565, f1pos = 0.9523, f1 = 0.9544
[Test_batch(13)(2000,28000)] 2017-05-02 21:19:54.576272: step 16000, loss = 0.1745, acc = 0.9385, f1neg = 0.9321, f1pos = 0.9438, f1 = 0.9379
[Test_batch(14)(2000,30000)] 2017-05-02 21:19:55.081971: step 16000, loss = 0.1453, acc = 0.9545, f1neg = 0.9485, f1pos = 0.9592, f1 = 0.9539
[Test_batch(15)(2000,32000)] 2017-05-02 21:19:55.585194: step 16000, loss = 0.1637, acc = 0.9475, f1neg = 0.9472, f1pos = 0.9478, f1 = 0.9475
[Test_batch(16)(2000,34000)] 2017-05-02 21:19:56.090446: step 16000, loss = 0.1480, acc = 0.9530, f1neg = 0.9522, f1pos = 0.9537, f1 = 0.9530
[Test_batch(17)(2000,36000)] 2017-05-02 21:19:56.586923: step 16000, loss = 0.1377, acc = 0.9590, f1neg = 0.9554, f1pos = 0.9620, f1 = 0.9587
[Test_batch(18)(2000,38000)] 2017-05-02 21:19:57.172903: step 16000, loss = 0.1268, acc = 0.9635, f1neg = 0.9629, f1pos = 0.9641, f1 = 0.9635
[Test] 2017-05-02 21:19:57.172989: step 16000, acc = 0.9476, f1 = 0.9473
[Status] 2017-05-02 21:19:57.173015: step 16000, maxindex = 16000, maxdev = 0.9558, maxtst = 0.9476
2017-05-02 21:20:09.684746: step 16010, loss = 0.1474, acc = 0.9560 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 21:20:18.824107: step 16020, loss = 0.1675, acc = 0.9360 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 21:20:28.004599: step 16030, loss = 0.1797, acc = 0.9420 (271.2 examples/sec; 0.236 sec/batch)
2017-05-02 21:20:37.021298: step 16040, loss = 0.1387, acc = 0.9580 (272.9 examples/sec; 0.234 sec/batch)
2017-05-02 21:20:46.404052: step 16050, loss = 0.1749, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 21:20:55.841198: step 16060, loss = 0.1481, acc = 0.9500 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 21:21:05.038203: step 16070, loss = 0.1470, acc = 0.9460 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 21:21:14.118243: step 16080, loss = 0.1555, acc = 0.9500 (280.1 examples/sec; 0.228 sec/batch)
2017-05-02 21:21:23.286085: step 16090, loss = 0.1459, acc = 0.9540 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 21:21:32.692455: step 16100, loss = 0.1687, acc = 0.9440 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 21:21:41.730876: step 16110, loss = 0.1716, acc = 0.9320 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 21:21:50.777953: step 16120, loss = 0.1434, acc = 0.9600 (293.2 examples/sec; 0.218 sec/batch)
2017-05-02 21:22:00.587801: step 16130, loss = 0.1174, acc = 0.9740 (288.4 examples/sec; 0.222 sec/batch)
2017-05-02 21:22:09.796758: step 16140, loss = 0.1458, acc = 0.9580 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 21:22:18.939529: step 16150, loss = 0.1492, acc = 0.9520 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 21:22:28.005071: step 16160, loss = 0.1263, acc = 0.9640 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 21:22:37.449090: step 16170, loss = 0.1736, acc = 0.9360 (283.8 examples/sec; 0.226 sec/batch)
2017-05-02 21:22:46.770531: step 16180, loss = 0.1680, acc = 0.9380 (291.9 examples/sec; 0.219 sec/batch)
2017-05-02 21:22:56.032503: step 16190, loss = 0.1646, acc = 0.9480 (279.9 examples/sec; 0.229 sec/batch)
2017-05-02 21:23:05.141870: step 16200, loss = 0.1698, acc = 0.9380 (263.2 examples/sec; 0.243 sec/batch)
2017-05-02 21:23:14.330238: step 16210, loss = 0.1726, acc = 0.9440 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 21:23:23.460162: step 16220, loss = 0.1569, acc = 0.9520 (297.5 examples/sec; 0.215 sec/batch)
2017-05-02 21:23:32.736390: step 16230, loss = 0.1288, acc = 0.9660 (268.1 examples/sec; 0.239 sec/batch)
2017-05-02 21:23:41.966068: step 16240, loss = 0.1476, acc = 0.9580 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 21:23:51.377374: step 16250, loss = 0.1629, acc = 0.9480 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 21:24:00.599542: step 16260, loss = 0.1762, acc = 0.9500 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 21:24:09.651549: step 16270, loss = 0.1503, acc = 0.9400 (293.0 examples/sec; 0.218 sec/batch)
2017-05-02 21:24:18.877962: step 16280, loss = 0.1716, acc = 0.9340 (286.9 examples/sec; 0.223 sec/batch)
2017-05-02 21:24:28.168605: step 16290, loss = 0.1936, acc = 0.9240 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 21:24:37.623242: step 16300, loss = 0.1685, acc = 0.9440 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 21:24:46.817152: step 16310, loss = 0.1633, acc = 0.9520 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 21:24:55.982750: step 16320, loss = 0.1567, acc = 0.9480 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 21:25:05.347050: step 16330, loss = 0.1501, acc = 0.9520 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 21:25:14.473020: step 16340, loss = 0.1544, acc = 0.9460 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 21:25:23.792609: step 16350, loss = 0.1682, acc = 0.9420 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 21:25:33.065308: step 16360, loss = 0.1711, acc = 0.9380 (293.8 examples/sec; 0.218 sec/batch)
2017-05-02 21:25:42.286494: step 16370, loss = 0.1597, acc = 0.9460 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 21:25:51.431292: step 16380, loss = 0.1452, acc = 0.9540 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 21:26:00.793476: step 16390, loss = 0.1440, acc = 0.9580 (257.3 examples/sec; 0.249 sec/batch)
2017-05-02 21:26:10.063369: step 16400, loss = 0.1659, acc = 0.9460 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 21:26:19.306951: step 16410, loss = 0.1314, acc = 0.9480 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 21:26:28.474323: step 16420, loss = 0.1631, acc = 0.9420 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 21:26:37.697212: step 16430, loss = 0.1580, acc = 0.9420 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 21:26:46.936284: step 16440, loss = 0.1399, acc = 0.9560 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 21:26:56.276904: step 16450, loss = 0.1404, acc = 0.9580 (270.0 examples/sec; 0.237 sec/batch)
2017-05-02 21:27:05.488395: step 16460, loss = 0.1495, acc = 0.9500 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 21:27:14.844764: step 16470, loss = 0.1615, acc = 0.9460 (254.1 examples/sec; 0.252 sec/batch)
2017-05-02 21:27:24.097373: step 16480, loss = 0.1592, acc = 0.9420 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 21:27:33.374161: step 16490, loss = 0.1600, acc = 0.9480 (270.5 examples/sec; 0.237 sec/batch)
2017-05-02 21:27:42.677120: step 16500, loss = 0.1635, acc = 0.9480 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 21:27:51.850287: step 16510, loss = 0.1453, acc = 0.9560 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 21:28:01.214544: step 16520, loss = 0.1944, acc = 0.9420 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 21:28:10.471848: step 16530, loss = 0.1511, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 21:28:19.787745: step 16540, loss = 0.1788, acc = 0.9520 (254.4 examples/sec; 0.252 sec/batch)
2017-05-02 21:28:29.003362: step 16550, loss = 0.1517, acc = 0.9540 (268.7 examples/sec; 0.238 sec/batch)
2017-05-02 21:28:38.342660: step 16560, loss = 0.1883, acc = 0.9380 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 21:28:47.515067: step 16570, loss = 0.1811, acc = 0.9440 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 21:28:56.641433: step 16580, loss = 0.1534, acc = 0.9540 (261.1 examples/sec; 0.245 sec/batch)
2017-05-02 21:29:05.765389: step 16590, loss = 0.1429, acc = 0.9580 (276.7 examples/sec; 0.231 sec/batch)
2017-05-02 21:29:14.869839: step 16600, loss = 0.1699, acc = 0.9380 (272.9 examples/sec; 0.234 sec/batch)
2017-05-02 21:29:24.114383: step 16610, loss = 0.1603, acc = 0.9540 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 21:29:33.344763: step 16620, loss = 0.1569, acc = 0.9440 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 21:29:42.512576: step 16630, loss = 0.1505, acc = 0.9540 (264.5 examples/sec; 0.242 sec/batch)
2017-05-02 21:29:51.715704: step 16640, loss = 0.1668, acc = 0.9360 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 21:30:00.933184: step 16650, loss = 0.2155, acc = 0.9220 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 21:30:10.146525: step 16660, loss = 0.1667, acc = 0.9520 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 21:30:19.502728: step 16670, loss = 0.1704, acc = 0.9400 (264.2 examples/sec; 0.242 sec/batch)
2017-05-02 21:30:28.725836: step 16680, loss = 0.1709, acc = 0.9380 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 21:30:37.721993: step 16690, loss = 0.1506, acc = 0.9440 (291.9 examples/sec; 0.219 sec/batch)
2017-05-02 21:30:46.806422: step 16700, loss = 0.1916, acc = 0.9360 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 21:30:55.901351: step 16710, loss = 0.1747, acc = 0.9540 (287.9 examples/sec; 0.222 sec/batch)
2017-05-02 21:31:04.998828: step 16720, loss = 0.1593, acc = 0.9500 (270.1 examples/sec; 0.237 sec/batch)
2017-05-02 21:31:14.142020: step 16730, loss = 0.1284, acc = 0.9580 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 21:31:23.443644: step 16740, loss = 0.1566, acc = 0.9480 (256.3 examples/sec; 0.250 sec/batch)
2017-05-02 21:31:32.741518: step 16750, loss = 0.1636, acc = 0.9400 (275.3 examples/sec; 0.233 sec/batch)
2017-05-02 21:31:41.963908: step 16760, loss = 0.1448, acc = 0.9560 (259.7 examples/sec; 0.246 sec/batch)
2017-05-02 21:31:51.140564: step 16770, loss = 0.1522, acc = 0.9480 (289.0 examples/sec; 0.221 sec/batch)
2017-05-02 21:32:00.331344: step 16780, loss = 0.1250, acc = 0.9680 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 21:32:09.413974: step 16790, loss = 0.1396, acc = 0.9620 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 21:32:18.644129: step 16800, loss = 0.1625, acc = 0.9460 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 21:32:27.916196: step 16810, loss = 0.1547, acc = 0.9540 (264.6 examples/sec; 0.242 sec/batch)
2017-05-02 21:32:37.827548: step 16820, loss = 0.1642, acc = 0.9480 (181.9 examples/sec; 0.352 sec/batch)
2017-05-02 21:32:46.938689: step 16830, loss = 0.1990, acc = 0.9380 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 21:32:56.170023: step 16840, loss = 0.1346, acc = 0.9520 (295.3 examples/sec; 0.217 sec/batch)
2017-05-02 21:33:05.522350: step 16850, loss = 0.1698, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 21:33:14.823369: step 16860, loss = 0.2019, acc = 0.9240 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 21:33:24.367899: step 16870, loss = 0.1359, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 21:33:33.548612: step 16880, loss = 0.1325, acc = 0.9540 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 21:33:42.752363: step 16890, loss = 0.1766, acc = 0.9380 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 21:33:52.061173: step 16900, loss = 0.1486, acc = 0.9540 (261.4 examples/sec; 0.245 sec/batch)
2017-05-02 21:34:01.495858: step 16910, loss = 0.1596, acc = 0.9440 (261.4 examples/sec; 0.245 sec/batch)
2017-05-02 21:34:10.647437: step 16920, loss = 0.1617, acc = 0.9500 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 21:34:19.962434: step 16930, loss = 0.1703, acc = 0.9400 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 21:34:29.260324: step 16940, loss = 0.1438, acc = 0.9580 (274.4 examples/sec; 0.233 sec/batch)
2017-05-02 21:34:38.622861: step 16950, loss = 0.1553, acc = 0.9480 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 21:34:47.703419: step 16960, loss = 0.1374, acc = 0.9560 (288.7 examples/sec; 0.222 sec/batch)
2017-05-02 21:34:56.988638: step 16970, loss = 0.1571, acc = 0.9380 (267.0 examples/sec; 0.240 sec/batch)
2017-05-02 21:35:06.162646: step 16980, loss = 0.1746, acc = 0.9260 (263.0 examples/sec; 0.243 sec/batch)
2017-05-02 21:35:15.337063: step 16990, loss = 0.1555, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 21:35:24.552856: step 17000, loss = 0.1374, acc = 0.9660 (275.1 examples/sec; 0.233 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 21:35:25.023810: step 17000, loss = 0.1288, acc = 0.9550, f1neg = 0.9510, f1pos = 0.9584, f1 = 0.9547
[Eval_batch(1)(2000,4000)] 2017-05-02 21:35:25.492530: step 17000, loss = 0.1322, acc = 0.9605, f1neg = 0.9544, f1pos = 0.9652, f1 = 0.9598
[Eval_batch(2)(2000,6000)] 2017-05-02 21:35:25.964128: step 17000, loss = 0.1438, acc = 0.9540, f1neg = 0.9514, f1pos = 0.9564, f1 = 0.9539
[Eval_batch(3)(2000,8000)] 2017-05-02 21:35:26.477793: step 17000, loss = 0.1491, acc = 0.9510, f1neg = 0.9514, f1pos = 0.9506, f1 = 0.9510
[Eval_batch(4)(2000,10000)] 2017-05-02 21:35:26.984281: step 17000, loss = 0.1488, acc = 0.9495, f1neg = 0.9511, f1pos = 0.9477, f1 = 0.9494
[Eval_batch(5)(2000,12000)] 2017-05-02 21:35:27.486014: step 17000, loss = 0.1450, acc = 0.9490, f1neg = 0.9457, f1pos = 0.9519, f1 = 0.9488
[Eval_batch(6)(2000,14000)] 2017-05-02 21:35:27.988880: step 17000, loss = 0.1355, acc = 0.9610, f1neg = 0.9604, f1pos = 0.9616, f1 = 0.9610
[Eval_batch(7)(2000,16000)] 2017-05-02 21:35:28.459406: step 17000, loss = 0.1348, acc = 0.9550, f1neg = 0.9467, f1pos = 0.9611, f1 = 0.9539
[Eval_batch(8)(2000,18000)] 2017-05-02 21:35:28.961478: step 17000, loss = 0.1322, acc = 0.9560, f1neg = 0.9502, f1pos = 0.9606, f1 = 0.9554
[Eval_batch(9)(2000,20000)] 2017-05-02 21:35:29.469656: step 17000, loss = 0.1346, acc = 0.9585, f1neg = 0.9559, f1pos = 0.9608, f1 = 0.9584
[Eval_batch(10)(2000,22000)] 2017-05-02 21:35:29.938486: step 17000, loss = 0.1447, acc = 0.9540, f1neg = 0.9509, f1pos = 0.9567, f1 = 0.9538
[Eval_batch(11)(2000,24000)] 2017-05-02 21:35:30.451934: step 17000, loss = 0.1450, acc = 0.9545, f1neg = 0.9477, f1pos = 0.9597, f1 = 0.9537
[Eval_batch(12)(2000,26000)] 2017-05-02 21:35:30.955930: step 17000, loss = 0.1407, acc = 0.9540, f1neg = 0.9530, f1pos = 0.9550, f1 = 0.9540
[Eval_batch(13)(2000,28000)] 2017-05-02 21:35:31.429743: step 17000, loss = 0.1284, acc = 0.9620, f1neg = 0.9527, f1pos = 0.9682, f1 = 0.9605
[Eval_batch(14)(2000,30000)] 2017-05-02 21:35:31.897312: step 17000, loss = 0.1540, acc = 0.9510, f1neg = 0.9415, f1pos = 0.9579, f1 = 0.9497
[Eval_batch(15)(2000,32000)] 2017-05-02 21:35:32.373059: step 17000, loss = 0.1363, acc = 0.9605, f1neg = 0.9578, f1pos = 0.9629, f1 = 0.9603
[Eval_batch(16)(2000,34000)] 2017-05-02 21:35:32.877682: step 17000, loss = 0.1309, acc = 0.9590, f1neg = 0.9575, f1pos = 0.9604, f1 = 0.9589
[Eval_batch(17)(2000,36000)] 2017-05-02 21:35:33.362874: step 17000, loss = 0.1515, acc = 0.9535, f1neg = 0.9540, f1pos = 0.9530, f1 = 0.9535
[Eval_batch(18)(2000,38000)] 2017-05-02 21:35:33.866978: step 17000, loss = 0.1485, acc = 0.9520, f1neg = 0.9541, f1pos = 0.9497, f1 = 0.9519
[Eval_batch(19)(2000,40000)] 2017-05-02 21:35:34.373063: step 17000, loss = 0.1303, acc = 0.9645, f1neg = 0.9592, f1pos = 0.9686, f1 = 0.9639
[Eval_batch(20)(2000,42000)] 2017-05-02 21:35:34.846932: step 17000, loss = 0.1306, acc = 0.9590, f1neg = 0.9638, f1pos = 0.9527, f1 = 0.9583
[Eval_batch(21)(2000,44000)] 2017-05-02 21:35:35.326626: step 17000, loss = 0.1275, acc = 0.9570, f1neg = 0.9535, f1pos = 0.9600, f1 = 0.9568
[Eval_batch(22)(2000,46000)] 2017-05-02 21:35:35.805650: step 17000, loss = 0.1326, acc = 0.9555, f1neg = 0.9561, f1pos = 0.9548, f1 = 0.9555
[Eval_batch(23)(2000,48000)] 2017-05-02 21:35:36.309432: step 17000, loss = 0.1398, acc = 0.9525, f1neg = 0.9467, f1pos = 0.9572, f1 = 0.9519
[Eval_batch(24)(2000,50000)] 2017-05-02 21:35:36.785587: step 17000, loss = 0.1280, acc = 0.9590, f1neg = 0.9534, f1pos = 0.9634, f1 = 0.9584
[Eval_batch(25)(2000,52000)] 2017-05-02 21:35:37.258431: step 17000, loss = 0.1181, acc = 0.9650, f1neg = 0.9617, f1pos = 0.9677, f1 = 0.9647
[Eval_batch(26)(2000,54000)] 2017-05-02 21:35:37.731875: step 17000, loss = 0.1274, acc = 0.9565, f1neg = 0.9592, f1pos = 0.9535, f1 = 0.9563
[Eval_batch(27)(2000,56000)] 2017-05-02 21:35:38.308395: step 17000, loss = 0.1168, acc = 0.9645, f1neg = 0.9628, f1pos = 0.9660, f1 = 0.9644
[Eval] 2017-05-02 21:35:38.308497: step 17000, acc = 0.9566, f1 = 0.9562
[Test_batch(0)(2000,2000)] 2017-05-02 21:35:38.783315: step 17000, loss = 0.1664, acc = 0.9495, f1neg = 0.9523, f1pos = 0.9463, f1 = 0.9493
[Test_batch(1)(2000,4000)] 2017-05-02 21:35:39.283741: step 17000, loss = 0.1510, acc = 0.9515, f1neg = 0.9572, f1pos = 0.9440, f1 = 0.9506
[Test_batch(2)(2000,6000)] 2017-05-02 21:35:39.788256: step 17000, loss = 0.1616, acc = 0.9530, f1neg = 0.9571, f1pos = 0.9480, f1 = 0.9526
[Test_batch(3)(2000,8000)] 2017-05-02 21:35:40.299272: step 17000, loss = 0.1717, acc = 0.9400, f1neg = 0.9443, f1pos = 0.9349, f1 = 0.9396
[Test_batch(4)(2000,10000)] 2017-05-02 21:35:40.782553: step 17000, loss = 0.1740, acc = 0.9415, f1neg = 0.9423, f1pos = 0.9407, f1 = 0.9415
[Test_batch(5)(2000,12000)] 2017-05-02 21:35:41.261963: step 17000, loss = 0.1828, acc = 0.9370, f1neg = 0.9383, f1pos = 0.9356, f1 = 0.9370
[Test_batch(6)(2000,14000)] 2017-05-02 21:35:41.767665: step 17000, loss = 0.1668, acc = 0.9465, f1neg = 0.9451, f1pos = 0.9478, f1 = 0.9465
[Test_batch(7)(2000,16000)] 2017-05-02 21:35:42.249624: step 17000, loss = 0.1566, acc = 0.9490, f1neg = 0.9541, f1pos = 0.9426, f1 = 0.9484
[Test_batch(8)(2000,18000)] 2017-05-02 21:35:42.727480: step 17000, loss = 0.1579, acc = 0.9485, f1neg = 0.9484, f1pos = 0.9486, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-02 21:35:43.234976: step 17000, loss = 0.1901, acc = 0.9300, f1neg = 0.9269, f1pos = 0.9329, f1 = 0.9299
[Test_batch(10)(2000,22000)] 2017-05-02 21:35:43.735878: step 17000, loss = 0.1666, acc = 0.9455, f1neg = 0.9392, f1pos = 0.9506, f1 = 0.9449
[Test_batch(11)(2000,24000)] 2017-05-02 21:35:44.203440: step 17000, loss = 0.1633, acc = 0.9475, f1neg = 0.9493, f1pos = 0.9455, f1 = 0.9474
[Test_batch(12)(2000,26000)] 2017-05-02 21:35:44.673409: step 17000, loss = 0.1439, acc = 0.9550, f1neg = 0.9570, f1pos = 0.9528, f1 = 0.9549
[Test_batch(13)(2000,28000)] 2017-05-02 21:35:45.136657: step 17000, loss = 0.1722, acc = 0.9410, f1neg = 0.9348, f1pos = 0.9461, f1 = 0.9405
[Test_batch(14)(2000,30000)] 2017-05-02 21:35:45.620918: step 17000, loss = 0.1441, acc = 0.9550, f1neg = 0.9491, f1pos = 0.9597, f1 = 0.9544
[Test_batch(15)(2000,32000)] 2017-05-02 21:35:46.121467: step 17000, loss = 0.1624, acc = 0.9485, f1neg = 0.9483, f1pos = 0.9487, f1 = 0.9485
[Test_batch(16)(2000,34000)] 2017-05-02 21:35:46.632403: step 17000, loss = 0.1469, acc = 0.9555, f1neg = 0.9548, f1pos = 0.9562, f1 = 0.9555
[Test_batch(17)(2000,36000)] 2017-05-02 21:35:47.145171: step 17000, loss = 0.1371, acc = 0.9580, f1neg = 0.9544, f1pos = 0.9610, f1 = 0.9577
[Test_batch(18)(2000,38000)] 2017-05-02 21:35:47.718723: step 17000, loss = 0.1266, acc = 0.9620, f1neg = 0.9614, f1pos = 0.9626, f1 = 0.9620
[Test] 2017-05-02 21:35:47.718784: step 17000, acc = 0.9481, f1 = 0.9479
[Status] 2017-05-02 21:35:47.718797: step 17000, maxindex = 17000, maxdev = 0.9566, maxtst = 0.9481
2017-05-02 21:36:00.061585: step 17010, loss = 0.1744, acc = 0.9380 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 21:36:09.231830: step 17020, loss = 0.1512, acc = 0.9480 (292.0 examples/sec; 0.219 sec/batch)
2017-05-02 21:36:18.535173: step 17030, loss = 0.1455, acc = 0.9520 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 21:36:27.696439: step 17040, loss = 0.1765, acc = 0.9480 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 21:36:36.948024: step 17050, loss = 0.1640, acc = 0.9500 (281.0 examples/sec; 0.228 sec/batch)
2017-05-02 21:36:46.090223: step 17060, loss = 0.1275, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 21:36:55.446744: step 17070, loss = 0.1649, acc = 0.9500 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 21:37:04.589157: step 17080, loss = 0.1379, acc = 0.9540 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 21:37:13.751463: step 17090, loss = 0.1708, acc = 0.9360 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 21:37:23.146710: step 17100, loss = 0.1482, acc = 0.9560 (264.2 examples/sec; 0.242 sec/batch)
2017-05-02 21:37:32.253282: step 17110, loss = 0.1683, acc = 0.9420 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 21:37:41.294617: step 17120, loss = 0.1372, acc = 0.9560 (287.8 examples/sec; 0.222 sec/batch)
2017-05-02 21:37:50.399764: step 17130, loss = 0.1522, acc = 0.9520 (288.2 examples/sec; 0.222 sec/batch)
2017-05-02 21:38:00.639698: step 17140, loss = 0.1460, acc = 0.9560 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 21:38:09.829614: step 17150, loss = 0.1330, acc = 0.9580 (272.1 examples/sec; 0.235 sec/batch)
2017-05-02 21:38:19.002950: step 17160, loss = 0.1528, acc = 0.9460 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 21:38:28.126271: step 17170, loss = 0.1213, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 21:38:37.452224: step 17180, loss = 0.1591, acc = 0.9480 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 21:38:46.544532: step 17190, loss = 0.1723, acc = 0.9400 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 21:38:55.681114: step 17200, loss = 0.1582, acc = 0.9540 (289.7 examples/sec; 0.221 sec/batch)
2017-05-02 21:39:04.849086: step 17210, loss = 0.1507, acc = 0.9500 (268.6 examples/sec; 0.238 sec/batch)
2017-05-02 21:39:14.034691: step 17220, loss = 0.1502, acc = 0.9460 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 21:39:23.215736: step 17230, loss = 0.1578, acc = 0.9520 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 21:39:32.413856: step 17240, loss = 0.1602, acc = 0.9460 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 21:39:41.588667: step 17250, loss = 0.1556, acc = 0.9480 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 21:39:50.789949: step 17260, loss = 0.1327, acc = 0.9520 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 21:40:00.003371: step 17270, loss = 0.1624, acc = 0.9500 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 21:40:09.380682: step 17280, loss = 0.1339, acc = 0.9560 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 21:40:18.737075: step 17290, loss = 0.1662, acc = 0.9440 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 21:40:27.929638: step 17300, loss = 0.1546, acc = 0.9420 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 21:40:37.122333: step 17310, loss = 0.1683, acc = 0.9400 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 21:40:46.159664: step 17320, loss = 0.1674, acc = 0.9500 (288.7 examples/sec; 0.222 sec/batch)
2017-05-02 21:40:55.530764: step 17330, loss = 0.1569, acc = 0.9380 (268.0 examples/sec; 0.239 sec/batch)
2017-05-02 21:41:04.715412: step 17340, loss = 0.1823, acc = 0.9280 (291.8 examples/sec; 0.219 sec/batch)
2017-05-02 21:41:13.894017: step 17350, loss = 0.1531, acc = 0.9560 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 21:41:23.193899: step 17360, loss = 0.1885, acc = 0.9320 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 21:41:32.334989: step 17370, loss = 0.1416, acc = 0.9560 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 21:41:41.536854: step 17380, loss = 0.1665, acc = 0.9500 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 21:41:50.794796: step 17390, loss = 0.1547, acc = 0.9500 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 21:41:59.979319: step 17400, loss = 0.1778, acc = 0.9360 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 21:42:09.120023: step 17410, loss = 0.1655, acc = 0.9400 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 21:42:18.309688: step 17420, loss = 0.1480, acc = 0.9540 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 21:42:27.496621: step 17430, loss = 0.1409, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 21:42:36.847337: step 17440, loss = 0.1620, acc = 0.9440 (289.1 examples/sec; 0.221 sec/batch)
2017-05-02 21:42:46.049016: step 17450, loss = 0.1840, acc = 0.9280 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 21:42:55.267126: step 17460, loss = 0.1425, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 21:43:04.528896: step 17470, loss = 0.1568, acc = 0.9440 (289.8 examples/sec; 0.221 sec/batch)
2017-05-02 21:43:13.656901: step 17480, loss = 0.1525, acc = 0.9540 (265.8 examples/sec; 0.241 sec/batch)
2017-05-02 21:43:22.816568: step 17490, loss = 0.1466, acc = 0.9500 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 21:43:32.358201: step 17500, loss = 0.1537, acc = 0.9480 (280.1 examples/sec; 0.228 sec/batch)
2017-05-02 21:43:41.653099: step 17510, loss = 0.1558, acc = 0.9580 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 21:43:50.864477: step 17520, loss = 0.1465, acc = 0.9520 (290.6 examples/sec; 0.220 sec/batch)
2017-05-02 21:43:59.992521: step 17530, loss = 0.1580, acc = 0.9480 (260.5 examples/sec; 0.246 sec/batch)
2017-05-02 21:44:09.791658: step 17540, loss = 0.1391, acc = 0.9600 (261.6 examples/sec; 0.245 sec/batch)
2017-05-02 21:44:19.148438: step 17550, loss = 0.2128, acc = 0.9300 (287.3 examples/sec; 0.223 sec/batch)
2017-05-02 21:44:28.314574: step 17560, loss = 0.1509, acc = 0.9520 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 21:44:37.623715: step 17570, loss = 0.1650, acc = 0.9480 (272.9 examples/sec; 0.235 sec/batch)
2017-05-02 21:44:46.742694: step 17580, loss = 0.1676, acc = 0.9360 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 21:44:56.025710: step 17590, loss = 0.1631, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 21:45:05.186811: step 17600, loss = 0.1076, acc = 0.9660 (263.9 examples/sec; 0.243 sec/batch)
2017-05-02 21:45:14.340529: step 17610, loss = 0.1524, acc = 0.9460 (281.3 examples/sec; 0.228 sec/batch)
2017-05-02 21:45:23.665002: step 17620, loss = 0.1562, acc = 0.9460 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 21:45:33.093158: step 17630, loss = 0.1296, acc = 0.9620 (266.9 examples/sec; 0.240 sec/batch)
2017-05-02 21:45:42.402550: step 17640, loss = 0.1507, acc = 0.9620 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 21:45:51.622790: step 17650, loss = 0.1386, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 21:46:00.854611: step 17660, loss = 0.1430, acc = 0.9540 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 21:46:09.968330: step 17670, loss = 0.1361, acc = 0.9500 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 21:46:19.094828: step 17680, loss = 0.1386, acc = 0.9480 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 21:46:28.425154: step 17690, loss = 0.1339, acc = 0.9580 (252.2 examples/sec; 0.254 sec/batch)
2017-05-02 21:46:37.554278: step 17700, loss = 0.1476, acc = 0.9580 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 21:46:46.542626: step 17710, loss = 0.1582, acc = 0.9500 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 21:46:55.662829: step 17720, loss = 0.1423, acc = 0.9600 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 21:47:04.826938: step 17730, loss = 0.1576, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 21:47:14.079527: step 17740, loss = 0.1465, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 21:47:23.160051: step 17750, loss = 0.1596, acc = 0.9480 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 21:47:32.403361: step 17760, loss = 0.1401, acc = 0.9560 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 21:47:41.417067: step 17770, loss = 0.1283, acc = 0.9640 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 21:47:50.614785: step 17780, loss = 0.1754, acc = 0.9380 (262.9 examples/sec; 0.243 sec/batch)
2017-05-02 21:47:59.846210: step 17790, loss = 0.1679, acc = 0.9420 (276.3 examples/sec; 0.232 sec/batch)
2017-05-02 21:48:09.062368: step 17800, loss = 0.1644, acc = 0.9360 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 21:48:18.179817: step 17810, loss = 0.1999, acc = 0.9420 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 21:48:27.435841: step 17820, loss = 0.1490, acc = 0.9580 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 21:48:36.610682: step 17830, loss = 0.1880, acc = 0.9280 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 21:48:45.854167: step 17840, loss = 0.1583, acc = 0.9440 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 21:48:55.158817: step 17850, loss = 0.1666, acc = 0.9360 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 21:49:04.397704: step 17860, loss = 0.1811, acc = 0.9420 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 21:49:13.617256: step 17870, loss = 0.1252, acc = 0.9660 (272.1 examples/sec; 0.235 sec/batch)
2017-05-02 21:49:22.708571: step 17880, loss = 0.1852, acc = 0.9400 (288.7 examples/sec; 0.222 sec/batch)
2017-05-02 21:49:31.908613: step 17890, loss = 0.1449, acc = 0.9640 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 21:49:41.067863: step 17900, loss = 0.1723, acc = 0.9300 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 21:49:50.290361: step 17910, loss = 0.1450, acc = 0.9540 (269.0 examples/sec; 0.238 sec/batch)
2017-05-02 21:49:59.529606: step 17920, loss = 0.1574, acc = 0.9400 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 21:50:08.700603: step 17930, loss = 0.1612, acc = 0.9600 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 21:50:17.921498: step 17940, loss = 0.1399, acc = 0.9540 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 21:50:27.158685: step 17950, loss = 0.1348, acc = 0.9520 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 21:50:36.382690: step 17960, loss = 0.1641, acc = 0.9460 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 21:50:45.436727: step 17970, loss = 0.1615, acc = 0.9420 (285.6 examples/sec; 0.224 sec/batch)
2017-05-02 21:50:54.874863: step 17980, loss = 0.1346, acc = 0.9620 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 21:51:04.255089: step 17990, loss = 0.1346, acc = 0.9640 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 21:51:13.557280: step 18000, loss = 0.1744, acc = 0.9340 (278.8 examples/sec; 0.230 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 21:51:14.054050: step 18000, loss = 0.1278, acc = 0.9545, f1neg = 0.9501, f1pos = 0.9582, f1 = 0.9541
[Eval_batch(1)(2000,4000)] 2017-05-02 21:51:14.562264: step 18000, loss = 0.1293, acc = 0.9620, f1neg = 0.9558, f1pos = 0.9667, f1 = 0.9612
[Eval_batch(2)(2000,6000)] 2017-05-02 21:51:15.069834: step 18000, loss = 0.1415, acc = 0.9560, f1neg = 0.9532, f1pos = 0.9585, f1 = 0.9558
[Eval_batch(3)(2000,8000)] 2017-05-02 21:51:15.551589: step 18000, loss = 0.1496, acc = 0.9525, f1neg = 0.9526, f1pos = 0.9524, f1 = 0.9525
[Eval_batch(4)(2000,10000)] 2017-05-02 21:51:16.025659: step 18000, loss = 0.1492, acc = 0.9475, f1neg = 0.9488, f1pos = 0.9461, f1 = 0.9475
[Eval_batch(5)(2000,12000)] 2017-05-02 21:51:16.497258: step 18000, loss = 0.1445, acc = 0.9510, f1neg = 0.9475, f1pos = 0.9541, f1 = 0.9508
[Eval_batch(6)(2000,14000)] 2017-05-02 21:51:17.004536: step 18000, loss = 0.1347, acc = 0.9630, f1neg = 0.9623, f1pos = 0.9637, f1 = 0.9630
[Eval_batch(7)(2000,16000)] 2017-05-02 21:51:17.518585: step 18000, loss = 0.1338, acc = 0.9550, f1neg = 0.9463, f1pos = 0.9613, f1 = 0.9538
[Eval_batch(8)(2000,18000)] 2017-05-02 21:51:18.026227: step 18000, loss = 0.1306, acc = 0.9565, f1neg = 0.9504, f1pos = 0.9613, f1 = 0.9558
[Eval_batch(9)(2000,20000)] 2017-05-02 21:51:18.533712: step 18000, loss = 0.1319, acc = 0.9630, f1neg = 0.9605, f1pos = 0.9652, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-02 21:51:19.037176: step 18000, loss = 0.1446, acc = 0.9510, f1neg = 0.9473, f1pos = 0.9542, f1 = 0.9508
[Eval_batch(11)(2000,24000)] 2017-05-02 21:51:19.527805: step 18000, loss = 0.1424, acc = 0.9605, f1neg = 0.9544, f1pos = 0.9652, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-02 21:51:20.033666: step 18000, loss = 0.1402, acc = 0.9540, f1neg = 0.9529, f1pos = 0.9551, f1 = 0.9540
[Eval_batch(13)(2000,28000)] 2017-05-02 21:51:20.531531: step 18000, loss = 0.1267, acc = 0.9615, f1neg = 0.9518, f1pos = 0.9680, f1 = 0.9599
[Eval_batch(14)(2000,30000)] 2017-05-02 21:51:21.044444: step 18000, loss = 0.1538, acc = 0.9525, f1neg = 0.9429, f1pos = 0.9593, f1 = 0.9511
[Eval_batch(15)(2000,32000)] 2017-05-02 21:51:21.533445: step 18000, loss = 0.1345, acc = 0.9630, f1neg = 0.9602, f1pos = 0.9654, f1 = 0.9628
[Eval_batch(16)(2000,34000)] 2017-05-02 21:51:22.050382: step 18000, loss = 0.1295, acc = 0.9600, f1neg = 0.9584, f1pos = 0.9615, f1 = 0.9599
[Eval_batch(17)(2000,36000)] 2017-05-02 21:51:22.561710: step 18000, loss = 0.1509, acc = 0.9550, f1neg = 0.9552, f1pos = 0.9548, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-02 21:51:23.068228: step 18000, loss = 0.1485, acc = 0.9520, f1neg = 0.9539, f1pos = 0.9499, f1 = 0.9519
[Eval_batch(19)(2000,40000)] 2017-05-02 21:51:23.565138: step 18000, loss = 0.1294, acc = 0.9655, f1neg = 0.9602, f1pos = 0.9696, f1 = 0.9649
[Eval_batch(20)(2000,42000)] 2017-05-02 21:51:24.067331: step 18000, loss = 0.1326, acc = 0.9595, f1neg = 0.9641, f1pos = 0.9535, f1 = 0.9588
[Eval_batch(21)(2000,44000)] 2017-05-02 21:51:24.575735: step 18000, loss = 0.1242, acc = 0.9610, f1neg = 0.9575, f1pos = 0.9640, f1 = 0.9607
[Eval_batch(22)(2000,46000)] 2017-05-02 21:51:25.073379: step 18000, loss = 0.1311, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9590, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-02 21:51:25.557395: step 18000, loss = 0.1390, acc = 0.9545, f1neg = 0.9486, f1pos = 0.9592, f1 = 0.9539
[Eval_batch(24)(2000,50000)] 2017-05-02 21:51:26.020805: step 18000, loss = 0.1263, acc = 0.9615, f1neg = 0.9558, f1pos = 0.9659, f1 = 0.9608
[Eval_batch(25)(2000,52000)] 2017-05-02 21:51:26.491307: step 18000, loss = 0.1165, acc = 0.9660, f1neg = 0.9627, f1pos = 0.9688, f1 = 0.9657
[Eval_batch(26)(2000,54000)] 2017-05-02 21:51:26.959811: step 18000, loss = 0.1274, acc = 0.9560, f1neg = 0.9585, f1pos = 0.9531, f1 = 0.9558
[Eval_batch(27)(2000,56000)] 2017-05-02 21:51:27.513502: step 18000, loss = 0.1158, acc = 0.9655, f1neg = 0.9638, f1pos = 0.9671, f1 = 0.9654
[Eval] 2017-05-02 21:51:27.513575: step 18000, acc = 0.9578, f1 = 0.9574
[Test_batch(0)(2000,2000)] 2017-05-02 21:51:28.000442: step 18000, loss = 0.1648, acc = 0.9495, f1neg = 0.9520, f1pos = 0.9467, f1 = 0.9494
[Test_batch(1)(2000,4000)] 2017-05-02 21:51:28.507209: step 18000, loss = 0.1512, acc = 0.9520, f1neg = 0.9575, f1pos = 0.9448, f1 = 0.9512
[Test_batch(2)(2000,6000)] 2017-05-02 21:51:28.986257: step 18000, loss = 0.1617, acc = 0.9490, f1neg = 0.9531, f1pos = 0.9441, f1 = 0.9486
[Test_batch(3)(2000,8000)] 2017-05-02 21:51:29.466632: step 18000, loss = 0.1710, acc = 0.9410, f1neg = 0.9448, f1pos = 0.9366, f1 = 0.9407
[Test_batch(4)(2000,10000)] 2017-05-02 21:51:29.930366: step 18000, loss = 0.1722, acc = 0.9400, f1neg = 0.9401, f1pos = 0.9399, f1 = 0.9400
[Test_batch(5)(2000,12000)] 2017-05-02 21:51:30.442900: step 18000, loss = 0.1817, acc = 0.9355, f1neg = 0.9363, f1pos = 0.9347, f1 = 0.9355
[Test_batch(6)(2000,14000)] 2017-05-02 21:51:30.939540: step 18000, loss = 0.1666, acc = 0.9425, f1neg = 0.9406, f1pos = 0.9443, f1 = 0.9424
[Test_batch(7)(2000,16000)] 2017-05-02 21:51:31.429383: step 18000, loss = 0.1552, acc = 0.9470, f1neg = 0.9519, f1pos = 0.9410, f1 = 0.9465
[Test_batch(8)(2000,18000)] 2017-05-02 21:51:31.932285: step 18000, loss = 0.1566, acc = 0.9495, f1neg = 0.9491, f1pos = 0.9499, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-02 21:51:32.445812: step 18000, loss = 0.1887, acc = 0.9325, f1neg = 0.9290, f1pos = 0.9357, f1 = 0.9323
[Test_batch(10)(2000,22000)] 2017-05-02 21:51:32.944305: step 18000, loss = 0.1640, acc = 0.9470, f1neg = 0.9406, f1pos = 0.9522, f1 = 0.9464
[Test_batch(11)(2000,24000)] 2017-05-02 21:51:33.454405: step 18000, loss = 0.1616, acc = 0.9470, f1neg = 0.9485, f1pos = 0.9454, f1 = 0.9470
[Test_batch(12)(2000,26000)] 2017-05-02 21:51:33.962363: step 18000, loss = 0.1410, acc = 0.9560, f1neg = 0.9577, f1pos = 0.9542, f1 = 0.9559
[Test_batch(13)(2000,28000)] 2017-05-02 21:51:34.468455: step 18000, loss = 0.1689, acc = 0.9425, f1neg = 0.9362, f1pos = 0.9477, f1 = 0.9419
[Test_batch(14)(2000,30000)] 2017-05-02 21:51:34.967550: step 18000, loss = 0.1408, acc = 0.9575, f1neg = 0.9516, f1pos = 0.9621, f1 = 0.9569
[Test_batch(15)(2000,32000)] 2017-05-02 21:51:35.468633: step 18000, loss = 0.1606, acc = 0.9470, f1neg = 0.9465, f1pos = 0.9475, f1 = 0.9470
[Test_batch(16)(2000,34000)] 2017-05-02 21:51:35.970595: step 18000, loss = 0.1445, acc = 0.9545, f1neg = 0.9535, f1pos = 0.9554, f1 = 0.9545
[Test_batch(17)(2000,36000)] 2017-05-02 21:51:36.482386: step 18000, loss = 0.1366, acc = 0.9585, f1neg = 0.9547, f1pos = 0.9617, f1 = 0.9582
[Test_batch(18)(2000,38000)] 2017-05-02 21:51:37.049520: step 18000, loss = 0.1258, acc = 0.9615, f1neg = 0.9608, f1pos = 0.9622, f1 = 0.9615
[Test] 2017-05-02 21:51:37.049627: step 18000, acc = 0.9479, f1 = 0.9476
[Status] 2017-05-02 21:51:37.049656: step 18000, maxindex = 18000, maxdev = 0.9578, maxtst = 0.9479
2017-05-02 21:51:49.632318: step 18010, loss = 0.1173, acc = 0.9700 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 21:51:58.798159: step 18020, loss = 0.1406, acc = 0.9620 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 21:52:07.971226: step 18030, loss = 0.1725, acc = 0.9400 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 21:52:17.294726: step 18040, loss = 0.1580, acc = 0.9480 (269.2 examples/sec; 0.238 sec/batch)
2017-05-02 21:52:26.456182: step 18050, loss = 0.1281, acc = 0.9680 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 21:52:35.625203: step 18060, loss = 0.1791, acc = 0.9400 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 21:52:44.764033: step 18070, loss = 0.1491, acc = 0.9600 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 21:52:53.973323: step 18080, loss = 0.1614, acc = 0.9560 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 21:53:03.312245: step 18090, loss = 0.1323, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 21:53:12.469745: step 18100, loss = 0.1408, acc = 0.9540 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 21:53:21.481076: step 18110, loss = 0.1519, acc = 0.9500 (288.7 examples/sec; 0.222 sec/batch)
2017-05-02 21:53:30.560000: step 18120, loss = 0.1312, acc = 0.9520 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 21:53:39.722362: step 18130, loss = 0.1626, acc = 0.9440 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 21:53:48.879435: step 18140, loss = 0.1805, acc = 0.9320 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 21:53:58.846479: step 18150, loss = 0.1730, acc = 0.9420 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 21:54:07.994117: step 18160, loss = 0.1561, acc = 0.9460 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 21:54:17.154697: step 18170, loss = 0.1466, acc = 0.9600 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 21:54:26.285750: step 18180, loss = 0.1358, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 21:54:35.356456: step 18190, loss = 0.1377, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 21:54:44.388479: step 18200, loss = 0.1798, acc = 0.9380 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 21:54:53.556104: step 18210, loss = 0.1629, acc = 0.9520 (290.3 examples/sec; 0.220 sec/batch)
2017-05-02 21:55:02.615017: step 18220, loss = 0.1225, acc = 0.9640 (288.8 examples/sec; 0.222 sec/batch)
2017-05-02 21:55:11.976470: step 18230, loss = 0.1787, acc = 0.9500 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 21:55:21.146113: step 18240, loss = 0.1587, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-02 21:55:30.234622: step 18250, loss = 0.1689, acc = 0.9400 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 21:55:39.467762: step 18260, loss = 0.1509, acc = 0.9500 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 21:55:48.650035: step 18270, loss = 0.1625, acc = 0.9400 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 21:55:57.626503: step 18280, loss = 0.1585, acc = 0.9440 (287.6 examples/sec; 0.223 sec/batch)
2017-05-02 21:56:06.838921: step 18290, loss = 0.1413, acc = 0.9600 (263.2 examples/sec; 0.243 sec/batch)
2017-05-02 21:56:15.921625: step 18300, loss = 0.1407, acc = 0.9520 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 21:56:25.020630: step 18310, loss = 0.1509, acc = 0.9440 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 21:56:34.041791: step 18320, loss = 0.1605, acc = 0.9520 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 21:56:43.053456: step 18330, loss = 0.1662, acc = 0.9400 (290.2 examples/sec; 0.221 sec/batch)
2017-05-02 21:56:52.259176: step 18340, loss = 0.1305, acc = 0.9620 (264.1 examples/sec; 0.242 sec/batch)
2017-05-02 21:57:01.554114: step 18350, loss = 0.1486, acc = 0.9540 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 21:57:10.753614: step 18360, loss = 0.1417, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 21:57:19.894080: step 18370, loss = 0.1579, acc = 0.9420 (288.2 examples/sec; 0.222 sec/batch)
2017-05-02 21:57:29.003567: step 18380, loss = 0.1618, acc = 0.9520 (266.5 examples/sec; 0.240 sec/batch)
2017-05-02 21:57:38.200240: step 18390, loss = 0.1513, acc = 0.9520 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 21:57:47.442933: step 18400, loss = 0.1605, acc = 0.9420 (277.9 examples/sec; 0.230 sec/batch)
2017-05-02 21:57:56.623298: step 18410, loss = 0.1849, acc = 0.9340 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 21:58:05.679435: step 18420, loss = 0.1875, acc = 0.9360 (289.1 examples/sec; 0.221 sec/batch)
2017-05-02 21:58:14.965693: step 18430, loss = 0.1365, acc = 0.9600 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 21:58:24.211357: step 18440, loss = 0.1844, acc = 0.9400 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 21:58:33.471395: step 18450, loss = 0.1466, acc = 0.9540 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 21:58:42.594584: step 18460, loss = 0.1111, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 21:58:51.982400: step 18470, loss = 0.1637, acc = 0.9520 (268.5 examples/sec; 0.238 sec/batch)
2017-05-02 21:59:01.140154: step 18480, loss = 0.1669, acc = 0.9400 (267.5 examples/sec; 0.239 sec/batch)
2017-05-02 21:59:10.374443: step 18490, loss = 0.1467, acc = 0.9440 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 21:59:19.528426: step 18500, loss = 0.1374, acc = 0.9560 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 21:59:28.571622: step 18510, loss = 0.1452, acc = 0.9600 (270.2 examples/sec; 0.237 sec/batch)
2017-05-02 21:59:37.817748: step 18520, loss = 0.1722, acc = 0.9500 (249.7 examples/sec; 0.256 sec/batch)
2017-05-02 21:59:46.979779: step 18530, loss = 0.1625, acc = 0.9540 (262.9 examples/sec; 0.243 sec/batch)
2017-05-02 21:59:56.079857: step 18540, loss = 0.1395, acc = 0.9520 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 22:00:05.361839: step 18550, loss = 0.1198, acc = 0.9660 (294.3 examples/sec; 0.217 sec/batch)
2017-05-02 22:00:14.508034: step 18560, loss = 0.1544, acc = 0.9480 (275.6 examples/sec; 0.232 sec/batch)
2017-05-02 22:00:23.662627: step 18570, loss = 0.1593, acc = 0.9460 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 22:00:32.879600: step 18580, loss = 0.1537, acc = 0.9420 (269.6 examples/sec; 0.237 sec/batch)
2017-05-02 22:00:42.040390: step 18590, loss = 0.1258, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 22:00:51.508053: step 18600, loss = 0.1546, acc = 0.9600 (271.3 examples/sec; 0.236 sec/batch)
2017-05-02 22:01:00.726487: step 18610, loss = 0.1580, acc = 0.9380 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 22:01:10.007728: step 18620, loss = 0.1546, acc = 0.9480 (270.1 examples/sec; 0.237 sec/batch)
2017-05-02 22:01:19.196055: step 18630, loss = 0.1550, acc = 0.9540 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 22:01:28.554741: step 18640, loss = 0.1419, acc = 0.9520 (262.2 examples/sec; 0.244 sec/batch)
2017-05-02 22:01:37.805365: step 18650, loss = 0.1866, acc = 0.9380 (289.1 examples/sec; 0.221 sec/batch)
2017-05-02 22:01:46.973007: step 18660, loss = 0.1693, acc = 0.9460 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 22:01:56.110672: step 18670, loss = 0.1519, acc = 0.9440 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 22:02:05.393008: step 18680, loss = 0.1365, acc = 0.9560 (262.3 examples/sec; 0.244 sec/batch)
2017-05-02 22:02:14.743399: step 18690, loss = 0.1966, acc = 0.9320 (261.9 examples/sec; 0.244 sec/batch)
2017-05-02 22:02:23.938264: step 18700, loss = 0.1304, acc = 0.9600 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 22:02:33.141662: step 18710, loss = 0.1391, acc = 0.9560 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 22:02:42.418540: step 18720, loss = 0.1746, acc = 0.9500 (258.9 examples/sec; 0.247 sec/batch)
2017-05-02 22:02:51.570752: step 18730, loss = 0.1305, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 22:03:00.743323: step 18740, loss = 0.1373, acc = 0.9580 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 22:03:10.014197: step 18750, loss = 0.1374, acc = 0.9500 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:03:19.341127: step 18760, loss = 0.1546, acc = 0.9440 (293.1 examples/sec; 0.218 sec/batch)
2017-05-02 22:03:28.491299: step 18770, loss = 0.1687, acc = 0.9460 (288.0 examples/sec; 0.222 sec/batch)
2017-05-02 22:03:37.734852: step 18780, loss = 0.1548, acc = 0.9560 (262.8 examples/sec; 0.244 sec/batch)
2017-05-02 22:03:47.374444: step 18790, loss = 0.1681, acc = 0.9440 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 22:03:56.532373: step 18800, loss = 0.1469, acc = 0.9600 (279.9 examples/sec; 0.229 sec/batch)
2017-05-02 22:04:05.746254: step 18810, loss = 0.1521, acc = 0.9500 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 22:04:14.948268: step 18820, loss = 0.1664, acc = 0.9500 (291.1 examples/sec; 0.220 sec/batch)
2017-05-02 22:04:24.263165: step 18830, loss = 0.1874, acc = 0.9320 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 22:04:33.359671: step 18840, loss = 0.1249, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-02 22:04:42.532239: step 18850, loss = 0.1724, acc = 0.9460 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 22:04:51.754596: step 18860, loss = 0.1319, acc = 0.9620 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 22:05:00.904947: step 18870, loss = 0.1383, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 22:05:10.069103: step 18880, loss = 0.1634, acc = 0.9480 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 22:05:19.273684: step 18890, loss = 0.1864, acc = 0.9340 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 22:05:28.610868: step 18900, loss = 0.1544, acc = 0.9440 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 22:05:37.751012: step 18910, loss = 0.1657, acc = 0.9400 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 22:05:47.072240: step 18920, loss = 0.1693, acc = 0.9460 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 22:05:56.200070: step 18930, loss = 0.1513, acc = 0.9540 (291.9 examples/sec; 0.219 sec/batch)
2017-05-02 22:06:05.470295: step 18940, loss = 0.1696, acc = 0.9480 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 22:06:14.704886: step 18950, loss = 0.1522, acc = 0.9480 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 22:06:23.838881: step 18960, loss = 0.1357, acc = 0.9560 (288.0 examples/sec; 0.222 sec/batch)
2017-05-02 22:06:33.055706: step 18970, loss = 0.1432, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 22:06:42.262071: step 18980, loss = 0.1400, acc = 0.9540 (259.6 examples/sec; 0.247 sec/batch)
2017-05-02 22:06:51.586752: step 18990, loss = 0.1698, acc = 0.9440 (275.1 examples/sec; 0.233 sec/batch)
2017-05-02 22:07:01.086142: step 19000, loss = 0.1973, acc = 0.9260 (269.0 examples/sec; 0.238 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 22:07:01.582067: step 19000, loss = 0.1284, acc = 0.9540, f1neg = 0.9495, f1pos = 0.9578, f1 = 0.9536
[Eval_batch(1)(2000,4000)] 2017-05-02 22:07:02.094147: step 19000, loss = 0.1284, acc = 0.9620, f1neg = 0.9557, f1pos = 0.9667, f1 = 0.9612
[Eval_batch(2)(2000,6000)] 2017-05-02 22:07:02.594592: step 19000, loss = 0.1409, acc = 0.9575, f1neg = 0.9548, f1pos = 0.9599, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-02 22:07:03.099475: step 19000, loss = 0.1486, acc = 0.9500, f1neg = 0.9499, f1pos = 0.9500, f1 = 0.9500
[Eval_batch(4)(2000,10000)] 2017-05-02 22:07:03.576530: step 19000, loss = 0.1498, acc = 0.9485, f1neg = 0.9497, f1pos = 0.9473, f1 = 0.9485
[Eval_batch(5)(2000,12000)] 2017-05-02 22:07:04.038655: step 19000, loss = 0.1430, acc = 0.9485, f1neg = 0.9446, f1pos = 0.9519, f1 = 0.9482
[Eval_batch(6)(2000,14000)] 2017-05-02 22:07:04.533512: step 19000, loss = 0.1359, acc = 0.9600, f1neg = 0.9592, f1pos = 0.9608, f1 = 0.9600
[Eval_batch(7)(2000,16000)] 2017-05-02 22:07:05.002294: step 19000, loss = 0.1317, acc = 0.9560, f1neg = 0.9473, f1pos = 0.9622, f1 = 0.9548
[Eval_batch(8)(2000,18000)] 2017-05-02 22:07:05.466493: step 19000, loss = 0.1301, acc = 0.9570, f1neg = 0.9509, f1pos = 0.9618, f1 = 0.9563
[Eval_batch(9)(2000,20000)] 2017-05-02 22:07:05.962239: step 19000, loss = 0.1318, acc = 0.9645, f1neg = 0.9620, f1pos = 0.9667, f1 = 0.9643
[Eval_batch(10)(2000,22000)] 2017-05-02 22:07:06.468002: step 19000, loss = 0.1423, acc = 0.9525, f1neg = 0.9489, f1pos = 0.9556, f1 = 0.9523
[Eval_batch(11)(2000,24000)] 2017-05-02 22:07:06.946124: step 19000, loss = 0.1415, acc = 0.9585, f1neg = 0.9520, f1pos = 0.9635, f1 = 0.9577
[Eval_batch(12)(2000,26000)] 2017-05-02 22:07:07.414286: step 19000, loss = 0.1394, acc = 0.9505, f1neg = 0.9493, f1pos = 0.9517, f1 = 0.9505
[Eval_batch(13)(2000,28000)] 2017-05-02 22:07:07.858119: step 19000, loss = 0.1264, acc = 0.9610, f1neg = 0.9511, f1pos = 0.9676, f1 = 0.9593
[Eval_batch(14)(2000,30000)] 2017-05-02 22:07:08.332678: step 19000, loss = 0.1531, acc = 0.9505, f1neg = 0.9405, f1pos = 0.9576, f1 = 0.9491
[Eval_batch(15)(2000,32000)] 2017-05-02 22:07:08.836852: step 19000, loss = 0.1337, acc = 0.9625, f1neg = 0.9596, f1pos = 0.9650, f1 = 0.9623
[Eval_batch(16)(2000,34000)] 2017-05-02 22:07:09.276832: step 19000, loss = 0.1290, acc = 0.9580, f1neg = 0.9562, f1pos = 0.9597, f1 = 0.9579
[Eval_batch(17)(2000,36000)] 2017-05-02 22:07:09.782463: step 19000, loss = 0.1507, acc = 0.9555, f1neg = 0.9557, f1pos = 0.9553, f1 = 0.9555
[Eval_batch(18)(2000,38000)] 2017-05-02 22:07:10.227377: step 19000, loss = 0.1481, acc = 0.9515, f1neg = 0.9532, f1pos = 0.9497, f1 = 0.9514
[Eval_batch(19)(2000,40000)] 2017-05-02 22:07:10.690486: step 19000, loss = 0.1288, acc = 0.9665, f1neg = 0.9612, f1pos = 0.9705, f1 = 0.9659
[Eval_batch(20)(2000,42000)] 2017-05-02 22:07:11.164432: step 19000, loss = 0.1326, acc = 0.9595, f1neg = 0.9640, f1pos = 0.9537, f1 = 0.9589
[Eval_batch(21)(2000,44000)] 2017-05-02 22:07:11.635169: step 19000, loss = 0.1232, acc = 0.9605, f1neg = 0.9570, f1pos = 0.9635, f1 = 0.9602
[Eval_batch(22)(2000,46000)] 2017-05-02 22:07:12.112180: step 19000, loss = 0.1320, acc = 0.9580, f1neg = 0.9583, f1pos = 0.9577, f1 = 0.9580
[Eval_batch(23)(2000,48000)] 2017-05-02 22:07:12.587977: step 19000, loss = 0.1374, acc = 0.9540, f1neg = 0.9480, f1pos = 0.9588, f1 = 0.9534
[Eval_batch(24)(2000,50000)] 2017-05-02 22:07:13.044593: step 19000, loss = 0.1256, acc = 0.9595, f1neg = 0.9534, f1pos = 0.9642, f1 = 0.9588
[Eval_batch(25)(2000,52000)] 2017-05-02 22:07:13.507524: step 19000, loss = 0.1161, acc = 0.9650, f1neg = 0.9615, f1pos = 0.9679, f1 = 0.9647
[Eval_batch(26)(2000,54000)] 2017-05-02 22:07:13.976967: step 19000, loss = 0.1281, acc = 0.9560, f1neg = 0.9584, f1pos = 0.9533, f1 = 0.9559
[Eval_batch(27)(2000,56000)] 2017-05-02 22:07:14.563549: step 19000, loss = 0.1158, acc = 0.9655, f1neg = 0.9637, f1pos = 0.9671, f1 = 0.9654
[Eval] 2017-05-02 22:07:14.563634: step 19000, acc = 0.9573, f1 = 0.9568
[Test_batch(0)(2000,2000)] 2017-05-02 22:07:15.041165: step 19000, loss = 0.1651, acc = 0.9490, f1neg = 0.9514, f1pos = 0.9464, f1 = 0.9489
[Test_batch(1)(2000,4000)] 2017-05-02 22:07:15.511073: step 19000, loss = 0.1496, acc = 0.9505, f1neg = 0.9560, f1pos = 0.9435, f1 = 0.9497
[Test_batch(2)(2000,6000)] 2017-05-02 22:07:15.968733: step 19000, loss = 0.1618, acc = 0.9515, f1neg = 0.9554, f1pos = 0.9468, f1 = 0.9511
[Test_batch(3)(2000,8000)] 2017-05-02 22:07:16.442525: step 19000, loss = 0.1707, acc = 0.9395, f1neg = 0.9432, f1pos = 0.9353, f1 = 0.9392
[Test_batch(4)(2000,10000)] 2017-05-02 22:07:16.922387: step 19000, loss = 0.1711, acc = 0.9390, f1neg = 0.9389, f1pos = 0.9391, f1 = 0.9390
[Test_batch(5)(2000,12000)] 2017-05-02 22:07:17.391750: step 19000, loss = 0.1805, acc = 0.9350, f1neg = 0.9355, f1pos = 0.9345, f1 = 0.9350
[Test_batch(6)(2000,14000)] 2017-05-02 22:07:17.853668: step 19000, loss = 0.1663, acc = 0.9405, f1neg = 0.9383, f1pos = 0.9425, f1 = 0.9404
[Test_batch(7)(2000,16000)] 2017-05-02 22:07:18.329455: step 19000, loss = 0.1554, acc = 0.9480, f1neg = 0.9526, f1pos = 0.9424, f1 = 0.9475
[Test_batch(8)(2000,18000)] 2017-05-02 22:07:18.778254: step 19000, loss = 0.1557, acc = 0.9475, f1neg = 0.9469, f1pos = 0.9480, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-02 22:07:19.243815: step 19000, loss = 0.1860, acc = 0.9305, f1neg = 0.9267, f1pos = 0.9339, f1 = 0.9303
[Test_batch(10)(2000,22000)] 2017-05-02 22:07:19.715304: step 19000, loss = 0.1614, acc = 0.9465, f1neg = 0.9398, f1pos = 0.9519, f1 = 0.9458
[Test_batch(11)(2000,24000)] 2017-05-02 22:07:20.193026: step 19000, loss = 0.1622, acc = 0.9460, f1neg = 0.9473, f1pos = 0.9447, f1 = 0.9460
[Test_batch(12)(2000,26000)] 2017-05-02 22:07:20.674521: step 19000, loss = 0.1402, acc = 0.9565, f1neg = 0.9581, f1pos = 0.9548, f1 = 0.9564
[Test_batch(13)(2000,28000)] 2017-05-02 22:07:21.137811: step 19000, loss = 0.1685, acc = 0.9385, f1neg = 0.9315, f1pos = 0.9442, f1 = 0.9378
[Test_batch(14)(2000,30000)] 2017-05-02 22:07:21.619256: step 19000, loss = 0.1394, acc = 0.9550, f1neg = 0.9486, f1pos = 0.9600, f1 = 0.9543
[Test_batch(15)(2000,32000)] 2017-05-02 22:07:22.087058: step 19000, loss = 0.1594, acc = 0.9470, f1neg = 0.9463, f1pos = 0.9477, f1 = 0.9470
[Test_batch(16)(2000,34000)] 2017-05-02 22:07:22.555019: step 19000, loss = 0.1446, acc = 0.9540, f1neg = 0.9529, f1pos = 0.9551, f1 = 0.9540
[Test_batch(17)(2000,36000)] 2017-05-02 22:07:23.028880: step 19000, loss = 0.1360, acc = 0.9615, f1neg = 0.9580, f1pos = 0.9645, f1 = 0.9612
[Test_batch(18)(2000,38000)] 2017-05-02 22:07:23.553215: step 19000, loss = 0.1269, acc = 0.9585, f1neg = 0.9575, f1pos = 0.9594, f1 = 0.9585
[Test] 2017-05-02 22:07:23.553287: step 19000, acc = 0.9471, f1 = 0.9468
[Status] 2017-05-02 22:07:23.553305: step 19000, maxindex = 18000, maxdev = 0.9578, maxtst = 0.9479
2017-05-02 22:07:32.733841: step 19010, loss = 0.1530, acc = 0.9560 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 22:07:41.820603: step 19020, loss = 0.1692, acc = 0.9440 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 22:07:51.286911: step 19030, loss = 0.1382, acc = 0.9520 (285.0 examples/sec; 0.225 sec/batch)
2017-05-02 22:08:00.425955: step 19040, loss = 0.1270, acc = 0.9640 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 22:08:09.555235: step 19050, loss = 0.1456, acc = 0.9620 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 22:08:19.015537: step 19060, loss = 0.1415, acc = 0.9620 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 22:08:28.360683: step 19070, loss = 0.1423, acc = 0.9620 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 22:08:37.574016: step 19080, loss = 0.1660, acc = 0.9440 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:08:46.745516: step 19090, loss = 0.1739, acc = 0.9340 (270.1 examples/sec; 0.237 sec/batch)
2017-05-02 22:08:55.814735: step 19100, loss = 0.1752, acc = 0.9460 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 22:09:05.115781: step 19110, loss = 0.1506, acc = 0.9540 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 22:09:14.357671: step 19120, loss = 0.1593, acc = 0.9360 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 22:09:23.495695: step 19130, loss = 0.1399, acc = 0.9560 (287.3 examples/sec; 0.223 sec/batch)
2017-05-02 22:09:32.689890: step 19140, loss = 0.1499, acc = 0.9460 (259.8 examples/sec; 0.246 sec/batch)
2017-05-02 22:09:41.900537: step 19150, loss = 0.1704, acc = 0.9480 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:09:52.099649: step 19160, loss = 0.1399, acc = 0.9560 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 22:10:01.298366: step 19170, loss = 0.1493, acc = 0.9480 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 22:10:10.386613: step 19180, loss = 0.1339, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 22:10:19.547574: step 19190, loss = 0.1234, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-02 22:10:28.682942: step 19200, loss = 0.1262, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 22:10:37.875855: step 19210, loss = 0.1599, acc = 0.9420 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 22:10:47.230628: step 19220, loss = 0.1524, acc = 0.9500 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 22:10:56.487047: step 19230, loss = 0.1276, acc = 0.9580 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 22:11:05.536540: step 19240, loss = 0.1391, acc = 0.9500 (299.4 examples/sec; 0.214 sec/batch)
2017-05-02 22:11:14.643585: step 19250, loss = 0.1674, acc = 0.9360 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 22:11:23.973836: step 19260, loss = 0.1452, acc = 0.9620 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 22:11:33.219816: step 19270, loss = 0.1652, acc = 0.9540 (291.1 examples/sec; 0.220 sec/batch)
2017-05-02 22:11:42.355691: step 19280, loss = 0.1564, acc = 0.9520 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 22:11:51.415763: step 19290, loss = 0.1646, acc = 0.9520 (278.6 examples/sec; 0.230 sec/batch)
2017-05-02 22:12:00.437517: step 19300, loss = 0.1552, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 22:12:09.649809: step 19310, loss = 0.1305, acc = 0.9560 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 22:12:18.715308: step 19320, loss = 0.1434, acc = 0.9580 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 22:12:27.842581: step 19330, loss = 0.1546, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 22:12:37.265251: step 19340, loss = 0.1345, acc = 0.9560 (266.2 examples/sec; 0.240 sec/batch)
2017-05-02 22:12:46.629222: step 19350, loss = 0.1511, acc = 0.9560 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 22:12:55.840491: step 19360, loss = 0.1527, acc = 0.9460 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 22:13:05.052524: step 19370, loss = 0.1325, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 22:13:14.119288: step 19380, loss = 0.1645, acc = 0.9420 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 22:13:23.166702: step 19390, loss = 0.1363, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 22:13:32.308919: step 19400, loss = 0.1404, acc = 0.9520 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 22:13:42.023124: step 19410, loss = 0.1288, acc = 0.9680 (267.4 examples/sec; 0.239 sec/batch)
2017-05-02 22:13:51.382921: step 19420, loss = 0.1839, acc = 0.9340 (262.6 examples/sec; 0.244 sec/batch)
2017-05-02 22:14:00.722640: step 19430, loss = 0.1495, acc = 0.9540 (254.2 examples/sec; 0.252 sec/batch)
2017-05-02 22:14:09.951756: step 19440, loss = 0.1633, acc = 0.9420 (253.3 examples/sec; 0.253 sec/batch)
2017-05-02 22:14:19.090217: step 19450, loss = 0.1767, acc = 0.9480 (269.4 examples/sec; 0.238 sec/batch)
2017-05-02 22:14:28.217949: step 19460, loss = 0.1770, acc = 0.9320 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 22:14:37.485721: step 19470, loss = 0.1697, acc = 0.9420 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 22:14:46.769762: step 19480, loss = 0.1393, acc = 0.9420 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 22:14:55.899780: step 19490, loss = 0.1685, acc = 0.9320 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 22:15:05.177502: step 19500, loss = 0.1265, acc = 0.9660 (272.4 examples/sec; 0.235 sec/batch)
2017-05-02 22:15:14.499984: step 19510, loss = 0.1408, acc = 0.9540 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 22:15:23.659950: step 19520, loss = 0.1791, acc = 0.9460 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 22:15:32.977884: step 19530, loss = 0.1664, acc = 0.9400 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 22:15:42.157844: step 19540, loss = 0.1248, acc = 0.9680 (267.0 examples/sec; 0.240 sec/batch)
2017-05-02 22:15:51.390595: step 19550, loss = 0.1681, acc = 0.9420 (279.5 examples/sec; 0.229 sec/batch)
2017-05-02 22:16:00.443514: step 19560, loss = 0.1294, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 22:16:09.528230: step 19570, loss = 0.1534, acc = 0.9420 (284.8 examples/sec; 0.225 sec/batch)
2017-05-02 22:16:18.605059: step 19580, loss = 0.1349, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 22:16:27.947666: step 19590, loss = 0.1339, acc = 0.9500 (270.4 examples/sec; 0.237 sec/batch)
2017-05-02 22:16:37.128261: step 19600, loss = 0.1695, acc = 0.9420 (290.8 examples/sec; 0.220 sec/batch)
2017-05-02 22:16:46.273991: step 19610, loss = 0.1467, acc = 0.9540 (285.1 examples/sec; 0.225 sec/batch)
2017-05-02 22:16:55.449036: step 19620, loss = 0.1454, acc = 0.9560 (290.1 examples/sec; 0.221 sec/batch)
2017-05-02 22:17:04.677251: step 19630, loss = 0.1533, acc = 0.9460 (269.1 examples/sec; 0.238 sec/batch)
2017-05-02 22:17:13.957471: step 19640, loss = 0.1419, acc = 0.9560 (265.9 examples/sec; 0.241 sec/batch)
2017-05-02 22:17:23.297902: step 19650, loss = 0.1534, acc = 0.9500 (267.4 examples/sec; 0.239 sec/batch)
2017-05-02 22:17:32.348376: step 19660, loss = 0.1859, acc = 0.9420 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 22:17:41.472072: step 19670, loss = 0.1550, acc = 0.9420 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 22:17:50.716784: step 19680, loss = 0.1455, acc = 0.9460 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 22:17:59.787218: step 19690, loss = 0.1455, acc = 0.9620 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 22:18:08.883119: step 19700, loss = 0.1459, acc = 0.9520 (259.9 examples/sec; 0.246 sec/batch)
2017-05-02 22:18:18.115230: step 19710, loss = 0.1457, acc = 0.9600 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:18:27.795474: step 19720, loss = 0.1583, acc = 0.9560 (290.3 examples/sec; 0.220 sec/batch)
2017-05-02 22:18:36.956964: step 19730, loss = 0.1875, acc = 0.9400 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 22:18:46.205073: step 19740, loss = 0.1814, acc = 0.9420 (253.8 examples/sec; 0.252 sec/batch)
2017-05-02 22:18:55.331467: step 19750, loss = 0.1521, acc = 0.9540 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 22:19:04.387293: step 19760, loss = 0.1624, acc = 0.9500 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 22:19:13.312364: step 19770, loss = 0.1385, acc = 0.9560 (301.9 examples/sec; 0.212 sec/batch)
2017-05-02 22:19:22.475671: step 19780, loss = 0.1430, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 22:19:31.614958: step 19790, loss = 0.1733, acc = 0.9320 (285.1 examples/sec; 0.224 sec/batch)
2017-05-02 22:19:40.771688: step 19800, loss = 0.1801, acc = 0.9320 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 22:19:49.948795: step 19810, loss = 0.1371, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 22:19:59.268854: step 19820, loss = 0.1560, acc = 0.9420 (281.0 examples/sec; 0.228 sec/batch)
2017-05-02 22:20:08.353049: step 19830, loss = 0.1585, acc = 0.9500 (279.6 examples/sec; 0.229 sec/batch)
2017-05-02 22:20:17.567875: step 19840, loss = 0.1499, acc = 0.9560 (278.9 examples/sec; 0.230 sec/batch)
2017-05-02 22:20:26.630187: step 19850, loss = 0.1443, acc = 0.9620 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 22:20:35.845967: step 19860, loss = 0.1696, acc = 0.9420 (253.2 examples/sec; 0.253 sec/batch)
2017-05-02 22:20:45.181684: step 19870, loss = 0.1430, acc = 0.9520 (270.1 examples/sec; 0.237 sec/batch)
2017-05-02 22:20:54.266515: step 19880, loss = 0.1246, acc = 0.9740 (291.0 examples/sec; 0.220 sec/batch)
2017-05-02 22:21:03.363575: step 19890, loss = 0.1469, acc = 0.9520 (263.4 examples/sec; 0.243 sec/batch)
2017-05-02 22:21:12.577740: step 19900, loss = 0.1696, acc = 0.9460 (266.2 examples/sec; 0.240 sec/batch)
2017-05-02 22:21:21.936145: step 19910, loss = 0.1460, acc = 0.9600 (266.5 examples/sec; 0.240 sec/batch)
2017-05-02 22:21:31.259878: step 19920, loss = 0.1274, acc = 0.9600 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:21:40.420434: step 19930, loss = 0.1444, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 22:21:49.633095: step 19940, loss = 0.1521, acc = 0.9480 (273.4 examples/sec; 0.234 sec/batch)
2017-05-02 22:21:58.693672: step 19950, loss = 0.1661, acc = 0.9460 (291.7 examples/sec; 0.219 sec/batch)
2017-05-02 22:22:07.761729: step 19960, loss = 0.1640, acc = 0.9420 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 22:22:16.793801: step 19970, loss = 0.1584, acc = 0.9540 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 22:22:26.040948: step 19980, loss = 0.1266, acc = 0.9720 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 22:22:35.326934: step 19990, loss = 0.1551, acc = 0.9400 (290.2 examples/sec; 0.221 sec/batch)
2017-05-02 22:22:44.560454: step 20000, loss = 0.1486, acc = 0.9580 (276.6 examples/sec; 0.231 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 22:22:45.020362: step 20000, loss = 0.1260, acc = 0.9550, f1neg = 0.9511, f1pos = 0.9583, f1 = 0.9547
[Eval_batch(1)(2000,4000)] 2017-05-02 22:22:45.464157: step 20000, loss = 0.1307, acc = 0.9610, f1neg = 0.9551, f1pos = 0.9655, f1 = 0.9603
[Eval_batch(2)(2000,6000)] 2017-05-02 22:22:45.944114: step 20000, loss = 0.1396, acc = 0.9560, f1neg = 0.9536, f1pos = 0.9581, f1 = 0.9559
[Eval_batch(3)(2000,8000)] 2017-05-02 22:22:46.428552: step 20000, loss = 0.1460, acc = 0.9515, f1neg = 0.9521, f1pos = 0.9508, f1 = 0.9515
[Eval_batch(4)(2000,10000)] 2017-05-02 22:22:46.927674: step 20000, loss = 0.1451, acc = 0.9510, f1neg = 0.9528, f1pos = 0.9490, f1 = 0.9509
[Eval_batch(5)(2000,12000)] 2017-05-02 22:22:47.407837: step 20000, loss = 0.1422, acc = 0.9490, f1neg = 0.9459, f1pos = 0.9518, f1 = 0.9488
[Eval_batch(6)(2000,14000)] 2017-05-02 22:22:47.867376: step 20000, loss = 0.1315, acc = 0.9610, f1neg = 0.9605, f1pos = 0.9615, f1 = 0.9610
[Eval_batch(7)(2000,16000)] 2017-05-02 22:22:48.370654: step 20000, loss = 0.1330, acc = 0.9550, f1neg = 0.9468, f1pos = 0.9610, f1 = 0.9539
[Eval_batch(8)(2000,18000)] 2017-05-02 22:22:48.837549: step 20000, loss = 0.1287, acc = 0.9555, f1neg = 0.9498, f1pos = 0.9600, f1 = 0.9549
[Eval_batch(9)(2000,20000)] 2017-05-02 22:22:49.316942: step 20000, loss = 0.1315, acc = 0.9590, f1neg = 0.9565, f1pos = 0.9612, f1 = 0.9589
[Eval_batch(10)(2000,22000)] 2017-05-02 22:22:49.830202: step 20000, loss = 0.1419, acc = 0.9560, f1neg = 0.9534, f1pos = 0.9583, f1 = 0.9559
[Eval_batch(11)(2000,24000)] 2017-05-02 22:22:50.311208: step 20000, loss = 0.1417, acc = 0.9565, f1neg = 0.9502, f1pos = 0.9614, f1 = 0.9558
[Eval_batch(12)(2000,26000)] 2017-05-02 22:22:50.797337: step 20000, loss = 0.1379, acc = 0.9515, f1neg = 0.9506, f1pos = 0.9524, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-02 22:22:51.297733: step 20000, loss = 0.1260, acc = 0.9630, f1neg = 0.9540, f1pos = 0.9690, f1 = 0.9615
[Eval_batch(14)(2000,30000)] 2017-05-02 22:22:51.780553: step 20000, loss = 0.1520, acc = 0.9510, f1neg = 0.9419, f1pos = 0.9576, f1 = 0.9498
[Eval_batch(15)(2000,32000)] 2017-05-02 22:22:52.253605: step 20000, loss = 0.1335, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9598
[Eval_batch(16)(2000,34000)] 2017-05-02 22:22:52.765173: step 20000, loss = 0.1283, acc = 0.9605, f1neg = 0.9592, f1pos = 0.9617, f1 = 0.9605
[Eval_batch(17)(2000,36000)] 2017-05-02 22:22:53.235662: step 20000, loss = 0.1486, acc = 0.9535, f1neg = 0.9540, f1pos = 0.9530, f1 = 0.9535
[Eval_batch(18)(2000,38000)] 2017-05-02 22:22:53.746788: step 20000, loss = 0.1448, acc = 0.9510, f1neg = 0.9532, f1pos = 0.9486, f1 = 0.9509
[Eval_batch(19)(2000,40000)] 2017-05-02 22:22:54.225284: step 20000, loss = 0.1281, acc = 0.9655, f1neg = 0.9604, f1pos = 0.9694, f1 = 0.9649
[Eval_batch(20)(2000,42000)] 2017-05-02 22:22:54.730648: step 20000, loss = 0.1267, acc = 0.9595, f1neg = 0.9643, f1pos = 0.9532, f1 = 0.9588
[Eval_batch(21)(2000,44000)] 2017-05-02 22:22:55.238196: step 20000, loss = 0.1255, acc = 0.9560, f1neg = 0.9526, f1pos = 0.9589, f1 = 0.9558
[Eval_batch(22)(2000,46000)] 2017-05-02 22:22:55.734416: step 20000, loss = 0.1300, acc = 0.9565, f1neg = 0.9572, f1pos = 0.9558, f1 = 0.9565
[Eval_batch(23)(2000,48000)] 2017-05-02 22:22:56.237030: step 20000, loss = 0.1376, acc = 0.9540, f1neg = 0.9485, f1pos = 0.9584, f1 = 0.9535
[Eval_batch(24)(2000,50000)] 2017-05-02 22:22:56.720670: step 20000, loss = 0.1248, acc = 0.9605, f1neg = 0.9551, f1pos = 0.9647, f1 = 0.9599
[Eval_batch(25)(2000,52000)] 2017-05-02 22:22:57.192347: step 20000, loss = 0.1147, acc = 0.9660, f1neg = 0.9630, f1pos = 0.9685, f1 = 0.9658
[Eval_batch(26)(2000,54000)] 2017-05-02 22:22:57.665259: step 20000, loss = 0.1240, acc = 0.9595, f1neg = 0.9620, f1pos = 0.9566, f1 = 0.9593
[Eval_batch(27)(2000,56000)] 2017-05-02 22:22:58.265604: step 20000, loss = 0.1129, acc = 0.9680, f1neg = 0.9666, f1pos = 0.9693, f1 = 0.9679
[Eval] 2017-05-02 22:22:58.265695: step 20000, acc = 0.9572, f1 = 0.9569
[Test_batch(0)(2000,2000)] 2017-05-02 22:22:58.751846: step 20000, loss = 0.1624, acc = 0.9515, f1neg = 0.9544, f1pos = 0.9482, f1 = 0.9513
[Test_batch(1)(2000,4000)] 2017-05-02 22:22:59.226005: step 20000, loss = 0.1466, acc = 0.9505, f1neg = 0.9565, f1pos = 0.9426, f1 = 0.9495
[Test_batch(2)(2000,6000)] 2017-05-02 22:22:59.724756: step 20000, loss = 0.1586, acc = 0.9525, f1neg = 0.9567, f1pos = 0.9474, f1 = 0.9521
[Test_batch(3)(2000,8000)] 2017-05-02 22:23:00.219284: step 20000, loss = 0.1689, acc = 0.9405, f1neg = 0.9450, f1pos = 0.9351, f1 = 0.9401
[Test_batch(4)(2000,10000)] 2017-05-02 22:23:00.727985: step 20000, loss = 0.1700, acc = 0.9430, f1neg = 0.9437, f1pos = 0.9422, f1 = 0.9430
[Test_batch(5)(2000,12000)] 2017-05-02 22:23:01.241188: step 20000, loss = 0.1787, acc = 0.9390, f1neg = 0.9404, f1pos = 0.9375, f1 = 0.9390
[Test_batch(6)(2000,14000)] 2017-05-02 22:23:01.745377: step 20000, loss = 0.1630, acc = 0.9455, f1neg = 0.9441, f1pos = 0.9468, f1 = 0.9455
[Test_batch(7)(2000,16000)] 2017-05-02 22:23:02.269722: step 20000, loss = 0.1529, acc = 0.9490, f1neg = 0.9542, f1pos = 0.9424, f1 = 0.9483
[Test_batch(8)(2000,18000)] 2017-05-02 22:23:02.766817: step 20000, loss = 0.1547, acc = 0.9490, f1neg = 0.9490, f1pos = 0.9490, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-02 22:23:03.274909: step 20000, loss = 0.1874, acc = 0.9285, f1neg = 0.9258, f1pos = 0.9310, f1 = 0.9284
[Test_batch(10)(2000,22000)] 2017-05-02 22:23:03.766359: step 20000, loss = 0.1649, acc = 0.9455, f1neg = 0.9397, f1pos = 0.9503, f1 = 0.9450
[Test_batch(11)(2000,24000)] 2017-05-02 22:23:04.247960: step 20000, loss = 0.1602, acc = 0.9460, f1neg = 0.9479, f1pos = 0.9439, f1 = 0.9459
[Test_batch(12)(2000,26000)] 2017-05-02 22:23:04.754965: step 20000, loss = 0.1416, acc = 0.9575, f1neg = 0.9594, f1pos = 0.9554, f1 = 0.9574
[Test_batch(13)(2000,28000)] 2017-05-02 22:23:05.257255: step 20000, loss = 0.1694, acc = 0.9410, f1neg = 0.9353, f1pos = 0.9458, f1 = 0.9405
[Test_batch(14)(2000,30000)] 2017-05-02 22:23:05.764244: step 20000, loss = 0.1417, acc = 0.9545, f1neg = 0.9487, f1pos = 0.9591, f1 = 0.9539
[Test_batch(15)(2000,32000)] 2017-05-02 22:23:06.264814: step 20000, loss = 0.1599, acc = 0.9490, f1neg = 0.9488, f1pos = 0.9492, f1 = 0.9490
[Test_batch(16)(2000,34000)] 2017-05-02 22:23:06.743580: step 20000, loss = 0.1432, acc = 0.9560, f1neg = 0.9554, f1pos = 0.9566, f1 = 0.9560
[Test_batch(17)(2000,36000)] 2017-05-02 22:23:07.248581: step 20000, loss = 0.1341, acc = 0.9585, f1neg = 0.9550, f1pos = 0.9615, f1 = 0.9582
[Test_batch(18)(2000,38000)] 2017-05-02 22:23:07.782431: step 20000, loss = 0.1230, acc = 0.9635, f1neg = 0.9630, f1pos = 0.9640, f1 = 0.9635
[Test] 2017-05-02 22:23:07.782526: step 20000, acc = 0.9484, f1 = 0.9482
[Status] 2017-05-02 22:23:07.782551: step 20000, maxindex = 18000, maxdev = 0.9578, maxtst = 0.9479
2017-05-02 22:23:16.988611: step 20010, loss = 0.1533, acc = 0.9560 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 22:23:26.470942: step 20020, loss = 0.1570, acc = 0.9520 (268.4 examples/sec; 0.238 sec/batch)
2017-05-02 22:23:35.654328: step 20030, loss = 0.1681, acc = 0.9520 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 22:23:44.921594: step 20040, loss = 0.1471, acc = 0.9520 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 22:23:54.042220: step 20050, loss = 0.1425, acc = 0.9540 (269.5 examples/sec; 0.237 sec/batch)
2017-05-02 22:24:03.179593: step 20060, loss = 0.1365, acc = 0.9620 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 22:24:12.542811: step 20070, loss = 0.2089, acc = 0.9200 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 22:24:21.709966: step 20080, loss = 0.1893, acc = 0.9360 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 22:24:30.794089: step 20090, loss = 0.1531, acc = 0.9520 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 22:24:39.885093: step 20100, loss = 0.1336, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 22:24:49.017551: step 20110, loss = 0.1729, acc = 0.9540 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 22:24:58.177304: step 20120, loss = 0.1447, acc = 0.9560 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 22:25:07.352021: step 20130, loss = 0.1494, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-02 22:25:16.434026: step 20140, loss = 0.1530, acc = 0.9500 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 22:25:25.646994: step 20150, loss = 0.1430, acc = 0.9520 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 22:25:35.627521: step 20160, loss = 0.1655, acc = 0.9420 (263.6 examples/sec; 0.243 sec/batch)
2017-05-02 22:25:44.886295: step 20170, loss = 0.1844, acc = 0.9340 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 22:25:54.006889: step 20180, loss = 0.1406, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 22:26:03.245135: step 20190, loss = 0.1743, acc = 0.9240 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 22:26:12.504712: step 20200, loss = 0.1294, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 22:26:21.605915: step 20210, loss = 0.1519, acc = 0.9580 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 22:26:30.668364: step 20220, loss = 0.1806, acc = 0.9440 (293.9 examples/sec; 0.218 sec/batch)
2017-05-02 22:26:39.853277: step 20230, loss = 0.1418, acc = 0.9480 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 22:26:49.083609: step 20240, loss = 0.1680, acc = 0.9440 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:26:58.140414: step 20250, loss = 0.1248, acc = 0.9640 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 22:27:07.345785: step 20260, loss = 0.1273, acc = 0.9680 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 22:27:16.449101: step 20270, loss = 0.1602, acc = 0.9380 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 22:27:25.756808: step 20280, loss = 0.1409, acc = 0.9500 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 22:27:34.753187: step 20290, loss = 0.1787, acc = 0.9380 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 22:27:44.266654: step 20300, loss = 0.1756, acc = 0.9280 (261.7 examples/sec; 0.245 sec/batch)
2017-05-02 22:27:53.346828: step 20310, loss = 0.1627, acc = 0.9440 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 22:28:02.523479: step 20320, loss = 0.1623, acc = 0.9520 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 22:28:11.663556: step 20330, loss = 0.1217, acc = 0.9720 (267.6 examples/sec; 0.239 sec/batch)
2017-05-02 22:28:21.146670: step 20340, loss = 0.1341, acc = 0.9560 (285.8 examples/sec; 0.224 sec/batch)
2017-05-02 22:28:30.434059: step 20350, loss = 0.1659, acc = 0.9420 (282.6 examples/sec; 0.227 sec/batch)
2017-05-02 22:28:39.568641: step 20360, loss = 0.1354, acc = 0.9480 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 22:28:48.664214: step 20370, loss = 0.1610, acc = 0.9460 (286.4 examples/sec; 0.223 sec/batch)
2017-05-02 22:28:57.772110: step 20380, loss = 0.1953, acc = 0.9300 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 22:29:06.950306: step 20390, loss = 0.1410, acc = 0.9600 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 22:29:16.115603: step 20400, loss = 0.1922, acc = 0.9300 (270.9 examples/sec; 0.236 sec/batch)
2017-05-02 22:29:25.065513: step 20410, loss = 0.1882, acc = 0.9360 (292.4 examples/sec; 0.219 sec/batch)
2017-05-02 22:29:34.214146: step 20420, loss = 0.1639, acc = 0.9440 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 22:29:43.412990: step 20430, loss = 0.1518, acc = 0.9440 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 22:29:52.611006: step 20440, loss = 0.1428, acc = 0.9520 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 22:30:01.634887: step 20450, loss = 0.1446, acc = 0.9500 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 22:30:10.797357: step 20460, loss = 0.1495, acc = 0.9580 (286.1 examples/sec; 0.224 sec/batch)
2017-05-02 22:30:19.853996: step 20470, loss = 0.1788, acc = 0.9340 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 22:30:29.385270: step 20480, loss = 0.1814, acc = 0.9500 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 22:30:38.570092: step 20490, loss = 0.1435, acc = 0.9440 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 22:30:47.801879: step 20500, loss = 0.1594, acc = 0.9460 (288.8 examples/sec; 0.222 sec/batch)
2017-05-02 22:30:57.075189: step 20510, loss = 0.1520, acc = 0.9480 (283.8 examples/sec; 0.226 sec/batch)
2017-05-02 22:31:06.288700: step 20520, loss = 0.1808, acc = 0.9380 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 22:31:15.601031: step 20530, loss = 0.1215, acc = 0.9680 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 22:31:24.764663: step 20540, loss = 0.1592, acc = 0.9460 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 22:31:34.012790: step 20550, loss = 0.1595, acc = 0.9500 (290.3 examples/sec; 0.220 sec/batch)
2017-05-02 22:31:43.227798: step 20560, loss = 0.1871, acc = 0.9340 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 22:31:52.589996: step 20570, loss = 0.1461, acc = 0.9560 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 22:32:01.694414: step 20580, loss = 0.1536, acc = 0.9460 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 22:32:10.836643: step 20590, loss = 0.1555, acc = 0.9500 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 22:32:20.142412: step 20600, loss = 0.1242, acc = 0.9640 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 22:32:29.110150: step 20610, loss = 0.1419, acc = 0.9580 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 22:32:38.284489: step 20620, loss = 0.1364, acc = 0.9620 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 22:32:47.525173: step 20630, loss = 0.1385, acc = 0.9600 (263.9 examples/sec; 0.243 sec/batch)
2017-05-02 22:32:56.702586: step 20640, loss = 0.1579, acc = 0.9600 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 22:33:05.854356: step 20650, loss = 0.1266, acc = 0.9580 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 22:33:15.085041: step 20660, loss = 0.1724, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 22:33:24.248465: step 20670, loss = 0.1221, acc = 0.9640 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 22:33:33.446245: step 20680, loss = 0.1960, acc = 0.9300 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 22:33:42.485071: step 20690, loss = 0.1679, acc = 0.9400 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 22:33:51.563230: step 20700, loss = 0.1510, acc = 0.9480 (292.7 examples/sec; 0.219 sec/batch)
2017-05-02 22:34:00.623426: step 20710, loss = 0.1339, acc = 0.9500 (285.6 examples/sec; 0.224 sec/batch)
2017-05-02 22:34:09.809103: step 20720, loss = 0.1622, acc = 0.9460 (264.5 examples/sec; 0.242 sec/batch)
2017-05-02 22:34:18.887375: step 20730, loss = 0.2018, acc = 0.9420 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 22:34:28.178931: step 20740, loss = 0.1432, acc = 0.9600 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 22:34:37.248369: step 20750, loss = 0.1596, acc = 0.9460 (291.7 examples/sec; 0.219 sec/batch)
2017-05-02 22:34:46.385898: step 20760, loss = 0.1715, acc = 0.9400 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 22:34:55.674818: step 20770, loss = 0.1479, acc = 0.9560 (285.8 examples/sec; 0.224 sec/batch)
2017-05-02 22:35:05.115948: step 20780, loss = 0.1452, acc = 0.9440 (296.4 examples/sec; 0.216 sec/batch)
2017-05-02 22:35:14.124995: step 20790, loss = 0.1597, acc = 0.9360 (288.7 examples/sec; 0.222 sec/batch)
2017-05-02 22:35:23.167240: step 20800, loss = 0.1507, acc = 0.9540 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 22:35:32.226221: step 20810, loss = 0.1578, acc = 0.9500 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 22:35:41.304040: step 20820, loss = 0.1321, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 22:35:50.431329: step 20830, loss = 0.1297, acc = 0.9660 (273.7 examples/sec; 0.234 sec/batch)
2017-05-02 22:35:59.541656: step 20840, loss = 0.1655, acc = 0.9400 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 22:36:08.648429: step 20850, loss = 0.1542, acc = 0.9460 (287.1 examples/sec; 0.223 sec/batch)
2017-05-02 22:36:17.794373: step 20860, loss = 0.1360, acc = 0.9560 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 22:36:26.993405: step 20870, loss = 0.1562, acc = 0.9440 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 22:36:36.172656: step 20880, loss = 0.1447, acc = 0.9500 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 22:36:45.349705: step 20890, loss = 0.1580, acc = 0.9400 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 22:36:54.668475: step 20900, loss = 0.1292, acc = 0.9660 (274.0 examples/sec; 0.234 sec/batch)
2017-05-02 22:37:03.786028: step 20910, loss = 0.1390, acc = 0.9620 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 22:37:12.885043: step 20920, loss = 0.1868, acc = 0.9400 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 22:37:22.339200: step 20930, loss = 0.1341, acc = 0.9580 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 22:37:31.363033: step 20940, loss = 0.1439, acc = 0.9500 (290.1 examples/sec; 0.221 sec/batch)
2017-05-02 22:37:40.501752: step 20950, loss = 0.1561, acc = 0.9600 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 22:37:49.759643: step 20960, loss = 0.1415, acc = 0.9560 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 22:37:58.974105: step 20970, loss = 0.1373, acc = 0.9580 (297.7 examples/sec; 0.215 sec/batch)
2017-05-02 22:38:08.073609: step 20980, loss = 0.1475, acc = 0.9500 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 22:38:17.214019: step 20990, loss = 0.1347, acc = 0.9560 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 22:38:26.318963: step 21000, loss = 0.1518, acc = 0.9480 (272.6 examples/sec; 0.235 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 22:38:26.824018: step 21000, loss = 0.1241, acc = 0.9565, f1neg = 0.9527, f1pos = 0.9597, f1 = 0.9562
[Eval_batch(1)(2000,4000)] 2017-05-02 22:38:27.334134: step 21000, loss = 0.1305, acc = 0.9615, f1neg = 0.9558, f1pos = 0.9659, f1 = 0.9609
[Eval_batch(2)(2000,6000)] 2017-05-02 22:38:27.838440: step 21000, loss = 0.1391, acc = 0.9540, f1neg = 0.9516, f1pos = 0.9561, f1 = 0.9539
[Eval_batch(3)(2000,8000)] 2017-05-02 22:38:28.343595: step 21000, loss = 0.1459, acc = 0.9500, f1neg = 0.9506, f1pos = 0.9494, f1 = 0.9500
[Eval_batch(4)(2000,10000)] 2017-05-02 22:38:28.818377: step 21000, loss = 0.1444, acc = 0.9520, f1neg = 0.9538, f1pos = 0.9501, f1 = 0.9519
[Eval_batch(5)(2000,12000)] 2017-05-02 22:38:29.292395: step 21000, loss = 0.1427, acc = 0.9495, f1neg = 0.9465, f1pos = 0.9522, f1 = 0.9493
[Eval_batch(6)(2000,14000)] 2017-05-02 22:38:29.769602: step 21000, loss = 0.1295, acc = 0.9610, f1neg = 0.9605, f1pos = 0.9615, f1 = 0.9610
[Eval_batch(7)(2000,16000)] 2017-05-02 22:38:30.232708: step 21000, loss = 0.1325, acc = 0.9550, f1neg = 0.9467, f1pos = 0.9610, f1 = 0.9539
[Eval_batch(8)(2000,18000)] 2017-05-02 22:38:30.694912: step 21000, loss = 0.1277, acc = 0.9575, f1neg = 0.9521, f1pos = 0.9618, f1 = 0.9570
[Eval_batch(9)(2000,20000)] 2017-05-02 22:38:31.166278: step 21000, loss = 0.1310, acc = 0.9590, f1neg = 0.9566, f1pos = 0.9612, f1 = 0.9589
[Eval_batch(10)(2000,22000)] 2017-05-02 22:38:31.648644: step 21000, loss = 0.1424, acc = 0.9555, f1neg = 0.9529, f1pos = 0.9578, f1 = 0.9554
[Eval_batch(11)(2000,24000)] 2017-05-02 22:38:32.111451: step 21000, loss = 0.1416, acc = 0.9560, f1neg = 0.9497, f1pos = 0.9609, f1 = 0.9553
[Eval_batch(12)(2000,26000)] 2017-05-02 22:38:32.590396: step 21000, loss = 0.1387, acc = 0.9510, f1neg = 0.9503, f1pos = 0.9517, f1 = 0.9510
[Eval_batch(13)(2000,28000)] 2017-05-02 22:38:33.064635: step 21000, loss = 0.1251, acc = 0.9645, f1neg = 0.9559, f1pos = 0.9703, f1 = 0.9631
[Eval_batch(14)(2000,30000)] 2017-05-02 22:38:33.539443: step 21000, loss = 0.1516, acc = 0.9530, f1neg = 0.9446, f1pos = 0.9592, f1 = 0.9519
[Eval_batch(15)(2000,32000)] 2017-05-02 22:38:34.013852: step 21000, loss = 0.1333, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9598
[Eval_batch(16)(2000,34000)] 2017-05-02 22:38:34.477432: step 21000, loss = 0.1280, acc = 0.9595, f1neg = 0.9581, f1pos = 0.9608, f1 = 0.9595
[Eval_batch(17)(2000,36000)] 2017-05-02 22:38:34.954662: step 21000, loss = 0.1487, acc = 0.9535, f1neg = 0.9541, f1pos = 0.9529, f1 = 0.9535
[Eval_batch(18)(2000,38000)] 2017-05-02 22:38:35.414528: step 21000, loss = 0.1437, acc = 0.9500, f1neg = 0.9523, f1pos = 0.9475, f1 = 0.9499
[Eval_batch(19)(2000,40000)] 2017-05-02 22:38:35.869042: step 21000, loss = 0.1279, acc = 0.9655, f1neg = 0.9605, f1pos = 0.9694, f1 = 0.9649
[Eval_batch(20)(2000,42000)] 2017-05-02 22:38:36.314577: step 21000, loss = 0.1266, acc = 0.9580, f1neg = 0.9630, f1pos = 0.9514, f1 = 0.9572
[Eval_batch(21)(2000,44000)] 2017-05-02 22:38:36.782431: step 21000, loss = 0.1258, acc = 0.9560, f1neg = 0.9526, f1pos = 0.9590, f1 = 0.9558
[Eval_batch(22)(2000,46000)] 2017-05-02 22:38:37.251477: step 21000, loss = 0.1289, acc = 0.9580, f1neg = 0.9587, f1pos = 0.9572, f1 = 0.9580
[Eval_batch(23)(2000,48000)] 2017-05-02 22:38:37.716391: step 21000, loss = 0.1377, acc = 0.9520, f1neg = 0.9464, f1pos = 0.9565, f1 = 0.9515
[Eval_batch(24)(2000,50000)] 2017-05-02 22:38:38.188740: step 21000, loss = 0.1242, acc = 0.9610, f1neg = 0.9557, f1pos = 0.9651, f1 = 0.9604
[Eval_batch(25)(2000,52000)] 2017-05-02 22:38:38.664522: step 21000, loss = 0.1141, acc = 0.9670, f1neg = 0.9642, f1pos = 0.9694, f1 = 0.9668
[Eval_batch(26)(2000,54000)] 2017-05-02 22:38:39.129537: step 21000, loss = 0.1235, acc = 0.9585, f1neg = 0.9612, f1pos = 0.9554, f1 = 0.9583
[Eval_batch(27)(2000,56000)] 2017-05-02 22:38:39.684425: step 21000, loss = 0.1116, acc = 0.9670, f1neg = 0.9656, f1pos = 0.9683, f1 = 0.9669
[Eval] 2017-05-02 22:38:39.684497: step 21000, acc = 0.9572, f1 = 0.9569
[Test_batch(0)(2000,2000)] 2017-05-02 22:38:40.142071: step 21000, loss = 0.1612, acc = 0.9535, f1neg = 0.9563, f1pos = 0.9503, f1 = 0.9533
[Test_batch(1)(2000,4000)] 2017-05-02 22:38:40.610136: step 21000, loss = 0.1459, acc = 0.9530, f1neg = 0.9586, f1pos = 0.9457, f1 = 0.9521
[Test_batch(2)(2000,6000)] 2017-05-02 22:38:41.076999: step 21000, loss = 0.1576, acc = 0.9530, f1neg = 0.9572, f1pos = 0.9478, f1 = 0.9525
[Test_batch(3)(2000,8000)] 2017-05-02 22:38:41.529671: step 21000, loss = 0.1681, acc = 0.9395, f1neg = 0.9441, f1pos = 0.9341, f1 = 0.9391
[Test_batch(4)(2000,10000)] 2017-05-02 22:38:42.002066: step 21000, loss = 0.1699, acc = 0.9400, f1neg = 0.9409, f1pos = 0.9390, f1 = 0.9400
[Test_batch(5)(2000,12000)] 2017-05-02 22:38:42.472600: step 21000, loss = 0.1798, acc = 0.9365, f1neg = 0.9381, f1pos = 0.9348, f1 = 0.9365
[Test_batch(6)(2000,14000)] 2017-05-02 22:38:42.917962: step 21000, loss = 0.1629, acc = 0.9470, f1neg = 0.9458, f1pos = 0.9482, f1 = 0.9470
[Test_batch(7)(2000,16000)] 2017-05-02 22:38:43.385191: step 21000, loss = 0.1519, acc = 0.9510, f1neg = 0.9560, f1pos = 0.9447, f1 = 0.9504
[Test_batch(8)(2000,18000)] 2017-05-02 22:38:43.864342: step 21000, loss = 0.1547, acc = 0.9490, f1neg = 0.9489, f1pos = 0.9491, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-02 22:38:44.334983: step 21000, loss = 0.1883, acc = 0.9315, f1neg = 0.9288, f1pos = 0.9340, f1 = 0.9314
[Test_batch(10)(2000,22000)] 2017-05-02 22:38:44.816559: step 21000, loss = 0.1657, acc = 0.9465, f1neg = 0.9407, f1pos = 0.9513, f1 = 0.9460
[Test_batch(11)(2000,24000)] 2017-05-02 22:38:45.294305: step 21000, loss = 0.1594, acc = 0.9485, f1neg = 0.9504, f1pos = 0.9464, f1 = 0.9484
[Test_batch(12)(2000,26000)] 2017-05-02 22:38:45.744276: step 21000, loss = 0.1406, acc = 0.9570, f1neg = 0.9589, f1pos = 0.9549, f1 = 0.9569
[Test_batch(13)(2000,28000)] 2017-05-02 22:38:46.213928: step 21000, loss = 0.1686, acc = 0.9440, f1neg = 0.9389, f1pos = 0.9483, f1 = 0.9436
[Test_batch(14)(2000,30000)] 2017-05-02 22:38:46.697504: step 21000, loss = 0.1415, acc = 0.9560, f1neg = 0.9503, f1pos = 0.9605, f1 = 0.9554
[Test_batch(15)(2000,32000)] 2017-05-02 22:38:47.178499: step 21000, loss = 0.1602, acc = 0.9485, f1neg = 0.9484, f1pos = 0.9486, f1 = 0.9485
[Test_batch(16)(2000,34000)] 2017-05-02 22:38:47.674503: step 21000, loss = 0.1433, acc = 0.9560, f1neg = 0.9554, f1pos = 0.9566, f1 = 0.9560
[Test_batch(17)(2000,36000)] 2017-05-02 22:38:48.150771: step 21000, loss = 0.1335, acc = 0.9595, f1neg = 0.9560, f1pos = 0.9624, f1 = 0.9592
[Test_batch(18)(2000,38000)] 2017-05-02 22:38:48.711246: step 21000, loss = 0.1220, acc = 0.9650, f1neg = 0.9646, f1pos = 0.9654, f1 = 0.9650
[Test] 2017-05-02 22:38:48.711337: step 21000, acc = 0.9492, f1 = 0.9490
[Status] 2017-05-02 22:38:48.711363: step 21000, maxindex = 18000, maxdev = 0.9578, maxtst = 0.9479
2017-05-02 22:38:57.854793: step 21010, loss = 0.1828, acc = 0.9380 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 22:39:07.185893: step 21020, loss = 0.1646, acc = 0.9420 (287.3 examples/sec; 0.223 sec/batch)
2017-05-02 22:39:16.394580: step 21030, loss = 0.1451, acc = 0.9520 (286.9 examples/sec; 0.223 sec/batch)
2017-05-02 22:39:25.457639: step 21040, loss = 0.1553, acc = 0.9500 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 22:39:34.608062: step 21050, loss = 0.1304, acc = 0.9620 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 22:39:43.821703: step 21060, loss = 0.1487, acc = 0.9480 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 22:39:52.952913: step 21070, loss = 0.1803, acc = 0.9340 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 22:40:02.208476: step 21080, loss = 0.1335, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 22:40:11.387314: step 21090, loss = 0.1513, acc = 0.9580 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 22:40:20.766230: step 21100, loss = 0.1608, acc = 0.9500 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 22:40:30.086120: step 21110, loss = 0.1449, acc = 0.9580 (285.8 examples/sec; 0.224 sec/batch)
2017-05-02 22:40:39.071792: step 21120, loss = 0.1404, acc = 0.9520 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 22:40:48.204841: step 21130, loss = 0.1658, acc = 0.9340 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 22:40:57.412182: step 21140, loss = 0.1460, acc = 0.9560 (291.0 examples/sec; 0.220 sec/batch)
2017-05-02 22:41:06.480937: step 21150, loss = 0.1506, acc = 0.9480 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 22:41:15.611448: step 21160, loss = 0.1510, acc = 0.9460 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 22:41:25.752120: step 21170, loss = 0.1329, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-02 22:41:35.122036: step 21180, loss = 0.1387, acc = 0.9580 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 22:41:44.143263: step 21190, loss = 0.1425, acc = 0.9560 (296.3 examples/sec; 0.216 sec/batch)
2017-05-02 22:41:53.442728: step 21200, loss = 0.1844, acc = 0.9420 (288.9 examples/sec; 0.222 sec/batch)
2017-05-02 22:42:02.577661: step 21210, loss = 0.1429, acc = 0.9540 (285.8 examples/sec; 0.224 sec/batch)
2017-05-02 22:42:11.778580: step 21220, loss = 0.1487, acc = 0.9460 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:42:21.132089: step 21230, loss = 0.1648, acc = 0.9280 (263.0 examples/sec; 0.243 sec/batch)
2017-05-02 22:42:30.209243: step 21240, loss = 0.1607, acc = 0.9540 (279.3 examples/sec; 0.229 sec/batch)
2017-05-02 22:42:39.288330: step 21250, loss = 0.1463, acc = 0.9520 (275.2 examples/sec; 0.233 sec/batch)
2017-05-02 22:42:48.628940: step 21260, loss = 0.1520, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 22:42:58.008454: step 21270, loss = 0.1351, acc = 0.9600 (264.8 examples/sec; 0.242 sec/batch)
2017-05-02 22:43:07.262551: step 21280, loss = 0.1395, acc = 0.9620 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 22:43:16.414534: step 21290, loss = 0.1397, acc = 0.9600 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 22:43:25.550741: step 21300, loss = 0.1527, acc = 0.9460 (271.5 examples/sec; 0.236 sec/batch)
2017-05-02 22:43:34.770039: step 21310, loss = 0.1458, acc = 0.9560 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 22:43:43.857425: step 21320, loss = 0.1616, acc = 0.9520 (288.9 examples/sec; 0.222 sec/batch)
2017-05-02 22:43:52.932112: step 21330, loss = 0.1599, acc = 0.9520 (282.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:44:02.134313: step 21340, loss = 0.1599, acc = 0.9540 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 22:44:11.364858: step 21350, loss = 0.1354, acc = 0.9580 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:44:20.559309: step 21360, loss = 0.1981, acc = 0.9440 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 22:44:29.742251: step 21370, loss = 0.1343, acc = 0.9640 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 22:44:38.852634: step 21380, loss = 0.1632, acc = 0.9500 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:44:47.955707: step 21390, loss = 0.1523, acc = 0.9540 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 22:44:57.071956: step 21400, loss = 0.1286, acc = 0.9500 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 22:45:06.380114: step 21410, loss = 0.1599, acc = 0.9500 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 22:45:15.484362: step 21420, loss = 0.1313, acc = 0.9660 (290.4 examples/sec; 0.220 sec/batch)
2017-05-02 22:45:24.776368: step 21430, loss = 0.1253, acc = 0.9620 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:45:34.003261: step 21440, loss = 0.1696, acc = 0.9440 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 22:45:43.293699: step 21450, loss = 0.1648, acc = 0.9360 (283.1 examples/sec; 0.226 sec/batch)
2017-05-02 22:45:52.592770: step 21460, loss = 0.1263, acc = 0.9680 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 22:46:01.803029: step 21470, loss = 0.1969, acc = 0.9340 (268.0 examples/sec; 0.239 sec/batch)
2017-05-02 22:46:10.939983: step 21480, loss = 0.1867, acc = 0.9380 (283.8 examples/sec; 0.225 sec/batch)
2017-05-02 22:46:20.079976: step 21490, loss = 0.1492, acc = 0.9540 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 22:46:29.276145: step 21500, loss = 0.1482, acc = 0.9560 (285.0 examples/sec; 0.225 sec/batch)
2017-05-02 22:46:38.338812: step 21510, loss = 0.1384, acc = 0.9560 (286.4 examples/sec; 0.223 sec/batch)
2017-05-02 22:46:47.451406: step 21520, loss = 0.1056, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-02 22:46:56.717374: step 21530, loss = 0.1227, acc = 0.9660 (270.0 examples/sec; 0.237 sec/batch)
2017-05-02 22:47:05.926654: step 21540, loss = 0.1536, acc = 0.9540 (260.0 examples/sec; 0.246 sec/batch)
2017-05-02 22:47:15.172923: step 21550, loss = 0.1648, acc = 0.9440 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 22:47:24.253665: step 21560, loss = 0.1385, acc = 0.9600 (273.5 examples/sec; 0.234 sec/batch)
2017-05-02 22:47:33.381801: step 21570, loss = 0.1678, acc = 0.9300 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 22:47:42.643227: step 21580, loss = 0.1752, acc = 0.9380 (263.8 examples/sec; 0.243 sec/batch)
2017-05-02 22:47:51.746515: step 21590, loss = 0.1504, acc = 0.9480 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:48:00.911186: step 21600, loss = 0.1486, acc = 0.9620 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 22:48:10.086794: step 21610, loss = 0.1257, acc = 0.9600 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 22:48:19.382466: step 21620, loss = 0.1553, acc = 0.9400 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 22:48:28.596628: step 21630, loss = 0.1526, acc = 0.9520 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 22:48:37.728695: step 21640, loss = 0.1730, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 22:48:46.885683: step 21650, loss = 0.1525, acc = 0.9380 (294.2 examples/sec; 0.218 sec/batch)
2017-05-02 22:48:56.106726: step 21660, loss = 0.1393, acc = 0.9580 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 22:49:05.376522: step 21670, loss = 0.1687, acc = 0.9360 (266.3 examples/sec; 0.240 sec/batch)
2017-05-02 22:49:14.540125: step 21680, loss = 0.1550, acc = 0.9560 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 22:49:23.639996: step 21690, loss = 0.1414, acc = 0.9500 (285.1 examples/sec; 0.225 sec/batch)
2017-05-02 22:49:32.923520: step 21700, loss = 0.1357, acc = 0.9580 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 22:49:41.886912: step 21710, loss = 0.1420, acc = 0.9520 (293.8 examples/sec; 0.218 sec/batch)
2017-05-02 22:49:51.121826: step 21720, loss = 0.1451, acc = 0.9520 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 22:50:00.474215: step 21730, loss = 0.1559, acc = 0.9380 (269.4 examples/sec; 0.238 sec/batch)
2017-05-02 22:50:09.759019: step 21740, loss = 0.1764, acc = 0.9400 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 22:50:18.960582: step 21750, loss = 0.1242, acc = 0.9540 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 22:50:28.175389: step 21760, loss = 0.1206, acc = 0.9600 (275.3 examples/sec; 0.233 sec/batch)
2017-05-02 22:50:37.430612: step 21770, loss = 0.1507, acc = 0.9540 (263.0 examples/sec; 0.243 sec/batch)
2017-05-02 22:50:46.625423: step 21780, loss = 0.1447, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 22:50:56.059590: step 21790, loss = 0.1731, acc = 0.9440 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 22:51:05.337685: step 21800, loss = 0.1525, acc = 0.9500 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 22:51:14.504260: step 21810, loss = 0.1692, acc = 0.9460 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 22:51:23.711840: step 21820, loss = 0.1720, acc = 0.9480 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 22:51:33.079473: step 21830, loss = 0.1411, acc = 0.9640 (281.5 examples/sec; 0.227 sec/batch)
2017-05-02 22:51:42.310794: step 21840, loss = 0.1825, acc = 0.9440 (287.1 examples/sec; 0.223 sec/batch)
2017-05-02 22:51:51.783024: step 21850, loss = 0.1246, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 22:52:00.990575: step 21860, loss = 0.1435, acc = 0.9560 (270.0 examples/sec; 0.237 sec/batch)
2017-05-02 22:52:10.056158: step 21870, loss = 0.1514, acc = 0.9480 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 22:52:19.217305: step 21880, loss = 0.1726, acc = 0.9400 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 22:52:28.216453: step 21890, loss = 0.1658, acc = 0.9380 (295.3 examples/sec; 0.217 sec/batch)
2017-05-02 22:52:37.555153: step 21900, loss = 0.1643, acc = 0.9460 (295.4 examples/sec; 0.217 sec/batch)
2017-05-02 22:52:46.745017: step 21910, loss = 0.1815, acc = 0.9300 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 22:52:55.869284: step 21920, loss = 0.1677, acc = 0.9380 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 22:53:04.974604: step 21930, loss = 0.1510, acc = 0.9520 (282.8 examples/sec; 0.226 sec/batch)
2017-05-02 22:53:14.149272: step 21940, loss = 0.1654, acc = 0.9420 (287.1 examples/sec; 0.223 sec/batch)
2017-05-02 22:53:23.384165: step 21950, loss = 0.1299, acc = 0.9500 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 22:53:32.674817: step 21960, loss = 0.1583, acc = 0.9520 (249.4 examples/sec; 0.257 sec/batch)
2017-05-02 22:53:42.022657: step 21970, loss = 0.1794, acc = 0.9300 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 22:53:51.328681: step 21980, loss = 0.1412, acc = 0.9540 (285.8 examples/sec; 0.224 sec/batch)
2017-05-02 22:54:00.393748: step 21990, loss = 0.1456, acc = 0.9560 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 22:54:09.618180: step 22000, loss = 0.1593, acc = 0.9380 (265.7 examples/sec; 0.241 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 22:54:10.091176: step 22000, loss = 0.1332, acc = 0.9535, f1neg = 0.9484, f1pos = 0.9577, f1 = 0.9530
[Eval_batch(1)(2000,4000)] 2017-05-02 22:54:10.556831: step 22000, loss = 0.1274, acc = 0.9580, f1neg = 0.9502, f1pos = 0.9637, f1 = 0.9570
[Eval_batch(2)(2000,6000)] 2017-05-02 22:54:11.027031: step 22000, loss = 0.1440, acc = 0.9515, f1neg = 0.9475, f1pos = 0.9549, f1 = 0.9512
[Eval_batch(3)(2000,8000)] 2017-05-02 22:54:11.493956: step 22000, loss = 0.1550, acc = 0.9455, f1neg = 0.9446, f1pos = 0.9463, f1 = 0.9455
[Eval_batch(4)(2000,10000)] 2017-05-02 22:54:11.961697: step 22000, loss = 0.1560, acc = 0.9475, f1neg = 0.9480, f1pos = 0.9469, f1 = 0.9475
[Eval_batch(5)(2000,12000)] 2017-05-02 22:54:12.427642: step 22000, loss = 0.1466, acc = 0.9495, f1neg = 0.9450, f1pos = 0.9533, f1 = 0.9492
[Eval_batch(6)(2000,14000)] 2017-05-02 22:54:12.898689: step 22000, loss = 0.1413, acc = 0.9550, f1neg = 0.9536, f1pos = 0.9564, f1 = 0.9550
[Eval_batch(7)(2000,16000)] 2017-05-02 22:54:13.366648: step 22000, loss = 0.1340, acc = 0.9585, f1neg = 0.9497, f1pos = 0.9647, f1 = 0.9572
[Eval_batch(8)(2000,18000)] 2017-05-02 22:54:13.833744: step 22000, loss = 0.1326, acc = 0.9560, f1neg = 0.9490, f1pos = 0.9613, f1 = 0.9551
[Eval_batch(9)(2000,20000)] 2017-05-02 22:54:14.288018: step 22000, loss = 0.1337, acc = 0.9595, f1neg = 0.9560, f1pos = 0.9624, f1 = 0.9592
[Eval_batch(10)(2000,22000)] 2017-05-02 22:54:14.791383: step 22000, loss = 0.1461, acc = 0.9475, f1neg = 0.9428, f1pos = 0.9515, f1 = 0.9471
[Eval_batch(11)(2000,24000)] 2017-05-02 22:54:15.293738: step 22000, loss = 0.1418, acc = 0.9540, f1neg = 0.9461, f1pos = 0.9599, f1 = 0.9530
[Eval_batch(12)(2000,26000)] 2017-05-02 22:54:15.772901: step 22000, loss = 0.1424, acc = 0.9490, f1neg = 0.9469, f1pos = 0.9510, f1 = 0.9489
[Eval_batch(13)(2000,28000)] 2017-05-02 22:54:16.269877: step 22000, loss = 0.1279, acc = 0.9580, f1neg = 0.9466, f1pos = 0.9654, f1 = 0.9560
[Eval_batch(14)(2000,30000)] 2017-05-02 22:54:16.721941: step 22000, loss = 0.1555, acc = 0.9465, f1neg = 0.9347, f1pos = 0.9547, f1 = 0.9447
[Eval_batch(15)(2000,32000)] 2017-05-02 22:54:17.223170: step 22000, loss = 0.1349, acc = 0.9605, f1neg = 0.9568, f1pos = 0.9636, f1 = 0.9602
[Eval_batch(16)(2000,34000)] 2017-05-02 22:54:17.681109: step 22000, loss = 0.1308, acc = 0.9585, f1neg = 0.9562, f1pos = 0.9605, f1 = 0.9584
[Eval_batch(17)(2000,36000)] 2017-05-02 22:54:18.185301: step 22000, loss = 0.1534, acc = 0.9505, f1neg = 0.9501, f1pos = 0.9509, f1 = 0.9505
[Eval_batch(18)(2000,38000)] 2017-05-02 22:54:18.682002: step 22000, loss = 0.1548, acc = 0.9475, f1neg = 0.9488, f1pos = 0.9461, f1 = 0.9475
[Eval_batch(19)(2000,40000)] 2017-05-02 22:54:19.181457: step 22000, loss = 0.1315, acc = 0.9640, f1neg = 0.9578, f1pos = 0.9686, f1 = 0.9632
[Eval_batch(20)(2000,42000)] 2017-05-02 22:54:19.663273: step 22000, loss = 0.1398, acc = 0.9585, f1neg = 0.9628, f1pos = 0.9530, f1 = 0.9579
[Eval_batch(21)(2000,44000)] 2017-05-02 22:54:20.132342: step 22000, loss = 0.1220, acc = 0.9605, f1neg = 0.9564, f1pos = 0.9639, f1 = 0.9601
[Eval_batch(22)(2000,46000)] 2017-05-02 22:54:20.602623: step 22000, loss = 0.1345, acc = 0.9555, f1neg = 0.9553, f1pos = 0.9557, f1 = 0.9555
[Eval_batch(23)(2000,48000)] 2017-05-02 22:54:21.067346: step 22000, loss = 0.1394, acc = 0.9515, f1neg = 0.9445, f1pos = 0.9569, f1 = 0.9507
[Eval_batch(24)(2000,50000)] 2017-05-02 22:54:21.540299: step 22000, loss = 0.1280, acc = 0.9565, f1neg = 0.9490, f1pos = 0.9621, f1 = 0.9555
[Eval_batch(25)(2000,52000)] 2017-05-02 22:54:22.045577: step 22000, loss = 0.1181, acc = 0.9625, f1neg = 0.9582, f1pos = 0.9660, f1 = 0.9621
[Eval_batch(26)(2000,54000)] 2017-05-02 22:54:22.553666: step 22000, loss = 0.1332, acc = 0.9565, f1neg = 0.9584, f1pos = 0.9544, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-02 22:54:23.133674: step 22000, loss = 0.1204, acc = 0.9635, f1neg = 0.9613, f1pos = 0.9655, f1 = 0.9634
[Eval] 2017-05-02 22:54:23.133737: step 22000, acc = 0.9548, f1 = 0.9543
[Test_batch(0)(2000,2000)] 2017-05-02 22:54:23.608076: step 22000, loss = 0.1693, acc = 0.9450, f1neg = 0.9470, f1pos = 0.9429, f1 = 0.9449
[Test_batch(1)(2000,4000)] 2017-05-02 22:54:24.087958: step 22000, loss = 0.1547, acc = 0.9460, f1neg = 0.9512, f1pos = 0.9396, f1 = 0.9454
[Test_batch(2)(2000,6000)] 2017-05-02 22:54:24.568860: step 22000, loss = 0.1681, acc = 0.9460, f1neg = 0.9496, f1pos = 0.9419, f1 = 0.9457
[Test_batch(3)(2000,8000)] 2017-05-02 22:54:25.021335: step 22000, loss = 0.1750, acc = 0.9345, f1neg = 0.9376, f1pos = 0.9311, f1 = 0.9343
[Test_batch(4)(2000,10000)] 2017-05-02 22:54:25.502963: step 22000, loss = 0.1739, acc = 0.9340, f1neg = 0.9330, f1pos = 0.9350, f1 = 0.9340
[Test_batch(5)(2000,12000)] 2017-05-02 22:54:25.978185: step 22000, loss = 0.1850, acc = 0.9300, f1neg = 0.9295, f1pos = 0.9305, f1 = 0.9300
[Test_batch(6)(2000,14000)] 2017-05-02 22:54:26.423968: step 22000, loss = 0.1722, acc = 0.9375, f1neg = 0.9345, f1pos = 0.9403, f1 = 0.9374
[Test_batch(7)(2000,16000)] 2017-05-02 22:54:26.928925: step 22000, loss = 0.1595, acc = 0.9455, f1neg = 0.9497, f1pos = 0.9406, f1 = 0.9451
[Test_batch(8)(2000,18000)] 2017-05-02 22:54:27.405220: step 22000, loss = 0.1580, acc = 0.9460, f1neg = 0.9448, f1pos = 0.9472, f1 = 0.9460
[Test_batch(9)(2000,20000)] 2017-05-02 22:54:27.892232: step 22000, loss = 0.1858, acc = 0.9345, f1neg = 0.9297, f1pos = 0.9387, f1 = 0.9342
[Test_batch(10)(2000,22000)] 2017-05-02 22:54:28.378998: step 22000, loss = 0.1598, acc = 0.9425, f1neg = 0.9343, f1pos = 0.9489, f1 = 0.9416
[Test_batch(11)(2000,24000)] 2017-05-02 22:54:28.886483: step 22000, loss = 0.1670, acc = 0.9440, f1neg = 0.9448, f1pos = 0.9431, f1 = 0.9440
[Test_batch(12)(2000,26000)] 2017-05-02 22:54:29.364505: step 22000, loss = 0.1408, acc = 0.9545, f1neg = 0.9557, f1pos = 0.9532, f1 = 0.9545
[Test_batch(13)(2000,28000)] 2017-05-02 22:54:29.869459: step 22000, loss = 0.1690, acc = 0.9360, f1neg = 0.9274, f1pos = 0.9428, f1 = 0.9351
[Test_batch(14)(2000,30000)] 2017-05-02 22:54:30.340910: step 22000, loss = 0.1380, acc = 0.9555, f1neg = 0.9480, f1pos = 0.9611, f1 = 0.9546
[Test_batch(15)(2000,32000)] 2017-05-02 22:54:30.843749: step 22000, loss = 0.1613, acc = 0.9455, f1neg = 0.9441, f1pos = 0.9469, f1 = 0.9455
[Test_batch(16)(2000,34000)] 2017-05-02 22:54:31.286163: step 22000, loss = 0.1464, acc = 0.9500, f1neg = 0.9479, f1pos = 0.9520, f1 = 0.9499
[Test_batch(17)(2000,36000)] 2017-05-02 22:54:31.796038: step 22000, loss = 0.1396, acc = 0.9590, f1neg = 0.9545, f1pos = 0.9627, f1 = 0.9586
[Test_batch(18)(2000,38000)] 2017-05-02 22:54:32.301690: step 22000, loss = 0.1328, acc = 0.9565, f1neg = 0.9551, f1pos = 0.9578, f1 = 0.9565
[Test] 2017-05-02 22:54:32.301783: step 22000, acc = 0.9443, f1 = 0.9441
[Status] 2017-05-02 22:54:32.301808: step 22000, maxindex = 18000, maxdev = 0.9578, maxtst = 0.9479
2017-05-02 22:54:41.273338: step 22010, loss = 0.1447, acc = 0.9480 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 22:54:50.410506: step 22020, loss = 0.1549, acc = 0.9540 (289.6 examples/sec; 0.221 sec/batch)
2017-05-02 22:54:59.562941: step 22030, loss = 0.1542, acc = 0.9420 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 22:55:08.802590: step 22040, loss = 0.1505, acc = 0.9480 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 22:55:18.002167: step 22050, loss = 0.1572, acc = 0.9480 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 22:55:27.393642: step 22060, loss = 0.1668, acc = 0.9380 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 22:55:36.657036: step 22070, loss = 0.1611, acc = 0.9480 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 22:55:46.103684: step 22080, loss = 0.1578, acc = 0.9520 (288.8 examples/sec; 0.222 sec/batch)
2017-05-02 22:55:55.298705: step 22090, loss = 0.1188, acc = 0.9560 (261.8 examples/sec; 0.244 sec/batch)
2017-05-02 22:56:04.760457: step 22100, loss = 0.1576, acc = 0.9500 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 22:56:13.805922: step 22110, loss = 0.1359, acc = 0.9640 (289.3 examples/sec; 0.221 sec/batch)
2017-05-02 22:56:22.935156: step 22120, loss = 0.1485, acc = 0.9500 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 22:56:32.169845: step 22130, loss = 0.1378, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 22:56:41.174380: step 22140, loss = 0.1284, acc = 0.9580 (277.3 examples/sec; 0.231 sec/batch)
2017-05-02 22:56:50.210639: step 22150, loss = 0.1682, acc = 0.9420 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 22:56:59.357753: step 22160, loss = 0.1281, acc = 0.9680 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 22:57:08.376874: step 22170, loss = 0.1381, acc = 0.9580 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 22:57:18.308574: step 22180, loss = 0.1397, acc = 0.9560 (276.4 examples/sec; 0.232 sec/batch)
2017-05-02 22:57:27.479581: step 22190, loss = 0.1217, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-02 22:57:36.606356: step 22200, loss = 0.1361, acc = 0.9600 (277.8 examples/sec; 0.230 sec/batch)
2017-05-02 22:57:45.787020: step 22210, loss = 0.1383, acc = 0.9560 (260.6 examples/sec; 0.246 sec/batch)
2017-05-02 22:57:55.025166: step 22220, loss = 0.1415, acc = 0.9540 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 22:58:04.359171: step 22230, loss = 0.1327, acc = 0.9560 (252.8 examples/sec; 0.253 sec/batch)
2017-05-02 22:58:13.448211: step 22240, loss = 0.1692, acc = 0.9440 (271.3 examples/sec; 0.236 sec/batch)
2017-05-02 22:58:22.778130: step 22250, loss = 0.1749, acc = 0.9340 (256.8 examples/sec; 0.249 sec/batch)
2017-05-02 22:58:31.871090: step 22260, loss = 0.1378, acc = 0.9660 (301.2 examples/sec; 0.213 sec/batch)
2017-05-02 22:58:41.096040: step 22270, loss = 0.1288, acc = 0.9540 (258.9 examples/sec; 0.247 sec/batch)
2017-05-02 22:58:50.306569: step 22280, loss = 0.1284, acc = 0.9640 (274.8 examples/sec; 0.233 sec/batch)
2017-05-02 22:58:59.438750: step 22290, loss = 0.1303, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 22:59:08.341447: step 22300, loss = 0.1380, acc = 0.9600 (299.3 examples/sec; 0.214 sec/batch)
2017-05-02 22:59:17.451053: step 22310, loss = 0.1660, acc = 0.9500 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 22:59:26.754960: step 22320, loss = 0.1499, acc = 0.9500 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 22:59:35.908283: step 22330, loss = 0.1377, acc = 0.9540 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 22:59:44.930066: step 22340, loss = 0.1254, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 22:59:54.004788: step 22350, loss = 0.1679, acc = 0.9460 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:00:03.136115: step 22360, loss = 0.1339, acc = 0.9540 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 23:00:12.234499: step 22370, loss = 0.1629, acc = 0.9380 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 23:00:21.545686: step 22380, loss = 0.1570, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 23:00:30.740433: step 22390, loss = 0.1538, acc = 0.9540 (271.6 examples/sec; 0.236 sec/batch)
2017-05-02 23:00:40.187012: step 22400, loss = 0.1477, acc = 0.9560 (278.8 examples/sec; 0.230 sec/batch)
2017-05-02 23:00:49.377881: step 22410, loss = 0.1572, acc = 0.9480 (297.3 examples/sec; 0.215 sec/batch)
2017-05-02 23:00:58.599971: step 22420, loss = 0.1448, acc = 0.9640 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 23:01:07.743350: step 22430, loss = 0.1361, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-02 23:01:16.983261: step 22440, loss = 0.1519, acc = 0.9440 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 23:01:26.119046: step 22450, loss = 0.1488, acc = 0.9560 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 23:01:35.355231: step 22460, loss = 0.1549, acc = 0.9520 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 23:01:44.619643: step 22470, loss = 0.1395, acc = 0.9540 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 23:01:53.752391: step 22480, loss = 0.1457, acc = 0.9560 (268.7 examples/sec; 0.238 sec/batch)
2017-05-02 23:02:02.948077: step 22490, loss = 0.1371, acc = 0.9640 (289.3 examples/sec; 0.221 sec/batch)
2017-05-02 23:02:12.116937: step 22500, loss = 0.1596, acc = 0.9580 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 23:02:21.207018: step 22510, loss = 0.1753, acc = 0.9400 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:02:30.495950: step 22520, loss = 0.1476, acc = 0.9480 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 23:02:39.870876: step 22530, loss = 0.1878, acc = 0.9320 (292.4 examples/sec; 0.219 sec/batch)
2017-05-02 23:02:48.938296: step 22540, loss = 0.1619, acc = 0.9520 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 23:02:57.995372: step 22550, loss = 0.1590, acc = 0.9500 (269.3 examples/sec; 0.238 sec/batch)
2017-05-02 23:03:07.142885: step 22560, loss = 0.1497, acc = 0.9460 (277.6 examples/sec; 0.231 sec/batch)
2017-05-02 23:03:16.308416: step 22570, loss = 0.1331, acc = 0.9480 (266.5 examples/sec; 0.240 sec/batch)
2017-05-02 23:03:25.453918: step 22580, loss = 0.1736, acc = 0.9440 (269.9 examples/sec; 0.237 sec/batch)
2017-05-02 23:03:34.524603: step 22590, loss = 0.1548, acc = 0.9420 (286.0 examples/sec; 0.224 sec/batch)
2017-05-02 23:03:43.840534: step 22600, loss = 0.1313, acc = 0.9620 (275.0 examples/sec; 0.233 sec/batch)
2017-05-02 23:03:52.884675: step 22610, loss = 0.1351, acc = 0.9540 (292.9 examples/sec; 0.218 sec/batch)
2017-05-02 23:04:01.980107: step 22620, loss = 0.1778, acc = 0.9340 (280.5 examples/sec; 0.228 sec/batch)
2017-05-02 23:04:10.998597: step 22630, loss = 0.1176, acc = 0.9640 (291.3 examples/sec; 0.220 sec/batch)
2017-05-02 23:04:20.220746: step 22640, loss = 0.1551, acc = 0.9480 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 23:04:29.443879: step 22650, loss = 0.1659, acc = 0.9380 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 23:04:38.645252: step 22660, loss = 0.1952, acc = 0.9300 (274.9 examples/sec; 0.233 sec/batch)
2017-05-02 23:04:47.910926: step 22670, loss = 0.1641, acc = 0.9340 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 23:04:57.062042: step 22680, loss = 0.1704, acc = 0.9540 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 23:05:06.314540: step 22690, loss = 0.1524, acc = 0.9480 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 23:05:15.558543: step 22700, loss = 0.1536, acc = 0.9460 (262.9 examples/sec; 0.243 sec/batch)
2017-05-02 23:05:24.712895: step 22710, loss = 0.1660, acc = 0.9460 (263.3 examples/sec; 0.243 sec/batch)
2017-05-02 23:05:33.935981: step 22720, loss = 0.1292, acc = 0.9600 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 23:05:43.074225: step 22730, loss = 0.1952, acc = 0.9460 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 23:05:52.211845: step 22740, loss = 0.1527, acc = 0.9440 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 23:06:01.395839: step 22750, loss = 0.1561, acc = 0.9520 (276.2 examples/sec; 0.232 sec/batch)
2017-05-02 23:06:10.632853: step 22760, loss = 0.1346, acc = 0.9640 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 23:06:19.886807: step 22770, loss = 0.1441, acc = 0.9560 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 23:06:29.088865: step 22780, loss = 0.1414, acc = 0.9500 (271.2 examples/sec; 0.236 sec/batch)
2017-05-02 23:06:38.117620: step 22790, loss = 0.1412, acc = 0.9620 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 23:06:47.170778: step 22800, loss = 0.1829, acc = 0.9380 (286.4 examples/sec; 0.223 sec/batch)
2017-05-02 23:06:56.341633: step 22810, loss = 0.1410, acc = 0.9500 (284.1 examples/sec; 0.225 sec/batch)
2017-05-02 23:07:05.446986: step 22820, loss = 0.1468, acc = 0.9580 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 23:07:14.595529: step 22830, loss = 0.1808, acc = 0.9400 (263.7 examples/sec; 0.243 sec/batch)
2017-05-02 23:07:23.971005: step 22840, loss = 0.1443, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:07:33.108059: step 22850, loss = 0.1570, acc = 0.9460 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 23:07:42.419655: step 22860, loss = 0.1328, acc = 0.9580 (271.0 examples/sec; 0.236 sec/batch)
2017-05-02 23:07:51.845186: step 22870, loss = 0.1368, acc = 0.9540 (261.5 examples/sec; 0.245 sec/batch)
2017-05-02 23:08:01.013666: step 22880, loss = 0.1443, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 23:08:10.051154: step 22890, loss = 0.1527, acc = 0.9440 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 23:08:19.021066: step 22900, loss = 0.1456, acc = 0.9520 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 23:08:28.206152: step 22910, loss = 0.1524, acc = 0.9400 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 23:08:37.382173: step 22920, loss = 0.1249, acc = 0.9620 (290.5 examples/sec; 0.220 sec/batch)
2017-05-02 23:08:46.410689: step 22930, loss = 0.1384, acc = 0.9600 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 23:08:55.562856: step 22940, loss = 0.1487, acc = 0.9560 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 23:09:04.736081: step 22950, loss = 0.1531, acc = 0.9500 (289.0 examples/sec; 0.221 sec/batch)
2017-05-02 23:09:14.110021: step 22960, loss = 0.1574, acc = 0.9520 (268.5 examples/sec; 0.238 sec/batch)
2017-05-02 23:09:23.324065: step 22970, loss = 0.1487, acc = 0.9560 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 23:09:32.564341: step 22980, loss = 0.1384, acc = 0.9500 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 23:09:41.897076: step 22990, loss = 0.1659, acc = 0.9520 (269.3 examples/sec; 0.238 sec/batch)
2017-05-02 23:09:51.131630: step 23000, loss = 0.1615, acc = 0.9460 (275.1 examples/sec; 0.233 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 23:09:51.607244: step 23000, loss = 0.1305, acc = 0.9550, f1neg = 0.9503, f1pos = 0.9589, f1 = 0.9546
[Eval_batch(1)(2000,4000)] 2017-05-02 23:09:52.067531: step 23000, loss = 0.1256, acc = 0.9615, f1neg = 0.9546, f1pos = 0.9666, f1 = 0.9606
[Eval_batch(2)(2000,6000)] 2017-05-02 23:09:52.551204: step 23000, loss = 0.1407, acc = 0.9530, f1neg = 0.9494, f1pos = 0.9561, f1 = 0.9528
[Eval_batch(3)(2000,8000)] 2017-05-02 23:09:53.062740: step 23000, loss = 0.1498, acc = 0.9515, f1neg = 0.9511, f1pos = 0.9519, f1 = 0.9515
[Eval_batch(4)(2000,10000)] 2017-05-02 23:09:53.568866: step 23000, loss = 0.1518, acc = 0.9475, f1neg = 0.9482, f1pos = 0.9468, f1 = 0.9475
[Eval_batch(5)(2000,12000)] 2017-05-02 23:09:54.071370: step 23000, loss = 0.1440, acc = 0.9495, f1neg = 0.9451, f1pos = 0.9533, f1 = 0.9492
[Eval_batch(6)(2000,14000)] 2017-05-02 23:09:54.575523: step 23000, loss = 0.1378, acc = 0.9570, f1neg = 0.9558, f1pos = 0.9582, f1 = 0.9570
[Eval_batch(7)(2000,16000)] 2017-05-02 23:09:55.044591: step 23000, loss = 0.1316, acc = 0.9580, f1neg = 0.9492, f1pos = 0.9642, f1 = 0.9567
[Eval_batch(8)(2000,18000)] 2017-05-02 23:09:55.521231: step 23000, loss = 0.1307, acc = 0.9590, f1neg = 0.9527, f1pos = 0.9638, f1 = 0.9583
[Eval_batch(9)(2000,20000)] 2017-05-02 23:09:56.030475: step 23000, loss = 0.1314, acc = 0.9625, f1neg = 0.9597, f1pos = 0.9650, f1 = 0.9623
[Eval_batch(10)(2000,22000)] 2017-05-02 23:09:56.533019: step 23000, loss = 0.1422, acc = 0.9500, f1neg = 0.9458, f1pos = 0.9536, f1 = 0.9497
[Eval_batch(11)(2000,24000)] 2017-05-02 23:09:57.037775: step 23000, loss = 0.1399, acc = 0.9545, f1neg = 0.9469, f1pos = 0.9602, f1 = 0.9536
[Eval_batch(12)(2000,26000)] 2017-05-02 23:09:57.498782: step 23000, loss = 0.1400, acc = 0.9485, f1neg = 0.9468, f1pos = 0.9501, f1 = 0.9484
[Eval_batch(13)(2000,28000)] 2017-05-02 23:09:57.978624: step 23000, loss = 0.1261, acc = 0.9590, f1neg = 0.9482, f1pos = 0.9661, f1 = 0.9571
[Eval_batch(14)(2000,30000)] 2017-05-02 23:09:58.454246: step 23000, loss = 0.1535, acc = 0.9475, f1neg = 0.9364, f1pos = 0.9553, f1 = 0.9459
[Eval_batch(15)(2000,32000)] 2017-05-02 23:09:58.928387: step 23000, loss = 0.1325, acc = 0.9630, f1neg = 0.9598, f1pos = 0.9657, f1 = 0.9628
[Eval_batch(16)(2000,34000)] 2017-05-02 23:09:59.396455: step 23000, loss = 0.1279, acc = 0.9595, f1neg = 0.9574, f1pos = 0.9614, f1 = 0.9594
[Eval_batch(17)(2000,36000)] 2017-05-02 23:09:59.864728: step 23000, loss = 0.1510, acc = 0.9510, f1neg = 0.9508, f1pos = 0.9512, f1 = 0.9510
[Eval_batch(18)(2000,38000)] 2017-05-02 23:10:00.322514: step 23000, loss = 0.1512, acc = 0.9490, f1neg = 0.9504, f1pos = 0.9475, f1 = 0.9490
[Eval_batch(19)(2000,40000)] 2017-05-02 23:10:00.771291: step 23000, loss = 0.1292, acc = 0.9630, f1neg = 0.9568, f1pos = 0.9677, f1 = 0.9622
[Eval_batch(20)(2000,42000)] 2017-05-02 23:10:01.252364: step 23000, loss = 0.1345, acc = 0.9585, f1neg = 0.9629, f1pos = 0.9529, f1 = 0.9579
[Eval_batch(21)(2000,44000)] 2017-05-02 23:10:01.732277: step 23000, loss = 0.1199, acc = 0.9620, f1neg = 0.9583, f1pos = 0.9651, f1 = 0.9617
[Eval_batch(22)(2000,46000)] 2017-05-02 23:10:02.208022: step 23000, loss = 0.1317, acc = 0.9560, f1neg = 0.9559, f1pos = 0.9561, f1 = 0.9560
[Eval_batch(23)(2000,48000)] 2017-05-02 23:10:02.684839: step 23000, loss = 0.1358, acc = 0.9535, f1neg = 0.9469, f1pos = 0.9586, f1 = 0.9528
[Eval_batch(24)(2000,50000)] 2017-05-02 23:10:03.165339: step 23000, loss = 0.1249, acc = 0.9585, f1neg = 0.9515, f1pos = 0.9637, f1 = 0.9576
[Eval_batch(25)(2000,52000)] 2017-05-02 23:10:03.660491: step 23000, loss = 0.1154, acc = 0.9630, f1neg = 0.9588, f1pos = 0.9664, f1 = 0.9626
[Eval_batch(26)(2000,54000)] 2017-05-02 23:10:04.123935: step 23000, loss = 0.1297, acc = 0.9555, f1neg = 0.9576, f1pos = 0.9532, f1 = 0.9554
[Eval_batch(27)(2000,56000)] 2017-05-02 23:10:04.717918: step 23000, loss = 0.1177, acc = 0.9635, f1neg = 0.9614, f1pos = 0.9654, f1 = 0.9634
[Eval] 2017-05-02 23:10:04.718008: step 23000, acc = 0.9561, f1 = 0.9556
[Test_batch(0)(2000,2000)] 2017-05-02 23:10:05.204521: step 23000, loss = 0.1660, acc = 0.9465, f1neg = 0.9487, f1pos = 0.9441, f1 = 0.9464
[Test_batch(1)(2000,4000)] 2017-05-02 23:10:05.677200: step 23000, loss = 0.1494, acc = 0.9480, f1neg = 0.9533, f1pos = 0.9414, f1 = 0.9473
[Test_batch(2)(2000,6000)] 2017-05-02 23:10:06.165937: step 23000, loss = 0.1629, acc = 0.9455, f1neg = 0.9494, f1pos = 0.9410, f1 = 0.9452
[Test_batch(3)(2000,8000)] 2017-05-02 23:10:06.605865: step 23000, loss = 0.1717, acc = 0.9390, f1neg = 0.9422, f1pos = 0.9354, f1 = 0.9388
[Test_batch(4)(2000,10000)] 2017-05-02 23:10:07.080901: step 23000, loss = 0.1699, acc = 0.9385, f1neg = 0.9378, f1pos = 0.9391, f1 = 0.9385
[Test_batch(5)(2000,12000)] 2017-05-02 23:10:07.557208: step 23000, loss = 0.1806, acc = 0.9315, f1neg = 0.9314, f1pos = 0.9316, f1 = 0.9315
[Test_batch(6)(2000,14000)] 2017-05-02 23:10:08.008491: step 23000, loss = 0.1675, acc = 0.9390, f1neg = 0.9363, f1pos = 0.9415, f1 = 0.9389
[Test_batch(7)(2000,16000)] 2017-05-02 23:10:08.512841: step 23000, loss = 0.1553, acc = 0.9485, f1neg = 0.9527, f1pos = 0.9435, f1 = 0.9481
[Test_batch(8)(2000,18000)] 2017-05-02 23:10:08.988343: step 23000, loss = 0.1548, acc = 0.9445, f1neg = 0.9435, f1pos = 0.9455, f1 = 0.9445
[Test_batch(9)(2000,20000)] 2017-05-02 23:10:09.469735: step 23000, loss = 0.1841, acc = 0.9335, f1neg = 0.9291, f1pos = 0.9374, f1 = 0.9332
[Test_batch(10)(2000,22000)] 2017-05-02 23:10:09.970372: step 23000, loss = 0.1586, acc = 0.9440, f1neg = 0.9363, f1pos = 0.9500, f1 = 0.9432
[Test_batch(11)(2000,24000)] 2017-05-02 23:10:10.462422: step 23000, loss = 0.1632, acc = 0.9460, f1neg = 0.9470, f1pos = 0.9450, f1 = 0.9460
[Test_batch(12)(2000,26000)] 2017-05-02 23:10:10.940465: step 23000, loss = 0.1384, acc = 0.9545, f1neg = 0.9558, f1pos = 0.9531, f1 = 0.9545
[Test_batch(13)(2000,28000)] 2017-05-02 23:10:11.419470: step 23000, loss = 0.1666, acc = 0.9395, f1neg = 0.9318, f1pos = 0.9456, f1 = 0.9387
[Test_batch(14)(2000,30000)] 2017-05-02 23:10:11.923391: step 23000, loss = 0.1363, acc = 0.9565, f1neg = 0.9496, f1pos = 0.9617, f1 = 0.9557
[Test_batch(15)(2000,32000)] 2017-05-02 23:10:12.402703: step 23000, loss = 0.1594, acc = 0.9485, f1neg = 0.9475, f1pos = 0.9495, f1 = 0.9485
[Test_batch(16)(2000,34000)] 2017-05-02 23:10:12.881453: step 23000, loss = 0.1445, acc = 0.9510, f1neg = 0.9491, f1pos = 0.9527, f1 = 0.9509
[Test_batch(17)(2000,36000)] 2017-05-02 23:10:13.363098: step 23000, loss = 0.1365, acc = 0.9625, f1neg = 0.9586, f1pos = 0.9657, f1 = 0.9622
[Test_batch(18)(2000,38000)] 2017-05-02 23:10:13.904082: step 23000, loss = 0.1290, acc = 0.9585, f1neg = 0.9573, f1pos = 0.9596, f1 = 0.9585
[Test] 2017-05-02 23:10:13.904189: step 23000, acc = 0.9461, f1 = 0.9458
[Status] 2017-05-02 23:10:13.904216: step 23000, maxindex = 18000, maxdev = 0.9578, maxtst = 0.9479
2017-05-02 23:10:22.997963: step 23010, loss = 0.1688, acc = 0.9440 (280.2 examples/sec; 0.228 sec/batch)
2017-05-02 23:10:32.304054: step 23020, loss = 0.1705, acc = 0.9420 (275.3 examples/sec; 0.232 sec/batch)
2017-05-02 23:10:41.578523: step 23030, loss = 0.1684, acc = 0.9480 (283.8 examples/sec; 0.226 sec/batch)
2017-05-02 23:10:50.825309: step 23040, loss = 0.1513, acc = 0.9480 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 23:10:59.819735: step 23050, loss = 0.1293, acc = 0.9680 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 23:11:09.086647: step 23060, loss = 0.1512, acc = 0.9500 (280.0 examples/sec; 0.229 sec/batch)
2017-05-02 23:11:18.486350: step 23070, loss = 0.1540, acc = 0.9460 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 23:11:27.592937: step 23080, loss = 0.1377, acc = 0.9520 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 23:11:36.816794: step 23090, loss = 0.1719, acc = 0.9420 (279.2 examples/sec; 0.229 sec/batch)
2017-05-02 23:11:45.968569: step 23100, loss = 0.1751, acc = 0.9380 (274.5 examples/sec; 0.233 sec/batch)
2017-05-02 23:11:55.111094: step 23110, loss = 0.1630, acc = 0.9420 (262.9 examples/sec; 0.243 sec/batch)
2017-05-02 23:12:04.275277: step 23120, loss = 0.1363, acc = 0.9620 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 23:12:13.382509: step 23130, loss = 0.1568, acc = 0.9560 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 23:12:22.608818: step 23140, loss = 0.1303, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 23:12:31.800227: step 23150, loss = 0.1320, acc = 0.9640 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 23:12:41.039710: step 23160, loss = 0.1768, acc = 0.9380 (279.7 examples/sec; 0.229 sec/batch)
2017-05-02 23:12:50.301823: step 23170, loss = 0.1553, acc = 0.9600 (272.7 examples/sec; 0.235 sec/batch)
2017-05-02 23:12:59.602863: step 23180, loss = 0.1568, acc = 0.9380 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 23:13:09.941819: step 23190, loss = 0.1287, acc = 0.9560 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 23:13:19.072005: step 23200, loss = 0.1510, acc = 0.9480 (284.3 examples/sec; 0.225 sec/batch)
2017-05-02 23:13:28.224138: step 23210, loss = 0.1359, acc = 0.9520 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 23:13:37.435505: step 23220, loss = 0.1571, acc = 0.9520 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 23:13:46.652360: step 23230, loss = 0.1398, acc = 0.9540 (276.8 examples/sec; 0.231 sec/batch)
2017-05-02 23:13:55.739449: step 23240, loss = 0.1713, acc = 0.9440 (277.1 examples/sec; 0.231 sec/batch)
2017-05-02 23:14:04.964952: step 23250, loss = 0.1439, acc = 0.9540 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 23:14:14.269410: step 23260, loss = 0.1426, acc = 0.9520 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 23:14:23.530999: step 23270, loss = 0.1190, acc = 0.9680 (272.0 examples/sec; 0.235 sec/batch)
2017-05-02 23:14:32.809211: step 23280, loss = 0.1427, acc = 0.9540 (288.4 examples/sec; 0.222 sec/batch)
2017-05-02 23:14:42.141510: step 23290, loss = 0.1587, acc = 0.9400 (264.0 examples/sec; 0.242 sec/batch)
2017-05-02 23:14:51.342374: step 23300, loss = 0.1678, acc = 0.9360 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 23:15:00.522338: step 23310, loss = 0.1241, acc = 0.9560 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 23:15:09.559818: step 23320, loss = 0.1464, acc = 0.9480 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 23:15:18.740957: step 23330, loss = 0.1220, acc = 0.9640 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 23:15:28.054083: step 23340, loss = 0.1424, acc = 0.9520 (280.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:15:37.148226: step 23350, loss = 0.1524, acc = 0.9440 (280.9 examples/sec; 0.228 sec/batch)
2017-05-02 23:15:46.427227: step 23360, loss = 0.1334, acc = 0.9500 (273.6 examples/sec; 0.234 sec/batch)
2017-05-02 23:15:55.566838: step 23370, loss = 0.1406, acc = 0.9560 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 23:16:04.759280: step 23380, loss = 0.1412, acc = 0.9540 (275.9 examples/sec; 0.232 sec/batch)
2017-05-02 23:16:13.931150: step 23390, loss = 0.1396, acc = 0.9580 (268.3 examples/sec; 0.239 sec/batch)
2017-05-02 23:16:23.005885: step 23400, loss = 0.1376, acc = 0.9500 (293.4 examples/sec; 0.218 sec/batch)
2017-05-02 23:16:32.147765: step 23410, loss = 0.1205, acc = 0.9700 (293.5 examples/sec; 0.218 sec/batch)
2017-05-02 23:16:41.422196: step 23420, loss = 0.1448, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 23:16:50.620814: step 23430, loss = 0.1480, acc = 0.9460 (289.5 examples/sec; 0.221 sec/batch)
2017-05-02 23:16:59.966253: step 23440, loss = 0.1560, acc = 0.9440 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 23:17:09.143570: step 23450, loss = 0.1268, acc = 0.9620 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 23:17:18.423629: step 23460, loss = 0.1411, acc = 0.9660 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 23:17:27.479999: step 23470, loss = 0.1496, acc = 0.9520 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 23:17:36.897308: step 23480, loss = 0.1596, acc = 0.9500 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 23:17:46.103835: step 23490, loss = 0.1605, acc = 0.9460 (270.3 examples/sec; 0.237 sec/batch)
2017-05-02 23:17:55.240986: step 23500, loss = 0.1539, acc = 0.9560 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 23:18:04.531934: step 23510, loss = 0.1403, acc = 0.9440 (265.9 examples/sec; 0.241 sec/batch)
2017-05-02 23:18:14.011792: step 23520, loss = 0.1407, acc = 0.9520 (290.7 examples/sec; 0.220 sec/batch)
2017-05-02 23:18:22.917471: step 23530, loss = 0.1610, acc = 0.9440 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 23:18:32.112538: step 23540, loss = 0.1935, acc = 0.9440 (262.9 examples/sec; 0.243 sec/batch)
2017-05-02 23:18:41.291066: step 23550, loss = 0.1430, acc = 0.9600 (289.4 examples/sec; 0.221 sec/batch)
2017-05-02 23:18:50.451737: step 23560, loss = 0.1441, acc = 0.9540 (270.5 examples/sec; 0.237 sec/batch)
2017-05-02 23:18:59.579847: step 23570, loss = 0.1573, acc = 0.9520 (270.2 examples/sec; 0.237 sec/batch)
2017-05-02 23:19:08.689226: step 23580, loss = 0.1309, acc = 0.9560 (283.2 examples/sec; 0.226 sec/batch)
2017-05-02 23:19:17.936013: step 23590, loss = 0.1616, acc = 0.9360 (287.5 examples/sec; 0.223 sec/batch)
2017-05-02 23:19:27.134763: step 23600, loss = 0.1385, acc = 0.9640 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 23:19:36.359295: step 23610, loss = 0.1722, acc = 0.9380 (281.2 examples/sec; 0.228 sec/batch)
2017-05-02 23:19:45.373590: step 23620, loss = 0.1382, acc = 0.9620 (283.8 examples/sec; 0.225 sec/batch)
2017-05-02 23:19:54.421757: step 23630, loss = 0.1366, acc = 0.9620 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 23:20:03.769799: step 23640, loss = 0.1529, acc = 0.9420 (256.5 examples/sec; 0.250 sec/batch)
2017-05-02 23:20:12.871155: step 23650, loss = 0.1420, acc = 0.9560 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 23:20:21.867169: step 23660, loss = 0.1455, acc = 0.9600 (288.8 examples/sec; 0.222 sec/batch)
2017-05-02 23:20:30.983617: step 23670, loss = 0.1427, acc = 0.9600 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 23:20:40.094674: step 23680, loss = 0.1579, acc = 0.9420 (278.4 examples/sec; 0.230 sec/batch)
2017-05-02 23:20:49.254480: step 23690, loss = 0.1842, acc = 0.9360 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 23:20:58.331056: step 23700, loss = 0.1642, acc = 0.9480 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 23:21:07.370247: step 23710, loss = 0.1546, acc = 0.9420 (289.9 examples/sec; 0.221 sec/batch)
2017-05-02 23:21:16.353964: step 23720, loss = 0.1296, acc = 0.9540 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 23:21:25.542052: step 23730, loss = 0.1732, acc = 0.9560 (290.9 examples/sec; 0.220 sec/batch)
2017-05-02 23:21:34.467946: step 23740, loss = 0.1342, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-02 23:21:43.583345: step 23750, loss = 0.1356, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 23:21:52.737663: step 23760, loss = 0.1511, acc = 0.9520 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 23:22:01.996892: step 23770, loss = 0.1349, acc = 0.9620 (272.6 examples/sec; 0.235 sec/batch)
2017-05-02 23:22:11.426342: step 23780, loss = 0.1471, acc = 0.9520 (228.8 examples/sec; 0.280 sec/batch)
2017-05-02 23:22:20.447236: step 23790, loss = 0.1609, acc = 0.9520 (274.4 examples/sec; 0.233 sec/batch)
2017-05-02 23:22:29.619176: step 23800, loss = 0.1610, acc = 0.9460 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 23:22:38.709165: step 23810, loss = 0.1267, acc = 0.9600 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:22:47.832027: step 23820, loss = 0.1284, acc = 0.9600 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 23:22:56.903971: step 23830, loss = 0.1435, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 23:23:06.058576: step 23840, loss = 0.2199, acc = 0.9280 (287.3 examples/sec; 0.223 sec/batch)
2017-05-02 23:23:15.359804: step 23850, loss = 0.1599, acc = 0.9300 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 23:23:24.401192: step 23860, loss = 0.1556, acc = 0.9500 (289.2 examples/sec; 0.221 sec/batch)
2017-05-02 23:23:33.577022: step 23870, loss = 0.1506, acc = 0.9500 (276.9 examples/sec; 0.231 sec/batch)
2017-05-02 23:23:42.792319: step 23880, loss = 0.1594, acc = 0.9540 (278.3 examples/sec; 0.230 sec/batch)
2017-05-02 23:23:51.785063: step 23890, loss = 0.1793, acc = 0.9380 (293.2 examples/sec; 0.218 sec/batch)
2017-05-02 23:24:00.933473: step 23900, loss = 0.1460, acc = 0.9480 (274.6 examples/sec; 0.233 sec/batch)
2017-05-02 23:24:09.978427: step 23910, loss = 0.1429, acc = 0.9540 (270.2 examples/sec; 0.237 sec/batch)
2017-05-02 23:24:19.161441: step 23920, loss = 0.1356, acc = 0.9540 (282.8 examples/sec; 0.226 sec/batch)
2017-05-02 23:24:28.468744: step 23930, loss = 0.1458, acc = 0.9580 (281.3 examples/sec; 0.227 sec/batch)
2017-05-02 23:24:37.711062: step 23940, loss = 0.1483, acc = 0.9460 (269.5 examples/sec; 0.237 sec/batch)
2017-05-02 23:24:46.969930: step 23950, loss = 0.1488, acc = 0.9380 (265.5 examples/sec; 0.241 sec/batch)
2017-05-02 23:24:56.067154: step 23960, loss = 0.1491, acc = 0.9520 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 23:25:05.228596: step 23970, loss = 0.1498, acc = 0.9480 (279.1 examples/sec; 0.229 sec/batch)
2017-05-02 23:25:14.411670: step 23980, loss = 0.1548, acc = 0.9560 (290.4 examples/sec; 0.220 sec/batch)
2017-05-02 23:25:23.593672: step 23990, loss = 0.1586, acc = 0.9440 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 23:25:32.598045: step 24000, loss = 0.1444, acc = 0.9460 (287.0 examples/sec; 0.223 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 23:25:33.102089: step 24000, loss = 0.1241, acc = 0.9560, f1neg = 0.9517, f1pos = 0.9596, f1 = 0.9556
[Eval_batch(1)(2000,4000)] 2017-05-02 23:25:33.608252: step 24000, loss = 0.1241, acc = 0.9630, f1neg = 0.9566, f1pos = 0.9677, f1 = 0.9622
[Eval_batch(2)(2000,6000)] 2017-05-02 23:25:34.091184: step 24000, loss = 0.1375, acc = 0.9575, f1neg = 0.9546, f1pos = 0.9601, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-02 23:25:34.567226: step 24000, loss = 0.1452, acc = 0.9520, f1neg = 0.9519, f1pos = 0.9521, f1 = 0.9520
[Eval_batch(4)(2000,10000)] 2017-05-02 23:25:35.037813: step 24000, loss = 0.1447, acc = 0.9505, f1neg = 0.9515, f1pos = 0.9494, f1 = 0.9505
[Eval_batch(5)(2000,12000)] 2017-05-02 23:25:35.515978: step 24000, loss = 0.1409, acc = 0.9500, f1neg = 0.9461, f1pos = 0.9534, f1 = 0.9497
[Eval_batch(6)(2000,14000)] 2017-05-02 23:25:36.018934: step 24000, loss = 0.1317, acc = 0.9600, f1neg = 0.9590, f1pos = 0.9609, f1 = 0.9600
[Eval_batch(7)(2000,16000)] 2017-05-02 23:25:36.483871: step 24000, loss = 0.1290, acc = 0.9575, f1neg = 0.9489, f1pos = 0.9636, f1 = 0.9563
[Eval_batch(8)(2000,18000)] 2017-05-02 23:25:36.989112: step 24000, loss = 0.1253, acc = 0.9590, f1neg = 0.9530, f1pos = 0.9637, f1 = 0.9583
[Eval_batch(9)(2000,20000)] 2017-05-02 23:25:37.490110: step 24000, loss = 0.1281, acc = 0.9630, f1neg = 0.9603, f1pos = 0.9654, f1 = 0.9628
[Eval_batch(10)(2000,22000)] 2017-05-02 23:25:37.994296: step 24000, loss = 0.1401, acc = 0.9515, f1neg = 0.9477, f1pos = 0.9548, f1 = 0.9512
[Eval_batch(11)(2000,24000)] 2017-05-02 23:25:38.475643: step 24000, loss = 0.1378, acc = 0.9585, f1neg = 0.9518, f1pos = 0.9636, f1 = 0.9577
[Eval_batch(12)(2000,26000)] 2017-05-02 23:25:38.979260: step 24000, loss = 0.1356, acc = 0.9510, f1neg = 0.9495, f1pos = 0.9524, f1 = 0.9510
[Eval_batch(13)(2000,28000)] 2017-05-02 23:25:39.491918: step 24000, loss = 0.1223, acc = 0.9625, f1neg = 0.9527, f1pos = 0.9689, f1 = 0.9608
[Eval_batch(14)(2000,30000)] 2017-05-02 23:25:39.970050: step 24000, loss = 0.1491, acc = 0.9485, f1neg = 0.9378, f1pos = 0.9561, f1 = 0.9469
[Eval_batch(15)(2000,32000)] 2017-05-02 23:25:40.477059: step 24000, loss = 0.1291, acc = 0.9645, f1neg = 0.9616, f1pos = 0.9670, f1 = 0.9643
[Eval_batch(16)(2000,34000)] 2017-05-02 23:25:40.940472: step 24000, loss = 0.1255, acc = 0.9605, f1neg = 0.9587, f1pos = 0.9621, f1 = 0.9604
[Eval_batch(17)(2000,36000)] 2017-05-02 23:25:41.404510: step 24000, loss = 0.1467, acc = 0.9545, f1neg = 0.9545, f1pos = 0.9545, f1 = 0.9545
[Eval_batch(18)(2000,38000)] 2017-05-02 23:25:41.874405: step 24000, loss = 0.1451, acc = 0.9520, f1neg = 0.9537, f1pos = 0.9502, f1 = 0.9519
[Eval_batch(19)(2000,40000)] 2017-05-02 23:25:42.341937: step 24000, loss = 0.1260, acc = 0.9655, f1neg = 0.9600, f1pos = 0.9697, f1 = 0.9648
[Eval_batch(20)(2000,42000)] 2017-05-02 23:25:42.816423: step 24000, loss = 0.1298, acc = 0.9595, f1neg = 0.9640, f1pos = 0.9538, f1 = 0.9589
[Eval_batch(21)(2000,44000)] 2017-05-02 23:25:43.283692: step 24000, loss = 0.1194, acc = 0.9625, f1neg = 0.9589, f1pos = 0.9655, f1 = 0.9622
[Eval_batch(22)(2000,46000)] 2017-05-02 23:25:43.759607: step 24000, loss = 0.1278, acc = 0.9565, f1neg = 0.9567, f1pos = 0.9563, f1 = 0.9565
[Eval_batch(23)(2000,48000)] 2017-05-02 23:25:44.258383: step 24000, loss = 0.1333, acc = 0.9555, f1neg = 0.9495, f1pos = 0.9603, f1 = 0.9549
[Eval_batch(24)(2000,50000)] 2017-05-02 23:25:44.705266: step 24000, loss = 0.1213, acc = 0.9610, f1neg = 0.9549, f1pos = 0.9656, f1 = 0.9603
[Eval_batch(25)(2000,52000)] 2017-05-02 23:25:45.180309: step 24000, loss = 0.1122, acc = 0.9670, f1neg = 0.9636, f1pos = 0.9698, f1 = 0.9667
[Eval_batch(26)(2000,54000)] 2017-05-02 23:25:45.658955: step 24000, loss = 0.1247, acc = 0.9560, f1neg = 0.9583, f1pos = 0.9535, f1 = 0.9559
[Eval_batch(27)(2000,56000)] 2017-05-02 23:25:46.218601: step 24000, loss = 0.1135, acc = 0.9650, f1neg = 0.9631, f1pos = 0.9667, f1 = 0.9649
[Eval] 2017-05-02 23:25:46.218723: step 24000, acc = 0.9579, f1 = 0.9574
[Test_batch(0)(2000,2000)] 2017-05-02 23:25:46.703702: step 24000, loss = 0.1603, acc = 0.9495, f1neg = 0.9518, f1pos = 0.9469, f1 = 0.9494
[Test_batch(1)(2000,4000)] 2017-05-02 23:25:47.182753: step 24000, loss = 0.1453, acc = 0.9510, f1neg = 0.9562, f1pos = 0.9444, f1 = 0.9503
[Test_batch(2)(2000,6000)] 2017-05-02 23:25:47.645298: step 24000, loss = 0.1595, acc = 0.9515, f1neg = 0.9552, f1pos = 0.9472, f1 = 0.9512
[Test_batch(3)(2000,8000)] 2017-05-02 23:25:48.153104: step 24000, loss = 0.1665, acc = 0.9415, f1neg = 0.9449, f1pos = 0.9376, f1 = 0.9413
[Test_batch(4)(2000,10000)] 2017-05-02 23:25:48.622178: step 24000, loss = 0.1658, acc = 0.9430, f1neg = 0.9428, f1pos = 0.9432, f1 = 0.9430
[Test_batch(5)(2000,12000)] 2017-05-02 23:25:49.097419: step 24000, loss = 0.1751, acc = 0.9380, f1neg = 0.9384, f1pos = 0.9376, f1 = 0.9380
[Test_batch(6)(2000,14000)] 2017-05-02 23:25:49.611519: step 24000, loss = 0.1619, acc = 0.9410, f1neg = 0.9386, f1pos = 0.9432, f1 = 0.9409
[Test_batch(7)(2000,16000)] 2017-05-02 23:25:50.081039: step 24000, loss = 0.1502, acc = 0.9490, f1neg = 0.9534, f1pos = 0.9437, f1 = 0.9485
[Test_batch(8)(2000,18000)] 2017-05-02 23:25:50.558980: step 24000, loss = 0.1514, acc = 0.9475, f1neg = 0.9467, f1pos = 0.9483, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-02 23:25:51.033177: step 24000, loss = 0.1809, acc = 0.9365, f1neg = 0.9328, f1pos = 0.9398, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-02 23:25:51.538972: step 24000, loss = 0.1564, acc = 0.9455, f1neg = 0.9384, f1pos = 0.9511, f1 = 0.9448
[Test_batch(11)(2000,24000)] 2017-05-02 23:25:52.013723: step 24000, loss = 0.1584, acc = 0.9460, f1neg = 0.9474, f1pos = 0.9445, f1 = 0.9460
[Test_batch(12)(2000,26000)] 2017-05-02 23:25:52.512147: step 24000, loss = 0.1352, acc = 0.9565, f1neg = 0.9580, f1pos = 0.9549, f1 = 0.9564
[Test_batch(13)(2000,28000)] 2017-05-02 23:25:52.981106: step 24000, loss = 0.1622, acc = 0.9400, f1neg = 0.9329, f1pos = 0.9458, f1 = 0.9393
[Test_batch(14)(2000,30000)] 2017-05-02 23:25:53.446781: step 24000, loss = 0.1336, acc = 0.9600, f1neg = 0.9541, f1pos = 0.9645, f1 = 0.9593
[Test_batch(15)(2000,32000)] 2017-05-02 23:25:53.922250: step 24000, loss = 0.1560, acc = 0.9470, f1neg = 0.9459, f1pos = 0.9480, f1 = 0.9470
[Test_batch(16)(2000,34000)] 2017-05-02 23:25:54.397685: step 24000, loss = 0.1395, acc = 0.9535, f1neg = 0.9522, f1pos = 0.9547, f1 = 0.9535
[Test_batch(17)(2000,36000)] 2017-05-02 23:25:54.864311: step 24000, loss = 0.1324, acc = 0.9595, f1neg = 0.9556, f1pos = 0.9628, f1 = 0.9592
[Test_batch(18)(2000,38000)] 2017-05-02 23:25:55.400174: step 24000, loss = 0.1235, acc = 0.9595, f1neg = 0.9584, f1pos = 0.9605, f1 = 0.9595
[Test] 2017-05-02 23:25:55.400262: step 24000, acc = 0.9482, f1 = 0.9480
[Status] 2017-05-02 23:25:55.400289: step 24000, maxindex = 24000, maxdev = 0.9579, maxtst = 0.9482
2017-05-02 23:26:07.831359: step 24010, loss = 0.1575, acc = 0.9480 (240.4 examples/sec; 0.266 sec/batch)
2017-05-02 23:26:16.901899: step 24020, loss = 0.1433, acc = 0.9500 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 23:26:25.986677: step 24030, loss = 0.1466, acc = 0.9440 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 23:26:35.026944: step 24040, loss = 0.1590, acc = 0.9460 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 23:26:44.149764: step 24050, loss = 0.1503, acc = 0.9480 (283.7 examples/sec; 0.226 sec/batch)
2017-05-02 23:26:53.356241: step 24060, loss = 0.1631, acc = 0.9460 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 23:27:02.565247: step 24070, loss = 0.1361, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-02 23:27:11.755137: step 24080, loss = 0.1917, acc = 0.9360 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 23:27:20.859577: step 24090, loss = 0.1315, acc = 0.9620 (269.8 examples/sec; 0.237 sec/batch)
2017-05-02 23:27:29.924433: step 24100, loss = 0.1612, acc = 0.9540 (293.8 examples/sec; 0.218 sec/batch)
2017-05-02 23:27:39.139702: step 24110, loss = 0.1539, acc = 0.9540 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 23:27:48.413677: step 24120, loss = 0.1415, acc = 0.9580 (272.8 examples/sec; 0.235 sec/batch)
2017-05-02 23:27:57.665601: step 24130, loss = 0.1501, acc = 0.9500 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 23:28:06.788250: step 24140, loss = 0.1276, acc = 0.9600 (278.5 examples/sec; 0.230 sec/batch)
2017-05-02 23:28:15.938291: step 24150, loss = 0.1396, acc = 0.9560 (265.5 examples/sec; 0.241 sec/batch)
2017-05-02 23:28:25.209945: step 24160, loss = 0.1356, acc = 0.9540 (276.1 examples/sec; 0.232 sec/batch)
2017-05-02 23:28:34.367705: step 24170, loss = 0.1593, acc = 0.9440 (290.1 examples/sec; 0.221 sec/batch)
2017-05-02 23:28:43.562752: step 24180, loss = 0.1644, acc = 0.9540 (267.3 examples/sec; 0.239 sec/batch)
2017-05-02 23:28:52.617473: step 24190, loss = 0.1308, acc = 0.9640 (274.3 examples/sec; 0.233 sec/batch)
2017-05-02 23:29:02.728885: step 24200, loss = 0.1331, acc = 0.9560 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 23:29:11.998227: step 24210, loss = 0.1517, acc = 0.9480 (271.8 examples/sec; 0.235 sec/batch)
2017-05-02 23:29:21.279881: step 24220, loss = 0.1613, acc = 0.9380 (296.1 examples/sec; 0.216 sec/batch)
2017-05-02 23:29:30.471401: step 24230, loss = 0.1450, acc = 0.9580 (291.9 examples/sec; 0.219 sec/batch)
2017-05-02 23:29:39.454421: step 24240, loss = 0.1572, acc = 0.9500 (291.4 examples/sec; 0.220 sec/batch)
2017-05-02 23:29:48.412536: step 24250, loss = 0.1374, acc = 0.9600 (294.5 examples/sec; 0.217 sec/batch)
2017-05-02 23:29:57.431130: step 24260, loss = 0.1493, acc = 0.9580 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 23:30:06.562951: step 24270, loss = 0.1332, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-02 23:30:15.676485: step 24280, loss = 0.1330, acc = 0.9560 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 23:30:24.952265: step 24290, loss = 0.1501, acc = 0.9520 (263.0 examples/sec; 0.243 sec/batch)
2017-05-02 23:30:34.299080: step 24300, loss = 0.1352, acc = 0.9500 (265.4 examples/sec; 0.241 sec/batch)
2017-05-02 23:30:43.319546: step 24310, loss = 0.1253, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 23:30:52.383968: step 24320, loss = 0.1521, acc = 0.9420 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 23:31:01.558761: step 24330, loss = 0.1939, acc = 0.9340 (269.7 examples/sec; 0.237 sec/batch)
2017-05-02 23:31:10.611491: step 24340, loss = 0.1383, acc = 0.9560 (286.5 examples/sec; 0.223 sec/batch)
2017-05-02 23:31:19.799546: step 24350, loss = 0.1618, acc = 0.9320 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 23:31:28.913385: step 24360, loss = 0.1386, acc = 0.9600 (294.2 examples/sec; 0.218 sec/batch)
2017-05-02 23:31:37.966604: step 24370, loss = 0.1354, acc = 0.9580 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 23:31:47.016566: step 24380, loss = 0.1744, acc = 0.9360 (295.9 examples/sec; 0.216 sec/batch)
2017-05-02 23:31:56.044076: step 24390, loss = 0.1770, acc = 0.9440 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 23:32:05.289418: step 24400, loss = 0.1438, acc = 0.9460 (282.2 examples/sec; 0.227 sec/batch)
2017-05-02 23:32:14.396659: step 24410, loss = 0.1338, acc = 0.9520 (289.0 examples/sec; 0.221 sec/batch)
2017-05-02 23:32:23.490497: step 24420, loss = 0.1577, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 23:32:32.756905: step 24430, loss = 0.1671, acc = 0.9420 (272.3 examples/sec; 0.235 sec/batch)
2017-05-02 23:32:41.992741: step 24440, loss = 0.1395, acc = 0.9420 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 23:32:51.216058: step 24450, loss = 0.1535, acc = 0.9460 (289.7 examples/sec; 0.221 sec/batch)
2017-05-02 23:33:00.290721: step 24460, loss = 0.1323, acc = 0.9580 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 23:33:09.500824: step 24470, loss = 0.1561, acc = 0.9420 (282.1 examples/sec; 0.227 sec/batch)
2017-05-02 23:33:18.770516: step 24480, loss = 0.1343, acc = 0.9600 (261.4 examples/sec; 0.245 sec/batch)
2017-05-02 23:33:27.832140: step 24490, loss = 0.1298, acc = 0.9580 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 23:33:37.126803: step 24500, loss = 0.1383, acc = 0.9580 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 23:33:46.354759: step 24510, loss = 0.1513, acc = 0.9520 (288.9 examples/sec; 0.222 sec/batch)
2017-05-02 23:33:55.578938: step 24520, loss = 0.1767, acc = 0.9340 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 23:34:04.928560: step 24530, loss = 0.1417, acc = 0.9560 (278.9 examples/sec; 0.229 sec/batch)
2017-05-02 23:34:14.056014: step 24540, loss = 0.1240, acc = 0.9620 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 23:34:23.494758: step 24550, loss = 0.1483, acc = 0.9440 (258.3 examples/sec; 0.248 sec/batch)
2017-05-02 23:34:32.542329: step 24560, loss = 0.1359, acc = 0.9560 (289.2 examples/sec; 0.221 sec/batch)
2017-05-02 23:34:41.609866: step 24570, loss = 0.1309, acc = 0.9460 (269.3 examples/sec; 0.238 sec/batch)
2017-05-02 23:34:50.744710: step 24580, loss = 0.1689, acc = 0.9380 (280.1 examples/sec; 0.229 sec/batch)
2017-05-02 23:34:59.990410: step 24590, loss = 0.1716, acc = 0.9480 (263.7 examples/sec; 0.243 sec/batch)
2017-05-02 23:35:09.156712: step 24600, loss = 0.1316, acc = 0.9520 (265.7 examples/sec; 0.241 sec/batch)
2017-05-02 23:35:18.189651: step 24610, loss = 0.1400, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:35:27.347096: step 24620, loss = 0.1576, acc = 0.9460 (280.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:35:36.577380: step 24630, loss = 0.1758, acc = 0.9480 (273.0 examples/sec; 0.234 sec/batch)
2017-05-02 23:35:45.709766: step 24640, loss = 0.1367, acc = 0.9560 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 23:35:54.905798: step 24650, loss = 0.1323, acc = 0.9620 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 23:36:04.047635: step 24660, loss = 0.1591, acc = 0.9440 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 23:36:13.347103: step 24670, loss = 0.1468, acc = 0.9600 (287.9 examples/sec; 0.222 sec/batch)
2017-05-02 23:36:22.449840: step 24680, loss = 0.1472, acc = 0.9580 (281.7 examples/sec; 0.227 sec/batch)
2017-05-02 23:36:31.702446: step 24690, loss = 0.1848, acc = 0.9200 (279.8 examples/sec; 0.229 sec/batch)
2017-05-02 23:36:40.723973: step 24700, loss = 0.1532, acc = 0.9500 (298.0 examples/sec; 0.215 sec/batch)
2017-05-02 23:36:49.988637: step 24710, loss = 0.1428, acc = 0.9560 (280.8 examples/sec; 0.228 sec/batch)
2017-05-02 23:36:59.379016: step 24720, loss = 0.1438, acc = 0.9560 (243.7 examples/sec; 0.263 sec/batch)
2017-05-02 23:37:08.585037: step 24730, loss = 0.1356, acc = 0.9580 (263.0 examples/sec; 0.243 sec/batch)
2017-05-02 23:37:17.867150: step 24740, loss = 0.1550, acc = 0.9480 (277.5 examples/sec; 0.231 sec/batch)
2017-05-02 23:37:27.183934: step 24750, loss = 0.1543, acc = 0.9600 (273.8 examples/sec; 0.234 sec/batch)
2017-05-02 23:37:36.442422: step 24760, loss = 0.1354, acc = 0.9640 (271.8 examples/sec; 0.235 sec/batch)
2017-05-02 23:37:45.669475: step 24770, loss = 0.1511, acc = 0.9520 (266.0 examples/sec; 0.241 sec/batch)
2017-05-02 23:37:54.662446: step 24780, loss = 0.1588, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 23:38:03.769055: step 24790, loss = 0.1625, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 23:38:12.896543: step 24800, loss = 0.1472, acc = 0.9460 (266.4 examples/sec; 0.240 sec/batch)
2017-05-02 23:38:21.899251: step 24810, loss = 0.1409, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-02 23:38:31.091127: step 24820, loss = 0.1724, acc = 0.9400 (256.0 examples/sec; 0.250 sec/batch)
2017-05-02 23:38:40.317521: step 24830, loss = 0.1620, acc = 0.9500 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 23:38:49.431359: step 24840, loss = 0.1489, acc = 0.9440 (286.8 examples/sec; 0.223 sec/batch)
2017-05-02 23:38:58.572332: step 24850, loss = 0.1694, acc = 0.9380 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 23:39:07.635882: step 24860, loss = 0.1453, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-02 23:39:16.792932: step 24870, loss = 0.1555, acc = 0.9420 (286.7 examples/sec; 0.223 sec/batch)
2017-05-02 23:39:25.752879: step 24880, loss = 0.1412, acc = 0.9620 (299.0 examples/sec; 0.214 sec/batch)
2017-05-02 23:39:34.997413: step 24890, loss = 0.1497, acc = 0.9460 (261.5 examples/sec; 0.245 sec/batch)
2017-05-02 23:39:44.073694: step 24900, loss = 0.1643, acc = 0.9480 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:39:53.221216: step 24910, loss = 0.1409, acc = 0.9560 (280.3 examples/sec; 0.228 sec/batch)
2017-05-02 23:40:02.425848: step 24920, loss = 0.1624, acc = 0.9460 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:40:11.493060: step 24930, loss = 0.1505, acc = 0.9500 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 23:40:20.708822: step 24940, loss = 0.1333, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-02 23:40:29.815330: step 24950, loss = 0.1416, acc = 0.9600 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 23:40:38.764779: step 24960, loss = 0.1311, acc = 0.9600 (286.3 examples/sec; 0.224 sec/batch)
2017-05-02 23:40:47.826232: step 24970, loss = 0.1568, acc = 0.9420 (280.1 examples/sec; 0.229 sec/batch)
2017-05-02 23:40:56.926649: step 24980, loss = 0.1554, acc = 0.9560 (282.5 examples/sec; 0.227 sec/batch)
2017-05-02 23:41:06.320111: step 24990, loss = 0.1510, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-02 23:41:15.378695: step 25000, loss = 0.1473, acc = 0.9560 (288.5 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 23:41:15.874213: step 25000, loss = 0.1231, acc = 0.9590, f1neg = 0.9558, f1pos = 0.9618, f1 = 0.9588
[Eval_batch(1)(2000,4000)] 2017-05-02 23:41:16.359062: step 25000, loss = 0.1319, acc = 0.9610, f1neg = 0.9555, f1pos = 0.9653, f1 = 0.9604
[Eval_batch(2)(2000,6000)] 2017-05-02 23:41:16.821367: step 25000, loss = 0.1383, acc = 0.9525, f1neg = 0.9504, f1pos = 0.9544, f1 = 0.9524
[Eval_batch(3)(2000,8000)] 2017-05-02 23:41:17.284917: step 25000, loss = 0.1428, acc = 0.9510, f1neg = 0.9520, f1pos = 0.9500, f1 = 0.9510
[Eval_batch(4)(2000,10000)] 2017-05-02 23:41:17.781915: step 25000, loss = 0.1420, acc = 0.9485, f1neg = 0.9510, f1pos = 0.9458, f1 = 0.9484
[Eval_batch(5)(2000,12000)] 2017-05-02 23:41:18.278364: step 25000, loss = 0.1416, acc = 0.9470, f1neg = 0.9446, f1pos = 0.9492, f1 = 0.9469
[Eval_batch(6)(2000,14000)] 2017-05-02 23:41:18.754163: step 25000, loss = 0.1278, acc = 0.9615, f1neg = 0.9611, f1pos = 0.9619, f1 = 0.9615
[Eval_batch(7)(2000,16000)] 2017-05-02 23:41:19.256701: step 25000, loss = 0.1319, acc = 0.9530, f1neg = 0.9450, f1pos = 0.9590, f1 = 0.9520
[Eval_batch(8)(2000,18000)] 2017-05-02 23:41:19.763333: step 25000, loss = 0.1267, acc = 0.9570, f1neg = 0.9520, f1pos = 0.9611, f1 = 0.9565
[Eval_batch(9)(2000,20000)] 2017-05-02 23:41:20.225461: step 25000, loss = 0.1313, acc = 0.9580, f1neg = 0.9557, f1pos = 0.9600, f1 = 0.9579
[Eval_batch(10)(2000,22000)] 2017-05-02 23:41:20.721544: step 25000, loss = 0.1396, acc = 0.9590, f1neg = 0.9570, f1pos = 0.9608, f1 = 0.9589
[Eval_batch(11)(2000,24000)] 2017-05-02 23:41:21.203123: step 25000, loss = 0.1415, acc = 0.9560, f1neg = 0.9500, f1pos = 0.9607, f1 = 0.9554
[Eval_batch(12)(2000,26000)] 2017-05-02 23:41:21.672278: step 25000, loss = 0.1368, acc = 0.9515, f1neg = 0.9509, f1pos = 0.9521, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-02 23:41:22.146000: step 25000, loss = 0.1251, acc = 0.9630, f1neg = 0.9544, f1pos = 0.9689, f1 = 0.9616
[Eval_batch(14)(2000,30000)] 2017-05-02 23:41:22.650004: step 25000, loss = 0.1513, acc = 0.9520, f1neg = 0.9439, f1pos = 0.9581, f1 = 0.9510
[Eval_batch(15)(2000,32000)] 2017-05-02 23:41:23.126603: step 25000, loss = 0.1328, acc = 0.9565, f1neg = 0.9539, f1pos = 0.9588, f1 = 0.9564
[Eval_batch(16)(2000,34000)] 2017-05-02 23:41:23.607116: step 25000, loss = 0.1273, acc = 0.9615, f1neg = 0.9604, f1pos = 0.9626, f1 = 0.9615
[Eval_batch(17)(2000,36000)] 2017-05-02 23:41:24.054678: step 25000, loss = 0.1476, acc = 0.9550, f1neg = 0.9559, f1pos = 0.9541, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-02 23:41:24.529762: step 25000, loss = 0.1419, acc = 0.9530, f1neg = 0.9555, f1pos = 0.9502, f1 = 0.9529
[Eval_batch(19)(2000,40000)] 2017-05-02 23:41:25.030552: step 25000, loss = 0.1269, acc = 0.9630, f1neg = 0.9578, f1pos = 0.9671, f1 = 0.9624
[Eval_batch(20)(2000,42000)] 2017-05-02 23:41:25.494308: step 25000, loss = 0.1227, acc = 0.9575, f1neg = 0.9626, f1pos = 0.9507, f1 = 0.9567
[Eval_batch(21)(2000,44000)] 2017-05-02 23:41:25.997777: step 25000, loss = 0.1269, acc = 0.9560, f1neg = 0.9529, f1pos = 0.9587, f1 = 0.9558
[Eval_batch(22)(2000,46000)] 2017-05-02 23:41:26.488202: step 25000, loss = 0.1288, acc = 0.9585, f1neg = 0.9594, f1pos = 0.9575, f1 = 0.9585
[Eval_batch(23)(2000,48000)] 2017-05-02 23:41:26.963486: step 25000, loss = 0.1365, acc = 0.9555, f1neg = 0.9507, f1pos = 0.9595, f1 = 0.9551
[Eval_batch(24)(2000,50000)] 2017-05-02 23:41:27.469096: step 25000, loss = 0.1245, acc = 0.9605, f1neg = 0.9555, f1pos = 0.9645, f1 = 0.9600
[Eval_batch(25)(2000,52000)] 2017-05-02 23:41:27.948243: step 25000, loss = 0.1120, acc = 0.9685, f1neg = 0.9659, f1pos = 0.9707, f1 = 0.9683
[Eval_batch(26)(2000,54000)] 2017-05-02 23:41:28.429089: step 25000, loss = 0.1214, acc = 0.9595, f1neg = 0.9623, f1pos = 0.9562, f1 = 0.9593
[Eval_batch(27)(2000,56000)] 2017-05-02 23:41:29.023906: step 25000, loss = 0.1095, acc = 0.9665, f1neg = 0.9652, f1pos = 0.9677, f1 = 0.9665
[Eval] 2017-05-02 23:41:29.023974: step 25000, acc = 0.9572, f1 = 0.9569
[Test_batch(0)(2000,2000)] 2017-05-02 23:41:29.536007: step 25000, loss = 0.1605, acc = 0.9490, f1neg = 0.9524, f1pos = 0.9451, f1 = 0.9487
[Test_batch(1)(2000,4000)] 2017-05-02 23:41:30.044451: step 25000, loss = 0.1429, acc = 0.9535, f1neg = 0.9594, f1pos = 0.9456, f1 = 0.9525
[Test_batch(2)(2000,6000)] 2017-05-02 23:41:30.544936: step 25000, loss = 0.1558, acc = 0.9540, f1neg = 0.9584, f1pos = 0.9486, f1 = 0.9535
[Test_batch(3)(2000,8000)] 2017-05-02 23:41:31.059225: step 25000, loss = 0.1675, acc = 0.9400, f1neg = 0.9450, f1pos = 0.9340, f1 = 0.9395
[Test_batch(4)(2000,10000)] 2017-05-02 23:41:31.530812: step 25000, loss = 0.1691, acc = 0.9400, f1neg = 0.9414, f1pos = 0.9385, f1 = 0.9400
[Test_batch(5)(2000,12000)] 2017-05-02 23:41:31.986524: step 25000, loss = 0.1789, acc = 0.9345, f1neg = 0.9367, f1pos = 0.9321, f1 = 0.9344
[Test_batch(6)(2000,14000)] 2017-05-02 23:41:32.493050: step 25000, loss = 0.1609, acc = 0.9490, f1neg = 0.9482, f1pos = 0.9498, f1 = 0.9490
[Test_batch(7)(2000,16000)] 2017-05-02 23:41:32.999559: step 25000, loss = 0.1515, acc = 0.9475, f1neg = 0.9533, f1pos = 0.9401, f1 = 0.9467
[Test_batch(8)(2000,18000)] 2017-05-02 23:41:33.502775: step 25000, loss = 0.1540, acc = 0.9475, f1neg = 0.9477, f1pos = 0.9473, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-02 23:41:33.983126: step 25000, loss = 0.1884, acc = 0.9285, f1neg = 0.9266, f1pos = 0.9303, f1 = 0.9284
[Test_batch(10)(2000,22000)] 2017-05-02 23:41:34.485096: step 25000, loss = 0.1673, acc = 0.9450, f1neg = 0.9396, f1pos = 0.9495, f1 = 0.9446
[Test_batch(11)(2000,24000)] 2017-05-02 23:41:34.995030: step 25000, loss = 0.1584, acc = 0.9510, f1neg = 0.9532, f1pos = 0.9486, f1 = 0.9509
[Test_batch(12)(2000,26000)] 2017-05-02 23:41:35.495014: step 25000, loss = 0.1411, acc = 0.9545, f1neg = 0.9568, f1pos = 0.9519, f1 = 0.9544
[Test_batch(13)(2000,28000)] 2017-05-02 23:41:35.975617: step 25000, loss = 0.1695, acc = 0.9455, f1neg = 0.9409, f1pos = 0.9495, f1 = 0.9452
[Test_batch(14)(2000,30000)] 2017-05-02 23:41:36.456925: step 25000, loss = 0.1437, acc = 0.9515, f1neg = 0.9456, f1pos = 0.9562, f1 = 0.9509
[Test_batch(15)(2000,32000)] 2017-05-02 23:41:36.935925: step 25000, loss = 0.1600, acc = 0.9505, f1neg = 0.9508, f1pos = 0.9502, f1 = 0.9505
[Test_batch(16)(2000,34000)] 2017-05-02 23:41:37.404830: step 25000, loss = 0.1425, acc = 0.9550, f1neg = 0.9547, f1pos = 0.9553, f1 = 0.9550
[Test_batch(17)(2000,36000)] 2017-05-02 23:41:37.881948: step 25000, loss = 0.1325, acc = 0.9585, f1neg = 0.9552, f1pos = 0.9613, f1 = 0.9583
[Test_batch(18)(2000,38000)] 2017-05-02 23:41:38.444903: step 25000, loss = 0.1205, acc = 0.9650, f1neg = 0.9649, f1pos = 0.9651, f1 = 0.9650
[Test] 2017-05-02 23:41:38.444976: step 25000, acc = 0.9484, f1 = 0.9482
[Status] 2017-05-02 23:41:38.444995: step 25000, maxindex = 24000, maxdev = 0.9579, maxtst = 0.9482
2017-05-02 23:41:47.729196: step 25010, loss = 0.1784, acc = 0.9400 (284.5 examples/sec; 0.225 sec/batch)
2017-05-02 23:41:56.681892: step 25020, loss = 0.1745, acc = 0.9440 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 23:42:05.913304: step 25030, loss = 0.1303, acc = 0.9540 (281.8 examples/sec; 0.227 sec/batch)
2017-05-02 23:42:15.016193: step 25040, loss = 0.1573, acc = 0.9460 (287.0 examples/sec; 0.223 sec/batch)
2017-05-02 23:42:24.074387: step 25050, loss = 0.1358, acc = 0.9500 (292.1 examples/sec; 0.219 sec/batch)
2017-05-02 23:42:33.249374: step 25060, loss = 0.1573, acc = 0.9440 (276.5 examples/sec; 0.231 sec/batch)
2017-05-02 23:42:42.399206: step 25070, loss = 0.1803, acc = 0.9280 (276.6 examples/sec; 0.231 sec/batch)
2017-05-02 23:42:51.581115: step 25080, loss = 0.1334, acc = 0.9640 (270.5 examples/sec; 0.237 sec/batch)
2017-05-02 23:43:00.634652: step 25090, loss = 0.1379, acc = 0.9500 (284.0 examples/sec; 0.225 sec/batch)
2017-05-02 23:43:09.631771: step 25100, loss = 0.1378, acc = 0.9540 (285.5 examples/sec; 0.224 sec/batch)
2017-05-02 23:43:18.762108: step 25110, loss = 0.1669, acc = 0.9400 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 23:43:28.032153: step 25120, loss = 0.1446, acc = 0.9440 (273.2 examples/sec; 0.234 sec/batch)
2017-05-02 23:43:36.974040: step 25130, loss = 0.1623, acc = 0.9380 (290.6 examples/sec; 0.220 sec/batch)
2017-05-02 23:43:46.022449: step 25140, loss = 0.1390, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 23:43:55.062862: step 25150, loss = 0.1345, acc = 0.9560 (275.1 examples/sec; 0.233 sec/batch)
2017-05-02 23:44:04.327559: step 25160, loss = 0.1327, acc = 0.9700 (268.8 examples/sec; 0.238 sec/batch)
2017-05-02 23:44:13.321208: step 25170, loss = 0.1584, acc = 0.9360 (289.0 examples/sec; 0.221 sec/batch)
2017-05-02 23:44:22.675232: step 25180, loss = 0.1619, acc = 0.9400 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 23:44:31.768931: step 25190, loss = 0.1369, acc = 0.9420 (284.9 examples/sec; 0.225 sec/batch)
2017-05-02 23:44:41.958856: step 25200, loss = 0.1823, acc = 0.9480 (277.0 examples/sec; 0.231 sec/batch)
2017-05-02 23:44:51.159050: step 25210, loss = 0.1310, acc = 0.9620 (277.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:45:00.336671: step 25220, loss = 0.1386, acc = 0.9500 (297.5 examples/sec; 0.215 sec/batch)
2017-05-02 23:45:09.505878: step 25230, loss = 0.1648, acc = 0.9480 (290.2 examples/sec; 0.221 sec/batch)
2017-05-02 23:45:18.608189: step 25240, loss = 0.1144, acc = 0.9700 (266.1 examples/sec; 0.241 sec/batch)
2017-05-02 23:45:27.671380: step 25250, loss = 0.1681, acc = 0.9460 (278.9 examples/sec; 0.230 sec/batch)
2017-05-02 23:45:36.756947: step 25260, loss = 0.1398, acc = 0.9520 (287.6 examples/sec; 0.223 sec/batch)
2017-05-02 23:45:45.848297: step 25270, loss = 0.1489, acc = 0.9600 (275.3 examples/sec; 0.233 sec/batch)
2017-05-02 23:45:55.035492: step 25280, loss = 0.1127, acc = 0.9740 (283.8 examples/sec; 0.226 sec/batch)
2017-05-02 23:46:04.234898: step 25290, loss = 0.1698, acc = 0.9460 (283.5 examples/sec; 0.226 sec/batch)
2017-05-02 23:46:13.412703: step 25300, loss = 0.1184, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-02 23:46:22.673336: step 25310, loss = 0.1556, acc = 0.9460 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:46:31.819337: step 25320, loss = 0.1490, acc = 0.9460 (288.0 examples/sec; 0.222 sec/batch)
2017-05-02 23:46:40.929181: step 25330, loss = 0.1763, acc = 0.9520 (278.2 examples/sec; 0.230 sec/batch)
2017-05-02 23:46:50.167753: step 25340, loss = 0.1191, acc = 0.9620 (257.7 examples/sec; 0.248 sec/batch)
2017-05-02 23:46:59.309005: step 25350, loss = 0.1526, acc = 0.9420 (284.2 examples/sec; 0.225 sec/batch)
2017-05-02 23:47:08.686021: step 25360, loss = 0.1135, acc = 0.9720 (272.2 examples/sec; 0.235 sec/batch)
2017-05-02 23:47:17.882175: step 25370, loss = 0.1471, acc = 0.9480 (282.4 examples/sec; 0.227 sec/batch)
2017-05-02 23:47:26.989324: step 25380, loss = 0.1317, acc = 0.9680 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 23:47:36.307812: step 25390, loss = 0.1769, acc = 0.9460 (291.4 examples/sec; 0.220 sec/batch)
2017-05-02 23:47:45.691115: step 25400, loss = 0.1671, acc = 0.9380 (263.7 examples/sec; 0.243 sec/batch)
2017-05-02 23:47:54.795744: step 25410, loss = 0.1420, acc = 0.9580 (278.9 examples/sec; 0.230 sec/batch)
2017-05-02 23:48:04.164527: step 25420, loss = 0.1793, acc = 0.9460 (266.8 examples/sec; 0.240 sec/batch)
2017-05-02 23:48:13.401814: step 25430, loss = 0.1479, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 23:48:22.650596: step 25440, loss = 0.1273, acc = 0.9760 (275.8 examples/sec; 0.232 sec/batch)
2017-05-02 23:48:31.911940: step 25450, loss = 0.1163, acc = 0.9740 (274.1 examples/sec; 0.234 sec/batch)
2017-05-02 23:48:41.082301: step 25460, loss = 0.1722, acc = 0.9340 (282.0 examples/sec; 0.227 sec/batch)
2017-05-02 23:48:50.116833: step 25470, loss = 0.1426, acc = 0.9540 (289.6 examples/sec; 0.221 sec/batch)
2017-05-02 23:48:59.258907: step 25480, loss = 0.1203, acc = 0.9720 (287.4 examples/sec; 0.223 sec/batch)
2017-05-02 23:49:08.359801: step 25490, loss = 0.1694, acc = 0.9440 (277.4 examples/sec; 0.231 sec/batch)
2017-05-02 23:49:17.431993: step 25500, loss = 0.1423, acc = 0.9520 (282.6 examples/sec; 0.226 sec/batch)
2017-05-02 23:49:26.612207: step 25510, loss = 0.1685, acc = 0.9520 (281.9 examples/sec; 0.227 sec/batch)
2017-05-02 23:49:35.777945: step 25520, loss = 0.1231, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-02 23:49:44.849282: step 25530, loss = 0.1458, acc = 0.9520 (294.5 examples/sec; 0.217 sec/batch)
2017-05-02 23:49:53.936207: step 25540, loss = 0.1239, acc = 0.9700 (280.1 examples/sec; 0.229 sec/batch)
2017-05-02 23:50:03.084268: step 25550, loss = 0.1592, acc = 0.9460 (275.7 examples/sec; 0.232 sec/batch)
2017-05-02 23:50:12.279176: step 25560, loss = 0.1419, acc = 0.9480 (271.9 examples/sec; 0.235 sec/batch)
2017-05-02 23:50:21.373690: step 25570, loss = 0.1640, acc = 0.9440 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 23:50:30.567076: step 25580, loss = 0.1067, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-02 23:50:39.673231: step 25590, loss = 0.1361, acc = 0.9560 (285.3 examples/sec; 0.224 sec/batch)
2017-05-02 23:50:48.857640: step 25600, loss = 0.1639, acc = 0.9360 (281.6 examples/sec; 0.227 sec/batch)
2017-05-02 23:50:57.886806: step 25610, loss = 0.1658, acc = 0.9420 (299.0 examples/sec; 0.214 sec/batch)
2017-05-02 23:51:07.099651: step 25620, loss = 0.1679, acc = 0.9400 (287.7 examples/sec; 0.222 sec/batch)
2017-05-02 23:51:16.200532: step 25630, loss = 0.1590, acc = 0.9460 (291.0 examples/sec; 0.220 sec/batch)
2017-05-02 23:51:25.477186: step 25640, loss = 0.1495, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-02 23:51:34.703938: step 25650, loss = 0.1143, acc = 0.9620 (288.1 examples/sec; 0.222 sec/batch)
2017-05-02 23:51:43.950887: step 25660, loss = 0.1738, acc = 0.9260 (257.0 examples/sec; 0.249 sec/batch)
2017-05-02 23:51:52.960126: step 25670, loss = 0.1170, acc = 0.9560 (265.5 examples/sec; 0.241 sec/batch)
2017-05-02 23:52:02.258635: step 25680, loss = 0.1487, acc = 0.9460 (284.7 examples/sec; 0.225 sec/batch)
2017-05-02 23:52:11.367534: step 25690, loss = 0.1506, acc = 0.9480 (283.3 examples/sec; 0.226 sec/batch)
2017-05-02 23:52:20.483705: step 25700, loss = 0.1538, acc = 0.9460 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 23:52:29.517832: step 25710, loss = 0.0993, acc = 0.9760 (280.7 examples/sec; 0.228 sec/batch)
2017-05-02 23:52:38.636392: step 25720, loss = 0.1346, acc = 0.9480 (286.2 examples/sec; 0.224 sec/batch)
2017-05-02 23:52:47.696155: step 25730, loss = 0.1301, acc = 0.9600 (291.6 examples/sec; 0.219 sec/batch)
2017-05-02 23:52:57.009690: step 25740, loss = 0.1425, acc = 0.9520 (268.1 examples/sec; 0.239 sec/batch)
2017-05-02 23:53:06.150987: step 25750, loss = 0.1678, acc = 0.9500 (284.4 examples/sec; 0.225 sec/batch)
2017-05-02 23:53:15.343709: step 25760, loss = 0.1410, acc = 0.9560 (285.6 examples/sec; 0.224 sec/batch)
2017-05-02 23:53:24.492514: step 25770, loss = 0.1435, acc = 0.9460 (291.5 examples/sec; 0.220 sec/batch)
2017-05-02 23:53:33.667892: step 25780, loss = 0.1410, acc = 0.9580 (270.0 examples/sec; 0.237 sec/batch)
2017-05-02 23:53:42.772033: step 25790, loss = 0.1368, acc = 0.9600 (288.6 examples/sec; 0.222 sec/batch)
2017-05-02 23:53:51.883268: step 25800, loss = 0.1421, acc = 0.9540 (275.4 examples/sec; 0.232 sec/batch)
2017-05-02 23:54:01.000957: step 25810, loss = 0.1379, acc = 0.9640 (260.6 examples/sec; 0.246 sec/batch)
2017-05-02 23:54:10.145052: step 25820, loss = 0.1391, acc = 0.9500 (283.6 examples/sec; 0.226 sec/batch)
2017-05-02 23:54:19.191884: step 25830, loss = 0.1418, acc = 0.9560 (278.0 examples/sec; 0.230 sec/batch)
2017-05-02 23:54:28.340187: step 25840, loss = 0.1231, acc = 0.9540 (286.6 examples/sec; 0.223 sec/batch)
2017-05-02 23:54:37.515238: step 25850, loss = 0.1423, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-02 23:54:46.629861: step 25860, loss = 0.1657, acc = 0.9420 (288.5 examples/sec; 0.222 sec/batch)
2017-05-02 23:54:55.756892: step 25870, loss = 0.1516, acc = 0.9500 (278.1 examples/sec; 0.230 sec/batch)
2017-05-02 23:55:04.948162: step 25880, loss = 0.1462, acc = 0.9560 (274.2 examples/sec; 0.233 sec/batch)
2017-05-02 23:55:14.163367: step 25890, loss = 0.1311, acc = 0.9660 (260.5 examples/sec; 0.246 sec/batch)
2017-05-02 23:55:23.330525: step 25900, loss = 0.1549, acc = 0.9460 (280.6 examples/sec; 0.228 sec/batch)
2017-05-02 23:55:32.810831: step 25910, loss = 0.1576, acc = 0.9500 (274.7 examples/sec; 0.233 sec/batch)
2017-05-02 23:55:41.867333: step 25920, loss = 0.1284, acc = 0.9680 (282.3 examples/sec; 0.227 sec/batch)
2017-05-02 23:55:51.049262: step 25930, loss = 0.1357, acc = 0.9560 (260.2 examples/sec; 0.246 sec/batch)
2017-05-02 23:56:00.280596: step 25940, loss = 0.1460, acc = 0.9460 (267.7 examples/sec; 0.239 sec/batch)
2017-05-02 23:56:09.467795: step 25950, loss = 0.1404, acc = 0.9560 (264.3 examples/sec; 0.242 sec/batch)
2017-05-02 23:56:18.536107: step 25960, loss = 0.1284, acc = 0.9660 (283.4 examples/sec; 0.226 sec/batch)
2017-05-02 23:56:27.510256: step 25970, loss = 0.1604, acc = 0.9380 (279.0 examples/sec; 0.229 sec/batch)
2017-05-02 23:56:36.742599: step 25980, loss = 0.1215, acc = 0.9620 (283.0 examples/sec; 0.226 sec/batch)
2017-05-02 23:56:45.866399: step 25990, loss = 0.1800, acc = 0.9340 (281.1 examples/sec; 0.228 sec/batch)
2017-05-02 23:56:55.059717: step 26000, loss = 0.1576, acc = 0.9400 (274.2 examples/sec; 0.233 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-02 23:56:55.529825: step 26000, loss = 0.1216, acc = 0.9555, f1neg = 0.9514, f1pos = 0.9590, f1 = 0.9552
[Eval_batch(1)(2000,4000)] 2017-05-02 23:56:56.028943: step 26000, loss = 0.1239, acc = 0.9640, f1neg = 0.9582, f1pos = 0.9684, f1 = 0.9633
[Eval_batch(2)(2000,6000)] 2017-05-02 23:56:56.502409: step 26000, loss = 0.1348, acc = 0.9575, f1neg = 0.9550, f1pos = 0.9598, f1 = 0.9574
[Eval_batch(3)(2000,8000)] 2017-05-02 23:56:56.956672: step 26000, loss = 0.1401, acc = 0.9555, f1neg = 0.9557, f1pos = 0.9553, f1 = 0.9555
[Eval_batch(4)(2000,10000)] 2017-05-02 23:56:57.461827: step 26000, loss = 0.1406, acc = 0.9535, f1neg = 0.9550, f1pos = 0.9519, f1 = 0.9534
[Eval_batch(5)(2000,12000)] 2017-05-02 23:56:57.937811: step 26000, loss = 0.1370, acc = 0.9505, f1neg = 0.9471, f1pos = 0.9535, f1 = 0.9503
[Eval_batch(6)(2000,14000)] 2017-05-02 23:56:58.419947: step 26000, loss = 0.1275, acc = 0.9630, f1neg = 0.9624, f1pos = 0.9636, f1 = 0.9630
[Eval_batch(7)(2000,16000)] 2017-05-02 23:56:58.923732: step 26000, loss = 0.1260, acc = 0.9575, f1neg = 0.9492, f1pos = 0.9635, f1 = 0.9563
[Eval_batch(8)(2000,18000)] 2017-05-02 23:56:59.397677: step 26000, loss = 0.1231, acc = 0.9605, f1neg = 0.9550, f1pos = 0.9648, f1 = 0.9599
[Eval_batch(9)(2000,20000)] 2017-05-02 23:56:59.867079: step 26000, loss = 0.1259, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9666, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-02 23:57:00.369339: step 26000, loss = 0.1354, acc = 0.9575, f1neg = 0.9547, f1pos = 0.9600, f1 = 0.9573
[Eval_batch(11)(2000,24000)] 2017-05-02 23:57:00.876732: step 26000, loss = 0.1359, acc = 0.9585, f1neg = 0.9522, f1pos = 0.9633, f1 = 0.9578
[Eval_batch(12)(2000,26000)] 2017-05-02 23:57:01.388622: step 26000, loss = 0.1325, acc = 0.9525, f1neg = 0.9513, f1pos = 0.9536, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-02 23:57:01.883551: step 26000, loss = 0.1207, acc = 0.9645, f1neg = 0.9557, f1pos = 0.9704, f1 = 0.9630
[Eval_batch(14)(2000,30000)] 2017-05-02 23:57:02.392350: step 26000, loss = 0.1465, acc = 0.9505, f1neg = 0.9408, f1pos = 0.9575, f1 = 0.9491
[Eval_batch(15)(2000,32000)] 2017-05-02 23:57:02.904204: step 26000, loss = 0.1265, acc = 0.9650, f1neg = 0.9624, f1pos = 0.9673, f1 = 0.9648
[Eval_batch(16)(2000,34000)] 2017-05-02 23:57:03.407208: step 26000, loss = 0.1228, acc = 0.9610, f1neg = 0.9594, f1pos = 0.9625, f1 = 0.9609
[Eval_batch(17)(2000,36000)] 2017-05-02 23:57:03.913349: step 26000, loss = 0.1441, acc = 0.9545, f1neg = 0.9548, f1pos = 0.9542, f1 = 0.9545
[Eval_batch(18)(2000,38000)] 2017-05-02 23:57:04.408017: step 26000, loss = 0.1411, acc = 0.9525, f1neg = 0.9544, f1pos = 0.9504, f1 = 0.9524
[Eval_batch(19)(2000,40000)] 2017-05-02 23:57:04.881424: step 26000, loss = 0.1231, acc = 0.9665, f1neg = 0.9614, f1pos = 0.9704, f1 = 0.9659
[Eval_batch(20)(2000,42000)] 2017-05-02 23:57:05.355559: step 26000, loss = 0.1235, acc = 0.9625, f1neg = 0.9668, f1pos = 0.9569, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-02 23:57:05.824266: step 26000, loss = 0.1191, acc = 0.9595, f1neg = 0.9560, f1pos = 0.9624, f1 = 0.9592
[Eval_batch(22)(2000,46000)] 2017-05-02 23:57:06.329739: step 26000, loss = 0.1254, acc = 0.9590, f1neg = 0.9595, f1pos = 0.9585, f1 = 0.9590
[Eval_batch(23)(2000,48000)] 2017-05-02 23:57:06.823727: step 26000, loss = 0.1310, acc = 0.9555, f1neg = 0.9498, f1pos = 0.9600, f1 = 0.9549
[Eval_batch(24)(2000,50000)] 2017-05-02 23:57:07.315420: step 26000, loss = 0.1192, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(25)(2000,52000)] 2017-05-02 23:57:07.790666: step 26000, loss = 0.1089, acc = 0.9680, f1neg = 0.9650, f1pos = 0.9706, f1 = 0.9678
[Eval_batch(26)(2000,54000)] 2017-05-02 23:57:08.293367: step 26000, loss = 0.1209, acc = 0.9575, f1neg = 0.9599, f1pos = 0.9548, f1 = 0.9573
[Eval_batch(27)(2000,56000)] 2017-05-02 23:57:08.880154: step 26000, loss = 0.1086, acc = 0.9670, f1neg = 0.9654, f1pos = 0.9685, f1 = 0.9669
[Eval] 2017-05-02 23:57:08.880237: step 26000, acc = 0.9591, f1 = 0.9588
[Test_batch(0)(2000,2000)] 2017-05-02 23:57:09.363341: step 26000, loss = 0.1575, acc = 0.9525, f1neg = 0.9550, f1pos = 0.9497, f1 = 0.9524
[Test_batch(1)(2000,4000)] 2017-05-02 23:57:09.828637: step 26000, loss = 0.1410, acc = 0.9535, f1neg = 0.9588, f1pos = 0.9466, f1 = 0.9527
[Test_batch(2)(2000,6000)] 2017-05-02 23:57:10.330224: step 26000, loss = 0.1552, acc = 0.9520, f1neg = 0.9561, f1pos = 0.9471, f1 = 0.9516
[Test_batch(3)(2000,8000)] 2017-05-02 23:57:10.817392: step 26000, loss = 0.1628, acc = 0.9440, f1neg = 0.9479, f1pos = 0.9395, f1 = 0.9437
[Test_batch(4)(2000,10000)] 2017-05-02 23:57:11.316093: step 26000, loss = 0.1627, acc = 0.9440, f1neg = 0.9443, f1pos = 0.9437, f1 = 0.9440
[Test_batch(5)(2000,12000)] 2017-05-02 23:57:11.809897: step 26000, loss = 0.1726, acc = 0.9380, f1neg = 0.9390, f1pos = 0.9369, f1 = 0.9380
[Test_batch(6)(2000,14000)] 2017-05-02 23:57:12.285503: step 26000, loss = 0.1578, acc = 0.9460, f1neg = 0.9443, f1pos = 0.9476, f1 = 0.9460
[Test_batch(7)(2000,16000)] 2017-05-02 23:57:12.757088: step 26000, loss = 0.1476, acc = 0.9505, f1neg = 0.9552, f1pos = 0.9447, f1 = 0.9500
[Test_batch(8)(2000,18000)] 2017-05-02 23:57:13.229656: step 26000, loss = 0.1486, acc = 0.9460, f1neg = 0.9457, f1pos = 0.9463, f1 = 0.9460
[Test_batch(9)(2000,20000)] 2017-05-02 23:57:13.702039: step 26000, loss = 0.1799, acc = 0.9335, f1neg = 0.9305, f1pos = 0.9363, f1 = 0.9334
[Test_batch(10)(2000,22000)] 2017-05-02 23:57:14.163865: step 26000, loss = 0.1564, acc = 0.9500, f1neg = 0.9439, f1pos = 0.9549, f1 = 0.9494
[Test_batch(11)(2000,24000)] 2017-05-02 23:57:14.637761: step 26000, loss = 0.1553, acc = 0.9465, f1neg = 0.9483, f1pos = 0.9446, f1 = 0.9464
[Test_batch(12)(2000,26000)] 2017-05-02 23:57:15.142799: step 26000, loss = 0.1340, acc = 0.9600, f1neg = 0.9616, f1pos = 0.9582, f1 = 0.9599
[Test_batch(13)(2000,28000)] 2017-05-02 23:57:15.637699: step 26000, loss = 0.1613, acc = 0.9445, f1neg = 0.9386, f1pos = 0.9494, f1 = 0.9440
[Test_batch(14)(2000,30000)] 2017-05-02 23:57:16.096096: step 26000, loss = 0.1340, acc = 0.9565, f1neg = 0.9505, f1pos = 0.9612, f1 = 0.9559
[Test_batch(15)(2000,32000)] 2017-05-02 23:57:16.535316: step 26000, loss = 0.1535, acc = 0.9500, f1neg = 0.9495, f1pos = 0.9505, f1 = 0.9500
[Test_batch(16)(2000,34000)] 2017-05-02 23:57:17.040263: step 26000, loss = 0.1378, acc = 0.9560, f1neg = 0.9551, f1pos = 0.9568, f1 = 0.9560
[Test_batch(17)(2000,36000)] 2017-05-02 23:57:17.542197: step 26000, loss = 0.1289, acc = 0.9615, f1neg = 0.9582, f1pos = 0.9643, f1 = 0.9613
[Test_batch(18)(2000,38000)] 2017-05-02 23:57:18.088051: step 26000, loss = 0.1203, acc = 0.9620, f1neg = 0.9613, f1pos = 0.9627, f1 = 0.9620
[Test] 2017-05-02 23:57:18.088141: step 26000, acc = 0.9498, f1 = 0.9496
[Status] 2017-05-02 23:57:18.088168: step 26000, maxindex = 26000, maxdev = 0.9591, maxtst = 0.9498
2017-05-02 23:57:30.183895: step 26010, loss = 0.1840, acc = 0.9200 (285.2 examples/sec; 0.224 sec/batch)
2017-05-02 23:57:39.496090: step 26020, loss = 0.1670, acc = 0.9480 (266.4 examples/sec; 0.240 sec/batch)
2017-05-02 23:57:48.728055: step 26030, loss = 0.1342, acc = 0.9580 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:57:57.816071: step 26040, loss = 0.1448, acc = 0.9680 (272.5 examples/sec; 0.235 sec/batch)
2017-05-02 23:58:06.905135: step 26050, loss = 0.1315, acc = 0.9600 (271.4 examples/sec; 0.236 sec/batch)
2017-05-02 23:58:16.029521: step 26060, loss = 0.1489, acc = 0.9520 (292.6 examples/sec; 0.219 sec/batch)
2017-05-02 23:58:25.317050: step 26070, loss = 0.1317, acc = 0.9560 (264.9 examples/sec; 0.242 sec/batch)
2017-05-02 23:58:34.502059: step 26080, loss = 0.1734, acc = 0.9460 (278.7 examples/sec; 0.230 sec/batch)
2017-05-02 23:58:43.643420: step 26090, loss = 0.1670, acc = 0.9460 (281.4 examples/sec; 0.227 sec/batch)
2017-05-02 23:58:52.599013: step 26100, loss = 0.1367, acc = 0.9500 (292.7 examples/sec; 0.219 sec/batch)
2017-05-02 23:59:01.690586: step 26110, loss = 0.1734, acc = 0.9260 (273.1 examples/sec; 0.234 sec/batch)
2017-05-02 23:59:10.787292: step 26120, loss = 0.1797, acc = 0.9360 (296.7 examples/sec; 0.216 sec/batch)
2017-05-02 23:59:19.913423: step 26130, loss = 0.1285, acc = 0.9700 (273.9 examples/sec; 0.234 sec/batch)
2017-05-02 23:59:29.213650: step 26140, loss = 0.1261, acc = 0.9700 (271.1 examples/sec; 0.236 sec/batch)
2017-05-02 23:59:38.366342: step 26150, loss = 0.1302, acc = 0.9600 (260.5 examples/sec; 0.246 sec/batch)
2017-05-02 23:59:47.464048: step 26160, loss = 0.1781, acc = 0.9440 (285.7 examples/sec; 0.224 sec/batch)
2017-05-02 23:59:56.508477: step 26170, loss = 0.1737, acc = 0.9300 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 00:00:05.541606: step 26180, loss = 0.1503, acc = 0.9460 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 00:00:14.702173: step 26190, loss = 0.1296, acc = 0.9480 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 00:00:23.823665: step 26200, loss = 0.1336, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 00:00:33.985928: step 26210, loss = 0.1625, acc = 0.9420 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 00:00:42.996723: step 26220, loss = 0.1515, acc = 0.9460 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 00:00:51.942005: step 26230, loss = 0.1268, acc = 0.9540 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:01:01.106489: step 26240, loss = 0.1619, acc = 0.9400 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 00:01:10.196067: step 26250, loss = 0.1684, acc = 0.9440 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 00:01:19.375700: step 26260, loss = 0.1432, acc = 0.9580 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 00:01:28.583184: step 26270, loss = 0.1318, acc = 0.9540 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 00:01:37.824659: step 26280, loss = 0.1380, acc = 0.9600 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 00:01:46.963335: step 26290, loss = 0.1724, acc = 0.9520 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 00:01:55.975646: step 26300, loss = 0.1580, acc = 0.9420 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 00:02:05.096614: step 26310, loss = 0.1380, acc = 0.9580 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 00:02:14.162298: step 26320, loss = 0.1383, acc = 0.9580 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 00:02:23.305730: step 26330, loss = 0.1444, acc = 0.9600 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 00:02:32.529026: step 26340, loss = 0.1466, acc = 0.9520 (262.4 examples/sec; 0.244 sec/batch)
2017-05-03 00:02:41.672228: step 26350, loss = 0.1351, acc = 0.9520 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 00:02:50.703384: step 26360, loss = 0.1733, acc = 0.9440 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 00:02:59.792980: step 26370, loss = 0.1379, acc = 0.9520 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 00:03:08.890110: step 26380, loss = 0.1337, acc = 0.9460 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 00:03:18.062079: step 26390, loss = 0.1580, acc = 0.9420 (266.3 examples/sec; 0.240 sec/batch)
2017-05-03 00:03:27.174702: step 26400, loss = 0.1248, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 00:03:36.299020: step 26410, loss = 0.1200, acc = 0.9560 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 00:03:45.484089: step 26420, loss = 0.1298, acc = 0.9580 (305.4 examples/sec; 0.210 sec/batch)
2017-05-03 00:03:54.531122: step 26430, loss = 0.1362, acc = 0.9480 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 00:04:03.588724: step 26440, loss = 0.1519, acc = 0.9380 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 00:04:12.930234: step 26450, loss = 0.1451, acc = 0.9560 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 00:04:22.016453: step 26460, loss = 0.1471, acc = 0.9440 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 00:04:31.101046: step 26470, loss = 0.1381, acc = 0.9600 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 00:04:40.209272: step 26480, loss = 0.1365, acc = 0.9580 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 00:04:49.343696: step 26490, loss = 0.1477, acc = 0.9620 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 00:04:58.468599: step 26500, loss = 0.1515, acc = 0.9400 (252.1 examples/sec; 0.254 sec/batch)
2017-05-03 00:05:07.728719: step 26510, loss = 0.1355, acc = 0.9600 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 00:05:16.845241: step 26520, loss = 0.1532, acc = 0.9540 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 00:05:26.126033: step 26530, loss = 0.1315, acc = 0.9660 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 00:05:35.200643: step 26540, loss = 0.1704, acc = 0.9440 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 00:05:44.720842: step 26550, loss = 0.1369, acc = 0.9640 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 00:05:53.958094: step 26560, loss = 0.1300, acc = 0.9480 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 00:06:03.151760: step 26570, loss = 0.1486, acc = 0.9460 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 00:06:12.327512: step 26580, loss = 0.1444, acc = 0.9600 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 00:06:21.552747: step 26590, loss = 0.1203, acc = 0.9620 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:06:30.634580: step 26600, loss = 0.1420, acc = 0.9440 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 00:06:39.799630: step 26610, loss = 0.1469, acc = 0.9540 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:06:49.015019: step 26620, loss = 0.1610, acc = 0.9400 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 00:06:58.105022: step 26630, loss = 0.2078, acc = 0.9160 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 00:07:07.157057: step 26640, loss = 0.1319, acc = 0.9600 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 00:07:16.322441: step 26650, loss = 0.1195, acc = 0.9660 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 00:07:25.441292: step 26660, loss = 0.1362, acc = 0.9560 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 00:07:34.691529: step 26670, loss = 0.1086, acc = 0.9740 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 00:07:43.968775: step 26680, loss = 0.1358, acc = 0.9560 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:07:53.182267: step 26690, loss = 0.1554, acc = 0.9460 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 00:08:02.400073: step 26700, loss = 0.1492, acc = 0.9460 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 00:08:11.742306: step 26710, loss = 0.1490, acc = 0.9540 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 00:08:20.859290: step 26720, loss = 0.1493, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 00:08:30.170414: step 26730, loss = 0.1732, acc = 0.9460 (248.0 examples/sec; 0.258 sec/batch)
2017-05-03 00:08:39.480912: step 26740, loss = 0.1378, acc = 0.9520 (295.1 examples/sec; 0.217 sec/batch)
2017-05-03 00:08:48.710049: step 26750, loss = 0.1701, acc = 0.9420 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 00:08:57.864231: step 26760, loss = 0.1589, acc = 0.9580 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 00:09:07.035604: step 26770, loss = 0.1204, acc = 0.9600 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 00:09:16.106288: step 26780, loss = 0.1466, acc = 0.9520 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 00:09:25.202365: step 26790, loss = 0.1221, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 00:09:34.570304: step 26800, loss = 0.1733, acc = 0.9420 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 00:09:43.884350: step 26810, loss = 0.1566, acc = 0.9520 (246.1 examples/sec; 0.260 sec/batch)
2017-05-03 00:09:53.169969: step 26820, loss = 0.1263, acc = 0.9560 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 00:10:02.310700: step 26830, loss = 0.1491, acc = 0.9420 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 00:10:11.568694: step 26840, loss = 0.1567, acc = 0.9460 (247.2 examples/sec; 0.259 sec/batch)
2017-05-03 00:10:20.689702: step 26850, loss = 0.1368, acc = 0.9520 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 00:10:29.893793: step 26860, loss = 0.1406, acc = 0.9560 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 00:10:39.038986: step 26870, loss = 0.1398, acc = 0.9440 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 00:10:48.157453: step 26880, loss = 0.1580, acc = 0.9360 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 00:10:57.237271: step 26890, loss = 0.1568, acc = 0.9500 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:11:06.218810: step 26900, loss = 0.1550, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 00:11:15.273356: step 26910, loss = 0.1476, acc = 0.9420 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 00:11:24.281388: step 26920, loss = 0.1425, acc = 0.9480 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 00:11:33.320348: step 26930, loss = 0.1203, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 00:11:42.212733: step 26940, loss = 0.1398, acc = 0.9620 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 00:11:51.274950: step 26950, loss = 0.1358, acc = 0.9580 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 00:12:00.420424: step 26960, loss = 0.1611, acc = 0.9480 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 00:12:09.507081: step 26970, loss = 0.1363, acc = 0.9620 (258.5 examples/sec; 0.248 sec/batch)
2017-05-03 00:12:18.723702: step 26980, loss = 0.1508, acc = 0.9560 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 00:12:27.796302: step 26990, loss = 0.1512, acc = 0.9420 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 00:12:36.919062: step 27000, loss = 0.1288, acc = 0.9560 (275.5 examples/sec; 0.232 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 00:12:37.404593: step 27000, loss = 0.1228, acc = 0.9570, f1neg = 0.9529, f1pos = 0.9605, f1 = 0.9567
[Eval_batch(1)(2000,4000)] 2017-05-03 00:12:37.877934: step 27000, loss = 0.1223, acc = 0.9640, f1neg = 0.9580, f1pos = 0.9685, f1 = 0.9632
[Eval_batch(2)(2000,6000)] 2017-05-03 00:12:38.381130: step 27000, loss = 0.1356, acc = 0.9575, f1neg = 0.9547, f1pos = 0.9600, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-03 00:12:38.888968: step 27000, loss = 0.1413, acc = 0.9530, f1neg = 0.9530, f1pos = 0.9530, f1 = 0.9530
[Eval_batch(4)(2000,10000)] 2017-05-03 00:12:39.365604: step 27000, loss = 0.1425, acc = 0.9530, f1neg = 0.9541, f1pos = 0.9518, f1 = 0.9530
[Eval_batch(5)(2000,12000)] 2017-05-03 00:12:39.837972: step 27000, loss = 0.1380, acc = 0.9500, f1neg = 0.9461, f1pos = 0.9534, f1 = 0.9497
[Eval_batch(6)(2000,14000)] 2017-05-03 00:12:40.273108: step 27000, loss = 0.1298, acc = 0.9600, f1neg = 0.9592, f1pos = 0.9608, f1 = 0.9600
[Eval_batch(7)(2000,16000)] 2017-05-03 00:12:40.777895: step 27000, loss = 0.1267, acc = 0.9595, f1neg = 0.9514, f1pos = 0.9653, f1 = 0.9583
[Eval_batch(8)(2000,18000)] 2017-05-03 00:12:41.241300: step 27000, loss = 0.1234, acc = 0.9590, f1neg = 0.9530, f1pos = 0.9637, f1 = 0.9583
[Eval_batch(9)(2000,20000)] 2017-05-03 00:12:41.715347: step 27000, loss = 0.1265, acc = 0.9635, f1neg = 0.9609, f1pos = 0.9657, f1 = 0.9633
[Eval_batch(10)(2000,22000)] 2017-05-03 00:12:42.191390: step 27000, loss = 0.1362, acc = 0.9540, f1neg = 0.9504, f1pos = 0.9571, f1 = 0.9538
[Eval_batch(11)(2000,24000)] 2017-05-03 00:12:42.659151: step 27000, loss = 0.1354, acc = 0.9580, f1neg = 0.9513, f1pos = 0.9631, f1 = 0.9572
[Eval_batch(12)(2000,26000)] 2017-05-03 00:12:43.134439: step 27000, loss = 0.1333, acc = 0.9525, f1neg = 0.9512, f1pos = 0.9537, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-03 00:12:43.640938: step 27000, loss = 0.1205, acc = 0.9640, f1neg = 0.9548, f1pos = 0.9701, f1 = 0.9625
[Eval_batch(14)(2000,30000)] 2017-05-03 00:12:44.103365: step 27000, loss = 0.1475, acc = 0.9505, f1neg = 0.9404, f1pos = 0.9577, f1 = 0.9490
[Eval_batch(15)(2000,32000)] 2017-05-03 00:12:44.604005: step 27000, loss = 0.1262, acc = 0.9665, f1neg = 0.9638, f1pos = 0.9688, f1 = 0.9663
[Eval_batch(16)(2000,34000)] 2017-05-03 00:12:45.107012: step 27000, loss = 0.1228, acc = 0.9620, f1neg = 0.9603, f1pos = 0.9635, f1 = 0.9619
[Eval_batch(17)(2000,36000)] 2017-05-03 00:12:45.582097: step 27000, loss = 0.1441, acc = 0.9560, f1neg = 0.9561, f1pos = 0.9559, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 00:12:46.051671: step 27000, loss = 0.1433, acc = 0.9535, f1neg = 0.9553, f1pos = 0.9516, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 00:12:46.556613: step 27000, loss = 0.1237, acc = 0.9665, f1neg = 0.9612, f1pos = 0.9705, f1 = 0.9659
[Eval_batch(20)(2000,42000)] 2017-05-03 00:12:47.022288: step 27000, loss = 0.1259, acc = 0.9625, f1neg = 0.9667, f1pos = 0.9571, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 00:12:47.494317: step 27000, loss = 0.1170, acc = 0.9615, f1neg = 0.9580, f1pos = 0.9644, f1 = 0.9612
[Eval_batch(22)(2000,46000)] 2017-05-03 00:12:48.017842: step 27000, loss = 0.1255, acc = 0.9580, f1neg = 0.9583, f1pos = 0.9577, f1 = 0.9580
[Eval_batch(23)(2000,48000)] 2017-05-03 00:12:48.531347: step 27000, loss = 0.1307, acc = 0.9565, f1neg = 0.9507, f1pos = 0.9611, f1 = 0.9559
[Eval_batch(24)(2000,50000)] 2017-05-03 00:12:49.040206: step 27000, loss = 0.1187, acc = 0.9630, f1neg = 0.9574, f1pos = 0.9673, f1 = 0.9624
[Eval_batch(25)(2000,52000)] 2017-05-03 00:12:49.514282: step 27000, loss = 0.1088, acc = 0.9700, f1neg = 0.9670, f1pos = 0.9725, f1 = 0.9698
[Eval_batch(26)(2000,54000)] 2017-05-03 00:12:49.990956: step 27000, loss = 0.1230, acc = 0.9565, f1neg = 0.9588, f1pos = 0.9539, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-03 00:12:50.553373: step 27000, loss = 0.1104, acc = 0.9645, f1neg = 0.9627, f1pos = 0.9661, f1 = 0.9644
[Eval] 2017-05-03 00:12:50.553435: step 27000, acc = 0.9590, f1 = 0.9586
[Test_batch(0)(2000,2000)] 2017-05-03 00:12:51.022496: step 27000, loss = 0.1580, acc = 0.9490, f1neg = 0.9515, f1pos = 0.9462, f1 = 0.9489
[Test_batch(1)(2000,4000)] 2017-05-03 00:12:51.529998: step 27000, loss = 0.1414, acc = 0.9540, f1neg = 0.9590, f1pos = 0.9476, f1 = 0.9533
[Test_batch(2)(2000,6000)] 2017-05-03 00:12:51.999649: step 27000, loss = 0.1564, acc = 0.9520, f1neg = 0.9558, f1pos = 0.9474, f1 = 0.9516
[Test_batch(3)(2000,8000)] 2017-05-03 00:12:52.505991: step 27000, loss = 0.1644, acc = 0.9420, f1neg = 0.9456, f1pos = 0.9379, f1 = 0.9417
[Test_batch(4)(2000,10000)] 2017-05-03 00:12:53.017382: step 27000, loss = 0.1621, acc = 0.9430, f1neg = 0.9429, f1pos = 0.9431, f1 = 0.9430
[Test_batch(5)(2000,12000)] 2017-05-03 00:12:53.489051: step 27000, loss = 0.1727, acc = 0.9355, f1neg = 0.9361, f1pos = 0.9349, f1 = 0.9355
[Test_batch(6)(2000,14000)] 2017-05-03 00:12:53.988832: step 27000, loss = 0.1588, acc = 0.9435, f1neg = 0.9414, f1pos = 0.9455, f1 = 0.9434
[Test_batch(7)(2000,16000)] 2017-05-03 00:12:54.464601: step 27000, loss = 0.1483, acc = 0.9495, f1neg = 0.9540, f1pos = 0.9440, f1 = 0.9490
[Test_batch(8)(2000,18000)] 2017-05-03 00:12:54.942352: step 27000, loss = 0.1483, acc = 0.9460, f1neg = 0.9456, f1pos = 0.9464, f1 = 0.9460
[Test_batch(9)(2000,20000)] 2017-05-03 00:12:55.423252: step 27000, loss = 0.1799, acc = 0.9370, f1neg = 0.9336, f1pos = 0.9401, f1 = 0.9368
[Test_batch(10)(2000,22000)] 2017-05-03 00:12:55.929165: step 27000, loss = 0.1547, acc = 0.9490, f1neg = 0.9425, f1pos = 0.9542, f1 = 0.9483
[Test_batch(11)(2000,24000)] 2017-05-03 00:12:56.424374: step 27000, loss = 0.1557, acc = 0.9470, f1neg = 0.9484, f1pos = 0.9455, f1 = 0.9470
[Test_batch(12)(2000,26000)] 2017-05-03 00:12:56.896773: step 27000, loss = 0.1334, acc = 0.9585, f1neg = 0.9601, f1pos = 0.9568, f1 = 0.9584
[Test_batch(13)(2000,28000)] 2017-05-03 00:12:57.375011: step 27000, loss = 0.1599, acc = 0.9430, f1neg = 0.9365, f1pos = 0.9483, f1 = 0.9424
[Test_batch(14)(2000,30000)] 2017-05-03 00:12:57.820322: step 27000, loss = 0.1319, acc = 0.9605, f1neg = 0.9546, f1pos = 0.9650, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 00:12:58.298438: step 27000, loss = 0.1544, acc = 0.9480, f1neg = 0.9473, f1pos = 0.9487, f1 = 0.9480
[Test_batch(16)(2000,34000)] 2017-05-03 00:12:58.804386: step 27000, loss = 0.1381, acc = 0.9565, f1neg = 0.9553, f1pos = 0.9576, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 00:12:59.311686: step 27000, loss = 0.1304, acc = 0.9620, f1neg = 0.9584, f1pos = 0.9650, f1 = 0.9617
[Test_batch(18)(2000,38000)] 2017-05-03 00:12:59.884994: step 27000, loss = 0.1212, acc = 0.9595, f1neg = 0.9585, f1pos = 0.9604, f1 = 0.9595
[Test] 2017-05-03 00:12:59.885056: step 27000, acc = 0.9492, f1 = 0.9490
[Status] 2017-05-03 00:12:59.885068: step 27000, maxindex = 26000, maxdev = 0.9591, maxtst = 0.9498
2017-05-03 00:13:08.947461: step 27010, loss = 0.1700, acc = 0.9500 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 00:13:18.026474: step 27020, loss = 0.1260, acc = 0.9600 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 00:13:27.362873: step 27030, loss = 0.1380, acc = 0.9560 (250.1 examples/sec; 0.256 sec/batch)
2017-05-03 00:13:36.702315: step 27040, loss = 0.1196, acc = 0.9600 (266.5 examples/sec; 0.240 sec/batch)
2017-05-03 00:13:45.907991: step 27050, loss = 0.1760, acc = 0.9360 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 00:13:55.015516: step 27060, loss = 0.1279, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 00:14:04.206081: step 27070, loss = 0.1418, acc = 0.9440 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 00:14:13.279556: step 27080, loss = 0.1643, acc = 0.9420 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 00:14:22.399889: step 27090, loss = 0.1502, acc = 0.9580 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 00:14:31.662826: step 27100, loss = 0.1795, acc = 0.9380 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:14:40.675700: step 27110, loss = 0.1429, acc = 0.9540 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 00:14:49.859520: step 27120, loss = 0.1347, acc = 0.9600 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 00:14:59.060590: step 27130, loss = 0.1204, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 00:15:08.228585: step 27140, loss = 0.1243, acc = 0.9620 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 00:15:17.386710: step 27150, loss = 0.1492, acc = 0.9520 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:15:26.691131: step 27160, loss = 0.1217, acc = 0.9680 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 00:15:36.010794: step 27170, loss = 0.1305, acc = 0.9640 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 00:15:45.232908: step 27180, loss = 0.1427, acc = 0.9500 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:15:54.358610: step 27190, loss = 0.1480, acc = 0.9460 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 00:16:03.525757: step 27200, loss = 0.1345, acc = 0.9540 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 00:16:12.774327: step 27210, loss = 0.1627, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 00:16:22.815854: step 27220, loss = 0.1426, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 00:16:32.093233: step 27230, loss = 0.1428, acc = 0.9620 (269.2 examples/sec; 0.238 sec/batch)
2017-05-03 00:16:41.188989: step 27240, loss = 0.1480, acc = 0.9520 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 00:16:50.342404: step 27250, loss = 0.1543, acc = 0.9560 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 00:16:59.525955: step 27260, loss = 0.1284, acc = 0.9540 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 00:17:08.517376: step 27270, loss = 0.1286, acc = 0.9620 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 00:17:17.713519: step 27280, loss = 0.1799, acc = 0.9420 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 00:17:26.837363: step 27290, loss = 0.1562, acc = 0.9420 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 00:17:35.982700: step 27300, loss = 0.1486, acc = 0.9460 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 00:17:45.303705: step 27310, loss = 0.1295, acc = 0.9580 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 00:17:54.509765: step 27320, loss = 0.1470, acc = 0.9620 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 00:18:03.903162: step 27330, loss = 0.1541, acc = 0.9340 (261.9 examples/sec; 0.244 sec/batch)
2017-05-03 00:18:13.478572: step 27340, loss = 0.1575, acc = 0.9420 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 00:18:22.572475: step 27350, loss = 0.1233, acc = 0.9660 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 00:18:31.750046: step 27360, loss = 0.1502, acc = 0.9540 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 00:18:40.952287: step 27370, loss = 0.1511, acc = 0.9560 (260.7 examples/sec; 0.245 sec/batch)
2017-05-03 00:18:49.845231: step 27380, loss = 0.1527, acc = 0.9520 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:18:58.914670: step 27390, loss = 0.1357, acc = 0.9700 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 00:19:08.056826: step 27400, loss = 0.1285, acc = 0.9620 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 00:19:17.100480: step 27410, loss = 0.1642, acc = 0.9340 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:19:26.321983: step 27420, loss = 0.0970, acc = 0.9740 (265.7 examples/sec; 0.241 sec/batch)
2017-05-03 00:19:35.644005: step 27430, loss = 0.1198, acc = 0.9600 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 00:19:44.820948: step 27440, loss = 0.1657, acc = 0.9400 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 00:19:53.862745: step 27450, loss = 0.1481, acc = 0.9460 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 00:20:02.968812: step 27460, loss = 0.1426, acc = 0.9480 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 00:20:12.028116: step 27470, loss = 0.1197, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 00:20:21.217408: step 27480, loss = 0.1474, acc = 0.9560 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 00:20:30.409141: step 27490, loss = 0.1203, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 00:20:39.533243: step 27500, loss = 0.1634, acc = 0.9380 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 00:20:48.450890: step 27510, loss = 0.1559, acc = 0.9460 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 00:20:57.723360: step 27520, loss = 0.1247, acc = 0.9660 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 00:21:06.828751: step 27530, loss = 0.1270, acc = 0.9560 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 00:21:15.920738: step 27540, loss = 0.1463, acc = 0.9540 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 00:21:25.105987: step 27550, loss = 0.1552, acc = 0.9520 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 00:21:34.312288: step 27560, loss = 0.1476, acc = 0.9460 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 00:21:43.656021: step 27570, loss = 0.1475, acc = 0.9480 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 00:21:53.133975: step 27580, loss = 0.1481, acc = 0.9500 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 00:22:02.197146: step 27590, loss = 0.1495, acc = 0.9520 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 00:22:11.427113: step 27600, loss = 0.1322, acc = 0.9580 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 00:22:20.554986: step 27610, loss = 0.1704, acc = 0.9400 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 00:22:29.887755: step 27620, loss = 0.1522, acc = 0.9500 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 00:22:39.070371: step 27630, loss = 0.1440, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 00:22:48.126733: step 27640, loss = 0.1150, acc = 0.9660 (297.0 examples/sec; 0.216 sec/batch)
2017-05-03 00:22:57.515066: step 27650, loss = 0.1436, acc = 0.9540 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 00:23:06.705415: step 27660, loss = 0.1263, acc = 0.9640 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 00:23:15.796381: step 27670, loss = 0.1453, acc = 0.9580 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 00:23:24.856129: step 27680, loss = 0.1491, acc = 0.9520 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 00:23:33.973590: step 27690, loss = 0.1636, acc = 0.9400 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 00:23:43.071108: step 27700, loss = 0.1321, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 00:23:52.350756: step 27710, loss = 0.1122, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 00:24:01.513578: step 27720, loss = 0.1573, acc = 0.9380 (265.1 examples/sec; 0.241 sec/batch)
2017-05-03 00:24:10.656249: step 27730, loss = 0.1063, acc = 0.9700 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 00:24:19.866726: step 27740, loss = 0.1495, acc = 0.9560 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 00:24:28.954262: step 27750, loss = 0.1401, acc = 0.9440 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 00:24:38.021262: step 27760, loss = 0.1405, acc = 0.9560 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 00:24:47.295111: step 27770, loss = 0.1803, acc = 0.9260 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 00:24:56.359830: step 27780, loss = 0.1267, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 00:25:05.367550: step 27790, loss = 0.1544, acc = 0.9480 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:25:14.576938: step 27800, loss = 0.1648, acc = 0.9500 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 00:25:23.770933: step 27810, loss = 0.1373, acc = 0.9500 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 00:25:32.918333: step 27820, loss = 0.1449, acc = 0.9540 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 00:25:42.057905: step 27830, loss = 0.1704, acc = 0.9540 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:25:51.239274: step 27840, loss = 0.1372, acc = 0.9640 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 00:26:00.249986: step 27850, loss = 0.1359, acc = 0.9620 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 00:26:09.358937: step 27860, loss = 0.1279, acc = 0.9580 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 00:26:18.522677: step 27870, loss = 0.1545, acc = 0.9380 (259.9 examples/sec; 0.246 sec/batch)
2017-05-03 00:26:27.540459: step 27880, loss = 0.1572, acc = 0.9480 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 00:26:36.624741: step 27890, loss = 0.1665, acc = 0.9440 (269.6 examples/sec; 0.237 sec/batch)
2017-05-03 00:26:45.656530: step 27900, loss = 0.1722, acc = 0.9340 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 00:26:54.855154: step 27910, loss = 0.1115, acc = 0.9720 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 00:27:04.080410: step 27920, loss = 0.1350, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 00:27:13.134287: step 27930, loss = 0.1583, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 00:27:22.413470: step 27940, loss = 0.1381, acc = 0.9480 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 00:27:31.371747: step 27950, loss = 0.1074, acc = 0.9700 (296.2 examples/sec; 0.216 sec/batch)
2017-05-03 00:27:40.409809: step 27960, loss = 0.1499, acc = 0.9500 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 00:27:49.494793: step 27970, loss = 0.1618, acc = 0.9620 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 00:27:58.540165: step 27980, loss = 0.1289, acc = 0.9600 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 00:28:07.483398: step 27990, loss = 0.1406, acc = 0.9560 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:28:16.443973: step 28000, loss = 0.1456, acc = 0.9540 (285.0 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 00:28:16.959683: step 28000, loss = 0.1192, acc = 0.9560, f1neg = 0.9521, f1pos = 0.9593, f1 = 0.9557
[Eval_batch(1)(2000,4000)] 2017-05-03 00:28:17.432811: step 28000, loss = 0.1255, acc = 0.9625, f1neg = 0.9569, f1pos = 0.9668, f1 = 0.9619
[Eval_batch(2)(2000,6000)] 2017-05-03 00:28:17.901289: step 28000, loss = 0.1340, acc = 0.9555, f1neg = 0.9531, f1pos = 0.9576, f1 = 0.9554
[Eval_batch(3)(2000,8000)] 2017-05-03 00:28:18.414027: step 28000, loss = 0.1385, acc = 0.9565, f1neg = 0.9569, f1pos = 0.9561, f1 = 0.9565
[Eval_batch(4)(2000,10000)] 2017-05-03 00:28:18.919653: step 28000, loss = 0.1390, acc = 0.9560, f1neg = 0.9577, f1pos = 0.9542, f1 = 0.9559
[Eval_batch(5)(2000,12000)] 2017-05-03 00:28:19.431415: step 28000, loss = 0.1372, acc = 0.9515, f1neg = 0.9485, f1pos = 0.9541, f1 = 0.9513
[Eval_batch(6)(2000,14000)] 2017-05-03 00:28:19.933228: step 28000, loss = 0.1251, acc = 0.9625, f1neg = 0.9620, f1pos = 0.9630, f1 = 0.9625
[Eval_batch(7)(2000,16000)] 2017-05-03 00:28:20.404688: step 28000, loss = 0.1267, acc = 0.9585, f1neg = 0.9509, f1pos = 0.9641, f1 = 0.9575
[Eval_batch(8)(2000,18000)] 2017-05-03 00:28:20.902081: step 28000, loss = 0.1226, acc = 0.9605, f1neg = 0.9554, f1pos = 0.9646, f1 = 0.9600
[Eval_batch(9)(2000,20000)] 2017-05-03 00:28:21.408502: step 28000, loss = 0.1256, acc = 0.9615, f1neg = 0.9591, f1pos = 0.9636, f1 = 0.9614
[Eval_batch(10)(2000,22000)] 2017-05-03 00:28:21.904927: step 28000, loss = 0.1349, acc = 0.9580, f1neg = 0.9555, f1pos = 0.9603, f1 = 0.9579
[Eval_batch(11)(2000,24000)] 2017-05-03 00:28:22.373368: step 28000, loss = 0.1363, acc = 0.9565, f1neg = 0.9502, f1pos = 0.9614, f1 = 0.9558
[Eval_batch(12)(2000,26000)] 2017-05-03 00:28:22.859737: step 28000, loss = 0.1319, acc = 0.9535, f1neg = 0.9526, f1pos = 0.9543, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 00:28:23.364534: step 28000, loss = 0.1201, acc = 0.9640, f1neg = 0.9552, f1pos = 0.9699, f1 = 0.9625
[Eval_batch(14)(2000,30000)] 2017-05-03 00:28:23.839547: step 28000, loss = 0.1470, acc = 0.9530, f1neg = 0.9444, f1pos = 0.9593, f1 = 0.9518
[Eval_batch(15)(2000,32000)] 2017-05-03 00:28:24.312858: step 28000, loss = 0.1265, acc = 0.9630, f1neg = 0.9604, f1pos = 0.9653, f1 = 0.9628
[Eval_batch(16)(2000,34000)] 2017-05-03 00:28:24.786825: step 28000, loss = 0.1222, acc = 0.9625, f1neg = 0.9612, f1pos = 0.9638, f1 = 0.9625
[Eval_batch(17)(2000,36000)] 2017-05-03 00:28:25.262900: step 28000, loss = 0.1440, acc = 0.9555, f1neg = 0.9560, f1pos = 0.9550, f1 = 0.9555
[Eval_batch(18)(2000,38000)] 2017-05-03 00:28:25.735542: step 28000, loss = 0.1398, acc = 0.9535, f1neg = 0.9557, f1pos = 0.9511, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 00:28:26.247321: step 28000, loss = 0.1233, acc = 0.9650, f1neg = 0.9598, f1pos = 0.9690, f1 = 0.9644
[Eval_batch(20)(2000,42000)] 2017-05-03 00:28:26.763568: step 28000, loss = 0.1218, acc = 0.9600, f1neg = 0.9647, f1pos = 0.9539, f1 = 0.9593
[Eval_batch(21)(2000,44000)] 2017-05-03 00:28:27.281093: step 28000, loss = 0.1212, acc = 0.9575, f1neg = 0.9541, f1pos = 0.9604, f1 = 0.9573
[Eval_batch(22)(2000,46000)] 2017-05-03 00:28:27.787288: step 28000, loss = 0.1247, acc = 0.9615, f1neg = 0.9621, f1pos = 0.9609, f1 = 0.9615
[Eval_batch(23)(2000,48000)] 2017-05-03 00:28:28.298576: step 28000, loss = 0.1327, acc = 0.9565, f1neg = 0.9514, f1pos = 0.9607, f1 = 0.9560
[Eval_batch(24)(2000,50000)] 2017-05-03 00:28:28.786293: step 28000, loss = 0.1188, acc = 0.9605, f1neg = 0.9551, f1pos = 0.9647, f1 = 0.9599
[Eval_batch(25)(2000,52000)] 2017-05-03 00:28:29.286030: step 28000, loss = 0.1083, acc = 0.9685, f1neg = 0.9658, f1pos = 0.9708, f1 = 0.9683
[Eval_batch(26)(2000,54000)] 2017-05-03 00:28:29.786783: step 28000, loss = 0.1192, acc = 0.9590, f1neg = 0.9616, f1pos = 0.9560, f1 = 0.9588
[Eval_batch(27)(2000,56000)] 2017-05-03 00:28:30.377030: step 28000, loss = 0.1067, acc = 0.9675, f1neg = 0.9660, f1pos = 0.9689, f1 = 0.9674
[Eval] 2017-05-03 00:28:30.377118: step 28000, acc = 0.9592, f1 = 0.9588
[Test_batch(0)(2000,2000)] 2017-05-03 00:28:30.890603: step 28000, loss = 0.1557, acc = 0.9525, f1neg = 0.9552, f1pos = 0.9494, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 00:28:31.404133: step 28000, loss = 0.1389, acc = 0.9555, f1neg = 0.9608, f1pos = 0.9485, f1 = 0.9547
[Test_batch(2)(2000,6000)] 2017-05-03 00:28:31.914181: step 28000, loss = 0.1531, acc = 0.9545, f1neg = 0.9585, f1pos = 0.9496, f1 = 0.9541
[Test_batch(3)(2000,8000)] 2017-05-03 00:28:32.424596: step 28000, loss = 0.1622, acc = 0.9430, f1neg = 0.9474, f1pos = 0.9378, f1 = 0.9426
[Test_batch(4)(2000,10000)] 2017-05-03 00:28:32.904779: step 28000, loss = 0.1635, acc = 0.9415, f1neg = 0.9423, f1pos = 0.9406, f1 = 0.9415
[Test_batch(5)(2000,12000)] 2017-05-03 00:28:33.421835: step 28000, loss = 0.1728, acc = 0.9375, f1neg = 0.9389, f1pos = 0.9360, f1 = 0.9375
[Test_batch(6)(2000,14000)] 2017-05-03 00:28:33.932753: step 28000, loss = 0.1566, acc = 0.9475, f1neg = 0.9461, f1pos = 0.9489, f1 = 0.9475
[Test_batch(7)(2000,16000)] 2017-05-03 00:28:34.400990: step 28000, loss = 0.1465, acc = 0.9520, f1neg = 0.9568, f1pos = 0.9459, f1 = 0.9514
[Test_batch(8)(2000,18000)] 2017-05-03 00:28:34.896673: step 28000, loss = 0.1488, acc = 0.9475, f1neg = 0.9474, f1pos = 0.9476, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 00:28:35.405501: step 28000, loss = 0.1813, acc = 0.9340, f1neg = 0.9312, f1pos = 0.9366, f1 = 0.9339
[Test_batch(10)(2000,22000)] 2017-05-03 00:28:35.924518: step 28000, loss = 0.1590, acc = 0.9485, f1neg = 0.9428, f1pos = 0.9532, f1 = 0.9480
[Test_batch(11)(2000,24000)] 2017-05-03 00:28:36.430846: step 28000, loss = 0.1549, acc = 0.9485, f1neg = 0.9504, f1pos = 0.9465, f1 = 0.9484
[Test_batch(12)(2000,26000)] 2017-05-03 00:28:36.929246: step 28000, loss = 0.1339, acc = 0.9580, f1neg = 0.9598, f1pos = 0.9560, f1 = 0.9579
[Test_batch(13)(2000,28000)] 2017-05-03 00:28:37.426565: step 28000, loss = 0.1617, acc = 0.9465, f1neg = 0.9414, f1pos = 0.9508, f1 = 0.9461
[Test_batch(14)(2000,30000)] 2017-05-03 00:28:37.904697: step 28000, loss = 0.1348, acc = 0.9545, f1neg = 0.9486, f1pos = 0.9592, f1 = 0.9539
[Test_batch(15)(2000,32000)] 2017-05-03 00:28:38.376956: step 28000, loss = 0.1554, acc = 0.9520, f1neg = 0.9519, f1pos = 0.9521, f1 = 0.9520
[Test_batch(16)(2000,34000)] 2017-05-03 00:28:38.884878: step 28000, loss = 0.1379, acc = 0.9570, f1neg = 0.9563, f1pos = 0.9576, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 00:28:39.383783: step 28000, loss = 0.1287, acc = 0.9620, f1neg = 0.9587, f1pos = 0.9648, f1 = 0.9618
[Test_batch(18)(2000,38000)] 2017-05-03 00:28:39.961284: step 28000, loss = 0.1179, acc = 0.9645, f1neg = 0.9640, f1pos = 0.9650, f1 = 0.9645
[Test] 2017-05-03 00:28:39.961371: step 28000, acc = 0.9504, f1 = 0.9501
[Status] 2017-05-03 00:28:39.961400: step 28000, maxindex = 28000, maxdev = 0.9592, maxtst = 0.9504
2017-05-03 00:28:52.287584: step 28010, loss = 0.1420, acc = 0.9500 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 00:29:01.476842: step 28020, loss = 0.1347, acc = 0.9560 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 00:29:10.449525: step 28030, loss = 0.1278, acc = 0.9620 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 00:29:19.444155: step 28040, loss = 0.1679, acc = 0.9440 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 00:29:28.501135: step 28050, loss = 0.1338, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 00:29:37.684476: step 28060, loss = 0.1328, acc = 0.9560 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 00:29:46.698141: step 28070, loss = 0.1257, acc = 0.9600 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 00:29:55.901203: step 28080, loss = 0.1717, acc = 0.9460 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 00:30:04.974447: step 28090, loss = 0.1483, acc = 0.9540 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 00:30:14.151596: step 28100, loss = 0.1263, acc = 0.9540 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 00:30:23.254681: step 28110, loss = 0.1536, acc = 0.9460 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 00:30:32.347197: step 28120, loss = 0.1260, acc = 0.9660 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 00:30:41.629117: step 28130, loss = 0.1551, acc = 0.9480 (295.4 examples/sec; 0.217 sec/batch)
2017-05-03 00:30:50.764006: step 28140, loss = 0.1590, acc = 0.9420 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 00:30:59.898698: step 28150, loss = 0.1235, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 00:31:08.877368: step 28160, loss = 0.1535, acc = 0.9500 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 00:31:17.885178: step 28170, loss = 0.1526, acc = 0.9520 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 00:31:27.144998: step 28180, loss = 0.1379, acc = 0.9580 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 00:31:36.345975: step 28190, loss = 0.1356, acc = 0.9500 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 00:31:45.475659: step 28200, loss = 0.1688, acc = 0.9460 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 00:31:54.669102: step 28210, loss = 0.1601, acc = 0.9440 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 00:32:03.798104: step 28220, loss = 0.1609, acc = 0.9540 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 00:32:13.542802: step 28230, loss = 0.1313, acc = 0.9620 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 00:32:22.615395: step 28240, loss = 0.1711, acc = 0.9360 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 00:32:31.770094: step 28250, loss = 0.1442, acc = 0.9480 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 00:32:40.902063: step 28260, loss = 0.1476, acc = 0.9480 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 00:32:50.115118: step 28270, loss = 0.1422, acc = 0.9460 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:32:59.247787: step 28280, loss = 0.1206, acc = 0.9560 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 00:33:08.509803: step 28290, loss = 0.1408, acc = 0.9560 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 00:33:17.828460: step 28300, loss = 0.1403, acc = 0.9640 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 00:33:26.864999: step 28310, loss = 0.1405, acc = 0.9560 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 00:33:36.042807: step 28320, loss = 0.1212, acc = 0.9700 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 00:33:45.188487: step 28330, loss = 0.1261, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:33:54.170548: step 28340, loss = 0.1367, acc = 0.9600 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 00:34:03.228679: step 28350, loss = 0.1453, acc = 0.9520 (272.9 examples/sec; 0.235 sec/batch)
2017-05-03 00:34:12.394609: step 28360, loss = 0.1368, acc = 0.9600 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 00:34:21.599619: step 28370, loss = 0.1436, acc = 0.9520 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 00:34:30.646198: step 28380, loss = 0.1642, acc = 0.9480 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 00:34:39.758913: step 28390, loss = 0.1626, acc = 0.9400 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 00:34:48.706971: step 28400, loss = 0.1710, acc = 0.9440 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 00:34:57.857717: step 28410, loss = 0.1648, acc = 0.9480 (264.0 examples/sec; 0.242 sec/batch)
2017-05-03 00:35:07.057446: step 28420, loss = 0.1753, acc = 0.9500 (269.5 examples/sec; 0.237 sec/batch)
2017-05-03 00:35:16.431681: step 28430, loss = 0.1559, acc = 0.9480 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 00:35:25.760149: step 28440, loss = 0.1096, acc = 0.9660 (221.0 examples/sec; 0.290 sec/batch)
2017-05-03 00:35:34.827680: step 28450, loss = 0.1825, acc = 0.9420 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 00:35:43.900145: step 28460, loss = 0.1420, acc = 0.9500 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 00:35:52.960463: step 28470, loss = 0.1483, acc = 0.9500 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 00:36:02.152220: step 28480, loss = 0.1604, acc = 0.9460 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 00:36:11.284153: step 28490, loss = 0.1602, acc = 0.9460 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 00:36:20.430884: step 28500, loss = 0.1399, acc = 0.9480 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 00:36:29.658774: step 28510, loss = 0.1367, acc = 0.9700 (261.7 examples/sec; 0.245 sec/batch)
2017-05-03 00:36:38.669288: step 28520, loss = 0.1792, acc = 0.9540 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 00:36:47.827778: step 28530, loss = 0.1302, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 00:36:56.832824: step 28540, loss = 0.1359, acc = 0.9580 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 00:37:06.062387: step 28550, loss = 0.1263, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 00:37:15.150695: step 28560, loss = 0.1425, acc = 0.9500 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 00:37:24.227645: step 28570, loss = 0.1524, acc = 0.9460 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 00:37:33.300460: step 28580, loss = 0.1473, acc = 0.9500 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 00:37:42.317955: step 28590, loss = 0.1443, acc = 0.9560 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:37:51.571817: step 28600, loss = 0.1576, acc = 0.9560 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 00:38:00.774828: step 28610, loss = 0.1761, acc = 0.9400 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 00:38:09.923015: step 28620, loss = 0.1558, acc = 0.9440 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 00:38:19.129224: step 28630, loss = 0.1526, acc = 0.9400 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 00:38:28.221811: step 28640, loss = 0.1815, acc = 0.9360 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 00:38:37.300577: step 28650, loss = 0.1165, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 00:38:46.429617: step 28660, loss = 0.1685, acc = 0.9460 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 00:38:55.584842: step 28670, loss = 0.1594, acc = 0.9440 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 00:39:04.729254: step 28680, loss = 0.1426, acc = 0.9520 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 00:39:13.860549: step 28690, loss = 0.1564, acc = 0.9420 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 00:39:22.948113: step 28700, loss = 0.1463, acc = 0.9580 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 00:39:32.308272: step 28710, loss = 0.1216, acc = 0.9640 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 00:39:41.454265: step 28720, loss = 0.1563, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 00:39:50.512402: step 28730, loss = 0.1520, acc = 0.9380 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:39:59.653008: step 28740, loss = 0.1484, acc = 0.9540 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 00:40:08.731081: step 28750, loss = 0.1644, acc = 0.9420 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 00:40:17.944418: step 28760, loss = 0.0966, acc = 0.9820 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 00:40:27.047790: step 28770, loss = 0.1157, acc = 0.9680 (270.6 examples/sec; 0.237 sec/batch)
2017-05-03 00:40:36.188237: step 28780, loss = 0.1208, acc = 0.9600 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 00:40:45.197612: step 28790, loss = 0.1335, acc = 0.9580 (297.8 examples/sec; 0.215 sec/batch)
2017-05-03 00:40:54.252021: step 28800, loss = 0.1345, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 00:41:03.395325: step 28810, loss = 0.1405, acc = 0.9480 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 00:41:12.461180: step 28820, loss = 0.1485, acc = 0.9560 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 00:41:21.383264: step 28830, loss = 0.1451, acc = 0.9460 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 00:41:30.392439: step 28840, loss = 0.1470, acc = 0.9520 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 00:41:39.416265: step 28850, loss = 0.1738, acc = 0.9380 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 00:41:48.727278: step 28860, loss = 0.1346, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 00:41:57.953363: step 28870, loss = 0.1390, acc = 0.9520 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:42:07.020016: step 28880, loss = 0.1487, acc = 0.9480 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 00:42:16.143649: step 28890, loss = 0.1502, acc = 0.9500 (259.3 examples/sec; 0.247 sec/batch)
2017-05-03 00:42:25.396391: step 28900, loss = 0.1455, acc = 0.9480 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 00:42:34.992046: step 28910, loss = 0.1595, acc = 0.9440 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 00:42:44.105403: step 28920, loss = 0.1258, acc = 0.9700 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 00:42:53.195388: step 28930, loss = 0.1359, acc = 0.9520 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 00:43:02.427631: step 28940, loss = 0.1416, acc = 0.9480 (267.2 examples/sec; 0.239 sec/batch)
2017-05-03 00:43:11.541286: step 28950, loss = 0.1444, acc = 0.9600 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 00:43:20.607988: step 28960, loss = 0.1601, acc = 0.9600 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 00:43:29.982159: step 28970, loss = 0.1604, acc = 0.9540 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 00:43:39.119010: step 28980, loss = 0.1291, acc = 0.9680 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 00:43:48.166085: step 28990, loss = 0.1705, acc = 0.9440 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 00:43:57.266424: step 29000, loss = 0.1612, acc = 0.9480 (291.4 examples/sec; 0.220 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 00:43:57.774350: step 29000, loss = 0.1189, acc = 0.9590, f1neg = 0.9555, f1pos = 0.9620, f1 = 0.9587
[Eval_batch(1)(2000,4000)] 2017-05-03 00:43:58.239141: step 29000, loss = 0.1276, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Eval_batch(2)(2000,6000)] 2017-05-03 00:43:58.736127: step 29000, loss = 0.1351, acc = 0.9550, f1neg = 0.9529, f1pos = 0.9569, f1 = 0.9549
[Eval_batch(3)(2000,8000)] 2017-05-03 00:43:59.240944: step 29000, loss = 0.1401, acc = 0.9510, f1neg = 0.9518, f1pos = 0.9502, f1 = 0.9510
[Eval_batch(4)(2000,10000)] 2017-05-03 00:43:59.714420: step 29000, loss = 0.1395, acc = 0.9510, f1neg = 0.9532, f1pos = 0.9486, f1 = 0.9509
[Eval_batch(5)(2000,12000)] 2017-05-03 00:44:00.215037: step 29000, loss = 0.1388, acc = 0.9495, f1neg = 0.9468, f1pos = 0.9520, f1 = 0.9494
[Eval_batch(6)(2000,14000)] 2017-05-03 00:44:00.727417: step 29000, loss = 0.1247, acc = 0.9620, f1neg = 0.9616, f1pos = 0.9624, f1 = 0.9620
[Eval_batch(7)(2000,16000)] 2017-05-03 00:44:01.168833: step 29000, loss = 0.1288, acc = 0.9565, f1neg = 0.9488, f1pos = 0.9622, f1 = 0.9555
[Eval_batch(8)(2000,18000)] 2017-05-03 00:44:01.645718: step 29000, loss = 0.1228, acc = 0.9605, f1neg = 0.9557, f1pos = 0.9643, f1 = 0.9600
[Eval_batch(9)(2000,20000)] 2017-05-03 00:44:02.121436: step 29000, loss = 0.1271, acc = 0.9590, f1neg = 0.9566, f1pos = 0.9611, f1 = 0.9589
[Eval_batch(10)(2000,22000)] 2017-05-03 00:44:02.624382: step 29000, loss = 0.1368, acc = 0.9600, f1neg = 0.9579, f1pos = 0.9619, f1 = 0.9599
[Eval_batch(11)(2000,24000)] 2017-05-03 00:44:03.086178: step 29000, loss = 0.1369, acc = 0.9570, f1neg = 0.9510, f1pos = 0.9617, f1 = 0.9563
[Eval_batch(12)(2000,26000)] 2017-05-03 00:44:03.533528: step 29000, loss = 0.1322, acc = 0.9515, f1neg = 0.9508, f1pos = 0.9522, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-03 00:44:04.003163: step 29000, loss = 0.1211, acc = 0.9650, f1neg = 0.9567, f1pos = 0.9706, f1 = 0.9637
[Eval_batch(14)(2000,30000)] 2017-05-03 00:44:04.483479: step 29000, loss = 0.1481, acc = 0.9525, f1neg = 0.9440, f1pos = 0.9587, f1 = 0.9514
[Eval_batch(15)(2000,32000)] 2017-05-03 00:44:04.982828: step 29000, loss = 0.1275, acc = 0.9610, f1neg = 0.9585, f1pos = 0.9632, f1 = 0.9609
[Eval_batch(16)(2000,34000)] 2017-05-03 00:44:05.454825: step 29000, loss = 0.1232, acc = 0.9625, f1neg = 0.9613, f1pos = 0.9636, f1 = 0.9625
[Eval_batch(17)(2000,36000)] 2017-05-03 00:44:05.920235: step 29000, loss = 0.1445, acc = 0.9560, f1neg = 0.9567, f1pos = 0.9553, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 00:44:06.374140: step 29000, loss = 0.1388, acc = 0.9535, f1neg = 0.9559, f1pos = 0.9509, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 00:44:06.839501: step 29000, loss = 0.1242, acc = 0.9640, f1neg = 0.9589, f1pos = 0.9680, f1 = 0.9634
[Eval_batch(20)(2000,42000)] 2017-05-03 00:44:07.313405: step 29000, loss = 0.1217, acc = 0.9605, f1neg = 0.9652, f1pos = 0.9543, f1 = 0.9598
[Eval_batch(21)(2000,44000)] 2017-05-03 00:44:07.787356: step 29000, loss = 0.1228, acc = 0.9575, f1neg = 0.9543, f1pos = 0.9603, f1 = 0.9573
[Eval_batch(22)(2000,46000)] 2017-05-03 00:44:08.259419: step 29000, loss = 0.1257, acc = 0.9605, f1neg = 0.9613, f1pos = 0.9597, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 00:44:08.731859: step 29000, loss = 0.1342, acc = 0.9560, f1neg = 0.9511, f1pos = 0.9600, f1 = 0.9555
[Eval_batch(24)(2000,50000)] 2017-05-03 00:44:09.238631: step 29000, loss = 0.1193, acc = 0.9635, f1neg = 0.9586, f1pos = 0.9673, f1 = 0.9630
[Eval_batch(25)(2000,52000)] 2017-05-03 00:44:09.703197: step 29000, loss = 0.1085, acc = 0.9700, f1neg = 0.9675, f1pos = 0.9722, f1 = 0.9698
[Eval_batch(26)(2000,54000)] 2017-05-03 00:44:10.174180: step 29000, loss = 0.1191, acc = 0.9595, f1neg = 0.9622, f1pos = 0.9564, f1 = 0.9593
[Eval_batch(27)(2000,56000)] 2017-05-03 00:44:10.740322: step 29000, loss = 0.1074, acc = 0.9665, f1neg = 0.9651, f1pos = 0.9678, f1 = 0.9664
[Eval] 2017-05-03 00:44:10.740412: step 29000, acc = 0.9586, f1 = 0.9583
[Test_batch(0)(2000,2000)] 2017-05-03 00:44:11.209918: step 29000, loss = 0.1566, acc = 0.9550, f1neg = 0.9577, f1pos = 0.9519, f1 = 0.9548
[Test_batch(1)(2000,4000)] 2017-05-03 00:44:11.716103: step 29000, loss = 0.1395, acc = 0.9530, f1neg = 0.9587, f1pos = 0.9454, f1 = 0.9521
[Test_batch(2)(2000,6000)] 2017-05-03 00:44:12.194769: step 29000, loss = 0.1535, acc = 0.9535, f1neg = 0.9578, f1pos = 0.9482, f1 = 0.9530
[Test_batch(3)(2000,8000)] 2017-05-03 00:44:12.700245: step 29000, loss = 0.1634, acc = 0.9420, f1neg = 0.9467, f1pos = 0.9364, f1 = 0.9415
[Test_batch(4)(2000,10000)] 2017-05-03 00:44:13.168899: step 29000, loss = 0.1644, acc = 0.9430, f1neg = 0.9441, f1pos = 0.9418, f1 = 0.9430
[Test_batch(5)(2000,12000)] 2017-05-03 00:44:13.638949: step 29000, loss = 0.1751, acc = 0.9380, f1neg = 0.9397, f1pos = 0.9361, f1 = 0.9379
[Test_batch(6)(2000,14000)] 2017-05-03 00:44:14.111056: step 29000, loss = 0.1568, acc = 0.9470, f1neg = 0.9459, f1pos = 0.9481, f1 = 0.9470
[Test_batch(7)(2000,16000)] 2017-05-03 00:44:14.615224: step 29000, loss = 0.1467, acc = 0.9505, f1neg = 0.9557, f1pos = 0.9438, f1 = 0.9498
[Test_batch(8)(2000,18000)] 2017-05-03 00:44:15.091834: step 29000, loss = 0.1498, acc = 0.9475, f1neg = 0.9475, f1pos = 0.9475, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 00:44:15.592937: step 29000, loss = 0.1839, acc = 0.9310, f1neg = 0.9286, f1pos = 0.9333, f1 = 0.9309
[Test_batch(10)(2000,22000)] 2017-05-03 00:44:16.060086: step 29000, loss = 0.1622, acc = 0.9460, f1neg = 0.9403, f1pos = 0.9507, f1 = 0.9455
[Test_batch(11)(2000,24000)] 2017-05-03 00:44:16.542208: step 29000, loss = 0.1554, acc = 0.9495, f1neg = 0.9515, f1pos = 0.9473, f1 = 0.9494
[Test_batch(12)(2000,26000)] 2017-05-03 00:44:17.021025: step 29000, loss = 0.1356, acc = 0.9590, f1neg = 0.9609, f1pos = 0.9569, f1 = 0.9589
[Test_batch(13)(2000,28000)] 2017-05-03 00:44:17.499135: step 29000, loss = 0.1628, acc = 0.9470, f1neg = 0.9422, f1pos = 0.9511, f1 = 0.9466
[Test_batch(14)(2000,30000)] 2017-05-03 00:44:17.967812: step 29000, loss = 0.1375, acc = 0.9550, f1neg = 0.9493, f1pos = 0.9595, f1 = 0.9544
[Test_batch(15)(2000,32000)] 2017-05-03 00:44:18.471017: step 29000, loss = 0.1571, acc = 0.9510, f1neg = 0.9510, f1pos = 0.9510, f1 = 0.9510
[Test_batch(16)(2000,34000)] 2017-05-03 00:44:18.945835: step 29000, loss = 0.1390, acc = 0.9560, f1neg = 0.9556, f1pos = 0.9564, f1 = 0.9560
[Test_batch(17)(2000,36000)] 2017-05-03 00:44:19.399788: step 29000, loss = 0.1291, acc = 0.9590, f1neg = 0.9557, f1pos = 0.9619, f1 = 0.9588
[Test_batch(18)(2000,38000)] 2017-05-03 00:44:19.960867: step 29000, loss = 0.1174, acc = 0.9675, f1neg = 0.9672, f1pos = 0.9678, f1 = 0.9675
[Test] 2017-05-03 00:44:19.960973: step 29000, acc = 0.9500, f1 = 0.9498
[Status] 2017-05-03 00:44:19.961000: step 29000, maxindex = 28000, maxdev = 0.9592, maxtst = 0.9504
2017-05-03 00:44:29.258940: step 29010, loss = 0.1452, acc = 0.9440 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 00:44:38.430151: step 29020, loss = 0.1565, acc = 0.9500 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 00:44:47.429335: step 29030, loss = 0.1448, acc = 0.9580 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 00:44:56.511204: step 29040, loss = 0.1364, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 00:45:05.550291: step 29050, loss = 0.1371, acc = 0.9640 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 00:45:14.610251: step 29060, loss = 0.1168, acc = 0.9580 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 00:45:23.731419: step 29070, loss = 0.1441, acc = 0.9520 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 00:45:32.822839: step 29080, loss = 0.1547, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 00:45:41.976056: step 29090, loss = 0.1541, acc = 0.9480 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 00:45:51.044167: step 29100, loss = 0.1343, acc = 0.9520 (269.6 examples/sec; 0.237 sec/batch)
2017-05-03 00:46:00.224169: step 29110, loss = 0.1260, acc = 0.9600 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 00:46:09.337081: step 29120, loss = 0.1787, acc = 0.9360 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 00:46:18.375534: step 29130, loss = 0.1390, acc = 0.9480 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 00:46:27.683960: step 29140, loss = 0.1405, acc = 0.9460 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 00:46:36.753436: step 29150, loss = 0.1648, acc = 0.9480 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 00:46:45.783571: step 29160, loss = 0.1286, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 00:46:54.863450: step 29170, loss = 0.1796, acc = 0.9440 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 00:47:04.032127: step 29180, loss = 0.1377, acc = 0.9540 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 00:47:13.294677: step 29190, loss = 0.1342, acc = 0.9560 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 00:47:22.421971: step 29200, loss = 0.1242, acc = 0.9560 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 00:47:31.565221: step 29210, loss = 0.1732, acc = 0.9420 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 00:47:40.643077: step 29220, loss = 0.1285, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 00:47:49.823599: step 29230, loss = 0.1711, acc = 0.9380 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 00:48:00.068978: step 29240, loss = 0.1473, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 00:48:09.157210: step 29250, loss = 0.1329, acc = 0.9480 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 00:48:18.268576: step 29260, loss = 0.1387, acc = 0.9520 (262.6 examples/sec; 0.244 sec/batch)
2017-05-03 00:48:27.312609: step 29270, loss = 0.1343, acc = 0.9620 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 00:48:36.432591: step 29280, loss = 0.1393, acc = 0.9540 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 00:48:45.615103: step 29290, loss = 0.1535, acc = 0.9440 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 00:48:54.707786: step 29300, loss = 0.1292, acc = 0.9480 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 00:49:03.933439: step 29310, loss = 0.1379, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 00:49:13.436547: step 29320, loss = 0.1175, acc = 0.9660 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 00:49:22.611644: step 29330, loss = 0.1353, acc = 0.9540 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 00:49:31.709446: step 29340, loss = 0.1504, acc = 0.9380 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 00:49:40.786077: step 29350, loss = 0.1282, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 00:49:50.033219: step 29360, loss = 0.1415, acc = 0.9600 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 00:49:59.194790: step 29370, loss = 0.1531, acc = 0.9500 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 00:50:08.250018: step 29380, loss = 0.1439, acc = 0.9500 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 00:50:17.388669: step 29390, loss = 0.1592, acc = 0.9460 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 00:50:26.442550: step 29400, loss = 0.1470, acc = 0.9480 (262.5 examples/sec; 0.244 sec/batch)
2017-05-03 00:50:35.531891: step 29410, loss = 0.1289, acc = 0.9660 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 00:50:44.678529: step 29420, loss = 0.1496, acc = 0.9500 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 00:50:53.945842: step 29430, loss = 0.1408, acc = 0.9580 (270.6 examples/sec; 0.237 sec/batch)
2017-05-03 00:51:03.129295: step 29440, loss = 0.1662, acc = 0.9400 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:51:12.271671: step 29450, loss = 0.1683, acc = 0.9380 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 00:51:21.685618: step 29460, loss = 0.1330, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 00:51:31.076214: step 29470, loss = 0.1894, acc = 0.9220 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 00:51:40.308026: step 29480, loss = 0.1253, acc = 0.9580 (251.5 examples/sec; 0.254 sec/batch)
2017-05-03 00:51:49.484744: step 29490, loss = 0.1322, acc = 0.9640 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 00:51:58.802586: step 29500, loss = 0.1293, acc = 0.9620 (262.6 examples/sec; 0.244 sec/batch)
2017-05-03 00:52:08.089614: step 29510, loss = 0.1462, acc = 0.9480 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 00:52:17.409402: step 29520, loss = 0.1250, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 00:52:26.729813: step 29530, loss = 0.1096, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 00:52:36.188473: step 29540, loss = 0.1571, acc = 0.9500 (270.6 examples/sec; 0.236 sec/batch)
2017-05-03 00:52:45.390282: step 29550, loss = 0.1160, acc = 0.9660 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 00:52:54.474766: step 29560, loss = 0.1235, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 00:53:03.590990: step 29570, loss = 0.1559, acc = 0.9400 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 00:53:12.786855: step 29580, loss = 0.1404, acc = 0.9540 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 00:53:21.935764: step 29590, loss = 0.1614, acc = 0.9520 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 00:53:31.206641: step 29600, loss = 0.1491, acc = 0.9460 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 00:53:40.236401: step 29610, loss = 0.1560, acc = 0.9360 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 00:53:49.471412: step 29620, loss = 0.1470, acc = 0.9520 (260.7 examples/sec; 0.246 sec/batch)
2017-05-03 00:53:58.760457: step 29630, loss = 0.1665, acc = 0.9420 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 00:54:07.832389: step 29640, loss = 0.1266, acc = 0.9600 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 00:54:16.931075: step 29650, loss = 0.1243, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 00:54:25.896654: step 29660, loss = 0.1582, acc = 0.9560 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 00:54:34.962141: step 29670, loss = 0.1200, acc = 0.9680 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 00:54:43.939314: step 29680, loss = 0.1391, acc = 0.9640 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 00:54:52.987455: step 29690, loss = 0.1190, acc = 0.9700 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 00:55:01.910648: step 29700, loss = 0.1355, acc = 0.9440 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 00:55:10.920812: step 29710, loss = 0.1746, acc = 0.9360 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 00:55:20.626594: step 29720, loss = 0.1428, acc = 0.9440 (182.6 examples/sec; 0.350 sec/batch)
2017-05-03 00:55:29.761870: step 29730, loss = 0.1458, acc = 0.9520 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 00:55:38.874904: step 29740, loss = 0.1543, acc = 0.9460 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 00:55:48.049659: step 29750, loss = 0.1877, acc = 0.9340 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 00:55:57.199552: step 29760, loss = 0.1467, acc = 0.9440 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 00:56:06.242655: step 29770, loss = 0.1715, acc = 0.9460 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 00:56:15.475247: step 29780, loss = 0.1750, acc = 0.9380 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 00:56:24.827709: step 29790, loss = 0.1633, acc = 0.9480 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 00:56:33.951398: step 29800, loss = 0.1273, acc = 0.9560 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 00:56:43.096170: step 29810, loss = 0.1466, acc = 0.9460 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 00:56:52.422903: step 29820, loss = 0.1084, acc = 0.9680 (260.3 examples/sec; 0.246 sec/batch)
2017-05-03 00:57:01.597181: step 29830, loss = 0.1354, acc = 0.9600 (256.9 examples/sec; 0.249 sec/batch)
2017-05-03 00:57:10.640205: step 29840, loss = 0.1243, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 00:57:19.650751: step 29850, loss = 0.1630, acc = 0.9500 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 00:57:28.727571: step 29860, loss = 0.1416, acc = 0.9500 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 00:57:38.202685: step 29870, loss = 0.1255, acc = 0.9680 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 00:57:47.407035: step 29880, loss = 0.1510, acc = 0.9480 (245.4 examples/sec; 0.261 sec/batch)
2017-05-03 00:57:56.519604: step 29890, loss = 0.1499, acc = 0.9440 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 00:58:05.388395: step 29900, loss = 0.1235, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 00:58:14.476160: step 29910, loss = 0.1399, acc = 0.9500 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 00:58:23.603174: step 29920, loss = 0.1121, acc = 0.9660 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 00:58:32.732070: step 29930, loss = 0.1501, acc = 0.9580 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 00:58:41.816275: step 29940, loss = 0.1816, acc = 0.9420 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 00:58:50.946188: step 29950, loss = 0.1434, acc = 0.9520 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 00:58:59.934498: step 29960, loss = 0.1428, acc = 0.9440 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 00:59:09.047092: step 29970, loss = 0.1620, acc = 0.9400 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 00:59:18.152781: step 29980, loss = 0.1333, acc = 0.9480 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 00:59:27.273430: step 29990, loss = 0.1335, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 00:59:36.281592: step 30000, loss = 0.1513, acc = 0.9580 (285.2 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 00:59:36.741107: step 30000, loss = 0.1189, acc = 0.9555, f1neg = 0.9516, f1pos = 0.9589, f1 = 0.9552
[Eval_batch(1)(2000,4000)] 2017-05-03 00:59:37.216337: step 30000, loss = 0.1238, acc = 0.9645, f1neg = 0.9590, f1pos = 0.9687, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 00:59:37.692058: step 30000, loss = 0.1318, acc = 0.9545, f1neg = 0.9521, f1pos = 0.9567, f1 = 0.9544
[Eval_batch(3)(2000,8000)] 2017-05-03 00:59:38.138508: step 30000, loss = 0.1373, acc = 0.9555, f1neg = 0.9558, f1pos = 0.9552, f1 = 0.9555
[Eval_batch(4)(2000,10000)] 2017-05-03 00:59:38.581883: step 30000, loss = 0.1380, acc = 0.9555, f1neg = 0.9571, f1pos = 0.9538, f1 = 0.9554
[Eval_batch(5)(2000,12000)] 2017-05-03 00:59:39.039557: step 30000, loss = 0.1355, acc = 0.9525, f1neg = 0.9496, f1pos = 0.9551, f1 = 0.9523
[Eval_batch(6)(2000,14000)] 2017-05-03 00:59:39.512520: step 30000, loss = 0.1242, acc = 0.9630, f1neg = 0.9625, f1pos = 0.9635, f1 = 0.9630
[Eval_batch(7)(2000,16000)] 2017-05-03 00:59:39.985332: step 30000, loss = 0.1251, acc = 0.9575, f1neg = 0.9496, f1pos = 0.9633, f1 = 0.9564
[Eval_batch(8)(2000,18000)] 2017-05-03 00:59:40.486079: step 30000, loss = 0.1207, acc = 0.9605, f1neg = 0.9554, f1pos = 0.9645, f1 = 0.9600
[Eval_batch(9)(2000,20000)] 2017-05-03 00:59:40.949831: step 30000, loss = 0.1247, acc = 0.9615, f1neg = 0.9592, f1pos = 0.9636, f1 = 0.9614
[Eval_batch(10)(2000,22000)] 2017-05-03 00:59:41.430364: step 30000, loss = 0.1341, acc = 0.9595, f1neg = 0.9570, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(11)(2000,24000)] 2017-05-03 00:59:41.936391: step 30000, loss = 0.1340, acc = 0.9595, f1neg = 0.9536, f1pos = 0.9640, f1 = 0.9588
[Eval_batch(12)(2000,26000)] 2017-05-03 00:59:42.415051: step 30000, loss = 0.1307, acc = 0.9535, f1neg = 0.9527, f1pos = 0.9543, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 00:59:42.922724: step 30000, loss = 0.1192, acc = 0.9630, f1neg = 0.9539, f1pos = 0.9691, f1 = 0.9615
[Eval_batch(14)(2000,30000)] 2017-05-03 00:59:43.397973: step 30000, loss = 0.1458, acc = 0.9525, f1neg = 0.9438, f1pos = 0.9589, f1 = 0.9513
[Eval_batch(15)(2000,32000)] 2017-05-03 00:59:43.900967: step 30000, loss = 0.1260, acc = 0.9630, f1neg = 0.9604, f1pos = 0.9653, f1 = 0.9628
[Eval_batch(16)(2000,34000)] 2017-05-03 00:59:44.375552: step 30000, loss = 0.1212, acc = 0.9635, f1neg = 0.9622, f1pos = 0.9648, f1 = 0.9635
[Eval_batch(17)(2000,36000)] 2017-05-03 00:59:44.853502: step 30000, loss = 0.1432, acc = 0.9550, f1neg = 0.9554, f1pos = 0.9546, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 00:59:45.323151: step 30000, loss = 0.1381, acc = 0.9515, f1neg = 0.9537, f1pos = 0.9491, f1 = 0.9514
[Eval_batch(19)(2000,40000)] 2017-05-03 00:59:45.796599: step 30000, loss = 0.1210, acc = 0.9670, f1neg = 0.9621, f1pos = 0.9708, f1 = 0.9664
[Eval_batch(20)(2000,42000)] 2017-05-03 00:59:46.256639: step 30000, loss = 0.1206, acc = 0.9615, f1neg = 0.9660, f1pos = 0.9556, f1 = 0.9608
[Eval_batch(21)(2000,44000)] 2017-05-03 00:59:46.729759: step 30000, loss = 0.1186, acc = 0.9585, f1neg = 0.9551, f1pos = 0.9614, f1 = 0.9583
[Eval_batch(22)(2000,46000)] 2017-05-03 00:59:47.203750: step 30000, loss = 0.1233, acc = 0.9595, f1neg = 0.9601, f1pos = 0.9589, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 00:59:47.678873: step 30000, loss = 0.1308, acc = 0.9580, f1neg = 0.9530, f1pos = 0.9621, f1 = 0.9575
[Eval_batch(24)(2000,50000)] 2017-05-03 00:59:48.152708: step 30000, loss = 0.1168, acc = 0.9630, f1neg = 0.9580, f1pos = 0.9670, f1 = 0.9625
[Eval_batch(25)(2000,52000)] 2017-05-03 00:59:48.655760: step 30000, loss = 0.1076, acc = 0.9685, f1neg = 0.9657, f1pos = 0.9709, f1 = 0.9683
[Eval_batch(26)(2000,54000)] 2017-05-03 00:59:49.106458: step 30000, loss = 0.1184, acc = 0.9605, f1neg = 0.9629, f1pos = 0.9577, f1 = 0.9603
[Eval_batch(27)(2000,56000)] 2017-05-03 00:59:49.677610: step 30000, loss = 0.1061, acc = 0.9690, f1neg = 0.9676, f1pos = 0.9702, f1 = 0.9689
[Eval] 2017-05-03 00:59:49.677702: step 30000, acc = 0.9595, f1 = 0.9592
[Test_batch(0)(2000,2000)] 2017-05-03 00:59:50.168521: step 30000, loss = 0.1549, acc = 0.9520, f1neg = 0.9548, f1pos = 0.9489, f1 = 0.9518
[Test_batch(1)(2000,4000)] 2017-05-03 00:59:50.646080: step 30000, loss = 0.1376, acc = 0.9560, f1neg = 0.9612, f1pos = 0.9492, f1 = 0.9552
[Test_batch(2)(2000,6000)] 2017-05-03 00:59:51.131849: step 30000, loss = 0.1521, acc = 0.9545, f1neg = 0.9585, f1pos = 0.9497, f1 = 0.9541
[Test_batch(3)(2000,8000)] 2017-05-03 00:59:51.594321: step 30000, loss = 0.1608, acc = 0.9465, f1neg = 0.9503, f1pos = 0.9420, f1 = 0.9462
[Test_batch(4)(2000,10000)] 2017-05-03 00:59:52.057399: step 30000, loss = 0.1602, acc = 0.9465, f1neg = 0.9471, f1pos = 0.9459, f1 = 0.9465
[Test_batch(5)(2000,12000)] 2017-05-03 00:59:52.535213: step 30000, loss = 0.1704, acc = 0.9390, f1neg = 0.9403, f1pos = 0.9376, f1 = 0.9390
[Test_batch(6)(2000,14000)] 2017-05-03 00:59:53.000235: step 30000, loss = 0.1553, acc = 0.9470, f1neg = 0.9454, f1pos = 0.9485, f1 = 0.9470
[Test_batch(7)(2000,16000)] 2017-05-03 00:59:53.479648: step 30000, loss = 0.1457, acc = 0.9515, f1neg = 0.9563, f1pos = 0.9455, f1 = 0.9509
[Test_batch(8)(2000,18000)] 2017-05-03 00:59:53.949213: step 30000, loss = 0.1468, acc = 0.9475, f1neg = 0.9474, f1pos = 0.9476, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 00:59:54.418213: step 30000, loss = 0.1793, acc = 0.9340, f1neg = 0.9313, f1pos = 0.9365, f1 = 0.9339
[Test_batch(10)(2000,22000)] 2017-05-03 00:59:54.886657: step 30000, loss = 0.1559, acc = 0.9485, f1neg = 0.9426, f1pos = 0.9533, f1 = 0.9479
[Test_batch(11)(2000,24000)] 2017-05-03 00:59:55.352136: step 30000, loss = 0.1532, acc = 0.9485, f1neg = 0.9504, f1pos = 0.9464, f1 = 0.9484
[Test_batch(12)(2000,26000)] 2017-05-03 00:59:55.825380: step 30000, loss = 0.1322, acc = 0.9595, f1neg = 0.9613, f1pos = 0.9576, f1 = 0.9594
[Test_batch(13)(2000,28000)] 2017-05-03 00:59:56.323869: step 30000, loss = 0.1595, acc = 0.9465, f1neg = 0.9413, f1pos = 0.9508, f1 = 0.9461
[Test_batch(14)(2000,30000)] 2017-05-03 00:59:56.796230: step 30000, loss = 0.1330, acc = 0.9555, f1neg = 0.9497, f1pos = 0.9601, f1 = 0.9549
[Test_batch(15)(2000,32000)] 2017-05-03 00:59:57.276612: step 30000, loss = 0.1525, acc = 0.9525, f1neg = 0.9523, f1pos = 0.9527, f1 = 0.9525
[Test_batch(16)(2000,34000)] 2017-05-03 00:59:57.747487: step 30000, loss = 0.1359, acc = 0.9580, f1neg = 0.9573, f1pos = 0.9587, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 00:59:58.262883: step 30000, loss = 0.1266, acc = 0.9610, f1neg = 0.9577, f1pos = 0.9639, f1 = 0.9608
[Test_batch(18)(2000,38000)] 2017-05-03 00:59:58.785374: step 30000, loss = 0.1179, acc = 0.9640, f1neg = 0.9635, f1pos = 0.9645, f1 = 0.9640
[Test] 2017-05-03 00:59:58.785463: step 30000, acc = 0.9510, f1 = 0.9507
[Status] 2017-05-03 00:59:58.785490: step 30000, maxindex = 30000, maxdev = 0.9595, maxtst = 0.9510
2017-05-03 01:00:11.068560: step 30010, loss = 0.1174, acc = 0.9720 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 01:00:20.131817: step 30020, loss = 0.1722, acc = 0.9260 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 01:00:29.467955: step 30030, loss = 0.1659, acc = 0.9420 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 01:00:38.572623: step 30040, loss = 0.1170, acc = 0.9720 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 01:00:47.662430: step 30050, loss = 0.1791, acc = 0.9360 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 01:00:56.838026: step 30060, loss = 0.1702, acc = 0.9540 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 01:01:05.901631: step 30070, loss = 0.1535, acc = 0.9420 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 01:01:15.080114: step 30080, loss = 0.1712, acc = 0.9360 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 01:01:24.418460: step 30090, loss = 0.1331, acc = 0.9600 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 01:01:33.625330: step 30100, loss = 0.1334, acc = 0.9600 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 01:01:42.725524: step 30110, loss = 0.1400, acc = 0.9560 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 01:01:51.838366: step 30120, loss = 0.1357, acc = 0.9560 (270.1 examples/sec; 0.237 sec/batch)
2017-05-03 01:02:00.997551: step 30130, loss = 0.1493, acc = 0.9440 (268.9 examples/sec; 0.238 sec/batch)
2017-05-03 01:02:10.177945: step 30140, loss = 0.1385, acc = 0.9600 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 01:02:19.320316: step 30150, loss = 0.1506, acc = 0.9500 (266.0 examples/sec; 0.241 sec/batch)
2017-05-03 01:02:28.415294: step 30160, loss = 0.1368, acc = 0.9640 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 01:02:37.465123: step 30170, loss = 0.1534, acc = 0.9400 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 01:02:46.609228: step 30180, loss = 0.1421, acc = 0.9500 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:02:55.712095: step 30190, loss = 0.1439, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 01:03:04.856794: step 30200, loss = 0.1421, acc = 0.9600 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 01:03:13.981200: step 30210, loss = 0.1548, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 01:03:23.150919: step 30220, loss = 0.1455, acc = 0.9560 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 01:03:32.418709: step 30230, loss = 0.1500, acc = 0.9560 (299.2 examples/sec; 0.214 sec/batch)
2017-05-03 01:03:42.164324: step 30240, loss = 0.1565, acc = 0.9500 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 01:03:51.237503: step 30250, loss = 0.1286, acc = 0.9600 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 01:04:00.445238: step 30260, loss = 0.1334, acc = 0.9440 (256.6 examples/sec; 0.249 sec/batch)
2017-05-03 01:04:09.952954: step 30270, loss = 0.1575, acc = 0.9480 (213.3 examples/sec; 0.300 sec/batch)
2017-05-03 01:04:19.051325: step 30280, loss = 0.1211, acc = 0.9660 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 01:04:28.246748: step 30290, loss = 0.1586, acc = 0.9380 (269.9 examples/sec; 0.237 sec/batch)
2017-05-03 01:04:37.535968: step 30300, loss = 0.1223, acc = 0.9580 (266.0 examples/sec; 0.241 sec/batch)
2017-05-03 01:04:46.880646: step 30310, loss = 0.1344, acc = 0.9540 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 01:04:56.123680: step 30320, loss = 0.1749, acc = 0.9300 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 01:05:05.269201: step 30330, loss = 0.1296, acc = 0.9560 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 01:05:14.553012: step 30340, loss = 0.1430, acc = 0.9700 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 01:05:23.651325: step 30350, loss = 0.1731, acc = 0.9420 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:05:32.667817: step 30360, loss = 0.1554, acc = 0.9440 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 01:05:41.872243: step 30370, loss = 0.1378, acc = 0.9440 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:05:51.059627: step 30380, loss = 0.1473, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 01:06:00.303654: step 30390, loss = 0.1253, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 01:06:09.359715: step 30400, loss = 0.1230, acc = 0.9640 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 01:06:18.483018: step 30410, loss = 0.1112, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 01:06:28.113679: step 30420, loss = 0.1554, acc = 0.9500 (178.8 examples/sec; 0.358 sec/batch)
2017-05-03 01:06:37.303938: step 30430, loss = 0.1536, acc = 0.9440 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 01:06:46.565277: step 30440, loss = 0.1381, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 01:06:55.572377: step 30450, loss = 0.1437, acc = 0.9500 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 01:07:04.608006: step 30460, loss = 0.1302, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 01:07:13.846751: step 30470, loss = 0.1719, acc = 0.9480 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 01:07:23.204678: step 30480, loss = 0.1153, acc = 0.9700 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 01:07:32.304135: step 30490, loss = 0.1448, acc = 0.9420 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 01:07:41.561786: step 30500, loss = 0.1267, acc = 0.9480 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:07:50.739428: step 30510, loss = 0.1295, acc = 0.9660 (273.4 examples/sec; 0.234 sec/batch)
2017-05-03 01:07:59.885474: step 30520, loss = 0.1489, acc = 0.9500 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 01:08:08.995435: step 30530, loss = 0.1408, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 01:08:18.549540: step 30540, loss = 0.1590, acc = 0.9540 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 01:08:27.682078: step 30550, loss = 0.1637, acc = 0.9540 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 01:08:36.894292: step 30560, loss = 0.1482, acc = 0.9600 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 01:08:46.049169: step 30570, loss = 0.1337, acc = 0.9520 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 01:08:55.369056: step 30580, loss = 0.1637, acc = 0.9400 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 01:09:04.440658: step 30590, loss = 0.1599, acc = 0.9480 (269.2 examples/sec; 0.238 sec/batch)
2017-05-03 01:09:13.491674: step 30600, loss = 0.1266, acc = 0.9620 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 01:09:22.462491: step 30610, loss = 0.1169, acc = 0.9700 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 01:09:31.558641: step 30620, loss = 0.1260, acc = 0.9660 (295.7 examples/sec; 0.216 sec/batch)
2017-05-03 01:09:40.713392: step 30630, loss = 0.1362, acc = 0.9500 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 01:09:49.919041: step 30640, loss = 0.1490, acc = 0.9520 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 01:09:58.974283: step 30650, loss = 0.1501, acc = 0.9520 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 01:10:08.093479: step 30660, loss = 0.1209, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 01:10:17.198402: step 30670, loss = 0.1434, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 01:10:26.624890: step 30680, loss = 0.1291, acc = 0.9600 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 01:10:36.089865: step 30690, loss = 0.1623, acc = 0.9440 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 01:10:45.317254: step 30700, loss = 0.1329, acc = 0.9560 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 01:10:54.409247: step 30710, loss = 0.1356, acc = 0.9520 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 01:11:03.409164: step 30720, loss = 0.2012, acc = 0.9240 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 01:11:12.529653: step 30730, loss = 0.1473, acc = 0.9460 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 01:11:22.070183: step 30740, loss = 0.1738, acc = 0.9380 (263.1 examples/sec; 0.243 sec/batch)
2017-05-03 01:11:31.162343: step 30750, loss = 0.1701, acc = 0.9600 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 01:11:40.225509: step 30760, loss = 0.1304, acc = 0.9600 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 01:11:49.268031: step 30770, loss = 0.1563, acc = 0.9460 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 01:11:58.350661: step 30780, loss = 0.1469, acc = 0.9460 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 01:12:07.345353: step 30790, loss = 0.1118, acc = 0.9700 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 01:12:16.530449: step 30800, loss = 0.1230, acc = 0.9660 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 01:12:25.624613: step 30810, loss = 0.1374, acc = 0.9560 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 01:12:34.604608: step 30820, loss = 0.1503, acc = 0.9500 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 01:12:43.727391: step 30830, loss = 0.1426, acc = 0.9580 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 01:12:52.836303: step 30840, loss = 0.1240, acc = 0.9540 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 01:13:01.845769: step 30850, loss = 0.1358, acc = 0.9540 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 01:13:10.886931: step 30860, loss = 0.1290, acc = 0.9540 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 01:13:19.959327: step 30870, loss = 0.1438, acc = 0.9560 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 01:13:29.170266: step 30880, loss = 0.1549, acc = 0.9360 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 01:13:38.463858: step 30890, loss = 0.1735, acc = 0.9280 (267.8 examples/sec; 0.239 sec/batch)
2017-05-03 01:13:47.451676: step 30900, loss = 0.1436, acc = 0.9440 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 01:13:56.643985: step 30910, loss = 0.1165, acc = 0.9700 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 01:14:05.718755: step 30920, loss = 0.1408, acc = 0.9540 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 01:14:14.823121: step 30930, loss = 0.1468, acc = 0.9540 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 01:14:23.843008: step 30940, loss = 0.1290, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 01:14:33.021142: step 30950, loss = 0.1250, acc = 0.9640 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:14:42.175147: step 30960, loss = 0.1376, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 01:14:51.294334: step 30970, loss = 0.1315, acc = 0.9600 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 01:15:00.410423: step 30980, loss = 0.1679, acc = 0.9460 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 01:15:09.565949: step 30990, loss = 0.1400, acc = 0.9500 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 01:15:18.657352: step 31000, loss = 0.1400, acc = 0.9520 (284.5 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 01:15:19.155066: step 31000, loss = 0.1220, acc = 0.9560, f1neg = 0.9515, f1pos = 0.9597, f1 = 0.9556
[Eval_batch(1)(2000,4000)] 2017-05-03 01:15:19.629878: step 31000, loss = 0.1199, acc = 0.9635, f1neg = 0.9571, f1pos = 0.9682, f1 = 0.9627
[Eval_batch(2)(2000,6000)] 2017-05-03 01:15:20.135050: step 31000, loss = 0.1343, acc = 0.9575, f1neg = 0.9544, f1pos = 0.9602, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-03 01:15:20.617824: step 31000, loss = 0.1403, acc = 0.9535, f1neg = 0.9532, f1pos = 0.9538, f1 = 0.9535
[Eval_batch(4)(2000,10000)] 2017-05-03 01:15:21.120972: step 31000, loss = 0.1424, acc = 0.9525, f1neg = 0.9533, f1pos = 0.9517, f1 = 0.9525
[Eval_batch(5)(2000,12000)] 2017-05-03 01:15:21.630165: step 31000, loss = 0.1376, acc = 0.9490, f1neg = 0.9448, f1pos = 0.9526, f1 = 0.9487
[Eval_batch(6)(2000,14000)] 2017-05-03 01:15:22.101865: step 31000, loss = 0.1292, acc = 0.9595, f1neg = 0.9585, f1pos = 0.9605, f1 = 0.9595
[Eval_batch(7)(2000,16000)] 2017-05-03 01:15:22.603249: step 31000, loss = 0.1250, acc = 0.9605, f1neg = 0.9524, f1pos = 0.9663, f1 = 0.9593
[Eval_batch(8)(2000,18000)] 2017-05-03 01:15:23.103120: step 31000, loss = 0.1226, acc = 0.9595, f1neg = 0.9534, f1pos = 0.9642, f1 = 0.9588
[Eval_batch(9)(2000,20000)] 2017-05-03 01:15:23.584911: step 31000, loss = 0.1251, acc = 0.9620, f1neg = 0.9591, f1pos = 0.9645, f1 = 0.9618
[Eval_batch(10)(2000,22000)] 2017-05-03 01:15:24.065755: step 31000, loss = 0.1353, acc = 0.9525, f1neg = 0.9487, f1pos = 0.9558, f1 = 0.9522
[Eval_batch(11)(2000,24000)] 2017-05-03 01:15:24.536370: step 31000, loss = 0.1338, acc = 0.9580, f1neg = 0.9512, f1pos = 0.9631, f1 = 0.9572
[Eval_batch(12)(2000,26000)] 2017-05-03 01:15:24.991496: step 31000, loss = 0.1318, acc = 0.9510, f1neg = 0.9493, f1pos = 0.9526, f1 = 0.9509
[Eval_batch(13)(2000,28000)] 2017-05-03 01:15:25.469959: step 31000, loss = 0.1195, acc = 0.9635, f1neg = 0.9541, f1pos = 0.9697, f1 = 0.9619
[Eval_batch(14)(2000,30000)] 2017-05-03 01:15:25.948664: step 31000, loss = 0.1471, acc = 0.9510, f1neg = 0.9407, f1pos = 0.9583, f1 = 0.9495
[Eval_batch(15)(2000,32000)] 2017-05-03 01:15:26.453051: step 31000, loss = 0.1241, acc = 0.9660, f1neg = 0.9631, f1pos = 0.9685, f1 = 0.9658
[Eval_batch(16)(2000,34000)] 2017-05-03 01:15:26.948683: step 31000, loss = 0.1215, acc = 0.9630, f1neg = 0.9613, f1pos = 0.9646, f1 = 0.9629
[Eval_batch(17)(2000,36000)] 2017-05-03 01:15:27.412205: step 31000, loss = 0.1443, acc = 0.9550, f1neg = 0.9550, f1pos = 0.9550, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 01:15:27.889181: step 31000, loss = 0.1440, acc = 0.9515, f1neg = 0.9530, f1pos = 0.9499, f1 = 0.9514
[Eval_batch(19)(2000,40000)] 2017-05-03 01:15:28.400994: step 31000, loss = 0.1228, acc = 0.9670, f1neg = 0.9616, f1pos = 0.9711, f1 = 0.9663
[Eval_batch(20)(2000,42000)] 2017-05-03 01:15:28.879106: step 31000, loss = 0.1269, acc = 0.9600, f1neg = 0.9644, f1pos = 0.9544, f1 = 0.9594
[Eval_batch(21)(2000,44000)] 2017-05-03 01:15:29.353807: step 31000, loss = 0.1149, acc = 0.9635, f1neg = 0.9601, f1pos = 0.9664, f1 = 0.9632
[Eval_batch(22)(2000,46000)] 2017-05-03 01:15:29.815874: step 31000, loss = 0.1246, acc = 0.9590, f1neg = 0.9591, f1pos = 0.9589, f1 = 0.9590
[Eval_batch(23)(2000,48000)] 2017-05-03 01:15:30.287012: step 31000, loss = 0.1292, acc = 0.9580, f1neg = 0.9523, f1pos = 0.9625, f1 = 0.9574
[Eval_batch(24)(2000,50000)] 2017-05-03 01:15:30.781340: step 31000, loss = 0.1167, acc = 0.9650, f1neg = 0.9595, f1pos = 0.9692, f1 = 0.9643
[Eval_batch(25)(2000,52000)] 2017-05-03 01:15:31.251828: step 31000, loss = 0.1077, acc = 0.9695, f1neg = 0.9663, f1pos = 0.9722, f1 = 0.9692
[Eval_batch(26)(2000,54000)] 2017-05-03 01:15:31.752995: step 31000, loss = 0.1226, acc = 0.9565, f1neg = 0.9587, f1pos = 0.9541, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-03 01:15:32.327249: step 31000, loss = 0.1103, acc = 0.9675, f1neg = 0.9657, f1pos = 0.9692, f1 = 0.9674
[Eval] 2017-05-03 01:15:32.327335: step 31000, acc = 0.9590, f1 = 0.9585
[Test_batch(0)(2000,2000)] 2017-05-03 01:15:32.809246: step 31000, loss = 0.1584, acc = 0.9500, f1neg = 0.9522, f1pos = 0.9476, f1 = 0.9499
[Test_batch(1)(2000,4000)] 2017-05-03 01:15:33.310303: step 31000, loss = 0.1406, acc = 0.9530, f1neg = 0.9580, f1pos = 0.9467, f1 = 0.9523
[Test_batch(2)(2000,6000)] 2017-05-03 01:15:33.793275: step 31000, loss = 0.1569, acc = 0.9490, f1neg = 0.9528, f1pos = 0.9445, f1 = 0.9487
[Test_batch(3)(2000,8000)] 2017-05-03 01:15:34.276375: step 31000, loss = 0.1636, acc = 0.9445, f1neg = 0.9477, f1pos = 0.9409, f1 = 0.9443
[Test_batch(4)(2000,10000)] 2017-05-03 01:15:34.758522: step 31000, loss = 0.1617, acc = 0.9450, f1neg = 0.9447, f1pos = 0.9453, f1 = 0.9450
[Test_batch(5)(2000,12000)] 2017-05-03 01:15:35.239509: step 31000, loss = 0.1728, acc = 0.9340, f1neg = 0.9341, f1pos = 0.9339, f1 = 0.9340
[Test_batch(6)(2000,14000)] 2017-05-03 01:15:35.721820: step 31000, loss = 0.1591, acc = 0.9420, f1neg = 0.9396, f1pos = 0.9442, f1 = 0.9419
[Test_batch(7)(2000,16000)] 2017-05-03 01:15:36.173607: step 31000, loss = 0.1473, acc = 0.9505, f1neg = 0.9547, f1pos = 0.9454, f1 = 0.9501
[Test_batch(8)(2000,18000)] 2017-05-03 01:15:36.640702: step 31000, loss = 0.1483, acc = 0.9470, f1neg = 0.9462, f1pos = 0.9477, f1 = 0.9470
[Test_batch(9)(2000,20000)] 2017-05-03 01:15:37.113739: step 31000, loss = 0.1775, acc = 0.9360, f1neg = 0.9320, f1pos = 0.9396, f1 = 0.9358
[Test_batch(10)(2000,22000)] 2017-05-03 01:15:37.580885: step 31000, loss = 0.1525, acc = 0.9490, f1neg = 0.9422, f1pos = 0.9543, f1 = 0.9483
[Test_batch(11)(2000,24000)] 2017-05-03 01:15:38.050904: step 31000, loss = 0.1564, acc = 0.9475, f1neg = 0.9486, f1pos = 0.9463, f1 = 0.9475
[Test_batch(12)(2000,26000)] 2017-05-03 01:15:38.529527: step 31000, loss = 0.1312, acc = 0.9580, f1neg = 0.9594, f1pos = 0.9565, f1 = 0.9579
[Test_batch(13)(2000,28000)] 2017-05-03 01:15:39.005601: step 31000, loss = 0.1584, acc = 0.9415, f1neg = 0.9344, f1pos = 0.9472, f1 = 0.9408
[Test_batch(14)(2000,30000)] 2017-05-03 01:15:39.479824: step 31000, loss = 0.1297, acc = 0.9595, f1neg = 0.9532, f1pos = 0.9643, f1 = 0.9588
[Test_batch(15)(2000,32000)] 2017-05-03 01:15:39.977146: step 31000, loss = 0.1535, acc = 0.9495, f1neg = 0.9485, f1pos = 0.9505, f1 = 0.9495
[Test_batch(16)(2000,34000)] 2017-05-03 01:15:40.425259: step 31000, loss = 0.1374, acc = 0.9555, f1neg = 0.9540, f1pos = 0.9569, f1 = 0.9555
[Test_batch(17)(2000,36000)] 2017-05-03 01:15:40.902788: step 31000, loss = 0.1294, acc = 0.9630, f1neg = 0.9593, f1pos = 0.9661, f1 = 0.9627
[Test_batch(18)(2000,38000)] 2017-05-03 01:15:41.455411: step 31000, loss = 0.1222, acc = 0.9590, f1neg = 0.9579, f1pos = 0.9600, f1 = 0.9590
[Test] 2017-05-03 01:15:41.455496: step 31000, acc = 0.9491, f1 = 0.9489
[Status] 2017-05-03 01:15:41.455521: step 31000, maxindex = 30000, maxdev = 0.9595, maxtst = 0.9510
2017-05-03 01:15:50.596711: step 31010, loss = 0.1315, acc = 0.9580 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 01:15:59.851839: step 31020, loss = 0.1850, acc = 0.9340 (249.9 examples/sec; 0.256 sec/batch)
2017-05-03 01:16:08.978323: step 31030, loss = 0.1496, acc = 0.9480 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 01:16:17.956808: step 31040, loss = 0.1362, acc = 0.9620 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 01:16:27.027197: step 31050, loss = 0.1342, acc = 0.9560 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 01:16:36.095120: step 31060, loss = 0.1183, acc = 0.9660 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 01:16:45.262600: step 31070, loss = 0.1354, acc = 0.9520 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 01:16:54.480730: step 31080, loss = 0.1155, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 01:17:03.506441: step 31090, loss = 0.1412, acc = 0.9480 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 01:17:12.696513: step 31100, loss = 0.1389, acc = 0.9580 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 01:17:21.895646: step 31110, loss = 0.1534, acc = 0.9400 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 01:17:30.938638: step 31120, loss = 0.1213, acc = 0.9560 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 01:17:39.979038: step 31130, loss = 0.1413, acc = 0.9480 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 01:17:49.066968: step 31140, loss = 0.1466, acc = 0.9480 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 01:17:58.231321: step 31150, loss = 0.1562, acc = 0.9480 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 01:18:07.427175: step 31160, loss = 0.1091, acc = 0.9680 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 01:18:16.423393: step 31170, loss = 0.1512, acc = 0.9360 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 01:18:25.478055: step 31180, loss = 0.1421, acc = 0.9580 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 01:18:34.738280: step 31190, loss = 0.1431, acc = 0.9560 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 01:18:43.927783: step 31200, loss = 0.1217, acc = 0.9580 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 01:18:52.949198: step 31210, loss = 0.1600, acc = 0.9460 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 01:19:02.069219: step 31220, loss = 0.1494, acc = 0.9540 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 01:19:11.153591: step 31230, loss = 0.1168, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 01:19:20.282744: step 31240, loss = 0.1550, acc = 0.9540 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 01:19:30.381926: step 31250, loss = 0.1446, acc = 0.9580 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 01:19:39.328319: step 31260, loss = 0.1398, acc = 0.9560 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 01:19:48.542932: step 31270, loss = 0.1438, acc = 0.9520 (262.9 examples/sec; 0.243 sec/batch)
2017-05-03 01:19:57.702499: step 31280, loss = 0.1560, acc = 0.9480 (266.6 examples/sec; 0.240 sec/batch)
2017-05-03 01:20:06.954061: step 31290, loss = 0.1392, acc = 0.9600 (249.0 examples/sec; 0.257 sec/batch)
2017-05-03 01:20:16.218682: step 31300, loss = 0.1179, acc = 0.9680 (260.0 examples/sec; 0.246 sec/batch)
2017-05-03 01:20:25.460262: step 31310, loss = 0.1265, acc = 0.9580 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 01:20:34.880175: step 31320, loss = 0.1365, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 01:20:44.030921: step 31330, loss = 0.1381, acc = 0.9640 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 01:20:53.112868: step 31340, loss = 0.1415, acc = 0.9520 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 01:21:02.239132: step 31350, loss = 0.1509, acc = 0.9540 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 01:21:11.383114: step 31360, loss = 0.1221, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 01:21:20.427794: step 31370, loss = 0.1367, acc = 0.9480 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 01:21:29.418314: step 31380, loss = 0.1093, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 01:21:38.417511: step 31390, loss = 0.1262, acc = 0.9620 (260.1 examples/sec; 0.246 sec/batch)
2017-05-03 01:21:47.452177: step 31400, loss = 0.1437, acc = 0.9460 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 01:21:56.405518: step 31410, loss = 0.1307, acc = 0.9480 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 01:22:05.465645: step 31420, loss = 0.1464, acc = 0.9540 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 01:22:14.717209: step 31430, loss = 0.1791, acc = 0.9440 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 01:22:23.659490: step 31440, loss = 0.1543, acc = 0.9500 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 01:22:32.740101: step 31450, loss = 0.1407, acc = 0.9460 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 01:22:41.761077: step 31460, loss = 0.1315, acc = 0.9540 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 01:22:50.943642: step 31470, loss = 0.1755, acc = 0.9500 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 01:22:59.986740: step 31480, loss = 0.1538, acc = 0.9460 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 01:23:08.967734: step 31490, loss = 0.1323, acc = 0.9620 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 01:23:18.173956: step 31500, loss = 0.1482, acc = 0.9580 (260.2 examples/sec; 0.246 sec/batch)
2017-05-03 01:23:27.319955: step 31510, loss = 0.1402, acc = 0.9480 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 01:23:36.646984: step 31520, loss = 0.1583, acc = 0.9460 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 01:23:45.633078: step 31530, loss = 0.1503, acc = 0.9360 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 01:23:54.716263: step 31540, loss = 0.1398, acc = 0.9560 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 01:24:03.823829: step 31550, loss = 0.1695, acc = 0.9460 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 01:24:13.090040: step 31560, loss = 0.1228, acc = 0.9720 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 01:24:22.373736: step 31570, loss = 0.1335, acc = 0.9520 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 01:24:31.451154: step 31580, loss = 0.1396, acc = 0.9480 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 01:24:40.520595: step 31590, loss = 0.1138, acc = 0.9680 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 01:24:49.691838: step 31600, loss = 0.1226, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 01:24:58.752302: step 31610, loss = 0.1491, acc = 0.9460 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 01:25:07.745585: step 31620, loss = 0.1536, acc = 0.9500 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 01:25:16.861727: step 31630, loss = 0.1481, acc = 0.9400 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 01:25:26.036373: step 31640, loss = 0.1382, acc = 0.9480 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 01:25:35.188905: step 31650, loss = 0.1605, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 01:25:44.316256: step 31660, loss = 0.1417, acc = 0.9600 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 01:25:53.418751: step 31670, loss = 0.1518, acc = 0.9420 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 01:26:02.363688: step 31680, loss = 0.1507, acc = 0.9540 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 01:26:11.593186: step 31690, loss = 0.1208, acc = 0.9560 (226.1 examples/sec; 0.283 sec/batch)
2017-05-03 01:26:20.723397: step 31700, loss = 0.1343, acc = 0.9460 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 01:26:29.682739: step 31710, loss = 0.1416, acc = 0.9560 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 01:26:38.652476: step 31720, loss = 0.1449, acc = 0.9620 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 01:26:47.728474: step 31730, loss = 0.1427, acc = 0.9500 (262.0 examples/sec; 0.244 sec/batch)
2017-05-03 01:26:57.359854: step 31740, loss = 0.1424, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:27:06.299902: step 31750, loss = 0.1485, acc = 0.9500 (299.1 examples/sec; 0.214 sec/batch)
2017-05-03 01:27:15.445144: step 31760, loss = 0.1489, acc = 0.9580 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 01:27:24.536481: step 31770, loss = 0.1322, acc = 0.9500 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 01:27:33.486972: step 31780, loss = 0.1272, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 01:27:42.724951: step 31790, loss = 0.1612, acc = 0.9520 (277.7 examples/sec; 0.231 sec/batch)
2017-05-03 01:27:51.634299: step 31800, loss = 0.1484, acc = 0.9540 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 01:28:00.630936: step 31810, loss = 0.1395, acc = 0.9560 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 01:28:09.761107: step 31820, loss = 0.1198, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 01:28:19.032907: step 31830, loss = 0.1371, acc = 0.9600 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 01:28:28.159626: step 31840, loss = 0.1480, acc = 0.9480 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 01:28:37.276757: step 31850, loss = 0.1459, acc = 0.9560 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 01:28:46.361148: step 31860, loss = 0.1711, acc = 0.9580 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 01:28:55.412787: step 31870, loss = 0.1405, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 01:29:04.422124: step 31880, loss = 0.1502, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 01:29:13.368913: step 31890, loss = 0.1467, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 01:29:22.427552: step 31900, loss = 0.1063, acc = 0.9700 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 01:29:31.441303: step 31910, loss = 0.1470, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 01:29:40.499969: step 31920, loss = 0.1228, acc = 0.9580 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 01:29:49.351343: step 31930, loss = 0.1473, acc = 0.9500 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 01:29:58.391936: step 31940, loss = 0.1495, acc = 0.9540 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 01:30:07.375849: step 31950, loss = 0.1324, acc = 0.9680 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 01:30:16.539575: step 31960, loss = 0.1389, acc = 0.9480 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 01:30:25.656290: step 31970, loss = 0.1441, acc = 0.9520 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 01:30:34.705402: step 31980, loss = 0.1018, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 01:30:43.751585: step 31990, loss = 0.1597, acc = 0.9500 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 01:30:52.799748: step 32000, loss = 0.1570, acc = 0.9500 (275.5 examples/sec; 0.232 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 01:30:53.301312: step 32000, loss = 0.1258, acc = 0.9570, f1neg = 0.9524, f1pos = 0.9608, f1 = 0.9566
[Eval_batch(1)(2000,4000)] 2017-05-03 01:30:53.774588: step 32000, loss = 0.1219, acc = 0.9610, f1neg = 0.9538, f1pos = 0.9663, f1 = 0.9600
[Eval_batch(2)(2000,6000)] 2017-05-03 01:30:54.269504: step 32000, loss = 0.1378, acc = 0.9540, f1neg = 0.9504, f1pos = 0.9571, f1 = 0.9538
[Eval_batch(3)(2000,8000)] 2017-05-03 01:30:54.762692: step 32000, loss = 0.1470, acc = 0.9505, f1neg = 0.9500, f1pos = 0.9510, f1 = 0.9505
[Eval_batch(4)(2000,10000)] 2017-05-03 01:30:55.260666: step 32000, loss = 0.1482, acc = 0.9500, f1neg = 0.9506, f1pos = 0.9493, f1 = 0.9500
[Eval_batch(5)(2000,12000)] 2017-05-03 01:30:55.778180: step 32000, loss = 0.1405, acc = 0.9505, f1neg = 0.9460, f1pos = 0.9543, f1 = 0.9502
[Eval_batch(6)(2000,14000)] 2017-05-03 01:30:56.296197: step 32000, loss = 0.1343, acc = 0.9580, f1neg = 0.9567, f1pos = 0.9592, f1 = 0.9580
[Eval_batch(7)(2000,16000)] 2017-05-03 01:30:56.808018: step 32000, loss = 0.1289, acc = 0.9570, f1neg = 0.9478, f1pos = 0.9634, f1 = 0.9556
[Eval_batch(8)(2000,18000)] 2017-05-03 01:30:57.318277: step 32000, loss = 0.1249, acc = 0.9570, f1neg = 0.9502, f1pos = 0.9622, f1 = 0.9562
[Eval_batch(9)(2000,20000)] 2017-05-03 01:30:57.828407: step 32000, loss = 0.1276, acc = 0.9600, f1neg = 0.9567, f1pos = 0.9628, f1 = 0.9598
[Eval_batch(10)(2000,22000)] 2017-05-03 01:30:58.329079: step 32000, loss = 0.1383, acc = 0.9490, f1neg = 0.9445, f1pos = 0.9528, f1 = 0.9487
[Eval_batch(11)(2000,24000)] 2017-05-03 01:30:58.835682: step 32000, loss = 0.1353, acc = 0.9550, f1neg = 0.9472, f1pos = 0.9608, f1 = 0.9540
[Eval_batch(12)(2000,26000)] 2017-05-03 01:30:59.335028: step 32000, loss = 0.1345, acc = 0.9520, f1neg = 0.9499, f1pos = 0.9539, f1 = 0.9519
[Eval_batch(13)(2000,28000)] 2017-05-03 01:30:59.848356: step 32000, loss = 0.1218, acc = 0.9600, f1neg = 0.9493, f1pos = 0.9670, f1 = 0.9581
[Eval_batch(14)(2000,30000)] 2017-05-03 01:31:00.356099: step 32000, loss = 0.1498, acc = 0.9500, f1neg = 0.9391, f1pos = 0.9576, f1 = 0.9483
[Eval_batch(15)(2000,32000)] 2017-05-03 01:31:00.858592: step 32000, loss = 0.1262, acc = 0.9650, f1neg = 0.9618, f1pos = 0.9677, f1 = 0.9648
[Eval_batch(16)(2000,34000)] 2017-05-03 01:31:01.365784: step 32000, loss = 0.1231, acc = 0.9605, f1neg = 0.9585, f1pos = 0.9623, f1 = 0.9604
[Eval_batch(17)(2000,36000)] 2017-05-03 01:31:01.866167: step 32000, loss = 0.1469, acc = 0.9520, f1neg = 0.9518, f1pos = 0.9522, f1 = 0.9520
[Eval_batch(18)(2000,38000)] 2017-05-03 01:31:02.355382: step 32000, loss = 0.1475, acc = 0.9500, f1neg = 0.9513, f1pos = 0.9486, f1 = 0.9500
[Eval_batch(19)(2000,40000)] 2017-05-03 01:31:02.860594: step 32000, loss = 0.1257, acc = 0.9635, f1neg = 0.9574, f1pos = 0.9681, f1 = 0.9627
[Eval_batch(20)(2000,42000)] 2017-05-03 01:31:03.371031: step 32000, loss = 0.1325, acc = 0.9595, f1neg = 0.9638, f1pos = 0.9541, f1 = 0.9589
[Eval_batch(21)(2000,44000)] 2017-05-03 01:31:03.876363: step 32000, loss = 0.1153, acc = 0.9620, f1neg = 0.9581, f1pos = 0.9652, f1 = 0.9617
[Eval_batch(22)(2000,46000)] 2017-05-03 01:31:04.362073: step 32000, loss = 0.1281, acc = 0.9565, f1neg = 0.9563, f1pos = 0.9567, f1 = 0.9565
[Eval_batch(23)(2000,48000)] 2017-05-03 01:31:04.850024: step 32000, loss = 0.1325, acc = 0.9530, f1neg = 0.9463, f1pos = 0.9582, f1 = 0.9523
[Eval_batch(24)(2000,50000)] 2017-05-03 01:31:05.354077: step 32000, loss = 0.1190, acc = 0.9610, f1neg = 0.9545, f1pos = 0.9659, f1 = 0.9602
[Eval_batch(25)(2000,52000)] 2017-05-03 01:31:05.850978: step 32000, loss = 0.1107, acc = 0.9655, f1neg = 0.9616, f1pos = 0.9687, f1 = 0.9651
[Eval_batch(26)(2000,54000)] 2017-05-03 01:31:06.310806: step 32000, loss = 0.1275, acc = 0.9565, f1neg = 0.9585, f1pos = 0.9543, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-03 01:31:06.933345: step 32000, loss = 0.1135, acc = 0.9645, f1neg = 0.9623, f1pos = 0.9664, f1 = 0.9644
[Eval] 2017-05-03 01:31:06.933415: step 32000, acc = 0.9568, f1 = 0.9563
[Test_batch(0)(2000,2000)] 2017-05-03 01:31:07.418442: step 32000, loss = 0.1626, acc = 0.9440, f1neg = 0.9461, f1pos = 0.9417, f1 = 0.9439
[Test_batch(1)(2000,4000)] 2017-05-03 01:31:07.857331: step 32000, loss = 0.1454, acc = 0.9490, f1neg = 0.9540, f1pos = 0.9428, f1 = 0.9484
[Test_batch(2)(2000,6000)] 2017-05-03 01:31:08.361610: step 32000, loss = 0.1625, acc = 0.9470, f1neg = 0.9507, f1pos = 0.9428, f1 = 0.9467
[Test_batch(3)(2000,8000)] 2017-05-03 01:31:08.845327: step 32000, loss = 0.1677, acc = 0.9385, f1neg = 0.9415, f1pos = 0.9352, f1 = 0.9383
[Test_batch(4)(2000,10000)] 2017-05-03 01:31:09.312334: step 32000, loss = 0.1645, acc = 0.9390, f1neg = 0.9382, f1pos = 0.9398, f1 = 0.9390
[Test_batch(5)(2000,12000)] 2017-05-03 01:31:09.790076: step 32000, loss = 0.1763, acc = 0.9315, f1neg = 0.9309, f1pos = 0.9321, f1 = 0.9315
[Test_batch(6)(2000,14000)] 2017-05-03 01:31:10.268395: step 32000, loss = 0.1638, acc = 0.9375, f1neg = 0.9343, f1pos = 0.9404, f1 = 0.9374
[Test_batch(7)(2000,16000)] 2017-05-03 01:31:10.743905: step 32000, loss = 0.1524, acc = 0.9465, f1neg = 0.9506, f1pos = 0.9416, f1 = 0.9461
[Test_batch(8)(2000,18000)] 2017-05-03 01:31:11.211343: step 32000, loss = 0.1507, acc = 0.9450, f1neg = 0.9439, f1pos = 0.9461, f1 = 0.9450
[Test_batch(9)(2000,20000)] 2017-05-03 01:31:11.681404: step 32000, loss = 0.1795, acc = 0.9315, f1neg = 0.9265, f1pos = 0.9359, f1 = 0.9312
[Test_batch(10)(2000,22000)] 2017-05-03 01:31:12.197473: step 32000, loss = 0.1536, acc = 0.9440, f1neg = 0.9361, f1pos = 0.9502, f1 = 0.9431
[Test_batch(11)(2000,24000)] 2017-05-03 01:31:12.708049: step 32000, loss = 0.1599, acc = 0.9460, f1neg = 0.9467, f1pos = 0.9453, f1 = 0.9460
[Test_batch(12)(2000,26000)] 2017-05-03 01:31:13.197803: step 32000, loss = 0.1336, acc = 0.9555, f1neg = 0.9567, f1pos = 0.9542, f1 = 0.9555
[Test_batch(13)(2000,28000)] 2017-05-03 01:31:13.676285: step 32000, loss = 0.1607, acc = 0.9420, f1neg = 0.9343, f1pos = 0.9481, f1 = 0.9412
[Test_batch(14)(2000,30000)] 2017-05-03 01:31:14.189133: step 32000, loss = 0.1310, acc = 0.9580, f1neg = 0.9511, f1pos = 0.9632, f1 = 0.9571
[Test_batch(15)(2000,32000)] 2017-05-03 01:31:14.700041: step 32000, loss = 0.1559, acc = 0.9485, f1neg = 0.9472, f1pos = 0.9497, f1 = 0.9485
[Test_batch(16)(2000,34000)] 2017-05-03 01:31:15.161602: step 32000, loss = 0.1400, acc = 0.9515, f1neg = 0.9495, f1pos = 0.9534, f1 = 0.9514
[Test_batch(17)(2000,36000)] 2017-05-03 01:31:15.641397: step 32000, loss = 0.1328, acc = 0.9625, f1neg = 0.9586, f1pos = 0.9657, f1 = 0.9622
[Test_batch(18)(2000,38000)] 2017-05-03 01:31:16.196069: step 32000, loss = 0.1261, acc = 0.9565, f1neg = 0.9551, f1pos = 0.9578, f1 = 0.9565
[Test] 2017-05-03 01:31:16.196177: step 32000, acc = 0.9460, f1 = 0.9457
[Status] 2017-05-03 01:31:16.196211: step 32000, maxindex = 30000, maxdev = 0.9595, maxtst = 0.9510
2017-05-03 01:31:25.335822: step 32010, loss = 0.1465, acc = 0.9480 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 01:31:34.534490: step 32020, loss = 0.1283, acc = 0.9660 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 01:31:43.652042: step 32030, loss = 0.1340, acc = 0.9580 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 01:31:52.718297: step 32040, loss = 0.1586, acc = 0.9460 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 01:32:01.640435: step 32050, loss = 0.1578, acc = 0.9460 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 01:32:10.622812: step 32060, loss = 0.1442, acc = 0.9640 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 01:32:19.661995: step 32070, loss = 0.1543, acc = 0.9520 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 01:32:28.955430: step 32080, loss = 0.1332, acc = 0.9580 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 01:32:38.337686: step 32090, loss = 0.1295, acc = 0.9620 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 01:32:47.377009: step 32100, loss = 0.1286, acc = 0.9580 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 01:32:56.338285: step 32110, loss = 0.1327, acc = 0.9640 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 01:33:05.544353: step 32120, loss = 0.1292, acc = 0.9640 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 01:33:14.549890: step 32130, loss = 0.1473, acc = 0.9620 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 01:33:23.728660: step 32140, loss = 0.1458, acc = 0.9520 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 01:33:32.834955: step 32150, loss = 0.1659, acc = 0.9480 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 01:33:41.978268: step 32160, loss = 0.1408, acc = 0.9540 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 01:33:51.061048: step 32170, loss = 0.1506, acc = 0.9420 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 01:34:00.120700: step 32180, loss = 0.1507, acc = 0.9460 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 01:34:09.077195: step 32190, loss = 0.1385, acc = 0.9640 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 01:34:18.276568: step 32200, loss = 0.1157, acc = 0.9660 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 01:34:27.176941: step 32210, loss = 0.1260, acc = 0.9600 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 01:34:36.384689: step 32220, loss = 0.1256, acc = 0.9560 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 01:34:45.563530: step 32230, loss = 0.1185, acc = 0.9660 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 01:34:54.564597: step 32240, loss = 0.1854, acc = 0.9380 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 01:35:03.667202: step 32250, loss = 0.1555, acc = 0.9560 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 01:35:13.580212: step 32260, loss = 0.1291, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 01:35:22.650464: step 32270, loss = 0.1557, acc = 0.9400 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 01:35:31.707214: step 32280, loss = 0.1390, acc = 0.9500 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 01:35:40.923950: step 32290, loss = 0.1299, acc = 0.9580 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 01:35:50.142172: step 32300, loss = 0.1261, acc = 0.9660 (262.2 examples/sec; 0.244 sec/batch)
2017-05-03 01:35:59.261399: step 32310, loss = 0.1423, acc = 0.9520 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:36:08.358351: step 32320, loss = 0.1361, acc = 0.9460 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 01:36:17.546968: step 32330, loss = 0.1746, acc = 0.9420 (254.9 examples/sec; 0.251 sec/batch)
2017-05-03 01:36:26.669655: step 32340, loss = 0.1379, acc = 0.9560 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 01:36:35.721790: step 32350, loss = 0.1404, acc = 0.9580 (266.4 examples/sec; 0.240 sec/batch)
2017-05-03 01:36:44.972959: step 32360, loss = 0.1264, acc = 0.9580 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 01:36:54.317092: step 32370, loss = 0.1456, acc = 0.9620 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 01:37:03.460996: step 32380, loss = 0.1368, acc = 0.9560 (264.2 examples/sec; 0.242 sec/batch)
2017-05-03 01:37:12.645154: step 32390, loss = 0.1459, acc = 0.9580 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 01:37:22.000641: step 32400, loss = 0.1472, acc = 0.9480 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 01:37:31.118811: step 32410, loss = 0.1611, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 01:37:40.319691: step 32420, loss = 0.1316, acc = 0.9520 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 01:37:49.868047: step 32430, loss = 0.1196, acc = 0.9660 (255.4 examples/sec; 0.251 sec/batch)
2017-05-03 01:37:59.666160: step 32440, loss = 0.1216, acc = 0.9680 (229.5 examples/sec; 0.279 sec/batch)
2017-05-03 01:38:08.728396: step 32450, loss = 0.1469, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 01:38:18.040287: step 32460, loss = 0.1484, acc = 0.9480 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 01:38:27.134244: step 32470, loss = 0.1479, acc = 0.9520 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 01:38:36.345197: step 32480, loss = 0.1508, acc = 0.9460 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 01:38:45.535782: step 32490, loss = 0.1262, acc = 0.9640 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 01:38:54.628024: step 32500, loss = 0.1245, acc = 0.9640 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 01:39:03.568793: step 32510, loss = 0.1442, acc = 0.9520 (297.6 examples/sec; 0.215 sec/batch)
2017-05-03 01:39:12.699848: step 32520, loss = 0.1509, acc = 0.9520 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 01:39:21.784695: step 32530, loss = 0.1457, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 01:39:31.043185: step 32540, loss = 0.1202, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 01:39:40.169102: step 32550, loss = 0.1724, acc = 0.9380 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 01:39:49.211847: step 32560, loss = 0.1583, acc = 0.9380 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 01:39:58.362046: step 32570, loss = 0.1619, acc = 0.9520 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 01:40:07.444067: step 32580, loss = 0.1434, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 01:40:16.608148: step 32590, loss = 0.1353, acc = 0.9560 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 01:40:25.608055: step 32600, loss = 0.1325, acc = 0.9640 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 01:40:34.639898: step 32610, loss = 0.1446, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 01:40:43.732742: step 32620, loss = 0.1360, acc = 0.9600 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 01:40:52.777681: step 32630, loss = 0.1408, acc = 0.9520 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 01:41:01.854643: step 32640, loss = 0.1613, acc = 0.9320 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 01:41:10.923863: step 32650, loss = 0.1307, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 01:41:19.951324: step 32660, loss = 0.1334, acc = 0.9520 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 01:41:29.025993: step 32670, loss = 0.1505, acc = 0.9520 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 01:41:38.282024: step 32680, loss = 0.1148, acc = 0.9680 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 01:41:47.388233: step 32690, loss = 0.1543, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 01:41:56.571747: step 32700, loss = 0.1392, acc = 0.9540 (247.2 examples/sec; 0.259 sec/batch)
2017-05-03 01:42:05.518525: step 32710, loss = 0.1306, acc = 0.9700 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 01:42:14.653707: step 32720, loss = 0.1682, acc = 0.9360 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 01:42:23.949093: step 32730, loss = 0.1289, acc = 0.9560 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 01:42:33.105138: step 32740, loss = 0.1775, acc = 0.9460 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 01:42:42.428433: step 32750, loss = 0.1321, acc = 0.9620 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 01:42:51.340256: step 32760, loss = 0.1431, acc = 0.9500 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 01:43:00.318051: step 32770, loss = 0.1226, acc = 0.9680 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 01:43:09.410400: step 32780, loss = 0.1535, acc = 0.9540 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 01:43:18.589752: step 32790, loss = 0.1462, acc = 0.9540 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 01:43:27.729645: step 32800, loss = 0.1557, acc = 0.9480 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 01:43:36.708514: step 32810, loss = 0.1192, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 01:43:46.704694: step 32820, loss = 0.1670, acc = 0.9380 (202.9 examples/sec; 0.315 sec/batch)
2017-05-03 01:43:55.851414: step 32830, loss = 0.1559, acc = 0.9460 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 01:44:04.958661: step 32840, loss = 0.1479, acc = 0.9480 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 01:44:14.108601: step 32850, loss = 0.1241, acc = 0.9680 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:44:23.135781: step 32860, loss = 0.1378, acc = 0.9560 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 01:44:32.139189: step 32870, loss = 0.1269, acc = 0.9500 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 01:44:41.274326: step 32880, loss = 0.1749, acc = 0.9400 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 01:44:50.334372: step 32890, loss = 0.1917, acc = 0.9380 (258.6 examples/sec; 0.247 sec/batch)
2017-05-03 01:44:59.415013: step 32900, loss = 0.1455, acc = 0.9460 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 01:45:08.648110: step 32910, loss = 0.1376, acc = 0.9580 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 01:45:17.722862: step 32920, loss = 0.1586, acc = 0.9420 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 01:45:26.884085: step 32930, loss = 0.1436, acc = 0.9480 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 01:45:35.954972: step 32940, loss = 0.1168, acc = 0.9560 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:45:45.236397: step 32950, loss = 0.1438, acc = 0.9440 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 01:45:54.420483: step 32960, loss = 0.1076, acc = 0.9660 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 01:46:03.520251: step 32970, loss = 0.1510, acc = 0.9420 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 01:46:12.712165: step 32980, loss = 0.1438, acc = 0.9540 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 01:46:21.767056: step 32990, loss = 0.1487, acc = 0.9660 (294.9 examples/sec; 0.217 sec/batch)
2017-05-03 01:46:30.914720: step 33000, loss = 0.1721, acc = 0.9500 (284.2 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 01:46:31.396965: step 33000, loss = 0.1162, acc = 0.9565, f1neg = 0.9525, f1pos = 0.9599, f1 = 0.9562
[Eval_batch(1)(2000,4000)] 2017-05-03 01:46:31.869381: step 33000, loss = 0.1218, acc = 0.9645, f1neg = 0.9589, f1pos = 0.9688, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 01:46:32.335779: step 33000, loss = 0.1303, acc = 0.9570, f1neg = 0.9546, f1pos = 0.9591, f1 = 0.9569
[Eval_batch(3)(2000,8000)] 2017-05-03 01:46:32.836195: step 33000, loss = 0.1349, acc = 0.9580, f1neg = 0.9583, f1pos = 0.9577, f1 = 0.9580
[Eval_batch(4)(2000,10000)] 2017-05-03 01:46:33.313119: step 33000, loss = 0.1370, acc = 0.9570, f1neg = 0.9584, f1pos = 0.9555, f1 = 0.9570
[Eval_batch(5)(2000,12000)] 2017-05-03 01:46:33.819487: step 33000, loss = 0.1335, acc = 0.9530, f1neg = 0.9500, f1pos = 0.9557, f1 = 0.9528
[Eval_batch(6)(2000,14000)] 2017-05-03 01:46:34.301450: step 33000, loss = 0.1227, acc = 0.9640, f1neg = 0.9635, f1pos = 0.9645, f1 = 0.9640
[Eval_batch(7)(2000,16000)] 2017-05-03 01:46:34.780849: step 33000, loss = 0.1241, acc = 0.9580, f1neg = 0.9502, f1pos = 0.9637, f1 = 0.9569
[Eval_batch(8)(2000,18000)] 2017-05-03 01:46:35.250974: step 33000, loss = 0.1187, acc = 0.9610, f1neg = 0.9558, f1pos = 0.9651, f1 = 0.9605
[Eval_batch(9)(2000,20000)] 2017-05-03 01:46:35.718793: step 33000, loss = 0.1230, acc = 0.9625, f1neg = 0.9601, f1pos = 0.9646, f1 = 0.9624
[Eval_batch(10)(2000,22000)] 2017-05-03 01:46:36.198034: step 33000, loss = 0.1317, acc = 0.9585, f1neg = 0.9559, f1pos = 0.9608, f1 = 0.9584
[Eval_batch(11)(2000,24000)] 2017-05-03 01:46:36.662308: step 33000, loss = 0.1323, acc = 0.9590, f1neg = 0.9529, f1pos = 0.9637, f1 = 0.9583
[Eval_batch(12)(2000,26000)] 2017-05-03 01:46:37.138492: step 33000, loss = 0.1288, acc = 0.9520, f1neg = 0.9509, f1pos = 0.9530, f1 = 0.9520
[Eval_batch(13)(2000,28000)] 2017-05-03 01:46:37.635821: step 33000, loss = 0.1162, acc = 0.9635, f1neg = 0.9545, f1pos = 0.9695, f1 = 0.9620
[Eval_batch(14)(2000,30000)] 2017-05-03 01:46:38.102856: step 33000, loss = 0.1435, acc = 0.9545, f1neg = 0.9461, f1pos = 0.9607, f1 = 0.9534
[Eval_batch(15)(2000,32000)] 2017-05-03 01:46:38.606531: step 33000, loss = 0.1228, acc = 0.9630, f1neg = 0.9603, f1pos = 0.9654, f1 = 0.9628
[Eval_batch(16)(2000,34000)] 2017-05-03 01:46:39.081357: step 33000, loss = 0.1189, acc = 0.9630, f1neg = 0.9616, f1pos = 0.9643, f1 = 0.9630
[Eval_batch(17)(2000,36000)] 2017-05-03 01:46:39.590438: step 33000, loss = 0.1416, acc = 0.9540, f1neg = 0.9543, f1pos = 0.9537, f1 = 0.9540
[Eval_batch(18)(2000,38000)] 2017-05-03 01:46:40.096321: step 33000, loss = 0.1369, acc = 0.9535, f1neg = 0.9556, f1pos = 0.9512, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 01:46:40.603316: step 33000, loss = 0.1193, acc = 0.9665, f1neg = 0.9615, f1pos = 0.9704, f1 = 0.9659
[Eval_batch(20)(2000,42000)] 2017-05-03 01:46:41.072300: step 33000, loss = 0.1199, acc = 0.9620, f1neg = 0.9664, f1pos = 0.9563, f1 = 0.9613
[Eval_batch(21)(2000,44000)] 2017-05-03 01:46:41.582672: step 33000, loss = 0.1172, acc = 0.9595, f1neg = 0.9561, f1pos = 0.9624, f1 = 0.9593
[Eval_batch(22)(2000,46000)] 2017-05-03 01:46:42.050185: step 33000, loss = 0.1222, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9589, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 01:46:42.518850: step 33000, loss = 0.1288, acc = 0.9580, f1neg = 0.9529, f1pos = 0.9621, f1 = 0.9575
[Eval_batch(24)(2000,50000)] 2017-05-03 01:46:42.998412: step 33000, loss = 0.1143, acc = 0.9625, f1neg = 0.9572, f1pos = 0.9666, f1 = 0.9619
[Eval_batch(25)(2000,52000)] 2017-05-03 01:46:43.504728: step 33000, loss = 0.1049, acc = 0.9710, f1neg = 0.9684, f1pos = 0.9732, f1 = 0.9708
[Eval_batch(26)(2000,54000)] 2017-05-03 01:46:44.012583: step 33000, loss = 0.1173, acc = 0.9610, f1neg = 0.9634, f1pos = 0.9582, f1 = 0.9608
[Eval_batch(27)(2000,56000)] 2017-05-03 01:46:44.589721: step 33000, loss = 0.1046, acc = 0.9685, f1neg = 0.9671, f1pos = 0.9698, f1 = 0.9684
[Eval] 2017-05-03 01:46:44.589813: step 33000, acc = 0.9600, f1 = 0.9597
[Test_batch(0)(2000,2000)] 2017-05-03 01:46:45.099051: step 33000, loss = 0.1535, acc = 0.9520, f1neg = 0.9547, f1pos = 0.9490, f1 = 0.9518
[Test_batch(1)(2000,4000)] 2017-05-03 01:46:45.602190: step 33000, loss = 0.1360, acc = 0.9565, f1neg = 0.9616, f1pos = 0.9499, f1 = 0.9557
[Test_batch(2)(2000,6000)] 2017-05-03 01:46:46.099084: step 33000, loss = 0.1500, acc = 0.9545, f1neg = 0.9584, f1pos = 0.9498, f1 = 0.9541
[Test_batch(3)(2000,8000)] 2017-05-03 01:46:46.581849: step 33000, loss = 0.1582, acc = 0.9470, f1neg = 0.9508, f1pos = 0.9426, f1 = 0.9467
[Test_batch(4)(2000,10000)] 2017-05-03 01:46:47.089864: step 33000, loss = 0.1584, acc = 0.9465, f1neg = 0.9470, f1pos = 0.9460, f1 = 0.9465
[Test_batch(5)(2000,12000)] 2017-05-03 01:46:47.574997: step 33000, loss = 0.1684, acc = 0.9390, f1neg = 0.9401, f1pos = 0.9379, f1 = 0.9390
[Test_batch(6)(2000,14000)] 2017-05-03 01:46:48.052398: step 33000, loss = 0.1540, acc = 0.9460, f1neg = 0.9444, f1pos = 0.9475, f1 = 0.9460
[Test_batch(7)(2000,16000)] 2017-05-03 01:46:48.556925: step 33000, loss = 0.1440, acc = 0.9495, f1neg = 0.9544, f1pos = 0.9434, f1 = 0.9489
[Test_batch(8)(2000,18000)] 2017-05-03 01:46:49.014017: step 33000, loss = 0.1456, acc = 0.9475, f1neg = 0.9472, f1pos = 0.9478, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 01:46:49.494528: step 33000, loss = 0.1763, acc = 0.9375, f1neg = 0.9348, f1pos = 0.9400, f1 = 0.9374
[Test_batch(10)(2000,22000)] 2017-05-03 01:46:49.969885: step 33000, loss = 0.1539, acc = 0.9500, f1neg = 0.9442, f1pos = 0.9547, f1 = 0.9495
[Test_batch(11)(2000,24000)] 2017-05-03 01:46:50.440868: step 33000, loss = 0.1516, acc = 0.9500, f1neg = 0.9516, f1pos = 0.9482, f1 = 0.9499
[Test_batch(12)(2000,26000)] 2017-05-03 01:46:50.910150: step 33000, loss = 0.1307, acc = 0.9600, f1neg = 0.9617, f1pos = 0.9581, f1 = 0.9599
[Test_batch(13)(2000,28000)] 2017-05-03 01:46:51.386841: step 33000, loss = 0.1570, acc = 0.9475, f1neg = 0.9424, f1pos = 0.9518, f1 = 0.9471
[Test_batch(14)(2000,30000)] 2017-05-03 01:46:51.870553: step 33000, loss = 0.1305, acc = 0.9580, f1neg = 0.9523, f1pos = 0.9625, f1 = 0.9574
[Test_batch(15)(2000,32000)] 2017-05-03 01:46:52.344758: step 33000, loss = 0.1517, acc = 0.9525, f1neg = 0.9523, f1pos = 0.9527, f1 = 0.9525
[Test_batch(16)(2000,34000)] 2017-05-03 01:46:52.819277: step 33000, loss = 0.1342, acc = 0.9570, f1neg = 0.9563, f1pos = 0.9577, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 01:46:53.300077: step 33000, loss = 0.1252, acc = 0.9615, f1neg = 0.9581, f1pos = 0.9644, f1 = 0.9612
[Test_batch(18)(2000,38000)] 2017-05-03 01:46:53.854259: step 33000, loss = 0.1161, acc = 0.9640, f1neg = 0.9635, f1pos = 0.9645, f1 = 0.9640
[Test] 2017-05-03 01:46:53.854356: step 33000, acc = 0.9514, f1 = 0.9512
[Status] 2017-05-03 01:46:53.854382: step 33000, maxindex = 33000, maxdev = 0.9600, maxtst = 0.9514
2017-05-03 01:47:06.287875: step 33010, loss = 0.1259, acc = 0.9600 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 01:47:15.401986: step 33020, loss = 0.1387, acc = 0.9500 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 01:47:25.123631: step 33030, loss = 0.1038, acc = 0.9760 (186.0 examples/sec; 0.344 sec/batch)
2017-05-03 01:47:34.193722: step 33040, loss = 0.1734, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 01:47:43.368441: step 33050, loss = 0.1492, acc = 0.9520 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:47:52.368412: step 33060, loss = 0.1406, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 01:48:01.498658: step 33070, loss = 0.1632, acc = 0.9480 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 01:48:10.651035: step 33080, loss = 0.1676, acc = 0.9520 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 01:48:19.683458: step 33090, loss = 0.1602, acc = 0.9360 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 01:48:28.859421: step 33100, loss = 0.1509, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 01:48:38.040229: step 33110, loss = 0.1245, acc = 0.9620 (266.7 examples/sec; 0.240 sec/batch)
2017-05-03 01:48:47.112160: step 33120, loss = 0.1330, acc = 0.9540 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 01:48:56.180639: step 33130, loss = 0.1436, acc = 0.9460 (266.7 examples/sec; 0.240 sec/batch)
2017-05-03 01:49:05.256869: step 33140, loss = 0.1120, acc = 0.9700 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 01:49:14.303385: step 33150, loss = 0.1303, acc = 0.9540 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 01:49:23.532418: step 33160, loss = 0.1230, acc = 0.9620 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 01:49:32.630117: step 33170, loss = 0.1196, acc = 0.9640 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 01:49:41.838274: step 33180, loss = 0.1552, acc = 0.9360 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 01:49:50.954049: step 33190, loss = 0.1601, acc = 0.9460 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 01:50:00.454528: step 33200, loss = 0.1511, acc = 0.9380 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 01:50:09.585428: step 33210, loss = 0.1221, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 01:50:18.682228: step 33220, loss = 0.1375, acc = 0.9580 (298.0 examples/sec; 0.215 sec/batch)
2017-05-03 01:50:27.857423: step 33230, loss = 0.1176, acc = 0.9680 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:50:37.141921: step 33240, loss = 0.1520, acc = 0.9500 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 01:50:46.221465: step 33250, loss = 0.1474, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:50:55.362430: step 33260, loss = 0.1445, acc = 0.9440 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 01:51:05.595865: step 33270, loss = 0.1251, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 01:51:14.825674: step 33280, loss = 0.1351, acc = 0.9540 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 01:51:23.856725: step 33290, loss = 0.1452, acc = 0.9500 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 01:51:32.977186: step 33300, loss = 0.1531, acc = 0.9440 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 01:51:42.145191: step 33310, loss = 0.1181, acc = 0.9660 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 01:51:51.161861: step 33320, loss = 0.1556, acc = 0.9460 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 01:52:00.318952: step 33330, loss = 0.1548, acc = 0.9520 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 01:52:09.411010: step 33340, loss = 0.1628, acc = 0.9380 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 01:52:18.397288: step 33350, loss = 0.1136, acc = 0.9640 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 01:52:27.332467: step 33360, loss = 0.1182, acc = 0.9680 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 01:52:37.302332: step 33370, loss = 0.1281, acc = 0.9640 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 01:52:46.305307: step 33380, loss = 0.1230, acc = 0.9640 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 01:52:55.593668: step 33390, loss = 0.1500, acc = 0.9500 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 01:53:04.901129: step 33400, loss = 0.1498, acc = 0.9540 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 01:53:14.350680: step 33410, loss = 0.1262, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:53:23.474462: step 33420, loss = 0.1268, acc = 0.9640 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 01:53:32.640365: step 33430, loss = 0.1575, acc = 0.9520 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 01:53:41.739656: step 33440, loss = 0.1449, acc = 0.9520 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 01:53:50.742986: step 33450, loss = 0.1729, acc = 0.9400 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 01:53:59.790996: step 33460, loss = 0.1293, acc = 0.9620 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 01:54:08.709677: step 33470, loss = 0.1426, acc = 0.9520 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 01:54:17.844732: step 33480, loss = 0.1485, acc = 0.9500 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 01:54:26.979935: step 33490, loss = 0.1284, acc = 0.9540 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 01:54:36.115597: step 33500, loss = 0.1317, acc = 0.9620 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 01:54:45.177624: step 33510, loss = 0.1320, acc = 0.9600 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 01:54:54.470721: step 33520, loss = 0.1126, acc = 0.9720 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 01:55:03.701464: step 33530, loss = 0.1415, acc = 0.9520 (261.8 examples/sec; 0.244 sec/batch)
2017-05-03 01:55:12.843745: step 33540, loss = 0.1479, acc = 0.9540 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 01:55:21.908444: step 33550, loss = 0.1258, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 01:55:31.061352: step 33560, loss = 0.1529, acc = 0.9560 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 01:55:40.412965: step 33570, loss = 0.1225, acc = 0.9600 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 01:55:49.513324: step 33580, loss = 0.1355, acc = 0.9600 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 01:55:58.516255: step 33590, loss = 0.1202, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 01:56:07.519616: step 33600, loss = 0.1309, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 01:56:16.907408: step 33610, loss = 0.1451, acc = 0.9520 (241.4 examples/sec; 0.265 sec/batch)
2017-05-03 01:56:26.040670: step 33620, loss = 0.1101, acc = 0.9700 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 01:56:35.258934: step 33630, loss = 0.1347, acc = 0.9620 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 01:56:44.450583: step 33640, loss = 0.1340, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 01:56:54.005137: step 33650, loss = 0.1517, acc = 0.9440 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 01:57:03.116531: step 33660, loss = 0.1423, acc = 0.9460 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 01:57:12.306422: step 33670, loss = 0.1401, acc = 0.9440 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 01:57:21.418440: step 33680, loss = 0.1255, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 01:57:30.432554: step 33690, loss = 0.1410, acc = 0.9420 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 01:57:39.600548: step 33700, loss = 0.1268, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 01:57:48.666563: step 33710, loss = 0.1080, acc = 0.9640 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 01:57:57.986730: step 33720, loss = 0.1255, acc = 0.9700 (238.3 examples/sec; 0.269 sec/batch)
2017-05-03 01:58:07.018653: step 33730, loss = 0.1389, acc = 0.9480 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 01:58:15.925965: step 33740, loss = 0.1724, acc = 0.9380 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 01:58:24.990090: step 33750, loss = 0.1717, acc = 0.9460 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 01:58:34.119591: step 33760, loss = 0.1185, acc = 0.9580 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 01:58:43.373392: step 33770, loss = 0.1211, acc = 0.9700 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 01:58:52.521480: step 33780, loss = 0.1622, acc = 0.9360 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 01:59:01.511760: step 33790, loss = 0.1074, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 01:59:10.472686: step 33800, loss = 0.1411, acc = 0.9440 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 01:59:19.539319: step 33810, loss = 0.1560, acc = 0.9440 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 01:59:28.638437: step 33820, loss = 0.1583, acc = 0.9460 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 01:59:37.661622: step 33830, loss = 0.1108, acc = 0.9760 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 01:59:46.722121: step 33840, loss = 0.1293, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 01:59:55.872620: step 33850, loss = 0.1600, acc = 0.9500 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 02:00:04.924173: step 33860, loss = 0.1417, acc = 0.9560 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 02:00:13.936960: step 33870, loss = 0.1510, acc = 0.9560 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 02:00:23.056801: step 33880, loss = 0.1210, acc = 0.9720 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 02:00:32.071797: step 33890, loss = 0.1733, acc = 0.9380 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 02:00:41.235454: step 33900, loss = 0.1027, acc = 0.9720 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 02:00:50.322734: step 33910, loss = 0.1436, acc = 0.9560 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 02:00:59.620541: step 33920, loss = 0.1185, acc = 0.9700 (269.9 examples/sec; 0.237 sec/batch)
2017-05-03 02:01:08.799523: step 33930, loss = 0.1288, acc = 0.9640 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 02:01:17.916069: step 33940, loss = 0.1484, acc = 0.9440 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 02:01:27.084182: step 33950, loss = 0.1503, acc = 0.9400 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 02:01:36.123359: step 33960, loss = 0.1371, acc = 0.9480 (265.7 examples/sec; 0.241 sec/batch)
2017-05-03 02:01:45.260424: step 33970, loss = 0.1205, acc = 0.9620 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 02:01:54.369051: step 33980, loss = 0.1209, acc = 0.9740 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 02:02:03.495682: step 33990, loss = 0.1327, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 02:02:12.668715: step 34000, loss = 0.1355, acc = 0.9480 (273.6 examples/sec; 0.234 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 02:02:13.178807: step 34000, loss = 0.1207, acc = 0.9565, f1neg = 0.9521, f1pos = 0.9601, f1 = 0.9561
[Eval_batch(1)(2000,4000)] 2017-05-03 02:02:13.652219: step 34000, loss = 0.1191, acc = 0.9625, f1neg = 0.9561, f1pos = 0.9673, f1 = 0.9617
[Eval_batch(2)(2000,6000)] 2017-05-03 02:02:14.124872: step 34000, loss = 0.1315, acc = 0.9605, f1neg = 0.9578, f1pos = 0.9629, f1 = 0.9603
[Eval_batch(3)(2000,8000)] 2017-05-03 02:02:14.631640: step 34000, loss = 0.1390, acc = 0.9550, f1neg = 0.9548, f1pos = 0.9552, f1 = 0.9550
[Eval_batch(4)(2000,10000)] 2017-05-03 02:02:15.133262: step 34000, loss = 0.1401, acc = 0.9540, f1neg = 0.9549, f1pos = 0.9530, f1 = 0.9540
[Eval_batch(5)(2000,12000)] 2017-05-03 02:02:15.629688: step 34000, loss = 0.1360, acc = 0.9495, f1neg = 0.9454, f1pos = 0.9530, f1 = 0.9492
[Eval_batch(6)(2000,14000)] 2017-05-03 02:02:16.091292: step 34000, loss = 0.1280, acc = 0.9600, f1neg = 0.9592, f1pos = 0.9608, f1 = 0.9600
[Eval_batch(7)(2000,16000)] 2017-05-03 02:02:16.574426: step 34000, loss = 0.1235, acc = 0.9600, f1neg = 0.9519, f1pos = 0.9658, f1 = 0.9588
[Eval_batch(8)(2000,18000)] 2017-05-03 02:02:17.048589: step 34000, loss = 0.1207, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Eval_batch(9)(2000,20000)] 2017-05-03 02:02:17.550455: step 34000, loss = 0.1241, acc = 0.9620, f1neg = 0.9591, f1pos = 0.9645, f1 = 0.9618
[Eval_batch(10)(2000,22000)] 2017-05-03 02:02:18.029965: step 34000, loss = 0.1335, acc = 0.9545, f1neg = 0.9510, f1pos = 0.9575, f1 = 0.9543
[Eval_batch(11)(2000,24000)] 2017-05-03 02:02:18.502373: step 34000, loss = 0.1321, acc = 0.9570, f1neg = 0.9502, f1pos = 0.9622, f1 = 0.9562
[Eval_batch(12)(2000,26000)] 2017-05-03 02:02:18.970800: step 34000, loss = 0.1309, acc = 0.9530, f1neg = 0.9515, f1pos = 0.9544, f1 = 0.9530
[Eval_batch(13)(2000,28000)] 2017-05-03 02:02:19.448728: step 34000, loss = 0.1186, acc = 0.9625, f1neg = 0.9528, f1pos = 0.9689, f1 = 0.9608
[Eval_batch(14)(2000,30000)] 2017-05-03 02:02:19.918180: step 34000, loss = 0.1449, acc = 0.9520, f1neg = 0.9421, f1pos = 0.9590, f1 = 0.9506
[Eval_batch(15)(2000,32000)] 2017-05-03 02:02:20.360160: step 34000, loss = 0.1230, acc = 0.9665, f1neg = 0.9637, f1pos = 0.9689, f1 = 0.9663
[Eval_batch(16)(2000,34000)] 2017-05-03 02:02:20.839184: step 34000, loss = 0.1195, acc = 0.9625, f1neg = 0.9608, f1pos = 0.9640, f1 = 0.9624
[Eval_batch(17)(2000,36000)] 2017-05-03 02:02:21.344745: step 34000, loss = 0.1429, acc = 0.9540, f1neg = 0.9540, f1pos = 0.9540, f1 = 0.9540
[Eval_batch(18)(2000,38000)] 2017-05-03 02:02:21.853625: step 34000, loss = 0.1413, acc = 0.9540, f1neg = 0.9557, f1pos = 0.9522, f1 = 0.9539
[Eval_batch(19)(2000,40000)] 2017-05-03 02:02:22.335516: step 34000, loss = 0.1212, acc = 0.9680, f1neg = 0.9630, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 02:02:22.839649: step 34000, loss = 0.1249, acc = 0.9595, f1neg = 0.9640, f1pos = 0.9538, f1 = 0.9589
[Eval_batch(21)(2000,44000)] 2017-05-03 02:02:23.339477: step 34000, loss = 0.1134, acc = 0.9635, f1neg = 0.9601, f1pos = 0.9663, f1 = 0.9632
[Eval_batch(22)(2000,46000)] 2017-05-03 02:02:23.842277: step 34000, loss = 0.1232, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 02:02:24.313625: step 34000, loss = 0.1279, acc = 0.9580, f1neg = 0.9524, f1pos = 0.9624, f1 = 0.9574
[Eval_batch(24)(2000,50000)] 2017-05-03 02:02:24.826760: step 34000, loss = 0.1157, acc = 0.9635, f1neg = 0.9579, f1pos = 0.9678, f1 = 0.9628
[Eval_batch(25)(2000,52000)] 2017-05-03 02:02:25.307772: step 34000, loss = 0.1062, acc = 0.9685, f1neg = 0.9653, f1pos = 0.9712, f1 = 0.9682
[Eval_batch(26)(2000,54000)] 2017-05-03 02:02:25.784642: step 34000, loss = 0.1212, acc = 0.9565, f1neg = 0.9587, f1pos = 0.9540, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-03 02:02:26.337061: step 34000, loss = 0.1087, acc = 0.9665, f1neg = 0.9646, f1pos = 0.9682, f1 = 0.9664
[Eval] 2017-05-03 02:02:26.337153: step 34000, acc = 0.9593, f1 = 0.9589
[Test_batch(0)(2000,2000)] 2017-05-03 02:02:26.802255: step 34000, loss = 0.1566, acc = 0.9510, f1neg = 0.9533, f1pos = 0.9484, f1 = 0.9509
[Test_batch(1)(2000,4000)] 2017-05-03 02:02:27.310694: step 34000, loss = 0.1386, acc = 0.9535, f1neg = 0.9585, f1pos = 0.9471, f1 = 0.9528
[Test_batch(2)(2000,6000)] 2017-05-03 02:02:27.815310: step 34000, loss = 0.1548, acc = 0.9505, f1neg = 0.9543, f1pos = 0.9460, f1 = 0.9502
[Test_batch(3)(2000,8000)] 2017-05-03 02:02:28.321665: step 34000, loss = 0.1616, acc = 0.9425, f1neg = 0.9459, f1pos = 0.9386, f1 = 0.9423
[Test_batch(4)(2000,10000)] 2017-05-03 02:02:28.825065: step 34000, loss = 0.1591, acc = 0.9440, f1neg = 0.9438, f1pos = 0.9442, f1 = 0.9440
[Test_batch(5)(2000,12000)] 2017-05-03 02:02:29.328199: step 34000, loss = 0.1700, acc = 0.9355, f1neg = 0.9357, f1pos = 0.9353, f1 = 0.9355
[Test_batch(6)(2000,14000)] 2017-05-03 02:02:29.832622: step 34000, loss = 0.1557, acc = 0.9435, f1neg = 0.9412, f1pos = 0.9456, f1 = 0.9434
[Test_batch(7)(2000,16000)] 2017-05-03 02:02:30.332543: step 34000, loss = 0.1458, acc = 0.9530, f1neg = 0.9572, f1pos = 0.9480, f1 = 0.9526
[Test_batch(8)(2000,18000)] 2017-05-03 02:02:30.843807: step 34000, loss = 0.1456, acc = 0.9460, f1neg = 0.9453, f1pos = 0.9467, f1 = 0.9460
[Test_batch(9)(2000,20000)] 2017-05-03 02:02:31.349961: step 34000, loss = 0.1749, acc = 0.9360, f1neg = 0.9324, f1pos = 0.9392, f1 = 0.9358
[Test_batch(10)(2000,22000)] 2017-05-03 02:02:31.851684: step 34000, loss = 0.1507, acc = 0.9495, f1neg = 0.9429, f1pos = 0.9547, f1 = 0.9488
[Test_batch(11)(2000,24000)] 2017-05-03 02:02:32.350003: step 34000, loss = 0.1543, acc = 0.9465, f1neg = 0.9477, f1pos = 0.9453, f1 = 0.9465
[Test_batch(12)(2000,26000)] 2017-05-03 02:02:32.848206: step 34000, loss = 0.1295, acc = 0.9590, f1neg = 0.9605, f1pos = 0.9574, f1 = 0.9589
[Test_batch(13)(2000,28000)] 2017-05-03 02:02:33.353920: step 34000, loss = 0.1565, acc = 0.9425, f1neg = 0.9356, f1pos = 0.9480, f1 = 0.9418
[Test_batch(14)(2000,30000)] 2017-05-03 02:02:33.864274: step 34000, loss = 0.1285, acc = 0.9585, f1neg = 0.9522, f1pos = 0.9634, f1 = 0.9578
[Test_batch(15)(2000,32000)] 2017-05-03 02:02:34.364196: step 34000, loss = 0.1511, acc = 0.9510, f1neg = 0.9501, f1pos = 0.9519, f1 = 0.9510
[Test_batch(16)(2000,34000)] 2017-05-03 02:02:34.863477: step 34000, loss = 0.1349, acc = 0.9570, f1neg = 0.9556, f1pos = 0.9583, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 02:02:35.341465: step 34000, loss = 0.1273, acc = 0.9660, f1neg = 0.9627, f1pos = 0.9688, f1 = 0.9657
[Test_batch(18)(2000,38000)] 2017-05-03 02:02:35.880281: step 34000, loss = 0.1205, acc = 0.9590, f1neg = 0.9580, f1pos = 0.9599, f1 = 0.9590
[Test] 2017-05-03 02:02:35.880373: step 34000, acc = 0.9497, f1 = 0.9495
[Status] 2017-05-03 02:02:35.880394: step 34000, maxindex = 33000, maxdev = 0.9600, maxtst = 0.9514
2017-05-03 02:02:45.007363: step 34010, loss = 0.1445, acc = 0.9540 (270.5 examples/sec; 0.237 sec/batch)
2017-05-03 02:02:54.145602: step 34020, loss = 0.1467, acc = 0.9480 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 02:03:03.217478: step 34030, loss = 0.1576, acc = 0.9460 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 02:03:12.249438: step 34040, loss = 0.1365, acc = 0.9520 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 02:03:21.329379: step 34050, loss = 0.1598, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 02:03:30.450735: step 34060, loss = 0.1396, acc = 0.9500 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 02:03:39.354625: step 34070, loss = 0.1555, acc = 0.9460 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 02:03:48.547467: step 34080, loss = 0.1221, acc = 0.9660 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 02:03:57.642046: step 34090, loss = 0.1372, acc = 0.9540 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 02:04:06.793400: step 34100, loss = 0.1499, acc = 0.9560 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 02:04:16.013715: step 34110, loss = 0.1520, acc = 0.9400 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 02:04:24.999679: step 34120, loss = 0.1208, acc = 0.9560 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 02:04:34.148848: step 34130, loss = 0.1192, acc = 0.9740 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 02:04:43.281839: step 34140, loss = 0.1495, acc = 0.9420 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 02:04:52.423810: step 34150, loss = 0.1610, acc = 0.9580 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 02:05:01.515428: step 34160, loss = 0.1477, acc = 0.9620 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 02:05:10.601132: step 34170, loss = 0.1427, acc = 0.9620 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 02:05:19.709490: step 34180, loss = 0.1648, acc = 0.9420 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 02:05:28.745946: step 34190, loss = 0.1333, acc = 0.9580 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 02:05:37.976973: step 34200, loss = 0.1248, acc = 0.9640 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 02:05:47.132704: step 34210, loss = 0.1468, acc = 0.9620 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 02:05:56.178437: step 34220, loss = 0.1480, acc = 0.9520 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 02:06:05.221531: step 34230, loss = 0.1552, acc = 0.9520 (263.3 examples/sec; 0.243 sec/batch)
2017-05-03 02:06:14.269584: step 34240, loss = 0.1563, acc = 0.9480 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 02:06:23.324036: step 34250, loss = 0.1309, acc = 0.9600 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 02:06:32.349275: step 34260, loss = 0.1354, acc = 0.9560 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 02:06:41.381839: step 34270, loss = 0.1895, acc = 0.9240 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 02:06:51.078055: step 34280, loss = 0.1438, acc = 0.9500 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 02:07:00.044960: step 34290, loss = 0.1669, acc = 0.9460 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 02:07:09.066401: step 34300, loss = 0.1207, acc = 0.9660 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 02:07:18.054446: step 34310, loss = 0.1456, acc = 0.9540 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 02:07:27.153197: step 34320, loss = 0.1558, acc = 0.9400 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 02:07:36.118854: step 34330, loss = 0.1427, acc = 0.9540 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 02:07:45.261298: step 34340, loss = 0.1140, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 02:07:54.235273: step 34350, loss = 0.1398, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 02:08:03.263670: step 34360, loss = 0.1355, acc = 0.9660 (264.2 examples/sec; 0.242 sec/batch)
2017-05-03 02:08:12.364554: step 34370, loss = 0.1309, acc = 0.9620 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 02:08:21.486774: step 34380, loss = 0.1714, acc = 0.9360 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 02:08:30.537300: step 34390, loss = 0.1268, acc = 0.9580 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 02:08:39.545936: step 34400, loss = 0.1519, acc = 0.9440 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 02:08:48.478264: step 34410, loss = 0.1354, acc = 0.9560 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 02:08:57.574050: step 34420, loss = 0.1504, acc = 0.9460 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 02:09:06.743557: step 34430, loss = 0.1515, acc = 0.9500 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 02:09:15.939367: step 34440, loss = 0.1332, acc = 0.9520 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 02:09:24.984713: step 34450, loss = 0.1381, acc = 0.9600 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 02:09:34.045622: step 34460, loss = 0.1424, acc = 0.9540 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 02:09:43.017780: step 34470, loss = 0.1242, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 02:09:52.084778: step 34480, loss = 0.1446, acc = 0.9580 (297.0 examples/sec; 0.216 sec/batch)
2017-05-03 02:10:01.224739: step 34490, loss = 0.1370, acc = 0.9640 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 02:10:10.408794: step 34500, loss = 0.1117, acc = 0.9740 (261.2 examples/sec; 0.245 sec/batch)
2017-05-03 02:10:19.414507: step 34510, loss = 0.1526, acc = 0.9480 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 02:10:28.388939: step 34520, loss = 0.1469, acc = 0.9560 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 02:10:37.516596: step 34530, loss = 0.1439, acc = 0.9460 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 02:10:46.559025: step 34540, loss = 0.1729, acc = 0.9420 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 02:10:55.701976: step 34550, loss = 0.1478, acc = 0.9560 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 02:11:04.767521: step 34560, loss = 0.1314, acc = 0.9520 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 02:11:13.888185: step 34570, loss = 0.1433, acc = 0.9520 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 02:11:22.986678: step 34580, loss = 0.1264, acc = 0.9640 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 02:11:32.081394: step 34590, loss = 0.1193, acc = 0.9700 (265.8 examples/sec; 0.241 sec/batch)
2017-05-03 02:11:41.536147: step 34600, loss = 0.1635, acc = 0.9440 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 02:11:50.609078: step 34610, loss = 0.1142, acc = 0.9720 (257.6 examples/sec; 0.248 sec/batch)
2017-05-03 02:11:59.647941: step 34620, loss = 0.1249, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 02:12:08.919602: step 34630, loss = 0.1509, acc = 0.9540 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 02:12:17.917520: step 34640, loss = 0.1465, acc = 0.9620 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 02:12:26.966922: step 34650, loss = 0.1803, acc = 0.9380 (266.8 examples/sec; 0.240 sec/batch)
2017-05-03 02:12:36.489044: step 34660, loss = 0.1320, acc = 0.9620 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 02:12:45.381463: step 34670, loss = 0.1104, acc = 0.9700 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 02:12:54.557305: step 34680, loss = 0.1630, acc = 0.9420 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 02:13:03.797012: step 34690, loss = 0.1532, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 02:13:12.869674: step 34700, loss = 0.1469, acc = 0.9560 (260.1 examples/sec; 0.246 sec/batch)
2017-05-03 02:13:21.850932: step 34710, loss = 0.1190, acc = 0.9580 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 02:13:30.976457: step 34720, loss = 0.1347, acc = 0.9560 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 02:13:40.281087: step 34730, loss = 0.1507, acc = 0.9560 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 02:13:49.335157: step 34740, loss = 0.1538, acc = 0.9500 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 02:13:58.589885: step 34750, loss = 0.1262, acc = 0.9640 (269.5 examples/sec; 0.237 sec/batch)
2017-05-03 02:14:07.711322: step 34760, loss = 0.1307, acc = 0.9600 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 02:14:16.741403: step 34770, loss = 0.1404, acc = 0.9500 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 02:14:25.776594: step 34780, loss = 0.1049, acc = 0.9660 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 02:14:34.747991: step 34790, loss = 0.1420, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 02:14:43.736860: step 34800, loss = 0.1225, acc = 0.9600 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 02:14:52.686265: step 34810, loss = 0.1396, acc = 0.9580 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 02:15:01.666469: step 34820, loss = 0.1159, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 02:15:10.931807: step 34830, loss = 0.1567, acc = 0.9540 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 02:15:20.130457: step 34840, loss = 0.1475, acc = 0.9440 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 02:15:29.172535: step 34850, loss = 0.1451, acc = 0.9500 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 02:15:38.169355: step 34860, loss = 0.1302, acc = 0.9500 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 02:15:47.294529: step 34870, loss = 0.1471, acc = 0.9400 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 02:15:56.280763: step 34880, loss = 0.1987, acc = 0.9180 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 02:16:05.348619: step 34890, loss = 0.1504, acc = 0.9460 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 02:16:14.372671: step 34900, loss = 0.1339, acc = 0.9520 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 02:16:23.636180: step 34910, loss = 0.1269, acc = 0.9480 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 02:16:32.635549: step 34920, loss = 0.1487, acc = 0.9540 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 02:16:41.730997: step 34930, loss = 0.1326, acc = 0.9520 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 02:16:50.688382: step 34940, loss = 0.1328, acc = 0.9560 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 02:16:59.765311: step 34950, loss = 0.1387, acc = 0.9560 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 02:17:08.811062: step 34960, loss = 0.1241, acc = 0.9560 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 02:17:17.824121: step 34970, loss = 0.1157, acc = 0.9600 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 02:17:27.073237: step 34980, loss = 0.1789, acc = 0.9440 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 02:17:36.144836: step 34990, loss = 0.1236, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 02:17:45.271331: step 35000, loss = 0.1458, acc = 0.9420 (278.2 examples/sec; 0.230 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 02:17:45.773183: step 35000, loss = 0.1155, acc = 0.9570, f1neg = 0.9532, f1pos = 0.9603, f1 = 0.9567
[Eval_batch(1)(2000,4000)] 2017-05-03 02:17:46.276008: step 35000, loss = 0.1228, acc = 0.9635, f1neg = 0.9579, f1pos = 0.9678, f1 = 0.9628
[Eval_batch(2)(2000,6000)] 2017-05-03 02:17:46.786702: step 35000, loss = 0.1303, acc = 0.9575, f1neg = 0.9552, f1pos = 0.9595, f1 = 0.9574
[Eval_batch(3)(2000,8000)] 2017-05-03 02:17:47.294172: step 35000, loss = 0.1354, acc = 0.9555, f1neg = 0.9560, f1pos = 0.9550, f1 = 0.9555
[Eval_batch(4)(2000,10000)] 2017-05-03 02:17:47.775742: step 35000, loss = 0.1348, acc = 0.9550, f1neg = 0.9568, f1pos = 0.9530, f1 = 0.9549
[Eval_batch(5)(2000,12000)] 2017-05-03 02:17:48.277761: step 35000, loss = 0.1341, acc = 0.9535, f1neg = 0.9507, f1pos = 0.9560, f1 = 0.9534
[Eval_batch(6)(2000,14000)] 2017-05-03 02:17:48.781604: step 35000, loss = 0.1214, acc = 0.9635, f1neg = 0.9630, f1pos = 0.9640, f1 = 0.9635
[Eval_batch(7)(2000,16000)] 2017-05-03 02:17:49.286485: step 35000, loss = 0.1234, acc = 0.9600, f1neg = 0.9526, f1pos = 0.9654, f1 = 0.9590
[Eval_batch(8)(2000,18000)] 2017-05-03 02:17:49.798178: step 35000, loss = 0.1184, acc = 0.9635, f1neg = 0.9588, f1pos = 0.9672, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 02:17:50.279157: step 35000, loss = 0.1233, acc = 0.9610, f1neg = 0.9586, f1pos = 0.9631, f1 = 0.9609
[Eval_batch(10)(2000,22000)] 2017-05-03 02:17:50.779475: step 35000, loss = 0.1315, acc = 0.9600, f1neg = 0.9577, f1pos = 0.9621, f1 = 0.9599
[Eval_batch(11)(2000,24000)] 2017-05-03 02:17:51.274232: step 35000, loss = 0.1326, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Eval_batch(12)(2000,26000)] 2017-05-03 02:17:51.772262: step 35000, loss = 0.1285, acc = 0.9510, f1neg = 0.9500, f1pos = 0.9520, f1 = 0.9510
[Eval_batch(13)(2000,28000)] 2017-05-03 02:17:52.283538: step 35000, loss = 0.1172, acc = 0.9650, f1neg = 0.9565, f1pos = 0.9707, f1 = 0.9636
[Eval_batch(14)(2000,30000)] 2017-05-03 02:17:52.795383: step 35000, loss = 0.1432, acc = 0.9540, f1neg = 0.9456, f1pos = 0.9601, f1 = 0.9529
[Eval_batch(15)(2000,32000)] 2017-05-03 02:17:53.305121: step 35000, loss = 0.1223, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9667, f1 = 0.9644
[Eval_batch(16)(2000,34000)] 2017-05-03 02:17:53.814915: step 35000, loss = 0.1185, acc = 0.9650, f1neg = 0.9638, f1pos = 0.9661, f1 = 0.9650
[Eval_batch(17)(2000,36000)] 2017-05-03 02:17:54.295439: step 35000, loss = 0.1408, acc = 0.9570, f1neg = 0.9575, f1pos = 0.9565, f1 = 0.9570
[Eval_batch(18)(2000,38000)] 2017-05-03 02:17:54.804793: step 35000, loss = 0.1361, acc = 0.9535, f1neg = 0.9557, f1pos = 0.9510, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 02:17:55.303408: step 35000, loss = 0.1199, acc = 0.9665, f1neg = 0.9616, f1pos = 0.9703, f1 = 0.9659
[Eval_batch(20)(2000,42000)] 2017-05-03 02:17:55.809285: step 35000, loss = 0.1188, acc = 0.9620, f1neg = 0.9665, f1pos = 0.9562, f1 = 0.9613
[Eval_batch(21)(2000,44000)] 2017-05-03 02:17:56.310573: step 35000, loss = 0.1177, acc = 0.9600, f1neg = 0.9568, f1pos = 0.9628, f1 = 0.9598
[Eval_batch(22)(2000,46000)] 2017-05-03 02:17:56.810692: step 35000, loss = 0.1218, acc = 0.9610, f1neg = 0.9616, f1pos = 0.9604, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 02:17:57.312226: step 35000, loss = 0.1294, acc = 0.9580, f1neg = 0.9530, f1pos = 0.9620, f1 = 0.9575
[Eval_batch(24)(2000,50000)] 2017-05-03 02:17:57.810566: step 35000, loss = 0.1156, acc = 0.9640, f1neg = 0.9590, f1pos = 0.9679, f1 = 0.9635
[Eval_batch(25)(2000,52000)] 2017-05-03 02:17:58.315482: step 35000, loss = 0.1048, acc = 0.9705, f1neg = 0.9680, f1pos = 0.9727, f1 = 0.9703
[Eval_batch(26)(2000,54000)] 2017-05-03 02:17:58.816717: step 35000, loss = 0.1164, acc = 0.9600, f1neg = 0.9625, f1pos = 0.9571, f1 = 0.9598
[Eval_batch(27)(2000,56000)] 2017-05-03 02:17:59.470861: step 35000, loss = 0.1042, acc = 0.9680, f1neg = 0.9666, f1pos = 0.9693, f1 = 0.9679
[Eval] 2017-05-03 02:17:59.470948: step 35000, acc = 0.9604, f1 = 0.9601
[Test_batch(0)(2000,2000)] 2017-05-03 02:17:59.981981: step 35000, loss = 0.1529, acc = 0.9510, f1neg = 0.9539, f1pos = 0.9478, f1 = 0.9508
[Test_batch(1)(2000,4000)] 2017-05-03 02:18:00.460770: step 35000, loss = 0.1355, acc = 0.9570, f1neg = 0.9621, f1pos = 0.9503, f1 = 0.9562
[Test_batch(2)(2000,6000)] 2017-05-03 02:18:00.958381: step 35000, loss = 0.1500, acc = 0.9540, f1neg = 0.9580, f1pos = 0.9491, f1 = 0.9536
[Test_batch(3)(2000,8000)] 2017-05-03 02:18:01.463157: step 35000, loss = 0.1588, acc = 0.9430, f1neg = 0.9473, f1pos = 0.9380, f1 = 0.9426
[Test_batch(4)(2000,10000)] 2017-05-03 02:18:01.944806: step 35000, loss = 0.1574, acc = 0.9460, f1neg = 0.9467, f1pos = 0.9453, f1 = 0.9460
[Test_batch(5)(2000,12000)] 2017-05-03 02:18:02.418317: step 35000, loss = 0.1681, acc = 0.9415, f1neg = 0.9428, f1pos = 0.9401, f1 = 0.9415
[Test_batch(6)(2000,14000)] 2017-05-03 02:18:02.931099: step 35000, loss = 0.1518, acc = 0.9475, f1neg = 0.9461, f1pos = 0.9489, f1 = 0.9475
[Test_batch(7)(2000,16000)] 2017-05-03 02:18:03.402878: step 35000, loss = 0.1419, acc = 0.9535, f1neg = 0.9582, f1pos = 0.9477, f1 = 0.9529
[Test_batch(8)(2000,18000)] 2017-05-03 02:18:03.910310: step 35000, loss = 0.1450, acc = 0.9475, f1neg = 0.9473, f1pos = 0.9477, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 02:18:04.420143: step 35000, loss = 0.1767, acc = 0.9355, f1neg = 0.9328, f1pos = 0.9380, f1 = 0.9354
[Test_batch(10)(2000,22000)] 2017-05-03 02:18:04.889825: step 35000, loss = 0.1546, acc = 0.9510, f1neg = 0.9455, f1pos = 0.9555, f1 = 0.9505
[Test_batch(11)(2000,24000)] 2017-05-03 02:18:05.389717: step 35000, loss = 0.1509, acc = 0.9515, f1neg = 0.9533, f1pos = 0.9496, f1 = 0.9514
[Test_batch(12)(2000,26000)] 2017-05-03 02:18:05.869176: step 35000, loss = 0.1309, acc = 0.9590, f1neg = 0.9608, f1pos = 0.9570, f1 = 0.9589
[Test_batch(13)(2000,28000)] 2017-05-03 02:18:06.374814: step 35000, loss = 0.1565, acc = 0.9495, f1neg = 0.9448, f1pos = 0.9535, f1 = 0.9491
[Test_batch(14)(2000,30000)] 2017-05-03 02:18:06.883440: step 35000, loss = 0.1316, acc = 0.9565, f1neg = 0.9507, f1pos = 0.9611, f1 = 0.9559
[Test_batch(15)(2000,32000)] 2017-05-03 02:18:07.353401: step 35000, loss = 0.1523, acc = 0.9520, f1neg = 0.9519, f1pos = 0.9521, f1 = 0.9520
[Test_batch(16)(2000,34000)] 2017-05-03 02:18:07.865093: step 35000, loss = 0.1334, acc = 0.9575, f1neg = 0.9570, f1pos = 0.9580, f1 = 0.9575
[Test_batch(17)(2000,36000)] 2017-05-03 02:18:08.327208: step 35000, loss = 0.1249, acc = 0.9635, f1neg = 0.9604, f1pos = 0.9662, f1 = 0.9633
[Test_batch(18)(2000,38000)] 2017-05-03 02:18:08.899542: step 35000, loss = 0.1152, acc = 0.9685, f1neg = 0.9681, f1pos = 0.9689, f1 = 0.9685
[Test] 2017-05-03 02:18:08.899608: step 35000, acc = 0.9519, f1 = 0.9516
[Status] 2017-05-03 02:18:08.899625: step 35000, maxindex = 35000, maxdev = 0.9604, maxtst = 0.9519
2017-05-03 02:18:21.120384: step 35010, loss = 0.1420, acc = 0.9560 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 02:18:30.140112: step 35020, loss = 0.1461, acc = 0.9580 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 02:18:39.180312: step 35030, loss = 0.1340, acc = 0.9560 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 02:18:48.276857: step 35040, loss = 0.1074, acc = 0.9660 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 02:18:57.552980: step 35050, loss = 0.1581, acc = 0.9540 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 02:19:06.507955: step 35060, loss = 0.1393, acc = 0.9600 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 02:19:15.522471: step 35070, loss = 0.1227, acc = 0.9680 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 02:19:24.840458: step 35080, loss = 0.1234, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 02:19:33.925433: step 35090, loss = 0.1305, acc = 0.9620 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 02:19:43.039580: step 35100, loss = 0.1276, acc = 0.9620 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 02:19:52.153208: step 35110, loss = 0.1307, acc = 0.9600 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 02:20:01.283838: step 35120, loss = 0.1648, acc = 0.9400 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 02:20:10.399926: step 35130, loss = 0.1316, acc = 0.9580 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 02:20:19.499502: step 35140, loss = 0.1560, acc = 0.9380 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 02:20:28.599450: step 35150, loss = 0.1060, acc = 0.9660 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 02:20:37.679935: step 35160, loss = 0.1756, acc = 0.9360 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 02:20:46.802112: step 35170, loss = 0.1337, acc = 0.9580 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 02:20:55.885530: step 35180, loss = 0.1382, acc = 0.9580 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 02:21:04.901673: step 35190, loss = 0.1377, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 02:21:13.943852: step 35200, loss = 0.1398, acc = 0.9520 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 02:21:23.005063: step 35210, loss = 0.1595, acc = 0.9520 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 02:21:32.192936: step 35220, loss = 0.1176, acc = 0.9660 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 02:21:41.348713: step 35230, loss = 0.1472, acc = 0.9460 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 02:21:50.585827: step 35240, loss = 0.1294, acc = 0.9700 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 02:21:59.667621: step 35250, loss = 0.1316, acc = 0.9640 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 02:22:08.744928: step 35260, loss = 0.1437, acc = 0.9520 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 02:22:17.827403: step 35270, loss = 0.1227, acc = 0.9500 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 02:22:27.909331: step 35280, loss = 0.1425, acc = 0.9500 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 02:22:36.811506: step 35290, loss = 0.1351, acc = 0.9580 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 02:22:45.836648: step 35300, loss = 0.1594, acc = 0.9400 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 02:22:54.911852: step 35310, loss = 0.1250, acc = 0.9600 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 02:23:03.971070: step 35320, loss = 0.1324, acc = 0.9540 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 02:23:13.014373: step 35330, loss = 0.1470, acc = 0.9520 (268.5 examples/sec; 0.238 sec/batch)
2017-05-03 02:23:22.003595: step 35340, loss = 0.1050, acc = 0.9680 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 02:23:30.990820: step 35350, loss = 0.1354, acc = 0.9600 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 02:23:40.549305: step 35360, loss = 0.1233, acc = 0.9680 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 02:23:49.929858: step 35370, loss = 0.1327, acc = 0.9600 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 02:23:59.042100: step 35380, loss = 0.1289, acc = 0.9640 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 02:24:08.018936: step 35390, loss = 0.1567, acc = 0.9480 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 02:24:17.446954: step 35400, loss = 0.1225, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 02:24:26.597665: step 35410, loss = 0.1188, acc = 0.9720 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 02:24:35.708325: step 35420, loss = 0.1457, acc = 0.9540 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 02:24:44.910373: step 35430, loss = 0.1393, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 02:24:54.026095: step 35440, loss = 0.1296, acc = 0.9580 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 02:25:03.149447: step 35450, loss = 0.1376, acc = 0.9540 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 02:25:12.268099: step 35460, loss = 0.1387, acc = 0.9560 (259.5 examples/sec; 0.247 sec/batch)
2017-05-03 02:25:21.347615: step 35470, loss = 0.1351, acc = 0.9520 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 02:25:30.431543: step 35480, loss = 0.1452, acc = 0.9440 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 02:25:39.333546: step 35490, loss = 0.1622, acc = 0.9480 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 02:25:48.512559: step 35500, loss = 0.1143, acc = 0.9620 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 02:25:57.525440: step 35510, loss = 0.1312, acc = 0.9680 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 02:26:06.561645: step 35520, loss = 0.1485, acc = 0.9440 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 02:26:15.786655: step 35530, loss = 0.1556, acc = 0.9500 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 02:26:24.974100: step 35540, loss = 0.1108, acc = 0.9720 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 02:26:34.008874: step 35550, loss = 0.1499, acc = 0.9480 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 02:26:43.041096: step 35560, loss = 0.1205, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 02:26:52.022205: step 35570, loss = 0.1285, acc = 0.9540 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 02:27:01.117261: step 35580, loss = 0.1309, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 02:27:10.246715: step 35590, loss = 0.1569, acc = 0.9380 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 02:27:19.503221: step 35600, loss = 0.1419, acc = 0.9600 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 02:27:28.443525: step 35610, loss = 0.1433, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 02:27:37.588065: step 35620, loss = 0.1280, acc = 0.9560 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 02:27:47.062330: step 35630, loss = 0.1693, acc = 0.9400 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 02:27:56.165566: step 35640, loss = 0.1410, acc = 0.9660 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 02:28:05.315711: step 35650, loss = 0.1266, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 02:28:14.400451: step 35660, loss = 0.1336, acc = 0.9600 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 02:28:23.780115: step 35670, loss = 0.1250, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 02:28:32.816269: step 35680, loss = 0.1418, acc = 0.9540 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 02:28:42.060588: step 35690, loss = 0.1442, acc = 0.9500 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 02:28:51.258282: step 35700, loss = 0.1510, acc = 0.9540 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 02:29:00.435251: step 35710, loss = 0.1337, acc = 0.9520 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 02:29:09.433743: step 35720, loss = 0.1295, acc = 0.9580 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 02:29:18.474928: step 35730, loss = 0.1314, acc = 0.9600 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 02:29:27.540679: step 35740, loss = 0.1522, acc = 0.9480 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 02:29:36.579273: step 35750, loss = 0.1133, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 02:29:45.610942: step 35760, loss = 0.1351, acc = 0.9600 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 02:29:54.542462: step 35770, loss = 0.1530, acc = 0.9440 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 02:30:03.843495: step 35780, loss = 0.1569, acc = 0.9480 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 02:30:12.875329: step 35790, loss = 0.1123, acc = 0.9680 (269.9 examples/sec; 0.237 sec/batch)
2017-05-03 02:30:21.964596: step 35800, loss = 0.1173, acc = 0.9680 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 02:30:30.979087: step 35810, loss = 0.1688, acc = 0.9400 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 02:30:40.049974: step 35820, loss = 0.1296, acc = 0.9620 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 02:30:49.131787: step 35830, loss = 0.1371, acc = 0.9500 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 02:30:58.242008: step 35840, loss = 0.1422, acc = 0.9480 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 02:31:07.366522: step 35850, loss = 0.1330, acc = 0.9640 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 02:31:16.441072: step 35860, loss = 0.1414, acc = 0.9540 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 02:31:25.497984: step 35870, loss = 0.1302, acc = 0.9620 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 02:31:34.534531: step 35880, loss = 0.1225, acc = 0.9640 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 02:31:43.438315: step 35890, loss = 0.1314, acc = 0.9520 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 02:31:52.625759: step 35900, loss = 0.1263, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 02:32:01.583201: step 35910, loss = 0.1531, acc = 0.9540 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 02:32:10.718361: step 35920, loss = 0.1454, acc = 0.9500 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 02:32:19.951745: step 35930, loss = 0.1309, acc = 0.9600 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 02:32:29.229252: step 35940, loss = 0.1193, acc = 0.9700 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 02:32:38.550547: step 35950, loss = 0.1312, acc = 0.9600 (249.1 examples/sec; 0.257 sec/batch)
2017-05-03 02:32:47.655420: step 35960, loss = 0.1268, acc = 0.9580 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 02:32:56.642929: step 35970, loss = 0.1564, acc = 0.9480 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 02:33:05.911910: step 35980, loss = 0.1220, acc = 0.9660 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 02:33:15.110106: step 35990, loss = 0.1077, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 02:33:24.237801: step 36000, loss = 0.1182, acc = 0.9660 (272.5 examples/sec; 0.235 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 02:33:24.701873: step 36000, loss = 0.1203, acc = 0.9570, f1neg = 0.9526, f1pos = 0.9606, f1 = 0.9566
[Eval_batch(1)(2000,4000)] 2017-05-03 02:33:25.208066: step 36000, loss = 0.1186, acc = 0.9620, f1neg = 0.9553, f1pos = 0.9670, f1 = 0.9611
[Eval_batch(2)(2000,6000)] 2017-05-03 02:33:25.714759: step 36000, loss = 0.1320, acc = 0.9575, f1neg = 0.9545, f1pos = 0.9601, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-03 02:33:26.220366: step 36000, loss = 0.1388, acc = 0.9540, f1neg = 0.9538, f1pos = 0.9542, f1 = 0.9540
[Eval_batch(4)(2000,10000)] 2017-05-03 02:33:26.702416: step 36000, loss = 0.1401, acc = 0.9545, f1neg = 0.9554, f1pos = 0.9536, f1 = 0.9545
[Eval_batch(5)(2000,12000)] 2017-05-03 02:33:27.172227: step 36000, loss = 0.1352, acc = 0.9500, f1neg = 0.9458, f1pos = 0.9536, f1 = 0.9497
[Eval_batch(6)(2000,14000)] 2017-05-03 02:33:27.654874: step 36000, loss = 0.1277, acc = 0.9600, f1neg = 0.9590, f1pos = 0.9610, f1 = 0.9600
[Eval_batch(7)(2000,16000)] 2017-05-03 02:33:28.160624: step 36000, loss = 0.1238, acc = 0.9595, f1neg = 0.9512, f1pos = 0.9654, f1 = 0.9583
[Eval_batch(8)(2000,18000)] 2017-05-03 02:33:28.668709: step 36000, loss = 0.1204, acc = 0.9600, f1neg = 0.9540, f1pos = 0.9646, f1 = 0.9593
[Eval_batch(9)(2000,20000)] 2017-05-03 02:33:29.150411: step 36000, loss = 0.1237, acc = 0.9640, f1neg = 0.9612, f1pos = 0.9664, f1 = 0.9638
[Eval_batch(10)(2000,22000)] 2017-05-03 02:33:29.657056: step 36000, loss = 0.1330, acc = 0.9545, f1neg = 0.9509, f1pos = 0.9576, f1 = 0.9543
[Eval_batch(11)(2000,24000)] 2017-05-03 02:33:30.130514: step 36000, loss = 0.1315, acc = 0.9575, f1neg = 0.9506, f1pos = 0.9627, f1 = 0.9566
[Eval_batch(12)(2000,26000)] 2017-05-03 02:33:30.628059: step 36000, loss = 0.1299, acc = 0.9530, f1neg = 0.9513, f1pos = 0.9546, f1 = 0.9529
[Eval_batch(13)(2000,28000)] 2017-05-03 02:33:31.131187: step 36000, loss = 0.1177, acc = 0.9625, f1neg = 0.9527, f1pos = 0.9689, f1 = 0.9608
[Eval_batch(14)(2000,30000)] 2017-05-03 02:33:31.635677: step 36000, loss = 0.1451, acc = 0.9515, f1neg = 0.9413, f1pos = 0.9587, f1 = 0.9500
[Eval_batch(15)(2000,32000)] 2017-05-03 02:33:32.136707: step 36000, loss = 0.1219, acc = 0.9655, f1neg = 0.9625, f1pos = 0.9681, f1 = 0.9653
[Eval_batch(16)(2000,34000)] 2017-05-03 02:33:32.644578: step 36000, loss = 0.1180, acc = 0.9635, f1neg = 0.9618, f1pos = 0.9650, f1 = 0.9634
[Eval_batch(17)(2000,36000)] 2017-05-03 02:33:33.151967: step 36000, loss = 0.1424, acc = 0.9535, f1neg = 0.9534, f1pos = 0.9536, f1 = 0.9535
[Eval_batch(18)(2000,38000)] 2017-05-03 02:33:33.650747: step 36000, loss = 0.1416, acc = 0.9535, f1neg = 0.9551, f1pos = 0.9518, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 02:33:34.150378: step 36000, loss = 0.1201, acc = 0.9675, f1neg = 0.9622, f1pos = 0.9715, f1 = 0.9669
[Eval_batch(20)(2000,42000)] 2017-05-03 02:33:34.657157: step 36000, loss = 0.1251, acc = 0.9600, f1neg = 0.9643, f1pos = 0.9544, f1 = 0.9594
[Eval_batch(21)(2000,44000)] 2017-05-03 02:33:35.123411: step 36000, loss = 0.1130, acc = 0.9630, f1neg = 0.9595, f1pos = 0.9660, f1 = 0.9627
[Eval_batch(22)(2000,46000)] 2017-05-03 02:33:35.592942: step 36000, loss = 0.1225, acc = 0.9580, f1neg = 0.9581, f1pos = 0.9579, f1 = 0.9580
[Eval_batch(23)(2000,48000)] 2017-05-03 02:33:36.040765: step 36000, loss = 0.1272, acc = 0.9580, f1neg = 0.9523, f1pos = 0.9625, f1 = 0.9574
[Eval_batch(24)(2000,50000)] 2017-05-03 02:33:36.505405: step 36000, loss = 0.1146, acc = 0.9635, f1neg = 0.9578, f1pos = 0.9678, f1 = 0.9628
[Eval_batch(25)(2000,52000)] 2017-05-03 02:33:37.002216: step 36000, loss = 0.1065, acc = 0.9695, f1neg = 0.9663, f1pos = 0.9722, f1 = 0.9692
[Eval_batch(26)(2000,54000)] 2017-05-03 02:33:37.457116: step 36000, loss = 0.1219, acc = 0.9550, f1neg = 0.9572, f1pos = 0.9526, f1 = 0.9549
[Eval_batch(27)(2000,56000)] 2017-05-03 02:33:38.044910: step 36000, loss = 0.1081, acc = 0.9660, f1neg = 0.9641, f1pos = 0.9677, f1 = 0.9659
[Eval] 2017-05-03 02:33:38.045001: step 36000, acc = 0.9591, f1 = 0.9586
[Test_batch(0)(2000,2000)] 2017-05-03 02:33:38.537292: step 36000, loss = 0.1564, acc = 0.9490, f1neg = 0.9513, f1pos = 0.9465, f1 = 0.9489
[Test_batch(1)(2000,4000)] 2017-05-03 02:33:39.016752: step 36000, loss = 0.1381, acc = 0.9515, f1neg = 0.9565, f1pos = 0.9452, f1 = 0.9508
[Test_batch(2)(2000,6000)] 2017-05-03 02:33:39.488972: step 36000, loss = 0.1546, acc = 0.9505, f1neg = 0.9543, f1pos = 0.9460, f1 = 0.9502
[Test_batch(3)(2000,8000)] 2017-05-03 02:33:39.952787: step 36000, loss = 0.1612, acc = 0.9430, f1neg = 0.9462, f1pos = 0.9394, f1 = 0.9428
[Test_batch(4)(2000,10000)] 2017-05-03 02:33:40.434035: step 36000, loss = 0.1584, acc = 0.9440, f1neg = 0.9437, f1pos = 0.9443, f1 = 0.9440
[Test_batch(5)(2000,12000)] 2017-05-03 02:33:40.881465: step 36000, loss = 0.1696, acc = 0.9345, f1neg = 0.9345, f1pos = 0.9345, f1 = 0.9345
[Test_batch(6)(2000,14000)] 2017-05-03 02:33:41.363454: step 36000, loss = 0.1555, acc = 0.9440, f1neg = 0.9416, f1pos = 0.9462, f1 = 0.9439
[Test_batch(7)(2000,16000)] 2017-05-03 02:33:41.815073: step 36000, loss = 0.1453, acc = 0.9490, f1neg = 0.9533, f1pos = 0.9439, f1 = 0.9486
[Test_batch(8)(2000,18000)] 2017-05-03 02:33:42.282221: step 36000, loss = 0.1455, acc = 0.9465, f1neg = 0.9457, f1pos = 0.9473, f1 = 0.9465
[Test_batch(9)(2000,20000)] 2017-05-03 02:33:42.780268: step 36000, loss = 0.1740, acc = 0.9370, f1neg = 0.9330, f1pos = 0.9405, f1 = 0.9368
[Test_batch(10)(2000,22000)] 2017-05-03 02:33:43.264943: step 36000, loss = 0.1497, acc = 0.9475, f1neg = 0.9405, f1pos = 0.9530, f1 = 0.9468
[Test_batch(11)(2000,24000)] 2017-05-03 02:33:43.740693: step 36000, loss = 0.1542, acc = 0.9490, f1neg = 0.9500, f1pos = 0.9480, f1 = 0.9490
[Test_batch(12)(2000,26000)] 2017-05-03 02:33:44.245646: step 36000, loss = 0.1289, acc = 0.9595, f1neg = 0.9609, f1pos = 0.9580, f1 = 0.9594
[Test_batch(13)(2000,28000)] 2017-05-03 02:33:44.716035: step 36000, loss = 0.1548, acc = 0.9435, f1neg = 0.9366, f1pos = 0.9491, f1 = 0.9428
[Test_batch(14)(2000,30000)] 2017-05-03 02:33:45.211807: step 36000, loss = 0.1270, acc = 0.9590, f1neg = 0.9525, f1pos = 0.9639, f1 = 0.9582
[Test_batch(15)(2000,32000)] 2017-05-03 02:33:45.698492: step 36000, loss = 0.1517, acc = 0.9490, f1neg = 0.9480, f1pos = 0.9500, f1 = 0.9490
[Test_batch(16)(2000,34000)] 2017-05-03 02:33:46.194605: step 36000, loss = 0.1347, acc = 0.9575, f1neg = 0.9561, f1pos = 0.9588, f1 = 0.9575
[Test_batch(17)(2000,36000)] 2017-05-03 02:33:46.693125: step 36000, loss = 0.1273, acc = 0.9645, f1neg = 0.9609, f1pos = 0.9675, f1 = 0.9642
[Test_batch(18)(2000,38000)] 2017-05-03 02:33:47.259206: step 36000, loss = 0.1204, acc = 0.9590, f1neg = 0.9579, f1pos = 0.9600, f1 = 0.9590
[Test] 2017-05-03 02:33:47.259311: step 36000, acc = 0.9493, f1 = 0.9491
[Status] 2017-05-03 02:33:47.259337: step 36000, maxindex = 35000, maxdev = 0.9604, maxtst = 0.9519
2017-05-03 02:33:56.997875: step 36010, loss = 0.1296, acc = 0.9660 (179.4 examples/sec; 0.357 sec/batch)
2017-05-03 02:34:06.530724: step 36020, loss = 0.1241, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 02:34:15.705761: step 36030, loss = 0.1475, acc = 0.9480 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 02:34:24.873423: step 36040, loss = 0.1570, acc = 0.9560 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 02:34:34.151650: step 36050, loss = 0.1312, acc = 0.9520 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 02:34:43.412050: step 36060, loss = 0.1452, acc = 0.9440 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 02:34:52.678753: step 36070, loss = 0.1396, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 02:35:01.998858: step 36080, loss = 0.1212, acc = 0.9620 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 02:35:11.049784: step 36090, loss = 0.1411, acc = 0.9540 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 02:35:20.225770: step 36100, loss = 0.1267, acc = 0.9620 (270.4 examples/sec; 0.237 sec/batch)
2017-05-03 02:35:29.267224: step 36110, loss = 0.1533, acc = 0.9540 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 02:35:38.312018: step 36120, loss = 0.1215, acc = 0.9680 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 02:35:47.406042: step 36130, loss = 0.1385, acc = 0.9520 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 02:35:56.605606: step 36140, loss = 0.1531, acc = 0.9600 (256.0 examples/sec; 0.250 sec/batch)
2017-05-03 02:36:05.792504: step 36150, loss = 0.1756, acc = 0.9440 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 02:36:14.744625: step 36160, loss = 0.1332, acc = 0.9660 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 02:36:23.841499: step 36170, loss = 0.1337, acc = 0.9600 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 02:36:32.916207: step 36180, loss = 0.1551, acc = 0.9480 (297.3 examples/sec; 0.215 sec/batch)
2017-05-03 02:36:42.071571: step 36190, loss = 0.1140, acc = 0.9640 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 02:36:51.209084: step 36200, loss = 0.1262, acc = 0.9520 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 02:37:00.336974: step 36210, loss = 0.1861, acc = 0.9320 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 02:37:09.446807: step 36220, loss = 0.1206, acc = 0.9600 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 02:37:18.582493: step 36230, loss = 0.1345, acc = 0.9660 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 02:37:27.796473: step 36240, loss = 0.1302, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 02:37:36.967196: step 36250, loss = 0.1411, acc = 0.9560 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 02:37:46.144587: step 36260, loss = 0.1622, acc = 0.9440 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 02:37:55.399820: step 36270, loss = 0.1250, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 02:38:04.332132: step 36280, loss = 0.1379, acc = 0.9540 (295.4 examples/sec; 0.217 sec/batch)
2017-05-03 02:38:14.441695: step 36290, loss = 0.1158, acc = 0.9700 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 02:38:23.458568: step 36300, loss = 0.1318, acc = 0.9660 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 02:38:32.536154: step 36310, loss = 0.1084, acc = 0.9800 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 02:38:42.062823: step 36320, loss = 0.1218, acc = 0.9600 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 02:38:51.045010: step 36330, loss = 0.1404, acc = 0.9580 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 02:39:00.155339: step 36340, loss = 0.1358, acc = 0.9580 (264.9 examples/sec; 0.242 sec/batch)
2017-05-03 02:39:09.140785: step 36350, loss = 0.1259, acc = 0.9580 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 02:39:18.349881: step 36360, loss = 0.1517, acc = 0.9400 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 02:39:27.457538: step 36370, loss = 0.1225, acc = 0.9640 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 02:39:36.554116: step 36380, loss = 0.1549, acc = 0.9520 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 02:39:45.735816: step 36390, loss = 0.1319, acc = 0.9600 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 02:39:54.990403: step 36400, loss = 0.1333, acc = 0.9580 (256.7 examples/sec; 0.249 sec/batch)
2017-05-03 02:40:04.240702: step 36410, loss = 0.1417, acc = 0.9560 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 02:40:13.346636: step 36420, loss = 0.1708, acc = 0.9420 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 02:40:22.289803: step 36430, loss = 0.1479, acc = 0.9460 (302.2 examples/sec; 0.212 sec/batch)
2017-05-03 02:40:31.346583: step 36440, loss = 0.1572, acc = 0.9440 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 02:40:40.377950: step 36450, loss = 0.1542, acc = 0.9440 (300.8 examples/sec; 0.213 sec/batch)
2017-05-03 02:40:49.358199: step 36460, loss = 0.1295, acc = 0.9580 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 02:40:58.474857: step 36470, loss = 0.1299, acc = 0.9600 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 02:41:07.452002: step 36480, loss = 0.1462, acc = 0.9600 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 02:41:16.421732: step 36490, loss = 0.1672, acc = 0.9480 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 02:41:25.493313: step 36500, loss = 0.1352, acc = 0.9540 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 02:41:34.476416: step 36510, loss = 0.1378, acc = 0.9560 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 02:41:43.589358: step 36520, loss = 0.1368, acc = 0.9620 (307.0 examples/sec; 0.208 sec/batch)
2017-05-03 02:41:52.743509: step 36530, loss = 0.1252, acc = 0.9760 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 02:42:01.655062: step 36540, loss = 0.1582, acc = 0.9540 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 02:42:10.696036: step 36550, loss = 0.1532, acc = 0.9560 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 02:42:19.851176: step 36560, loss = 0.1283, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 02:42:28.909098: step 36570, loss = 0.1075, acc = 0.9720 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 02:42:37.894789: step 36580, loss = 0.1432, acc = 0.9660 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 02:42:46.999019: step 36590, loss = 0.1533, acc = 0.9540 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 02:42:56.157583: step 36600, loss = 0.1203, acc = 0.9640 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 02:43:05.285770: step 36610, loss = 0.1248, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 02:43:14.337840: step 36620, loss = 0.1447, acc = 0.9580 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 02:43:23.274033: step 36630, loss = 0.1615, acc = 0.9420 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 02:43:32.301815: step 36640, loss = 0.1386, acc = 0.9580 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 02:43:41.525382: step 36650, loss = 0.1295, acc = 0.9660 (258.4 examples/sec; 0.248 sec/batch)
2017-05-03 02:43:50.627588: step 36660, loss = 0.1450, acc = 0.9620 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 02:43:59.719435: step 36670, loss = 0.1468, acc = 0.9460 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 02:44:08.698619: step 36680, loss = 0.1286, acc = 0.9580 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 02:44:18.044000: step 36690, loss = 0.1277, acc = 0.9500 (265.6 examples/sec; 0.241 sec/batch)
2017-05-03 02:44:27.157413: step 36700, loss = 0.1383, acc = 0.9620 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 02:44:36.219154: step 36710, loss = 0.1518, acc = 0.9520 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 02:44:45.282209: step 36720, loss = 0.1636, acc = 0.9520 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 02:44:54.270684: step 36730, loss = 0.1176, acc = 0.9680 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 02:45:03.355744: step 36740, loss = 0.1358, acc = 0.9560 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 02:45:12.417475: step 36750, loss = 0.1362, acc = 0.9620 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 02:45:21.455119: step 36760, loss = 0.1562, acc = 0.9500 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 02:45:30.510498: step 36770, loss = 0.1200, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 02:45:39.770892: step 36780, loss = 0.1425, acc = 0.9540 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 02:45:49.403041: step 36790, loss = 0.1538, acc = 0.9480 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 02:45:58.417841: step 36800, loss = 0.1353, acc = 0.9520 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 02:46:07.648860: step 36810, loss = 0.1345, acc = 0.9640 (234.6 examples/sec; 0.273 sec/batch)
2017-05-03 02:46:16.629991: step 36820, loss = 0.1372, acc = 0.9520 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 02:46:25.727387: step 36830, loss = 0.1478, acc = 0.9560 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 02:46:34.748857: step 36840, loss = 0.1150, acc = 0.9660 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 02:46:43.743695: step 36850, loss = 0.1367, acc = 0.9580 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 02:46:52.965176: step 36860, loss = 0.1230, acc = 0.9540 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 02:47:02.039731: step 36870, loss = 0.1191, acc = 0.9560 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 02:47:11.039303: step 36880, loss = 0.1312, acc = 0.9520 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 02:47:20.035984: step 36890, loss = 0.1329, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 02:47:29.106046: step 36900, loss = 0.1157, acc = 0.9600 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 02:47:38.211709: step 36910, loss = 0.1370, acc = 0.9560 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 02:47:47.278908: step 36920, loss = 0.1252, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 02:47:56.533368: step 36930, loss = 0.1630, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 02:48:05.691012: step 36940, loss = 0.1213, acc = 0.9620 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 02:48:14.591397: step 36950, loss = 0.1329, acc = 0.9540 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 02:48:23.733143: step 36960, loss = 0.1124, acc = 0.9600 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 02:48:32.871394: step 36970, loss = 0.1319, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 02:48:42.008989: step 36980, loss = 0.1238, acc = 0.9680 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 02:48:51.323257: step 36990, loss = 0.1566, acc = 0.9480 (235.9 examples/sec; 0.271 sec/batch)
2017-05-03 02:49:01.009010: step 37000, loss = 0.1192, acc = 0.9700 (187.2 examples/sec; 0.342 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 02:49:01.493642: step 37000, loss = 0.1168, acc = 0.9585, f1neg = 0.9544, f1pos = 0.9619, f1 = 0.9582
[Eval_batch(1)(2000,4000)] 2017-05-03 02:49:01.995934: step 37000, loss = 0.1188, acc = 0.9660, f1neg = 0.9604, f1pos = 0.9702, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 02:49:02.499886: step 37000, loss = 0.1296, acc = 0.9560, f1neg = 0.9534, f1pos = 0.9583, f1 = 0.9559
[Eval_batch(3)(2000,8000)] 2017-05-03 02:49:02.979792: step 37000, loss = 0.1345, acc = 0.9575, f1neg = 0.9575, f1pos = 0.9575, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 02:49:03.462066: step 37000, loss = 0.1362, acc = 0.9585, f1neg = 0.9596, f1pos = 0.9573, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 02:49:03.938420: step 37000, loss = 0.1332, acc = 0.9515, f1neg = 0.9480, f1pos = 0.9546, f1 = 0.9513
[Eval_batch(6)(2000,14000)] 2017-05-03 02:49:04.404553: step 37000, loss = 0.1231, acc = 0.9625, f1neg = 0.9619, f1pos = 0.9631, f1 = 0.9625
[Eval_batch(7)(2000,16000)] 2017-05-03 02:49:04.913754: step 37000, loss = 0.1222, acc = 0.9615, f1neg = 0.9539, f1pos = 0.9669, f1 = 0.9604
[Eval_batch(8)(2000,18000)] 2017-05-03 02:49:05.390234: step 37000, loss = 0.1174, acc = 0.9590, f1neg = 0.9532, f1pos = 0.9635, f1 = 0.9584
[Eval_batch(9)(2000,20000)] 2017-05-03 02:49:05.896073: step 37000, loss = 0.1216, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9667, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 02:49:06.408625: step 37000, loss = 0.1302, acc = 0.9575, f1neg = 0.9546, f1pos = 0.9601, f1 = 0.9573
[Eval_batch(11)(2000,24000)] 2017-05-03 02:49:06.875321: step 37000, loss = 0.1301, acc = 0.9580, f1neg = 0.9514, f1pos = 0.9630, f1 = 0.9572
[Eval_batch(12)(2000,26000)] 2017-05-03 02:49:07.337967: step 37000, loss = 0.1275, acc = 0.9535, f1neg = 0.9522, f1pos = 0.9547, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 02:49:07.835784: step 37000, loss = 0.1155, acc = 0.9655, f1neg = 0.9569, f1pos = 0.9712, f1 = 0.9641
[Eval_batch(14)(2000,30000)] 2017-05-03 02:49:08.338402: step 37000, loss = 0.1428, acc = 0.9545, f1neg = 0.9457, f1pos = 0.9608, f1 = 0.9533
[Eval_batch(15)(2000,32000)] 2017-05-03 02:49:08.847650: step 37000, loss = 0.1204, acc = 0.9675, f1neg = 0.9649, f1pos = 0.9698, f1 = 0.9673
[Eval_batch(16)(2000,34000)] 2017-05-03 02:49:09.327967: step 37000, loss = 0.1164, acc = 0.9655, f1neg = 0.9641, f1pos = 0.9668, f1 = 0.9654
[Eval_batch(17)(2000,36000)] 2017-05-03 02:49:09.834965: step 37000, loss = 0.1396, acc = 0.9555, f1neg = 0.9557, f1pos = 0.9553, f1 = 0.9555
[Eval_batch(18)(2000,38000)] 2017-05-03 02:49:10.333074: step 37000, loss = 0.1360, acc = 0.9560, f1neg = 0.9577, f1pos = 0.9541, f1 = 0.9559
[Eval_batch(19)(2000,40000)] 2017-05-03 02:49:10.821302: step 37000, loss = 0.1179, acc = 0.9670, f1neg = 0.9618, f1pos = 0.9709, f1 = 0.9664
[Eval_batch(20)(2000,42000)] 2017-05-03 02:49:11.330260: step 37000, loss = 0.1198, acc = 0.9630, f1neg = 0.9673, f1pos = 0.9575, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 02:49:11.827079: step 37000, loss = 0.1132, acc = 0.9625, f1neg = 0.9592, f1pos = 0.9653, f1 = 0.9623
[Eval_batch(22)(2000,46000)] 2017-05-03 02:49:12.313334: step 37000, loss = 0.1200, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9590, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 02:49:12.819690: step 37000, loss = 0.1260, acc = 0.9575, f1neg = 0.9520, f1pos = 0.9619, f1 = 0.9569
[Eval_batch(24)(2000,50000)] 2017-05-03 02:49:13.318802: step 37000, loss = 0.1125, acc = 0.9640, f1neg = 0.9587, f1pos = 0.9681, f1 = 0.9634
[Eval_batch(25)(2000,52000)] 2017-05-03 02:49:13.797363: step 37000, loss = 0.1041, acc = 0.9690, f1neg = 0.9660, f1pos = 0.9715, f1 = 0.9688
[Eval_batch(26)(2000,54000)] 2017-05-03 02:49:14.274813: step 37000, loss = 0.1170, acc = 0.9580, f1neg = 0.9603, f1pos = 0.9554, f1 = 0.9579
[Eval_batch(27)(2000,56000)] 2017-05-03 02:49:14.853957: step 37000, loss = 0.1049, acc = 0.9685, f1neg = 0.9670, f1pos = 0.9699, f1 = 0.9684
[Eval] 2017-05-03 02:49:14.854043: step 37000, acc = 0.9606, f1 = 0.9603
[Test_batch(0)(2000,2000)] 2017-05-03 02:49:15.356763: step 37000, loss = 0.1532, acc = 0.9500, f1neg = 0.9525, f1pos = 0.9472, f1 = 0.9499
[Test_batch(1)(2000,4000)] 2017-05-03 02:49:15.855157: step 37000, loss = 0.1343, acc = 0.9595, f1neg = 0.9641, f1pos = 0.9535, f1 = 0.9588
[Test_batch(2)(2000,6000)] 2017-05-03 02:49:16.354743: step 37000, loss = 0.1507, acc = 0.9540, f1neg = 0.9578, f1pos = 0.9495, f1 = 0.9536
[Test_batch(3)(2000,8000)] 2017-05-03 02:49:16.854542: step 37000, loss = 0.1579, acc = 0.9475, f1neg = 0.9509, f1pos = 0.9436, f1 = 0.9473
[Test_batch(4)(2000,10000)] 2017-05-03 02:49:17.352125: step 37000, loss = 0.1552, acc = 0.9490, f1neg = 0.9493, f1pos = 0.9487, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 02:49:17.862986: step 37000, loss = 0.1657, acc = 0.9405, f1neg = 0.9411, f1pos = 0.9399, f1 = 0.9405
[Test_batch(6)(2000,14000)] 2017-05-03 02:49:18.363204: step 37000, loss = 0.1518, acc = 0.9460, f1neg = 0.9441, f1pos = 0.9478, f1 = 0.9459
[Test_batch(7)(2000,16000)] 2017-05-03 02:49:18.862864: step 37000, loss = 0.1418, acc = 0.9520, f1neg = 0.9565, f1pos = 0.9465, f1 = 0.9515
[Test_batch(8)(2000,18000)] 2017-05-03 02:49:19.340868: step 37000, loss = 0.1430, acc = 0.9470, f1neg = 0.9466, f1pos = 0.9474, f1 = 0.9470
[Test_batch(9)(2000,20000)] 2017-05-03 02:49:19.841760: step 37000, loss = 0.1739, acc = 0.9375, f1neg = 0.9342, f1pos = 0.9405, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 02:49:20.342149: step 37000, loss = 0.1494, acc = 0.9510, f1neg = 0.9449, f1pos = 0.9559, f1 = 0.9504
[Test_batch(11)(2000,24000)] 2017-05-03 02:49:20.843701: step 37000, loss = 0.1506, acc = 0.9485, f1neg = 0.9499, f1pos = 0.9470, f1 = 0.9485
[Test_batch(12)(2000,26000)] 2017-05-03 02:49:21.352968: step 37000, loss = 0.1275, acc = 0.9610, f1neg = 0.9625, f1pos = 0.9593, f1 = 0.9609
[Test_batch(13)(2000,28000)] 2017-05-03 02:49:21.851593: step 37000, loss = 0.1533, acc = 0.9475, f1neg = 0.9418, f1pos = 0.9522, f1 = 0.9470
[Test_batch(14)(2000,30000)] 2017-05-03 02:49:22.355451: step 37000, loss = 0.1267, acc = 0.9615, f1neg = 0.9559, f1pos = 0.9659, f1 = 0.9609
[Test_batch(15)(2000,32000)] 2017-05-03 02:49:22.832123: step 37000, loss = 0.1500, acc = 0.9525, f1neg = 0.9520, f1pos = 0.9530, f1 = 0.9525
[Test_batch(16)(2000,34000)] 2017-05-03 02:49:23.311186: step 37000, loss = 0.1321, acc = 0.9595, f1neg = 0.9586, f1pos = 0.9604, f1 = 0.9595
[Test_batch(17)(2000,36000)] 2017-05-03 02:49:23.816549: step 37000, loss = 0.1239, acc = 0.9630, f1neg = 0.9596, f1pos = 0.9659, f1 = 0.9627
[Test_batch(18)(2000,38000)] 2017-05-03 02:49:24.365639: step 37000, loss = 0.1165, acc = 0.9635, f1neg = 0.9627, f1pos = 0.9642, f1 = 0.9635
[Test] 2017-05-03 02:49:24.365722: step 37000, acc = 0.9522, f1 = 0.9519
[Status] 2017-05-03 02:49:24.365748: step 37000, maxindex = 37000, maxdev = 0.9606, maxtst = 0.9522
2017-05-03 02:49:36.587338: step 37010, loss = 0.1472, acc = 0.9440 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 02:49:45.682638: step 37020, loss = 0.1492, acc = 0.9420 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 02:49:54.710841: step 37030, loss = 0.1297, acc = 0.9560 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 02:50:03.636101: step 37040, loss = 0.1382, acc = 0.9480 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 02:50:13.063591: step 37050, loss = 0.1468, acc = 0.9480 (270.4 examples/sec; 0.237 sec/batch)
2017-05-03 02:50:22.119975: step 37060, loss = 0.1358, acc = 0.9560 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 02:50:31.177053: step 37070, loss = 0.1487, acc = 0.9580 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 02:50:40.203826: step 37080, loss = 0.1290, acc = 0.9520 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 02:50:49.118819: step 37090, loss = 0.1596, acc = 0.9480 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 02:50:58.409658: step 37100, loss = 0.1494, acc = 0.9500 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 02:51:07.511196: step 37110, loss = 0.1440, acc = 0.9380 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 02:51:16.412828: step 37120, loss = 0.1036, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 02:51:25.369390: step 37130, loss = 0.1365, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 02:51:34.565131: step 37140, loss = 0.1460, acc = 0.9500 (261.8 examples/sec; 0.244 sec/batch)
2017-05-03 02:51:43.572652: step 37150, loss = 0.1416, acc = 0.9380 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 02:51:52.622928: step 37160, loss = 0.1389, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 02:52:01.620025: step 37170, loss = 0.1048, acc = 0.9720 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 02:52:10.727958: step 37180, loss = 0.1276, acc = 0.9560 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 02:52:19.864199: step 37190, loss = 0.1294, acc = 0.9540 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 02:52:28.929494: step 37200, loss = 0.1459, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 02:52:38.153296: step 37210, loss = 0.1356, acc = 0.9540 (268.3 examples/sec; 0.239 sec/batch)
2017-05-03 02:52:47.289354: step 37220, loss = 0.1423, acc = 0.9600 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 02:52:56.562656: step 37230, loss = 0.1403, acc = 0.9540 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 02:53:05.571494: step 37240, loss = 0.1743, acc = 0.9400 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 02:53:14.611337: step 37250, loss = 0.1518, acc = 0.9500 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 02:53:23.819255: step 37260, loss = 0.1109, acc = 0.9740 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 02:53:32.852874: step 37270, loss = 0.1122, acc = 0.9800 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 02:53:42.038172: step 37280, loss = 0.1286, acc = 0.9660 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 02:53:51.247682: step 37290, loss = 0.1439, acc = 0.9540 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 02:54:01.373137: step 37300, loss = 0.1495, acc = 0.9400 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 02:54:10.556474: step 37310, loss = 0.1165, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 02:54:19.603216: step 37320, loss = 0.1621, acc = 0.9400 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 02:54:28.569667: step 37330, loss = 0.1224, acc = 0.9680 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 02:54:37.662590: step 37340, loss = 0.1195, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 02:54:46.708706: step 37350, loss = 0.1466, acc = 0.9600 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 02:54:55.736734: step 37360, loss = 0.1468, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 02:55:04.808470: step 37370, loss = 0.1432, acc = 0.9480 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 02:55:13.814579: step 37380, loss = 0.1465, acc = 0.9560 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 02:55:23.178909: step 37390, loss = 0.1265, acc = 0.9600 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 02:55:32.284126: step 37400, loss = 0.1201, acc = 0.9680 (267.8 examples/sec; 0.239 sec/batch)
2017-05-03 02:55:41.423605: step 37410, loss = 0.1631, acc = 0.9480 (302.0 examples/sec; 0.212 sec/batch)
2017-05-03 02:55:50.517903: step 37420, loss = 0.1155, acc = 0.9620 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 02:55:59.546762: step 37430, loss = 0.1250, acc = 0.9620 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 02:56:08.753462: step 37440, loss = 0.1404, acc = 0.9500 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 02:56:17.747451: step 37450, loss = 0.1584, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 02:56:26.806904: step 37460, loss = 0.1368, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 02:56:35.882933: step 37470, loss = 0.1626, acc = 0.9440 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 02:56:45.025895: step 37480, loss = 0.1462, acc = 0.9480 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 02:56:54.149573: step 37490, loss = 0.1252, acc = 0.9520 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 02:57:03.375211: step 37500, loss = 0.1285, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 02:57:12.485283: step 37510, loss = 0.1402, acc = 0.9560 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 02:57:21.444423: step 37520, loss = 0.1581, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 02:57:30.852791: step 37530, loss = 0.1366, acc = 0.9520 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 02:57:40.064703: step 37540, loss = 0.0967, acc = 0.9820 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 02:57:49.081710: step 37550, loss = 0.1401, acc = 0.9620 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 02:57:58.122366: step 37560, loss = 0.1089, acc = 0.9740 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 02:58:07.295093: step 37570, loss = 0.1136, acc = 0.9760 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 02:58:16.230030: step 37580, loss = 0.1556, acc = 0.9360 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 02:58:25.371176: step 37590, loss = 0.1385, acc = 0.9620 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 02:58:34.417947: step 37600, loss = 0.1431, acc = 0.9480 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 02:58:43.651083: step 37610, loss = 0.1297, acc = 0.9660 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 02:58:52.630799: step 37620, loss = 0.1145, acc = 0.9640 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 02:59:01.634445: step 37630, loss = 0.1522, acc = 0.9600 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 02:59:10.823598: step 37640, loss = 0.1734, acc = 0.9300 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 02:59:19.850813: step 37650, loss = 0.1292, acc = 0.9600 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 02:59:29.199878: step 37660, loss = 0.1504, acc = 0.9500 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 02:59:38.259383: step 37670, loss = 0.1568, acc = 0.9360 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 02:59:47.790413: step 37680, loss = 0.1200, acc = 0.9700 (207.0 examples/sec; 0.309 sec/batch)
2017-05-03 02:59:56.981291: step 37690, loss = 0.1362, acc = 0.9500 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 03:00:05.966551: step 37700, loss = 0.1395, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 03:00:15.225158: step 37710, loss = 0.1136, acc = 0.9640 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 03:00:24.355829: step 37720, loss = 0.1311, acc = 0.9540 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 03:00:33.367247: step 37730, loss = 0.1223, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 03:00:42.401153: step 37740, loss = 0.1098, acc = 0.9780 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 03:00:51.294743: step 37750, loss = 0.1325, acc = 0.9620 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 03:01:00.427680: step 37760, loss = 0.1406, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 03:01:09.551416: step 37770, loss = 0.1111, acc = 0.9640 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 03:01:18.510372: step 37780, loss = 0.1438, acc = 0.9480 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 03:01:27.580119: step 37790, loss = 0.1619, acc = 0.9420 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 03:01:36.707632: step 37800, loss = 0.1151, acc = 0.9740 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 03:01:45.751905: step 37810, loss = 0.1159, acc = 0.9700 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 03:01:54.915363: step 37820, loss = 0.1430, acc = 0.9440 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 03:02:04.088844: step 37830, loss = 0.1529, acc = 0.9480 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 03:02:13.055407: step 37840, loss = 0.1190, acc = 0.9680 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 03:02:22.083104: step 37850, loss = 0.1439, acc = 0.9520 (298.1 examples/sec; 0.215 sec/batch)
2017-05-03 03:02:31.293179: step 37860, loss = 0.1158, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 03:02:40.316097: step 37870, loss = 0.1251, acc = 0.9620 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 03:02:49.320386: step 37880, loss = 0.1281, acc = 0.9680 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 03:02:58.435297: step 37890, loss = 0.1218, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 03:03:07.764711: step 37900, loss = 0.1165, acc = 0.9660 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 03:03:17.026137: step 37910, loss = 0.1405, acc = 0.9540 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 03:03:26.090160: step 37920, loss = 0.1367, acc = 0.9580 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 03:03:35.151312: step 37930, loss = 0.1548, acc = 0.9480 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 03:03:44.241111: step 37940, loss = 0.1297, acc = 0.9580 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 03:03:53.344794: step 37950, loss = 0.1596, acc = 0.9520 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 03:04:02.278945: step 37960, loss = 0.1250, acc = 0.9580 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 03:04:11.213455: step 37970, loss = 0.1101, acc = 0.9700 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 03:04:20.297463: step 37980, loss = 0.1421, acc = 0.9560 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 03:04:29.480523: step 37990, loss = 0.1188, acc = 0.9560 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 03:04:38.574688: step 38000, loss = 0.1241, acc = 0.9680 (270.6 examples/sec; 0.236 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 03:04:39.050351: step 38000, loss = 0.1144, acc = 0.9620, f1neg = 0.9588, f1pos = 0.9647, f1 = 0.9618
[Eval_batch(1)(2000,4000)] 2017-05-03 03:04:39.522902: step 38000, loss = 0.1241, acc = 0.9615, f1neg = 0.9558, f1pos = 0.9659, f1 = 0.9609
[Eval_batch(2)(2000,6000)] 2017-05-03 03:04:40.012420: step 38000, loss = 0.1302, acc = 0.9560, f1neg = 0.9539, f1pos = 0.9579, f1 = 0.9559
[Eval_batch(3)(2000,8000)] 2017-05-03 03:04:40.488570: step 38000, loss = 0.1351, acc = 0.9540, f1neg = 0.9547, f1pos = 0.9533, f1 = 0.9540
[Eval_batch(4)(2000,10000)] 2017-05-03 03:04:40.988987: step 38000, loss = 0.1359, acc = 0.9535, f1neg = 0.9556, f1pos = 0.9512, f1 = 0.9534
[Eval_batch(5)(2000,12000)] 2017-05-03 03:04:41.465136: step 38000, loss = 0.1353, acc = 0.9500, f1neg = 0.9473, f1pos = 0.9524, f1 = 0.9499
[Eval_batch(6)(2000,14000)] 2017-05-03 03:04:41.944480: step 38000, loss = 0.1206, acc = 0.9655, f1neg = 0.9652, f1pos = 0.9658, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 03:04:42.419703: step 38000, loss = 0.1247, acc = 0.9590, f1neg = 0.9517, f1pos = 0.9644, f1 = 0.9580
[Eval_batch(8)(2000,18000)] 2017-05-03 03:04:42.888386: step 38000, loss = 0.1191, acc = 0.9605, f1neg = 0.9558, f1pos = 0.9643, f1 = 0.9601
[Eval_batch(9)(2000,20000)] 2017-05-03 03:04:43.352852: step 38000, loss = 0.1237, acc = 0.9595, f1neg = 0.9572, f1pos = 0.9616, f1 = 0.9594
[Eval_batch(10)(2000,22000)] 2017-05-03 03:04:43.828465: step 38000, loss = 0.1333, acc = 0.9580, f1neg = 0.9557, f1pos = 0.9601, f1 = 0.9579
[Eval_batch(11)(2000,24000)] 2017-05-03 03:04:44.312255: step 38000, loss = 0.1337, acc = 0.9585, f1neg = 0.9528, f1pos = 0.9630, f1 = 0.9579
[Eval_batch(12)(2000,26000)] 2017-05-03 03:04:44.812345: step 38000, loss = 0.1291, acc = 0.9515, f1neg = 0.9508, f1pos = 0.9521, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-03 03:04:45.319934: step 38000, loss = 0.1172, acc = 0.9670, f1neg = 0.9592, f1pos = 0.9723, f1 = 0.9658
[Eval_batch(14)(2000,30000)] 2017-05-03 03:04:45.795464: step 38000, loss = 0.1441, acc = 0.9530, f1neg = 0.9448, f1pos = 0.9591, f1 = 0.9519
[Eval_batch(15)(2000,32000)] 2017-05-03 03:04:46.275768: step 38000, loss = 0.1239, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9599
[Eval_batch(16)(2000,34000)] 2017-05-03 03:04:46.755293: step 38000, loss = 0.1189, acc = 0.9655, f1neg = 0.9644, f1pos = 0.9665, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 03:04:47.253194: step 38000, loss = 0.1416, acc = 0.9580, f1neg = 0.9587, f1pos = 0.9573, f1 = 0.9580
[Eval_batch(18)(2000,38000)] 2017-05-03 03:04:47.761304: step 38000, loss = 0.1348, acc = 0.9540, f1neg = 0.9563, f1pos = 0.9515, f1 = 0.9539
[Eval_batch(19)(2000,40000)] 2017-05-03 03:04:48.267634: step 38000, loss = 0.1198, acc = 0.9660, f1neg = 0.9611, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(20)(2000,42000)] 2017-05-03 03:04:48.767279: step 38000, loss = 0.1178, acc = 0.9600, f1neg = 0.9648, f1pos = 0.9537, f1 = 0.9592
[Eval_batch(21)(2000,44000)] 2017-05-03 03:04:49.277667: step 38000, loss = 0.1194, acc = 0.9595, f1neg = 0.9565, f1pos = 0.9621, f1 = 0.9593
[Eval_batch(22)(2000,46000)] 2017-05-03 03:04:49.754070: step 38000, loss = 0.1217, acc = 0.9620, f1neg = 0.9627, f1pos = 0.9612, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 03:04:50.214080: step 38000, loss = 0.1308, acc = 0.9575, f1neg = 0.9527, f1pos = 0.9614, f1 = 0.9571
[Eval_batch(24)(2000,50000)] 2017-05-03 03:04:50.692421: step 38000, loss = 0.1162, acc = 0.9650, f1neg = 0.9604, f1pos = 0.9686, f1 = 0.9645
[Eval_batch(25)(2000,52000)] 2017-05-03 03:04:51.201043: step 38000, loss = 0.1046, acc = 0.9710, f1neg = 0.9686, f1pos = 0.9731, f1 = 0.9708
[Eval_batch(26)(2000,54000)] 2017-05-03 03:04:51.701227: step 38000, loss = 0.1161, acc = 0.9610, f1neg = 0.9636, f1pos = 0.9580, f1 = 0.9608
[Eval_batch(27)(2000,56000)] 2017-05-03 03:04:52.326086: step 38000, loss = 0.1030, acc = 0.9690, f1neg = 0.9678, f1pos = 0.9701, f1 = 0.9690
[Eval] 2017-05-03 03:04:52.326119: step 38000, acc = 0.9599, f1 = 0.9596
[Test_batch(0)(2000,2000)] 2017-05-03 03:04:52.797236: step 38000, loss = 0.1528, acc = 0.9530, f1neg = 0.9560, f1pos = 0.9496, f1 = 0.9528
[Test_batch(1)(2000,4000)] 2017-05-03 03:04:53.290523: step 38000, loss = 0.1343, acc = 0.9565, f1neg = 0.9618, f1pos = 0.9494, f1 = 0.9556
[Test_batch(2)(2000,6000)] 2017-05-03 03:04:53.767591: step 38000, loss = 0.1488, acc = 0.9555, f1neg = 0.9597, f1pos = 0.9504, f1 = 0.9550
[Test_batch(3)(2000,8000)] 2017-05-03 03:04:54.247567: step 38000, loss = 0.1588, acc = 0.9415, f1neg = 0.9462, f1pos = 0.9359, f1 = 0.9410
[Test_batch(4)(2000,10000)] 2017-05-03 03:04:54.747230: step 38000, loss = 0.1585, acc = 0.9470, f1neg = 0.9480, f1pos = 0.9460, f1 = 0.9470
[Test_batch(5)(2000,12000)] 2017-05-03 03:04:55.234216: step 38000, loss = 0.1704, acc = 0.9405, f1neg = 0.9423, f1pos = 0.9386, f1 = 0.9404
[Test_batch(6)(2000,14000)] 2017-05-03 03:04:55.738591: step 38000, loss = 0.1511, acc = 0.9495, f1neg = 0.9485, f1pos = 0.9504, f1 = 0.9495
[Test_batch(7)(2000,16000)] 2017-05-03 03:04:56.234437: step 38000, loss = 0.1423, acc = 0.9515, f1neg = 0.9566, f1pos = 0.9451, f1 = 0.9508
[Test_batch(8)(2000,18000)] 2017-05-03 03:04:56.712207: step 38000, loss = 0.1462, acc = 0.9490, f1neg = 0.9489, f1pos = 0.9491, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 03:04:57.183037: step 38000, loss = 0.1787, acc = 0.9320, f1neg = 0.9299, f1pos = 0.9340, f1 = 0.9319
[Test_batch(10)(2000,22000)] 2017-05-03 03:04:57.690616: step 38000, loss = 0.1576, acc = 0.9465, f1neg = 0.9410, f1pos = 0.9511, f1 = 0.9460
[Test_batch(11)(2000,24000)] 2017-05-03 03:04:58.177503: step 38000, loss = 0.1518, acc = 0.9510, f1neg = 0.9531, f1pos = 0.9487, f1 = 0.9509
[Test_batch(12)(2000,26000)] 2017-05-03 03:04:58.664450: step 38000, loss = 0.1311, acc = 0.9570, f1neg = 0.9590, f1pos = 0.9547, f1 = 0.9569
[Test_batch(13)(2000,28000)] 2017-05-03 03:04:59.131633: step 38000, loss = 0.1571, acc = 0.9525, f1neg = 0.9483, f1pos = 0.9561, f1 = 0.9522
[Test_batch(14)(2000,30000)] 2017-05-03 03:04:59.613280: step 38000, loss = 0.1331, acc = 0.9555, f1neg = 0.9499, f1pos = 0.9600, f1 = 0.9549
[Test_batch(15)(2000,32000)] 2017-05-03 03:05:00.082791: step 38000, loss = 0.1534, acc = 0.9515, f1neg = 0.9515, f1pos = 0.9515, f1 = 0.9515
[Test_batch(16)(2000,34000)] 2017-05-03 03:05:00.594733: step 38000, loss = 0.1353, acc = 0.9575, f1neg = 0.9570, f1pos = 0.9579, f1 = 0.9575
[Test_batch(17)(2000,36000)] 2017-05-03 03:05:01.111402: step 38000, loss = 0.1255, acc = 0.9600, f1neg = 0.9568, f1pos = 0.9628, f1 = 0.9598
[Test_batch(18)(2000,38000)] 2017-05-03 03:05:01.721134: step 38000, loss = 0.1142, acc = 0.9670, f1neg = 0.9667, f1pos = 0.9673, f1 = 0.9670
[Test] 2017-05-03 03:05:01.721209: step 38000, acc = 0.9513, f1 = 0.9510
[Status] 2017-05-03 03:05:01.721230: step 38000, maxindex = 37000, maxdev = 0.9606, maxtst = 0.9522
2017-05-03 03:05:10.760593: step 38010, loss = 0.1413, acc = 0.9560 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 03:05:19.841419: step 38020, loss = 0.1593, acc = 0.9560 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 03:05:29.073540: step 38030, loss = 0.1375, acc = 0.9600 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 03:05:38.066096: step 38040, loss = 0.1304, acc = 0.9480 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 03:05:47.035979: step 38050, loss = 0.1194, acc = 0.9640 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 03:05:56.051445: step 38060, loss = 0.1261, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 03:06:05.140306: step 38070, loss = 0.1463, acc = 0.9460 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 03:06:14.300751: step 38080, loss = 0.1241, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 03:06:23.296655: step 38090, loss = 0.1263, acc = 0.9580 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 03:06:32.278585: step 38100, loss = 0.1582, acc = 0.9360 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 03:06:41.395629: step 38110, loss = 0.1417, acc = 0.9540 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 03:06:50.541748: step 38120, loss = 0.1412, acc = 0.9540 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 03:06:59.700272: step 38130, loss = 0.1586, acc = 0.9400 (268.7 examples/sec; 0.238 sec/batch)
2017-05-03 03:07:08.739603: step 38140, loss = 0.1162, acc = 0.9680 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 03:07:17.794790: step 38150, loss = 0.1424, acc = 0.9560 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 03:07:26.739318: step 38160, loss = 0.1299, acc = 0.9500 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 03:07:35.903033: step 38170, loss = 0.1372, acc = 0.9520 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 03:07:45.007571: step 38180, loss = 0.1690, acc = 0.9480 (267.2 examples/sec; 0.240 sec/batch)
2017-05-03 03:07:53.973491: step 38190, loss = 0.1413, acc = 0.9420 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 03:08:03.019987: step 38200, loss = 0.1468, acc = 0.9380 (295.1 examples/sec; 0.217 sec/batch)
2017-05-03 03:08:12.201259: step 38210, loss = 0.1550, acc = 0.9560 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 03:08:21.402983: step 38220, loss = 0.1434, acc = 0.9440 (256.5 examples/sec; 0.250 sec/batch)
2017-05-03 03:08:30.479676: step 38230, loss = 0.1470, acc = 0.9460 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 03:08:39.607932: step 38240, loss = 0.1293, acc = 0.9540 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 03:08:48.567190: step 38250, loss = 0.1296, acc = 0.9520 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 03:08:57.596166: step 38260, loss = 0.1114, acc = 0.9680 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 03:09:06.535226: step 38270, loss = 0.1327, acc = 0.9580 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 03:09:15.598362: step 38280, loss = 0.1515, acc = 0.9480 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 03:09:24.790831: step 38290, loss = 0.1386, acc = 0.9600 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 03:09:33.884894: step 38300, loss = 0.1459, acc = 0.9480 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 03:09:43.511485: step 38310, loss = 0.1206, acc = 0.9600 (302.4 examples/sec; 0.212 sec/batch)
2017-05-03 03:09:52.639641: step 38320, loss = 0.1171, acc = 0.9600 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 03:10:01.653893: step 38330, loss = 0.1410, acc = 0.9500 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 03:10:10.908538: step 38340, loss = 0.1588, acc = 0.9540 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 03:10:20.002181: step 38350, loss = 0.1456, acc = 0.9480 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 03:10:29.029785: step 38360, loss = 0.1557, acc = 0.9560 (295.1 examples/sec; 0.217 sec/batch)
2017-05-03 03:10:38.015321: step 38370, loss = 0.1120, acc = 0.9680 (300.6 examples/sec; 0.213 sec/batch)
2017-05-03 03:10:47.024994: step 38380, loss = 0.1293, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 03:10:55.891032: step 38390, loss = 0.1534, acc = 0.9460 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 03:11:05.366574: step 38400, loss = 0.1418, acc = 0.9620 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 03:11:14.500791: step 38410, loss = 0.1320, acc = 0.9560 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 03:11:23.544366: step 38420, loss = 0.1120, acc = 0.9660 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 03:11:32.607124: step 38430, loss = 0.1538, acc = 0.9360 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 03:11:41.952296: step 38440, loss = 0.1142, acc = 0.9660 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 03:11:51.076878: step 38450, loss = 0.1518, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 03:12:00.060780: step 38460, loss = 0.1386, acc = 0.9520 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 03:12:09.476118: step 38470, loss = 0.1331, acc = 0.9600 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 03:12:18.498021: step 38480, loss = 0.1561, acc = 0.9460 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 03:12:27.780023: step 38490, loss = 0.1256, acc = 0.9580 (241.5 examples/sec; 0.265 sec/batch)
2017-05-03 03:12:36.953413: step 38500, loss = 0.1503, acc = 0.9580 (272.9 examples/sec; 0.234 sec/batch)
2017-05-03 03:12:46.029053: step 38510, loss = 0.1849, acc = 0.9460 (264.2 examples/sec; 0.242 sec/batch)
2017-05-03 03:12:54.988404: step 38520, loss = 0.1420, acc = 0.9420 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 03:13:04.123626: step 38530, loss = 0.1661, acc = 0.9460 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 03:13:13.083649: step 38540, loss = 0.1013, acc = 0.9700 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 03:13:22.108496: step 38550, loss = 0.1617, acc = 0.9500 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 03:13:31.298518: step 38560, loss = 0.1400, acc = 0.9500 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:13:40.402472: step 38570, loss = 0.1137, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 03:13:49.430209: step 38580, loss = 0.1452, acc = 0.9500 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 03:13:58.415585: step 38590, loss = 0.1315, acc = 0.9620 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:14:07.644950: step 38600, loss = 0.1320, acc = 0.9540 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:14:16.841673: step 38610, loss = 0.1237, acc = 0.9540 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 03:14:25.990042: step 38620, loss = 0.1442, acc = 0.9540 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 03:14:35.081275: step 38630, loss = 0.1498, acc = 0.9540 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 03:14:44.144961: step 38640, loss = 0.1452, acc = 0.9460 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 03:14:53.873512: step 38650, loss = 0.1531, acc = 0.9380 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 03:15:02.826021: step 38660, loss = 0.1646, acc = 0.9480 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 03:15:11.860632: step 38670, loss = 0.1171, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 03:15:21.145688: step 38680, loss = 0.1131, acc = 0.9760 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 03:15:30.255313: step 38690, loss = 0.1372, acc = 0.9580 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 03:15:39.417768: step 38700, loss = 0.1255, acc = 0.9660 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 03:15:48.498132: step 38710, loss = 0.1433, acc = 0.9520 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 03:15:57.451585: step 38720, loss = 0.1336, acc = 0.9580 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 03:16:06.591906: step 38730, loss = 0.1752, acc = 0.9320 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 03:16:16.018249: step 38740, loss = 0.1372, acc = 0.9540 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 03:16:25.084159: step 38750, loss = 0.1211, acc = 0.9580 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 03:16:34.200529: step 38760, loss = 0.1415, acc = 0.9560 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 03:16:43.228734: step 38770, loss = 0.1194, acc = 0.9580 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 03:16:52.185341: step 38780, loss = 0.1266, acc = 0.9640 (300.8 examples/sec; 0.213 sec/batch)
2017-05-03 03:17:01.300154: step 38790, loss = 0.1221, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 03:17:10.341962: step 38800, loss = 0.1362, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 03:17:19.760197: step 38810, loss = 0.1376, acc = 0.9600 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 03:17:28.818023: step 38820, loss = 0.1503, acc = 0.9400 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 03:17:38.122031: step 38830, loss = 0.1413, acc = 0.9600 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 03:17:47.628817: step 38840, loss = 0.1414, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 03:17:56.755538: step 38850, loss = 0.1521, acc = 0.9520 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 03:18:05.739491: step 38860, loss = 0.1147, acc = 0.9620 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 03:18:14.855897: step 38870, loss = 0.1243, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 03:18:23.936130: step 38880, loss = 0.1596, acc = 0.9480 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 03:18:33.093683: step 38890, loss = 0.1440, acc = 0.9480 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 03:18:42.118815: step 38900, loss = 0.1520, acc = 0.9400 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 03:18:51.095562: step 38910, loss = 0.1225, acc = 0.9520 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 03:19:00.340421: step 38920, loss = 0.1457, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 03:19:09.426010: step 38930, loss = 0.1589, acc = 0.9460 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 03:19:18.407790: step 38940, loss = 0.1502, acc = 0.9500 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 03:19:27.513845: step 38950, loss = 0.1262, acc = 0.9580 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 03:19:36.527806: step 38960, loss = 0.1799, acc = 0.9240 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 03:19:45.705158: step 38970, loss = 0.1397, acc = 0.9480 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 03:19:54.610796: step 38980, loss = 0.1085, acc = 0.9680 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 03:20:03.728268: step 38990, loss = 0.1445, acc = 0.9560 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 03:20:13.092872: step 39000, loss = 0.1165, acc = 0.9660 (281.6 examples/sec; 0.227 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 03:20:13.603334: step 39000, loss = 0.1148, acc = 0.9590, f1neg = 0.9554, f1pos = 0.9620, f1 = 0.9587
[Eval_batch(1)(2000,4000)] 2017-05-03 03:20:14.075317: step 39000, loss = 0.1224, acc = 0.9630, f1neg = 0.9575, f1pos = 0.9672, f1 = 0.9624
[Eval_batch(2)(2000,6000)] 2017-05-03 03:20:14.547026: step 39000, loss = 0.1286, acc = 0.9565, f1neg = 0.9544, f1pos = 0.9584, f1 = 0.9564
[Eval_batch(3)(2000,8000)] 2017-05-03 03:20:15.064707: step 39000, loss = 0.1336, acc = 0.9540, f1neg = 0.9546, f1pos = 0.9534, f1 = 0.9540
[Eval_batch(4)(2000,10000)] 2017-05-03 03:20:15.567976: step 39000, loss = 0.1353, acc = 0.9535, f1neg = 0.9554, f1pos = 0.9514, f1 = 0.9534
[Eval_batch(5)(2000,12000)] 2017-05-03 03:20:16.031806: step 39000, loss = 0.1329, acc = 0.9535, f1neg = 0.9508, f1pos = 0.9559, f1 = 0.9534
[Eval_batch(6)(2000,14000)] 2017-05-03 03:20:16.511652: step 39000, loss = 0.1205, acc = 0.9635, f1neg = 0.9631, f1pos = 0.9639, f1 = 0.9635
[Eval_batch(7)(2000,16000)] 2017-05-03 03:20:16.984210: step 39000, loss = 0.1237, acc = 0.9585, f1neg = 0.9511, f1pos = 0.9640, f1 = 0.9575
[Eval_batch(8)(2000,18000)] 2017-05-03 03:20:17.420314: step 39000, loss = 0.1176, acc = 0.9620, f1neg = 0.9574, f1pos = 0.9657, f1 = 0.9616
[Eval_batch(9)(2000,20000)] 2017-05-03 03:20:17.927130: step 39000, loss = 0.1225, acc = 0.9615, f1neg = 0.9592, f1pos = 0.9635, f1 = 0.9614
[Eval_batch(10)(2000,22000)] 2017-05-03 03:20:18.389890: step 39000, loss = 0.1308, acc = 0.9580, f1neg = 0.9556, f1pos = 0.9602, f1 = 0.9579
[Eval_batch(11)(2000,24000)] 2017-05-03 03:20:18.842464: step 39000, loss = 0.1313, acc = 0.9585, f1neg = 0.9527, f1pos = 0.9630, f1 = 0.9579
[Eval_batch(12)(2000,26000)] 2017-05-03 03:20:19.351721: step 39000, loss = 0.1277, acc = 0.9535, f1neg = 0.9528, f1pos = 0.9542, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 03:20:19.854204: step 39000, loss = 0.1162, acc = 0.9645, f1neg = 0.9559, f1pos = 0.9703, f1 = 0.9631
[Eval_batch(14)(2000,30000)] 2017-05-03 03:20:20.319290: step 39000, loss = 0.1435, acc = 0.9540, f1neg = 0.9459, f1pos = 0.9600, f1 = 0.9530
[Eval_batch(15)(2000,32000)] 2017-05-03 03:20:20.796036: step 39000, loss = 0.1228, acc = 0.9615, f1neg = 0.9589, f1pos = 0.9638, f1 = 0.9613
[Eval_batch(16)(2000,34000)] 2017-05-03 03:20:21.252339: step 39000, loss = 0.1172, acc = 0.9655, f1neg = 0.9643, f1pos = 0.9666, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 03:20:21.710903: step 39000, loss = 0.1412, acc = 0.9565, f1neg = 0.9571, f1pos = 0.9559, f1 = 0.9565
[Eval_batch(18)(2000,38000)] 2017-05-03 03:20:22.174874: step 39000, loss = 0.1344, acc = 0.9535, f1neg = 0.9558, f1pos = 0.9510, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 03:20:22.646084: step 39000, loss = 0.1180, acc = 0.9640, f1neg = 0.9588, f1pos = 0.9680, f1 = 0.9634
[Eval_batch(20)(2000,42000)] 2017-05-03 03:20:23.114056: step 39000, loss = 0.1168, acc = 0.9630, f1neg = 0.9673, f1pos = 0.9573, f1 = 0.9623
[Eval_batch(21)(2000,44000)] 2017-05-03 03:20:23.617722: step 39000, loss = 0.1173, acc = 0.9585, f1neg = 0.9554, f1pos = 0.9612, f1 = 0.9583
[Eval_batch(22)(2000,46000)] 2017-05-03 03:20:24.089407: step 39000, loss = 0.1211, acc = 0.9640, f1neg = 0.9647, f1pos = 0.9633, f1 = 0.9640
[Eval_batch(23)(2000,48000)] 2017-05-03 03:20:24.595622: step 39000, loss = 0.1291, acc = 0.9595, f1neg = 0.9549, f1pos = 0.9633, f1 = 0.9591
[Eval_batch(24)(2000,50000)] 2017-05-03 03:20:25.070586: step 39000, loss = 0.1134, acc = 0.9645, f1neg = 0.9597, f1pos = 0.9683, f1 = 0.9640
[Eval_batch(25)(2000,52000)] 2017-05-03 03:20:25.541625: step 39000, loss = 0.1039, acc = 0.9725, f1neg = 0.9702, f1pos = 0.9745, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 03:20:26.017284: step 39000, loss = 0.1149, acc = 0.9630, f1neg = 0.9654, f1pos = 0.9603, f1 = 0.9628
[Eval_batch(27)(2000,56000)] 2017-05-03 03:20:26.615486: step 39000, loss = 0.1027, acc = 0.9705, f1neg = 0.9693, f1pos = 0.9716, f1 = 0.9705
[Eval] 2017-05-03 03:20:26.615585: step 39000, acc = 0.9604, f1 = 0.9600
[Test_batch(0)(2000,2000)] 2017-05-03 03:20:27.127484: step 39000, loss = 0.1526, acc = 0.9530, f1neg = 0.9559, f1pos = 0.9497, f1 = 0.9528
[Test_batch(1)(2000,4000)] 2017-05-03 03:20:27.637912: step 39000, loss = 0.1333, acc = 0.9580, f1neg = 0.9631, f1pos = 0.9513, f1 = 0.9572
[Test_batch(2)(2000,6000)] 2017-05-03 03:20:28.140579: step 39000, loss = 0.1475, acc = 0.9565, f1neg = 0.9605, f1pos = 0.9516, f1 = 0.9561
[Test_batch(3)(2000,8000)] 2017-05-03 03:20:28.581157: step 39000, loss = 0.1581, acc = 0.9460, f1neg = 0.9501, f1pos = 0.9411, f1 = 0.9456
[Test_batch(4)(2000,10000)] 2017-05-03 03:20:29.091308: step 39000, loss = 0.1557, acc = 0.9450, f1neg = 0.9460, f1pos = 0.9440, f1 = 0.9450
[Test_batch(5)(2000,12000)] 2017-05-03 03:20:29.535828: step 39000, loss = 0.1682, acc = 0.9410, f1neg = 0.9427, f1pos = 0.9392, f1 = 0.9410
[Test_batch(6)(2000,14000)] 2017-05-03 03:20:29.998144: step 39000, loss = 0.1506, acc = 0.9490, f1neg = 0.9479, f1pos = 0.9500, f1 = 0.9490
[Test_batch(7)(2000,16000)] 2017-05-03 03:20:30.464859: step 39000, loss = 0.1416, acc = 0.9515, f1neg = 0.9564, f1pos = 0.9454, f1 = 0.9509
[Test_batch(8)(2000,18000)] 2017-05-03 03:20:30.970422: step 39000, loss = 0.1441, acc = 0.9480, f1neg = 0.9479, f1pos = 0.9481, f1 = 0.9480
[Test_batch(9)(2000,20000)] 2017-05-03 03:20:31.445141: step 39000, loss = 0.1771, acc = 0.9320, f1neg = 0.9296, f1pos = 0.9342, f1 = 0.9319
[Test_batch(10)(2000,22000)] 2017-05-03 03:20:31.926243: step 39000, loss = 0.1545, acc = 0.9490, f1neg = 0.9434, f1pos = 0.9536, f1 = 0.9485
[Test_batch(11)(2000,24000)] 2017-05-03 03:20:32.422079: step 39000, loss = 0.1503, acc = 0.9505, f1neg = 0.9525, f1pos = 0.9484, f1 = 0.9504
[Test_batch(12)(2000,26000)] 2017-05-03 03:20:32.866559: step 39000, loss = 0.1303, acc = 0.9580, f1neg = 0.9599, f1pos = 0.9559, f1 = 0.9579
[Test_batch(13)(2000,28000)] 2017-05-03 03:20:33.312492: step 39000, loss = 0.1556, acc = 0.9500, f1neg = 0.9454, f1pos = 0.9539, f1 = 0.9496
[Test_batch(14)(2000,30000)] 2017-05-03 03:20:33.792986: step 39000, loss = 0.1307, acc = 0.9540, f1neg = 0.9481, f1pos = 0.9587, f1 = 0.9534
[Test_batch(15)(2000,32000)] 2017-05-03 03:20:34.257484: step 39000, loss = 0.1517, acc = 0.9535, f1neg = 0.9535, f1pos = 0.9535, f1 = 0.9535
[Test_batch(16)(2000,34000)] 2017-05-03 03:20:34.718653: step 39000, loss = 0.1330, acc = 0.9565, f1neg = 0.9559, f1pos = 0.9571, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 03:20:35.185303: step 39000, loss = 0.1230, acc = 0.9610, f1neg = 0.9577, f1pos = 0.9638, f1 = 0.9608
[Test_batch(18)(2000,38000)] 2017-05-03 03:20:35.738258: step 39000, loss = 0.1139, acc = 0.9655, f1neg = 0.9652, f1pos = 0.9658, f1 = 0.9655
[Test] 2017-05-03 03:20:35.738363: step 39000, acc = 0.9515, f1 = 0.9512
[Status] 2017-05-03 03:20:35.738388: step 39000, maxindex = 37000, maxdev = 0.9606, maxtst = 0.9522
2017-05-03 03:20:44.910192: step 39010, loss = 0.1327, acc = 0.9580 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 03:20:53.993834: step 39020, loss = 0.1705, acc = 0.9420 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 03:21:03.075368: step 39030, loss = 0.1389, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 03:21:12.083472: step 39040, loss = 0.1397, acc = 0.9480 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 03:21:21.687089: step 39050, loss = 0.0949, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 03:21:30.767401: step 39060, loss = 0.1266, acc = 0.9560 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 03:21:39.762393: step 39070, loss = 0.1455, acc = 0.9540 (264.3 examples/sec; 0.242 sec/batch)
2017-05-03 03:21:48.770789: step 39080, loss = 0.1482, acc = 0.9460 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 03:21:57.817755: step 39090, loss = 0.1243, acc = 0.9520 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 03:22:07.019331: step 39100, loss = 0.1016, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 03:22:16.170918: step 39110, loss = 0.1406, acc = 0.9540 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 03:22:25.188074: step 39120, loss = 0.1656, acc = 0.9340 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 03:22:34.458381: step 39130, loss = 0.1301, acc = 0.9680 (261.6 examples/sec; 0.245 sec/batch)
2017-05-03 03:22:43.495250: step 39140, loss = 0.1624, acc = 0.9440 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 03:22:52.437965: step 39150, loss = 0.1571, acc = 0.9480 (298.8 examples/sec; 0.214 sec/batch)
2017-05-03 03:23:01.877931: step 39160, loss = 0.1136, acc = 0.9700 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 03:23:11.094887: step 39170, loss = 0.1362, acc = 0.9540 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 03:23:20.170397: step 39180, loss = 0.1198, acc = 0.9660 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 03:23:29.146046: step 39190, loss = 0.1445, acc = 0.9560 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:23:38.236941: step 39200, loss = 0.1414, acc = 0.9480 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 03:23:47.277329: step 39210, loss = 0.1421, acc = 0.9580 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 03:23:56.345044: step 39220, loss = 0.1487, acc = 0.9520 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 03:24:05.439501: step 39230, loss = 0.1638, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 03:24:14.897542: step 39240, loss = 0.1369, acc = 0.9640 (221.6 examples/sec; 0.289 sec/batch)
2017-05-03 03:24:24.040877: step 39250, loss = 0.1372, acc = 0.9520 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 03:24:33.163895: step 39260, loss = 0.1358, acc = 0.9520 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 03:24:42.140361: step 39270, loss = 0.1151, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 03:24:51.104212: step 39280, loss = 0.1460, acc = 0.9500 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 03:25:00.209842: step 39290, loss = 0.1298, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 03:25:09.345614: step 39300, loss = 0.1032, acc = 0.9620 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 03:25:18.525033: step 39310, loss = 0.1537, acc = 0.9560 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 03:25:28.913902: step 39320, loss = 0.1055, acc = 0.9680 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 03:25:37.979491: step 39330, loss = 0.1209, acc = 0.9640 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 03:25:47.042413: step 39340, loss = 0.1561, acc = 0.9500 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 03:25:56.073469: step 39350, loss = 0.1580, acc = 0.9460 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 03:26:05.153031: step 39360, loss = 0.1312, acc = 0.9540 (298.1 examples/sec; 0.215 sec/batch)
2017-05-03 03:26:14.178104: step 39370, loss = 0.1642, acc = 0.9420 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 03:26:23.203834: step 39380, loss = 0.1623, acc = 0.9520 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 03:26:32.329515: step 39390, loss = 0.1298, acc = 0.9640 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 03:26:41.354085: step 39400, loss = 0.1339, acc = 0.9680 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:26:50.253696: step 39410, loss = 0.1358, acc = 0.9540 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 03:26:59.342536: step 39420, loss = 0.1113, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:27:08.517672: step 39430, loss = 0.1425, acc = 0.9540 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 03:27:17.601293: step 39440, loss = 0.1081, acc = 0.9720 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 03:27:27.123928: step 39450, loss = 0.1276, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 03:27:36.146726: step 39460, loss = 0.1602, acc = 0.9420 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 03:27:45.239987: step 39470, loss = 0.1224, acc = 0.9620 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 03:27:54.181816: step 39480, loss = 0.1676, acc = 0.9420 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 03:28:03.411362: step 39490, loss = 0.1084, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 03:28:12.471129: step 39500, loss = 0.1292, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 03:28:21.514820: step 39510, loss = 0.1178, acc = 0.9700 (296.7 examples/sec; 0.216 sec/batch)
2017-05-03 03:28:30.609893: step 39520, loss = 0.1085, acc = 0.9660 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 03:28:39.616701: step 39530, loss = 0.1357, acc = 0.9560 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 03:28:48.686468: step 39540, loss = 0.1111, acc = 0.9640 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 03:28:57.674477: step 39550, loss = 0.1269, acc = 0.9580 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 03:29:06.836435: step 39560, loss = 0.1274, acc = 0.9540 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 03:29:16.030261: step 39570, loss = 0.1275, acc = 0.9620 (303.6 examples/sec; 0.211 sec/batch)
2017-05-03 03:29:25.031046: step 39580, loss = 0.0972, acc = 0.9820 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 03:29:34.302837: step 39590, loss = 0.1629, acc = 0.9460 (233.8 examples/sec; 0.274 sec/batch)
2017-05-03 03:29:43.566780: step 39600, loss = 0.1212, acc = 0.9680 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 03:29:52.563983: step 39610, loss = 0.1185, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 03:30:01.492803: step 39620, loss = 0.1251, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 03:30:10.600657: step 39630, loss = 0.1346, acc = 0.9560 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 03:30:19.613454: step 39640, loss = 0.1342, acc = 0.9540 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 03:30:28.568073: step 39650, loss = 0.1464, acc = 0.9500 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 03:30:37.675967: step 39660, loss = 0.1478, acc = 0.9480 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 03:30:46.821659: step 39670, loss = 0.1316, acc = 0.9700 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 03:30:55.824605: step 39680, loss = 0.1353, acc = 0.9580 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 03:31:04.967844: step 39690, loss = 0.1308, acc = 0.9640 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 03:31:14.264297: step 39700, loss = 0.1563, acc = 0.9620 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 03:31:23.461241: step 39710, loss = 0.1516, acc = 0.9540 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 03:31:32.608620: step 39720, loss = 0.1065, acc = 0.9680 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 03:31:41.543883: step 39730, loss = 0.1191, acc = 0.9580 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 03:31:50.804784: step 39740, loss = 0.1377, acc = 0.9420 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 03:31:59.998569: step 39750, loss = 0.1167, acc = 0.9620 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 03:32:09.086734: step 39760, loss = 0.1513, acc = 0.9460 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 03:32:18.245667: step 39770, loss = 0.1382, acc = 0.9480 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 03:32:27.438911: step 39780, loss = 0.1509, acc = 0.9420 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 03:32:36.699315: step 39790, loss = 0.1424, acc = 0.9540 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 03:32:45.706321: step 39800, loss = 0.1127, acc = 0.9660 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 03:32:54.731219: step 39810, loss = 0.1305, acc = 0.9540 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 03:33:03.961289: step 39820, loss = 0.1506, acc = 0.9460 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 03:33:12.945727: step 39830, loss = 0.1370, acc = 0.9540 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 03:33:22.299685: step 39840, loss = 0.1306, acc = 0.9600 (239.9 examples/sec; 0.267 sec/batch)
2017-05-03 03:33:31.397378: step 39850, loss = 0.1164, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 03:33:40.665112: step 39860, loss = 0.1378, acc = 0.9520 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 03:33:49.663842: step 39870, loss = 0.1191, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 03:33:58.693611: step 39880, loss = 0.1354, acc = 0.9560 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 03:34:07.830777: step 39890, loss = 0.1234, acc = 0.9600 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 03:34:16.820919: step 39900, loss = 0.1358, acc = 0.9580 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 03:34:25.757147: step 39910, loss = 0.1114, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 03:34:34.934546: step 39920, loss = 0.1210, acc = 0.9640 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 03:34:44.018663: step 39930, loss = 0.1388, acc = 0.9600 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 03:34:53.013686: step 39940, loss = 0.1294, acc = 0.9620 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 03:35:02.161402: step 39950, loss = 0.1309, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 03:35:11.078662: step 39960, loss = 0.1247, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 03:35:20.231272: step 39970, loss = 0.1153, acc = 0.9600 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 03:35:29.309549: step 39980, loss = 0.1114, acc = 0.9680 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 03:35:38.375337: step 39990, loss = 0.1084, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 03:35:47.559303: step 40000, loss = 0.1228, acc = 0.9700 (290.9 examples/sec; 0.220 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 03:35:48.068310: step 40000, loss = 0.1167, acc = 0.9570, f1neg = 0.9527, f1pos = 0.9606, f1 = 0.9566
[Eval_batch(1)(2000,4000)] 2017-05-03 03:35:48.564351: step 40000, loss = 0.1174, acc = 0.9640, f1neg = 0.9578, f1pos = 0.9686, f1 = 0.9632
[Eval_batch(2)(2000,6000)] 2017-05-03 03:35:49.071628: step 40000, loss = 0.1298, acc = 0.9580, f1neg = 0.9552, f1pos = 0.9605, f1 = 0.9578
[Eval_batch(3)(2000,8000)] 2017-05-03 03:35:49.579889: step 40000, loss = 0.1355, acc = 0.9560, f1neg = 0.9558, f1pos = 0.9562, f1 = 0.9560
[Eval_batch(4)(2000,10000)] 2017-05-03 03:35:50.081034: step 40000, loss = 0.1373, acc = 0.9565, f1neg = 0.9575, f1pos = 0.9555, f1 = 0.9565
[Eval_batch(5)(2000,12000)] 2017-05-03 03:35:50.579000: step 40000, loss = 0.1326, acc = 0.9515, f1neg = 0.9476, f1pos = 0.9549, f1 = 0.9512
[Eval_batch(6)(2000,14000)] 2017-05-03 03:35:51.042894: step 40000, loss = 0.1244, acc = 0.9610, f1neg = 0.9602, f1pos = 0.9618, f1 = 0.9610
[Eval_batch(7)(2000,16000)] 2017-05-03 03:35:51.545097: step 40000, loss = 0.1217, acc = 0.9595, f1neg = 0.9512, f1pos = 0.9654, f1 = 0.9583
[Eval_batch(8)(2000,18000)] 2017-05-03 03:35:52.020531: step 40000, loss = 0.1173, acc = 0.9600, f1neg = 0.9541, f1pos = 0.9646, f1 = 0.9593
[Eval_batch(9)(2000,20000)] 2017-05-03 03:35:52.501924: step 40000, loss = 0.1211, acc = 0.9655, f1neg = 0.9630, f1pos = 0.9677, f1 = 0.9653
[Eval_batch(10)(2000,22000)] 2017-05-03 03:35:52.981737: step 40000, loss = 0.1303, acc = 0.9555, f1neg = 0.9521, f1pos = 0.9584, f1 = 0.9553
[Eval_batch(11)(2000,24000)] 2017-05-03 03:35:53.458439: step 40000, loss = 0.1295, acc = 0.9590, f1neg = 0.9524, f1pos = 0.9640, f1 = 0.9582
[Eval_batch(12)(2000,26000)] 2017-05-03 03:35:53.954151: step 40000, loss = 0.1270, acc = 0.9545, f1neg = 0.9530, f1pos = 0.9559, f1 = 0.9545
[Eval_batch(13)(2000,28000)] 2017-05-03 03:35:54.463273: step 40000, loss = 0.1155, acc = 0.9640, f1neg = 0.9547, f1pos = 0.9701, f1 = 0.9624
[Eval_batch(14)(2000,30000)] 2017-05-03 03:35:54.963208: step 40000, loss = 0.1427, acc = 0.9540, f1neg = 0.9449, f1pos = 0.9605, f1 = 0.9527
[Eval_batch(15)(2000,32000)] 2017-05-03 03:35:55.428496: step 40000, loss = 0.1191, acc = 0.9670, f1neg = 0.9642, f1pos = 0.9694, f1 = 0.9668
[Eval_batch(16)(2000,34000)] 2017-05-03 03:35:55.895695: step 40000, loss = 0.1153, acc = 0.9640, f1neg = 0.9625, f1pos = 0.9654, f1 = 0.9639
[Eval_batch(17)(2000,36000)] 2017-05-03 03:35:56.333792: step 40000, loss = 0.1409, acc = 0.9540, f1neg = 0.9541, f1pos = 0.9539, f1 = 0.9540
[Eval_batch(18)(2000,38000)] 2017-05-03 03:35:56.807248: step 40000, loss = 0.1376, acc = 0.9545, f1neg = 0.9561, f1pos = 0.9528, f1 = 0.9544
[Eval_batch(19)(2000,40000)] 2017-05-03 03:35:57.317812: step 40000, loss = 0.1183, acc = 0.9680, f1neg = 0.9629, f1pos = 0.9719, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 03:35:57.790711: step 40000, loss = 0.1222, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9565, f1 = 0.9614
[Eval_batch(21)(2000,44000)] 2017-05-03 03:35:58.263271: step 40000, loss = 0.1119, acc = 0.9640, f1neg = 0.9607, f1pos = 0.9668, f1 = 0.9637
[Eval_batch(22)(2000,46000)] 2017-05-03 03:35:58.771973: step 40000, loss = 0.1207, acc = 0.9590, f1neg = 0.9592, f1pos = 0.9588, f1 = 0.9590
[Eval_batch(23)(2000,48000)] 2017-05-03 03:35:59.231057: step 40000, loss = 0.1260, acc = 0.9580, f1neg = 0.9523, f1pos = 0.9625, f1 = 0.9574
[Eval_batch(24)(2000,50000)] 2017-05-03 03:35:59.737910: step 40000, loss = 0.1120, acc = 0.9650, f1neg = 0.9597, f1pos = 0.9691, f1 = 0.9644
[Eval_batch(25)(2000,52000)] 2017-05-03 03:36:00.175736: step 40000, loss = 0.1035, acc = 0.9695, f1neg = 0.9664, f1pos = 0.9721, f1 = 0.9692
[Eval_batch(26)(2000,54000)] 2017-05-03 03:36:00.681620: step 40000, loss = 0.1187, acc = 0.9565, f1neg = 0.9587, f1pos = 0.9541, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-03 03:36:01.280804: step 40000, loss = 0.1056, acc = 0.9655, f1neg = 0.9635, f1pos = 0.9673, f1 = 0.9654
[Eval] 2017-05-03 03:36:01.280894: step 40000, acc = 0.9601, f1 = 0.9597
[Test_batch(0)(2000,2000)] 2017-05-03 03:36:01.789142: step 40000, loss = 0.1538, acc = 0.9490, f1neg = 0.9513, f1pos = 0.9464, f1 = 0.9489
[Test_batch(1)(2000,4000)] 2017-05-03 03:36:02.284230: step 40000, loss = 0.1354, acc = 0.9550, f1neg = 0.9599, f1pos = 0.9488, f1 = 0.9543
[Test_batch(2)(2000,6000)] 2017-05-03 03:36:02.758290: step 40000, loss = 0.1515, acc = 0.9515, f1neg = 0.9553, f1pos = 0.9470, f1 = 0.9511
[Test_batch(3)(2000,8000)] 2017-05-03 03:36:03.256945: step 40000, loss = 0.1574, acc = 0.9480, f1neg = 0.9511, f1pos = 0.9444, f1 = 0.9478
[Test_batch(4)(2000,10000)] 2017-05-03 03:36:03.768562: step 40000, loss = 0.1555, acc = 0.9450, f1neg = 0.9449, f1pos = 0.9451, f1 = 0.9450
[Test_batch(5)(2000,12000)] 2017-05-03 03:36:04.245758: step 40000, loss = 0.1658, acc = 0.9400, f1neg = 0.9403, f1pos = 0.9397, f1 = 0.9400
[Test_batch(6)(2000,14000)] 2017-05-03 03:36:04.722561: step 40000, loss = 0.1520, acc = 0.9440, f1neg = 0.9418, f1pos = 0.9461, f1 = 0.9439
[Test_batch(7)(2000,16000)] 2017-05-03 03:36:05.192286: step 40000, loss = 0.1421, acc = 0.9535, f1neg = 0.9576, f1pos = 0.9485, f1 = 0.9531
[Test_batch(8)(2000,18000)] 2017-05-03 03:36:05.670646: step 40000, loss = 0.1423, acc = 0.9475, f1neg = 0.9468, f1pos = 0.9481, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 03:36:06.177661: step 40000, loss = 0.1726, acc = 0.9375, f1neg = 0.9337, f1pos = 0.9409, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 03:36:06.660503: step 40000, loss = 0.1479, acc = 0.9510, f1neg = 0.9446, f1pos = 0.9561, f1 = 0.9503
[Test_batch(11)(2000,24000)] 2017-05-03 03:36:07.141097: step 40000, loss = 0.1510, acc = 0.9465, f1neg = 0.9478, f1pos = 0.9451, f1 = 0.9465
[Test_batch(12)(2000,26000)] 2017-05-03 03:36:07.620725: step 40000, loss = 0.1264, acc = 0.9605, f1neg = 0.9620, f1pos = 0.9589, f1 = 0.9604
[Test_batch(13)(2000,28000)] 2017-05-03 03:36:08.126061: step 40000, loss = 0.1525, acc = 0.9460, f1neg = 0.9397, f1pos = 0.9511, f1 = 0.9454
[Test_batch(14)(2000,30000)] 2017-05-03 03:36:08.604988: step 40000, loss = 0.1253, acc = 0.9620, f1neg = 0.9562, f1pos = 0.9664, f1 = 0.9613
[Test_batch(15)(2000,32000)] 2017-05-03 03:36:09.114207: step 40000, loss = 0.1488, acc = 0.9535, f1neg = 0.9529, f1pos = 0.9541, f1 = 0.9535
[Test_batch(16)(2000,34000)] 2017-05-03 03:36:09.597409: step 40000, loss = 0.1315, acc = 0.9565, f1neg = 0.9552, f1pos = 0.9577, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 03:36:10.101090: step 40000, loss = 0.1243, acc = 0.9660, f1neg = 0.9627, f1pos = 0.9688, f1 = 0.9657
[Test_batch(18)(2000,38000)] 2017-05-03 03:36:10.646549: step 40000, loss = 0.1175, acc = 0.9610, f1neg = 0.9601, f1pos = 0.9619, f1 = 0.9610
[Test] 2017-05-03 03:36:10.646644: step 40000, acc = 0.9513, f1 = 0.9510
[Status] 2017-05-03 03:36:10.646670: step 40000, maxindex = 37000, maxdev = 0.9606, maxtst = 0.9522
2017-05-03 03:36:20.072671: step 40010, loss = 0.1516, acc = 0.9460 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 03:36:29.146932: step 40020, loss = 0.1211, acc = 0.9640 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 03:36:38.284048: step 40030, loss = 0.1755, acc = 0.9500 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 03:36:47.352349: step 40040, loss = 0.1198, acc = 0.9580 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 03:36:56.516721: step 40050, loss = 0.1492, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 03:37:05.615819: step 40060, loss = 0.1479, acc = 0.9480 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 03:37:14.887585: step 40070, loss = 0.1167, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 03:37:24.201914: step 40080, loss = 0.1314, acc = 0.9720 (261.1 examples/sec; 0.245 sec/batch)
2017-05-03 03:37:33.242808: step 40090, loss = 0.1210, acc = 0.9620 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 03:37:42.304871: step 40100, loss = 0.1118, acc = 0.9720 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 03:37:51.251947: step 40110, loss = 0.1456, acc = 0.9560 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 03:38:00.384716: step 40120, loss = 0.1527, acc = 0.9460 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 03:38:09.420984: step 40130, loss = 0.1161, acc = 0.9520 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 03:38:18.361117: step 40140, loss = 0.1253, acc = 0.9580 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 03:38:27.395567: step 40150, loss = 0.1543, acc = 0.9540 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 03:38:36.350675: step 40160, loss = 0.1619, acc = 0.9500 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 03:38:45.353852: step 40170, loss = 0.1250, acc = 0.9600 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 03:38:54.725700: step 40180, loss = 0.1110, acc = 0.9700 (200.2 examples/sec; 0.320 sec/batch)
2017-05-03 03:39:03.741274: step 40190, loss = 0.1350, acc = 0.9560 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 03:39:12.864061: step 40200, loss = 0.1318, acc = 0.9500 (262.2 examples/sec; 0.244 sec/batch)
2017-05-03 03:39:21.984339: step 40210, loss = 0.1615, acc = 0.9560 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 03:39:31.050394: step 40220, loss = 0.1506, acc = 0.9540 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 03:39:40.137414: step 40230, loss = 0.1297, acc = 0.9660 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 03:39:49.279763: step 40240, loss = 0.1301, acc = 0.9660 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 03:39:58.255016: step 40250, loss = 0.1158, acc = 0.9700 (298.4 examples/sec; 0.214 sec/batch)
2017-05-03 03:40:07.307831: step 40260, loss = 0.1374, acc = 0.9580 (297.8 examples/sec; 0.215 sec/batch)
2017-05-03 03:40:16.488767: step 40270, loss = 0.1478, acc = 0.9540 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 03:40:25.578745: step 40280, loss = 0.1533, acc = 0.9460 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 03:40:34.901943: step 40290, loss = 0.1262, acc = 0.9440 (263.2 examples/sec; 0.243 sec/batch)
2017-05-03 03:40:44.036792: step 40300, loss = 0.1469, acc = 0.9580 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:40:52.976000: step 40310, loss = 0.1529, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 03:41:02.946932: step 40320, loss = 0.1360, acc = 0.9540 (296.9 examples/sec; 0.216 sec/batch)
2017-05-03 03:41:11.913685: step 40330, loss = 0.1391, acc = 0.9500 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 03:41:20.946496: step 40340, loss = 0.1460, acc = 0.9580 (296.8 examples/sec; 0.216 sec/batch)
2017-05-03 03:41:30.133538: step 40350, loss = 0.1687, acc = 0.9380 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:41:39.204698: step 40360, loss = 0.1259, acc = 0.9620 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:41:48.225437: step 40370, loss = 0.1432, acc = 0.9620 (260.1 examples/sec; 0.246 sec/batch)
2017-05-03 03:41:57.405293: step 40380, loss = 0.1380, acc = 0.9540 (267.3 examples/sec; 0.239 sec/batch)
2017-05-03 03:42:06.670748: step 40390, loss = 0.1443, acc = 0.9560 (265.1 examples/sec; 0.241 sec/batch)
2017-05-03 03:42:15.783746: step 40400, loss = 0.1660, acc = 0.9440 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 03:42:24.854408: step 40410, loss = 0.1382, acc = 0.9520 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 03:42:34.277924: step 40420, loss = 0.1350, acc = 0.9580 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 03:42:43.475211: step 40430, loss = 0.1335, acc = 0.9600 (254.4 examples/sec; 0.252 sec/batch)
2017-05-03 03:42:52.984105: step 40440, loss = 0.1194, acc = 0.9700 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 03:43:02.089861: step 40450, loss = 0.1260, acc = 0.9540 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 03:43:11.346691: step 40460, loss = 0.1455, acc = 0.9520 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 03:43:20.764113: step 40470, loss = 0.1533, acc = 0.9420 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 03:43:30.137585: step 40480, loss = 0.1385, acc = 0.9660 (260.9 examples/sec; 0.245 sec/batch)
2017-05-03 03:43:39.223321: step 40490, loss = 0.1759, acc = 0.9300 (291.6 examples/sec; 0.220 sec/batch)
2017-05-03 03:43:48.472213: step 40500, loss = 0.1307, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 03:43:57.700469: step 40510, loss = 0.1394, acc = 0.9560 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 03:44:06.877922: step 40520, loss = 0.1306, acc = 0.9520 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 03:44:16.025798: step 40530, loss = 0.1512, acc = 0.9460 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 03:44:25.147865: step 40540, loss = 0.1077, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 03:44:34.065076: step 40550, loss = 0.1255, acc = 0.9580 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 03:44:43.247880: step 40560, loss = 0.1221, acc = 0.9660 (265.9 examples/sec; 0.241 sec/batch)
2017-05-03 03:44:52.306844: step 40570, loss = 0.1646, acc = 0.9420 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 03:45:01.771713: step 40580, loss = 0.1335, acc = 0.9480 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 03:45:10.965490: step 40590, loss = 0.1256, acc = 0.9580 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 03:45:20.049883: step 40600, loss = 0.1450, acc = 0.9500 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 03:45:29.279432: step 40610, loss = 0.1935, acc = 0.9360 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 03:45:38.391221: step 40620, loss = 0.1474, acc = 0.9440 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 03:45:47.483669: step 40630, loss = 0.1357, acc = 0.9520 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 03:45:56.660744: step 40640, loss = 0.1488, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 03:46:05.635493: step 40650, loss = 0.1346, acc = 0.9600 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 03:46:14.688610: step 40660, loss = 0.1217, acc = 0.9600 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 03:46:23.688307: step 40670, loss = 0.1397, acc = 0.9540 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 03:46:32.852693: step 40680, loss = 0.1079, acc = 0.9680 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 03:46:41.881067: step 40690, loss = 0.1410, acc = 0.9500 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 03:46:51.002988: step 40700, loss = 0.1146, acc = 0.9700 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 03:47:00.571612: step 40710, loss = 0.1345, acc = 0.9420 (256.0 examples/sec; 0.250 sec/batch)
2017-05-03 03:47:09.961516: step 40720, loss = 0.1197, acc = 0.9700 (194.7 examples/sec; 0.329 sec/batch)
2017-05-03 03:47:19.002311: step 40730, loss = 0.1479, acc = 0.9600 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 03:47:28.333627: step 40740, loss = 0.1016, acc = 0.9720 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 03:47:37.891478: step 40750, loss = 0.1233, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 03:47:47.142244: step 40760, loss = 0.1235, acc = 0.9680 (236.9 examples/sec; 0.270 sec/batch)
2017-05-03 03:47:56.212991: step 40770, loss = 0.1244, acc = 0.9640 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 03:48:05.252691: step 40780, loss = 0.1393, acc = 0.9560 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 03:48:14.315323: step 40790, loss = 0.1442, acc = 0.9500 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:48:23.418474: step 40800, loss = 0.1322, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 03:48:32.697230: step 40810, loss = 0.1271, acc = 0.9620 (261.6 examples/sec; 0.245 sec/batch)
2017-05-03 03:48:41.935710: step 40820, loss = 0.1143, acc = 0.9680 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 03:48:51.199857: step 40830, loss = 0.1554, acc = 0.9440 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 03:49:00.585721: step 40840, loss = 0.1482, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 03:49:09.665121: step 40850, loss = 0.1194, acc = 0.9560 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 03:49:18.815161: step 40860, loss = 0.1689, acc = 0.9380 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 03:49:27.848418: step 40870, loss = 0.1320, acc = 0.9580 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 03:49:36.760643: step 40880, loss = 0.1403, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 03:49:45.793413: step 40890, loss = 0.1243, acc = 0.9560 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 03:49:54.872158: step 40900, loss = 0.1263, acc = 0.9600 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 03:50:03.870574: step 40910, loss = 0.1522, acc = 0.9440 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:50:13.136051: step 40920, loss = 0.1433, acc = 0.9500 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 03:50:22.157702: step 40930, loss = 0.1318, acc = 0.9540 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 03:50:31.066141: step 40940, loss = 0.1503, acc = 0.9520 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 03:50:40.163438: step 40950, loss = 0.1425, acc = 0.9480 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 03:50:49.099093: step 40960, loss = 0.1350, acc = 0.9640 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 03:50:58.006000: step 40970, loss = 0.1371, acc = 0.9620 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 03:51:07.174290: step 40980, loss = 0.1207, acc = 0.9640 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 03:51:16.342060: step 40990, loss = 0.1515, acc = 0.9600 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 03:51:25.396409: step 41000, loss = 0.1155, acc = 0.9680 (298.1 examples/sec; 0.215 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 03:51:25.869158: step 41000, loss = 0.1215, acc = 0.9570, f1neg = 0.9524, f1pos = 0.9608, f1 = 0.9566
[Eval_batch(1)(2000,4000)] 2017-05-03 03:51:26.334692: step 41000, loss = 0.1183, acc = 0.9605, f1neg = 0.9533, f1pos = 0.9658, f1 = 0.9595
[Eval_batch(2)(2000,6000)] 2017-05-03 03:51:26.833165: step 41000, loss = 0.1341, acc = 0.9545, f1neg = 0.9509, f1pos = 0.9576, f1 = 0.9543
[Eval_batch(3)(2000,8000)] 2017-05-03 03:51:27.343577: step 41000, loss = 0.1418, acc = 0.9515, f1neg = 0.9510, f1pos = 0.9520, f1 = 0.9515
[Eval_batch(4)(2000,10000)] 2017-05-03 03:51:27.846395: step 41000, loss = 0.1430, acc = 0.9525, f1neg = 0.9531, f1pos = 0.9518, f1 = 0.9525
[Eval_batch(5)(2000,12000)] 2017-05-03 03:51:28.308036: step 41000, loss = 0.1372, acc = 0.9510, f1neg = 0.9465, f1pos = 0.9548, f1 = 0.9507
[Eval_batch(6)(2000,14000)] 2017-05-03 03:51:28.788473: step 41000, loss = 0.1295, acc = 0.9620, f1neg = 0.9609, f1pos = 0.9631, f1 = 0.9620
[Eval_batch(7)(2000,16000)] 2017-05-03 03:51:29.255535: step 41000, loss = 0.1241, acc = 0.9580, f1neg = 0.9490, f1pos = 0.9643, f1 = 0.9567
[Eval_batch(8)(2000,18000)] 2017-05-03 03:51:29.720989: step 41000, loss = 0.1208, acc = 0.9590, f1neg = 0.9526, f1pos = 0.9639, f1 = 0.9582
[Eval_batch(9)(2000,20000)] 2017-05-03 03:51:30.214760: step 41000, loss = 0.1249, acc = 0.9610, f1neg = 0.9577, f1pos = 0.9638, f1 = 0.9608
[Eval_batch(10)(2000,22000)] 2017-05-03 03:51:30.714323: step 41000, loss = 0.1347, acc = 0.9525, f1neg = 0.9485, f1pos = 0.9559, f1 = 0.9522
[Eval_batch(11)(2000,24000)] 2017-05-03 03:51:31.189501: step 41000, loss = 0.1322, acc = 0.9560, f1neg = 0.9485, f1pos = 0.9616, f1 = 0.9551
[Eval_batch(12)(2000,26000)] 2017-05-03 03:51:31.662726: step 41000, loss = 0.1310, acc = 0.9540, f1neg = 0.9520, f1pos = 0.9559, f1 = 0.9539
[Eval_batch(13)(2000,28000)] 2017-05-03 03:51:32.168281: step 41000, loss = 0.1183, acc = 0.9600, f1neg = 0.9492, f1pos = 0.9670, f1 = 0.9581
[Eval_batch(14)(2000,30000)] 2017-05-03 03:51:32.673396: step 41000, loss = 0.1460, acc = 0.9485, f1neg = 0.9372, f1pos = 0.9563, f1 = 0.9468
[Eval_batch(15)(2000,32000)] 2017-05-03 03:51:33.146008: step 41000, loss = 0.1221, acc = 0.9670, f1neg = 0.9641, f1pos = 0.9695, f1 = 0.9668
[Eval_batch(16)(2000,34000)] 2017-05-03 03:51:33.624781: step 41000, loss = 0.1182, acc = 0.9620, f1neg = 0.9600, f1pos = 0.9638, f1 = 0.9619
[Eval_batch(17)(2000,36000)] 2017-05-03 03:51:34.131991: step 41000, loss = 0.1449, acc = 0.9530, f1neg = 0.9528, f1pos = 0.9532, f1 = 0.9530
[Eval_batch(18)(2000,38000)] 2017-05-03 03:51:34.586823: step 41000, loss = 0.1436, acc = 0.9505, f1neg = 0.9519, f1pos = 0.9490, f1 = 0.9505
[Eval_batch(19)(2000,40000)] 2017-05-03 03:51:35.061946: step 41000, loss = 0.1211, acc = 0.9655, f1neg = 0.9598, f1pos = 0.9698, f1 = 0.9648
[Eval_batch(20)(2000,42000)] 2017-05-03 03:51:35.547329: step 41000, loss = 0.1294, acc = 0.9600, f1neg = 0.9642, f1pos = 0.9546, f1 = 0.9594
[Eval_batch(21)(2000,44000)] 2017-05-03 03:51:36.013274: step 41000, loss = 0.1120, acc = 0.9645, f1neg = 0.9609, f1pos = 0.9675, f1 = 0.9642
[Eval_batch(22)(2000,46000)] 2017-05-03 03:51:36.476836: step 41000, loss = 0.1240, acc = 0.9570, f1neg = 0.9570, f1pos = 0.9570, f1 = 0.9570
[Eval_batch(23)(2000,48000)] 2017-05-03 03:51:36.951368: step 41000, loss = 0.1279, acc = 0.9575, f1neg = 0.9515, f1pos = 0.9622, f1 = 0.9568
[Eval_batch(24)(2000,50000)] 2017-05-03 03:51:37.423277: step 41000, loss = 0.1148, acc = 0.9640, f1neg = 0.9582, f1pos = 0.9684, f1 = 0.9633
[Eval_batch(25)(2000,52000)] 2017-05-03 03:51:37.900620: step 41000, loss = 0.1068, acc = 0.9660, f1neg = 0.9622, f1pos = 0.9691, f1 = 0.9657
[Eval_batch(26)(2000,54000)] 2017-05-03 03:51:38.404186: step 41000, loss = 0.1233, acc = 0.9575, f1neg = 0.9594, f1pos = 0.9554, f1 = 0.9574
[Eval_batch(27)(2000,56000)] 2017-05-03 03:51:38.992504: step 41000, loss = 0.1101, acc = 0.9660, f1neg = 0.9640, f1pos = 0.9678, f1 = 0.9659
[Eval] 2017-05-03 03:51:38.992596: step 41000, acc = 0.9582, f1 = 0.9577
[Test_batch(0)(2000,2000)] 2017-05-03 03:51:39.475048: step 41000, loss = 0.1591, acc = 0.9460, f1neg = 0.9481, f1pos = 0.9437, f1 = 0.9459
[Test_batch(1)(2000,4000)] 2017-05-03 03:51:39.981852: step 41000, loss = 0.1418, acc = 0.9490, f1neg = 0.9540, f1pos = 0.9428, f1 = 0.9484
[Test_batch(2)(2000,6000)] 2017-05-03 03:51:40.458044: step 41000, loss = 0.1588, acc = 0.9500, f1neg = 0.9535, f1pos = 0.9459, f1 = 0.9497
[Test_batch(3)(2000,8000)] 2017-05-03 03:51:40.966619: step 41000, loss = 0.1637, acc = 0.9410, f1neg = 0.9440, f1pos = 0.9377, f1 = 0.9408
[Test_batch(4)(2000,10000)] 2017-05-03 03:51:41.447467: step 41000, loss = 0.1596, acc = 0.9410, f1neg = 0.9402, f1pos = 0.9418, f1 = 0.9410
[Test_batch(5)(2000,12000)] 2017-05-03 03:51:41.933888: step 41000, loss = 0.1710, acc = 0.9335, f1neg = 0.9331, f1pos = 0.9339, f1 = 0.9335
[Test_batch(6)(2000,14000)] 2017-05-03 03:51:42.437202: step 41000, loss = 0.1587, acc = 0.9390, f1neg = 0.9359, f1pos = 0.9418, f1 = 0.9389
[Test_batch(7)(2000,16000)] 2017-05-03 03:51:42.942826: step 41000, loss = 0.1471, acc = 0.9495, f1neg = 0.9535, f1pos = 0.9448, f1 = 0.9491
[Test_batch(8)(2000,18000)] 2017-05-03 03:51:43.426637: step 41000, loss = 0.1463, acc = 0.9475, f1neg = 0.9465, f1pos = 0.9485, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 03:51:43.928009: step 41000, loss = 0.1743, acc = 0.9350, f1neg = 0.9304, f1pos = 0.9390, f1 = 0.9347
[Test_batch(10)(2000,22000)] 2017-05-03 03:51:44.427268: step 41000, loss = 0.1490, acc = 0.9475, f1neg = 0.9400, f1pos = 0.9533, f1 = 0.9467
[Test_batch(11)(2000,24000)] 2017-05-03 03:51:44.936486: step 41000, loss = 0.1557, acc = 0.9450, f1neg = 0.9458, f1pos = 0.9442, f1 = 0.9450
[Test_batch(12)(2000,26000)] 2017-05-03 03:51:45.432199: step 41000, loss = 0.1287, acc = 0.9560, f1neg = 0.9572, f1pos = 0.9547, f1 = 0.9560
[Test_batch(13)(2000,28000)] 2017-05-03 03:51:45.946842: step 41000, loss = 0.1548, acc = 0.9420, f1neg = 0.9342, f1pos = 0.9481, f1 = 0.9412
[Test_batch(14)(2000,30000)] 2017-05-03 03:51:46.428620: step 41000, loss = 0.1262, acc = 0.9585, f1neg = 0.9517, f1pos = 0.9636, f1 = 0.9577
[Test_batch(15)(2000,32000)] 2017-05-03 03:51:46.933215: step 41000, loss = 0.1522, acc = 0.9490, f1neg = 0.9478, f1pos = 0.9501, f1 = 0.9490
[Test_batch(16)(2000,34000)] 2017-05-03 03:51:47.434079: step 41000, loss = 0.1351, acc = 0.9545, f1neg = 0.9527, f1pos = 0.9562, f1 = 0.9544
[Test_batch(17)(2000,36000)] 2017-05-03 03:51:47.929553: step 41000, loss = 0.1284, acc = 0.9630, f1neg = 0.9591, f1pos = 0.9662, f1 = 0.9627
[Test_batch(18)(2000,38000)] 2017-05-03 03:51:48.493030: step 41000, loss = 0.1231, acc = 0.9580, f1neg = 0.9566, f1pos = 0.9593, f1 = 0.9580
[Test] 2017-05-03 03:51:48.493117: step 41000, acc = 0.9476, f1 = 0.9474
[Status] 2017-05-03 03:51:48.493142: step 41000, maxindex = 37000, maxdev = 0.9606, maxtst = 0.9522
2017-05-03 03:51:57.530461: step 41010, loss = 0.1339, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 03:52:06.511056: step 41020, loss = 0.1454, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 03:52:15.470671: step 41030, loss = 0.1583, acc = 0.9380 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 03:52:24.560574: step 41040, loss = 0.1199, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:52:33.478084: step 41050, loss = 0.1210, acc = 0.9600 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 03:52:42.560372: step 41060, loss = 0.1345, acc = 0.9520 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 03:52:51.588088: step 41070, loss = 0.1435, acc = 0.9580 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 03:53:00.962825: step 41080, loss = 0.1620, acc = 0.9540 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 03:53:10.106923: step 41090, loss = 0.1382, acc = 0.9560 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 03:53:19.078281: step 41100, loss = 0.1303, acc = 0.9600 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 03:53:28.197172: step 41110, loss = 0.1262, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 03:53:37.250159: step 41120, loss = 0.1272, acc = 0.9600 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 03:53:46.214413: step 41130, loss = 0.1265, acc = 0.9640 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 03:53:55.609196: step 41140, loss = 0.1152, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 03:54:04.553286: step 41150, loss = 0.1273, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 03:54:13.634918: step 41160, loss = 0.1807, acc = 0.9440 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 03:54:22.740450: step 41170, loss = 0.1445, acc = 0.9440 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 03:54:31.818595: step 41180, loss = 0.1282, acc = 0.9660 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 03:54:40.772437: step 41190, loss = 0.1232, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 03:54:49.966455: step 41200, loss = 0.1314, acc = 0.9580 (244.3 examples/sec; 0.262 sec/batch)
2017-05-03 03:54:59.346898: step 41210, loss = 0.1116, acc = 0.9680 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 03:55:08.505619: step 41220, loss = 0.1360, acc = 0.9600 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 03:55:17.572933: step 41230, loss = 0.1273, acc = 0.9560 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 03:55:26.592802: step 41240, loss = 0.1837, acc = 0.9380 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 03:55:35.669754: step 41250, loss = 0.1496, acc = 0.9440 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 03:55:44.628599: step 41260, loss = 0.1298, acc = 0.9560 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 03:55:53.688227: step 41270, loss = 0.1189, acc = 0.9600 (257.2 examples/sec; 0.249 sec/batch)
2017-05-03 03:56:02.675292: step 41280, loss = 0.1515, acc = 0.9520 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 03:56:11.704008: step 41290, loss = 0.1532, acc = 0.9440 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 03:56:20.774050: step 41300, loss = 0.1366, acc = 0.9540 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 03:56:30.226963: step 41310, loss = 0.1215, acc = 0.9680 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:56:39.208849: step 41320, loss = 0.1267, acc = 0.9600 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 03:56:49.387784: step 41330, loss = 0.1310, acc = 0.9580 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 03:56:58.426373: step 41340, loss = 0.1186, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 03:57:07.500906: step 41350, loss = 0.1249, acc = 0.9680 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 03:57:16.600800: step 41360, loss = 0.1370, acc = 0.9640 (266.4 examples/sec; 0.240 sec/batch)
2017-05-03 03:57:25.762470: step 41370, loss = 0.1491, acc = 0.9380 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 03:57:35.710542: step 41380, loss = 0.1100, acc = 0.9700 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 03:57:44.703330: step 41390, loss = 0.1139, acc = 0.9720 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 03:57:53.833057: step 41400, loss = 0.1288, acc = 0.9560 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 03:58:02.658631: step 41410, loss = 0.1434, acc = 0.9580 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 03:58:11.842052: step 41420, loss = 0.1114, acc = 0.9760 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 03:58:20.902072: step 41430, loss = 0.1344, acc = 0.9560 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 03:58:30.074819: step 41440, loss = 0.1199, acc = 0.9700 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 03:58:39.112708: step 41450, loss = 0.1448, acc = 0.9500 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 03:58:48.166543: step 41460, loss = 0.1171, acc = 0.9600 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 03:58:57.309736: step 41470, loss = 0.1460, acc = 0.9500 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 03:59:06.591117: step 41480, loss = 0.1337, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:59:15.640589: step 41490, loss = 0.1341, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 03:59:24.680344: step 41500, loss = 0.1171, acc = 0.9660 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 03:59:33.836431: step 41510, loss = 0.1416, acc = 0.9540 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 03:59:42.848546: step 41520, loss = 0.1358, acc = 0.9520 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 03:59:51.891460: step 41530, loss = 0.1391, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 04:00:00.944343: step 41540, loss = 0.1371, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 04:00:09.935547: step 41550, loss = 0.1382, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 04:00:18.967363: step 41560, loss = 0.1594, acc = 0.9560 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 04:00:28.082133: step 41570, loss = 0.1146, acc = 0.9720 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 04:00:37.054381: step 41580, loss = 0.1137, acc = 0.9600 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 04:00:45.991259: step 41590, loss = 0.1384, acc = 0.9420 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 04:00:55.275968: step 41600, loss = 0.1332, acc = 0.9560 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 04:01:04.520344: step 41610, loss = 0.1675, acc = 0.9360 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 04:01:13.496939: step 41620, loss = 0.1498, acc = 0.9460 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 04:01:22.503786: step 41630, loss = 0.1328, acc = 0.9600 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 04:01:31.570167: step 41640, loss = 0.1396, acc = 0.9540 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 04:01:41.027655: step 41650, loss = 0.1136, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 04:01:50.056624: step 41660, loss = 0.1335, acc = 0.9600 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 04:01:59.131672: step 41670, loss = 0.1136, acc = 0.9660 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 04:02:08.398217: step 41680, loss = 0.1397, acc = 0.9500 (259.0 examples/sec; 0.247 sec/batch)
2017-05-03 04:02:17.383507: step 41690, loss = 0.1443, acc = 0.9440 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 04:02:26.560376: step 41700, loss = 0.1399, acc = 0.9320 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 04:02:35.557651: step 41710, loss = 0.1213, acc = 0.9580 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 04:02:44.624333: step 41720, loss = 0.1400, acc = 0.9500 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 04:02:53.616583: step 41730, loss = 0.1565, acc = 0.9440 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 04:03:02.688790: step 41740, loss = 0.1472, acc = 0.9500 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 04:03:11.808381: step 41750, loss = 0.1256, acc = 0.9560 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 04:03:20.943263: step 41760, loss = 0.1530, acc = 0.9580 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 04:03:30.087704: step 41770, loss = 0.1604, acc = 0.9420 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 04:03:39.167698: step 41780, loss = 0.1750, acc = 0.9380 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 04:03:48.248520: step 41790, loss = 0.1408, acc = 0.9540 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 04:03:57.294126: step 41800, loss = 0.1153, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 04:04:06.462427: step 41810, loss = 0.1316, acc = 0.9560 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 04:04:15.764809: step 41820, loss = 0.1358, acc = 0.9520 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 04:04:24.761711: step 41830, loss = 0.1710, acc = 0.9480 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 04:04:34.132395: step 41840, loss = 0.1178, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 04:04:43.272539: step 41850, loss = 0.1183, acc = 0.9640 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 04:04:52.346786: step 41860, loss = 0.1308, acc = 0.9620 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 04:05:01.531198: step 41870, loss = 0.1182, acc = 0.9620 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 04:05:10.652269: step 41880, loss = 0.1374, acc = 0.9500 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 04:05:19.706778: step 41890, loss = 0.1151, acc = 0.9600 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 04:05:28.830906: step 41900, loss = 0.1384, acc = 0.9580 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 04:05:37.866154: step 41910, loss = 0.1232, acc = 0.9560 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 04:05:46.977651: step 41920, loss = 0.1424, acc = 0.9500 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 04:05:56.112775: step 41930, loss = 0.1246, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 04:06:05.303349: step 41940, loss = 0.1562, acc = 0.9480 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 04:06:14.613724: step 41950, loss = 0.1526, acc = 0.9460 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 04:06:23.848322: step 41960, loss = 0.1278, acc = 0.9480 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 04:06:33.079801: step 41970, loss = 0.0837, acc = 0.9740 (294.9 examples/sec; 0.217 sec/batch)
2017-05-03 04:06:42.209255: step 41980, loss = 0.1248, acc = 0.9580 (264.0 examples/sec; 0.242 sec/batch)
2017-05-03 04:06:51.176566: step 41990, loss = 0.1278, acc = 0.9600 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 04:07:00.447128: step 42000, loss = 0.1584, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 04:07:00.921689: step 42000, loss = 0.1136, acc = 0.9595, f1neg = 0.9556, f1pos = 0.9628, f1 = 0.9592
[Eval_batch(1)(2000,4000)] 2017-05-03 04:07:01.430079: step 42000, loss = 0.1169, acc = 0.9660, f1neg = 0.9605, f1pos = 0.9702, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 04:07:01.933377: step 42000, loss = 0.1266, acc = 0.9585, f1neg = 0.9559, f1pos = 0.9608, f1 = 0.9584
[Eval_batch(3)(2000,8000)] 2017-05-03 04:07:02.410139: step 42000, loss = 0.1324, acc = 0.9565, f1neg = 0.9565, f1pos = 0.9565, f1 = 0.9565
[Eval_batch(4)(2000,10000)] 2017-05-03 04:07:02.916164: step 42000, loss = 0.1343, acc = 0.9565, f1neg = 0.9578, f1pos = 0.9551, f1 = 0.9565
[Eval_batch(5)(2000,12000)] 2017-05-03 04:07:03.425944: step 42000, loss = 0.1305, acc = 0.9540, f1neg = 0.9507, f1pos = 0.9568, f1 = 0.9538
[Eval_batch(6)(2000,14000)] 2017-05-03 04:07:03.930297: step 42000, loss = 0.1203, acc = 0.9635, f1neg = 0.9629, f1pos = 0.9641, f1 = 0.9635
[Eval_batch(7)(2000,16000)] 2017-05-03 04:07:04.378666: step 42000, loss = 0.1206, acc = 0.9600, f1neg = 0.9522, f1pos = 0.9656, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 04:07:04.885762: step 42000, loss = 0.1147, acc = 0.9610, f1neg = 0.9556, f1pos = 0.9652, f1 = 0.9604
[Eval_batch(9)(2000,20000)] 2017-05-03 04:07:05.396351: step 42000, loss = 0.1193, acc = 0.9635, f1neg = 0.9611, f1pos = 0.9656, f1 = 0.9634
[Eval_batch(10)(2000,22000)] 2017-05-03 04:07:05.876696: step 42000, loss = 0.1281, acc = 0.9585, f1neg = 0.9557, f1pos = 0.9610, f1 = 0.9583
[Eval_batch(11)(2000,24000)] 2017-05-03 04:07:06.349615: step 42000, loss = 0.1280, acc = 0.9590, f1neg = 0.9527, f1pos = 0.9638, f1 = 0.9583
[Eval_batch(12)(2000,26000)] 2017-05-03 04:07:06.823704: step 42000, loss = 0.1252, acc = 0.9525, f1neg = 0.9512, f1pos = 0.9537, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-03 04:07:07.299829: step 42000, loss = 0.1132, acc = 0.9650, f1neg = 0.9564, f1pos = 0.9708, f1 = 0.9636
[Eval_batch(14)(2000,30000)] 2017-05-03 04:07:07.812301: step 42000, loss = 0.1409, acc = 0.9565, f1neg = 0.9482, f1pos = 0.9625, f1 = 0.9553
[Eval_batch(15)(2000,32000)] 2017-05-03 04:07:08.287426: step 42000, loss = 0.1180, acc = 0.9680, f1neg = 0.9654, f1pos = 0.9702, f1 = 0.9678
[Eval_batch(16)(2000,34000)] 2017-05-03 04:07:08.787197: step 42000, loss = 0.1142, acc = 0.9660, f1neg = 0.9646, f1pos = 0.9673, f1 = 0.9659
[Eval_batch(17)(2000,36000)] 2017-05-03 04:07:09.264013: step 42000, loss = 0.1390, acc = 0.9560, f1neg = 0.9562, f1pos = 0.9558, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 04:07:09.727148: step 42000, loss = 0.1341, acc = 0.9565, f1neg = 0.9582, f1pos = 0.9546, f1 = 0.9564
[Eval_batch(19)(2000,40000)] 2017-05-03 04:07:10.221186: step 42000, loss = 0.1164, acc = 0.9680, f1neg = 0.9630, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 04:07:10.671474: step 42000, loss = 0.1179, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9564, f1 = 0.9614
[Eval_batch(21)(2000,44000)] 2017-05-03 04:07:11.145879: step 42000, loss = 0.1118, acc = 0.9620, f1neg = 0.9587, f1pos = 0.9648, f1 = 0.9617
[Eval_batch(22)(2000,46000)] 2017-05-03 04:07:11.618284: step 42000, loss = 0.1187, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9590, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 04:07:12.091700: step 42000, loss = 0.1247, acc = 0.9585, f1neg = 0.9532, f1pos = 0.9627, f1 = 0.9580
[Eval_batch(24)(2000,50000)] 2017-05-03 04:07:12.551551: step 42000, loss = 0.1104, acc = 0.9665, f1neg = 0.9616, f1pos = 0.9703, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 04:07:13.013201: step 42000, loss = 0.1016, acc = 0.9710, f1neg = 0.9682, f1pos = 0.9733, f1 = 0.9708
[Eval_batch(26)(2000,54000)] 2017-05-03 04:07:13.472151: step 42000, loss = 0.1147, acc = 0.9615, f1neg = 0.9637, f1pos = 0.9591, f1 = 0.9614
[Eval_batch(27)(2000,56000)] 2017-05-03 04:07:14.067956: step 42000, loss = 0.1026, acc = 0.9690, f1neg = 0.9675, f1pos = 0.9704, f1 = 0.9689
[Eval] 2017-05-03 04:07:14.068058: step 42000, acc = 0.9613, f1 = 0.9609
[Test_batch(0)(2000,2000)] 2017-05-03 04:07:14.575038: step 42000, loss = 0.1509, acc = 0.9515, f1neg = 0.9540, f1pos = 0.9487, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 04:07:15.073208: step 42000, loss = 0.1325, acc = 0.9580, f1neg = 0.9627, f1pos = 0.9519, f1 = 0.9573
[Test_batch(2)(2000,6000)] 2017-05-03 04:07:15.583181: step 42000, loss = 0.1482, acc = 0.9550, f1neg = 0.9587, f1pos = 0.9506, f1 = 0.9546
[Test_batch(3)(2000,8000)] 2017-05-03 04:07:16.059267: step 42000, loss = 0.1553, acc = 0.9465, f1neg = 0.9499, f1pos = 0.9426, f1 = 0.9462
[Test_batch(4)(2000,10000)] 2017-05-03 04:07:16.568017: step 42000, loss = 0.1526, acc = 0.9490, f1neg = 0.9492, f1pos = 0.9488, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 04:07:17.058661: step 42000, loss = 0.1643, acc = 0.9435, f1neg = 0.9442, f1pos = 0.9428, f1 = 0.9435
[Test_batch(6)(2000,14000)] 2017-05-03 04:07:17.570240: step 42000, loss = 0.1495, acc = 0.9460, f1neg = 0.9441, f1pos = 0.9478, f1 = 0.9459
[Test_batch(7)(2000,16000)] 2017-05-03 04:07:18.076451: step 42000, loss = 0.1402, acc = 0.9530, f1neg = 0.9574, f1pos = 0.9477, f1 = 0.9525
[Test_batch(8)(2000,18000)] 2017-05-03 04:07:18.555191: step 42000, loss = 0.1416, acc = 0.9475, f1neg = 0.9470, f1pos = 0.9480, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 04:07:19.065659: step 42000, loss = 0.1697, acc = 0.9355, f1neg = 0.9323, f1pos = 0.9384, f1 = 0.9354
[Test_batch(10)(2000,22000)] 2017-05-03 04:07:19.504540: step 42000, loss = 0.1477, acc = 0.9500, f1neg = 0.9439, f1pos = 0.9549, f1 = 0.9494
[Test_batch(11)(2000,24000)] 2017-05-03 04:07:19.980641: step 42000, loss = 0.1487, acc = 0.9480, f1neg = 0.9496, f1pos = 0.9463, f1 = 0.9479
[Test_batch(12)(2000,26000)] 2017-05-03 04:07:20.459553: step 42000, loss = 0.1249, acc = 0.9620, f1neg = 0.9635, f1pos = 0.9604, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 04:07:20.939423: step 42000, loss = 0.1502, acc = 0.9485, f1neg = 0.9429, f1pos = 0.9531, f1 = 0.9480
[Test_batch(14)(2000,30000)] 2017-05-03 04:07:21.419176: step 42000, loss = 0.1247, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Test_batch(15)(2000,32000)] 2017-05-03 04:07:21.897245: step 42000, loss = 0.1474, acc = 0.9540, f1neg = 0.9535, f1pos = 0.9545, f1 = 0.9540
[Test_batch(16)(2000,34000)] 2017-05-03 04:07:22.372421: step 42000, loss = 0.1294, acc = 0.9570, f1neg = 0.9560, f1pos = 0.9580, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 04:07:22.879502: step 42000, loss = 0.1216, acc = 0.9635, f1neg = 0.9602, f1pos = 0.9663, f1 = 0.9632
[Test_batch(18)(2000,38000)] 2017-05-03 04:07:23.418965: step 42000, loss = 0.1142, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 04:07:23.419025: step 42000, acc = 0.9524, f1 = 0.9522
[Status] 2017-05-03 04:07:23.419038: step 42000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 04:07:35.667076: step 42010, loss = 0.1339, acc = 0.9580 (244.3 examples/sec; 0.262 sec/batch)
2017-05-03 04:07:44.781607: step 42020, loss = 0.1096, acc = 0.9660 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 04:07:54.006617: step 42030, loss = 0.1252, acc = 0.9580 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 04:08:03.222974: step 42040, loss = 0.1358, acc = 0.9680 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 04:08:12.452860: step 42050, loss = 0.1180, acc = 0.9600 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 04:08:21.465403: step 42060, loss = 0.1314, acc = 0.9580 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 04:08:30.719413: step 42070, loss = 0.1426, acc = 0.9520 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 04:08:39.725117: step 42080, loss = 0.1252, acc = 0.9540 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:08:48.683294: step 42090, loss = 0.1392, acc = 0.9560 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 04:08:57.594388: step 42100, loss = 0.1200, acc = 0.9620 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 04:09:06.787198: step 42110, loss = 0.1380, acc = 0.9500 (266.2 examples/sec; 0.240 sec/batch)
2017-05-03 04:09:16.174628: step 42120, loss = 0.1240, acc = 0.9620 (262.3 examples/sec; 0.244 sec/batch)
2017-05-03 04:09:25.244271: step 42130, loss = 0.1141, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:09:34.278440: step 42140, loss = 0.1279, acc = 0.9580 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 04:09:43.333011: step 42150, loss = 0.1204, acc = 0.9620 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 04:09:52.513632: step 42160, loss = 0.1156, acc = 0.9580 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 04:10:01.649625: step 42170, loss = 0.1268, acc = 0.9540 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 04:10:10.747121: step 42180, loss = 0.1131, acc = 0.9680 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 04:10:20.170314: step 42190, loss = 0.1549, acc = 0.9420 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 04:10:29.274902: step 42200, loss = 0.1251, acc = 0.9620 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 04:10:38.735423: step 42210, loss = 0.1359, acc = 0.9540 (202.2 examples/sec; 0.317 sec/batch)
2017-05-03 04:10:47.873831: step 42220, loss = 0.1242, acc = 0.9720 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 04:10:57.488438: step 42230, loss = 0.1065, acc = 0.9660 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 04:11:06.728551: step 42240, loss = 0.1286, acc = 0.9600 (263.1 examples/sec; 0.243 sec/batch)
2017-05-03 04:11:15.742114: step 42250, loss = 0.1261, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 04:11:24.751317: step 42260, loss = 0.1119, acc = 0.9700 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 04:11:33.892333: step 42270, loss = 0.1537, acc = 0.9520 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 04:11:43.001609: step 42280, loss = 0.1489, acc = 0.9540 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 04:11:52.415232: step 42290, loss = 0.1822, acc = 0.9440 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 04:12:01.903602: step 42300, loss = 0.1212, acc = 0.9620 (203.3 examples/sec; 0.315 sec/batch)
2017-05-03 04:12:11.006047: step 42310, loss = 0.1080, acc = 0.9680 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 04:12:20.048017: step 42320, loss = 0.1342, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 04:12:29.217460: step 42330, loss = 0.1275, acc = 0.9620 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 04:12:39.163157: step 42340, loss = 0.1492, acc = 0.9480 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 04:12:48.186295: step 42350, loss = 0.1455, acc = 0.9540 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 04:12:57.144708: step 42360, loss = 0.1272, acc = 0.9580 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 04:13:06.179328: step 42370, loss = 0.1514, acc = 0.9420 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 04:13:15.345845: step 42380, loss = 0.1168, acc = 0.9680 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 04:13:24.617758: step 42390, loss = 0.1219, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 04:13:33.733595: step 42400, loss = 0.1322, acc = 0.9540 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 04:13:43.234161: step 42410, loss = 0.1372, acc = 0.9620 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:13:52.716161: step 42420, loss = 0.1422, acc = 0.9460 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 04:14:01.729635: step 42430, loss = 0.1396, acc = 0.9520 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 04:14:11.000085: step 42440, loss = 0.1479, acc = 0.9420 (268.9 examples/sec; 0.238 sec/batch)
2017-05-03 04:14:20.231913: step 42450, loss = 0.1150, acc = 0.9640 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 04:14:29.582367: step 42460, loss = 0.1481, acc = 0.9600 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 04:14:38.699498: step 42470, loss = 0.1388, acc = 0.9480 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 04:14:47.665002: step 42480, loss = 0.1325, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 04:14:56.625692: step 42490, loss = 0.1322, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 04:15:05.741057: step 42500, loss = 0.1236, acc = 0.9540 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 04:15:14.662235: step 42510, loss = 0.1341, acc = 0.9520 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 04:15:24.184123: step 42520, loss = 0.1211, acc = 0.9560 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 04:15:33.537303: step 42530, loss = 0.1812, acc = 0.9380 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 04:15:42.631920: step 42540, loss = 0.1335, acc = 0.9600 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 04:15:51.993005: step 42550, loss = 0.1282, acc = 0.9660 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 04:16:01.231524: step 42560, loss = 0.1198, acc = 0.9660 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 04:16:10.306640: step 42570, loss = 0.1131, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 04:16:19.332049: step 42580, loss = 0.1331, acc = 0.9600 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 04:16:28.508410: step 42590, loss = 0.1165, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 04:16:37.722684: step 42600, loss = 0.1344, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 04:16:46.766539: step 42610, loss = 0.1267, acc = 0.9560 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 04:16:55.803242: step 42620, loss = 0.1470, acc = 0.9520 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 04:17:04.856831: step 42630, loss = 0.1432, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 04:17:13.933492: step 42640, loss = 0.1323, acc = 0.9580 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 04:17:23.022587: step 42650, loss = 0.1337, acc = 0.9600 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 04:17:32.245726: step 42660, loss = 0.1560, acc = 0.9440 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 04:17:41.523517: step 42670, loss = 0.1280, acc = 0.9600 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 04:17:50.891950: step 42680, loss = 0.1284, acc = 0.9580 (202.6 examples/sec; 0.316 sec/batch)
2017-05-03 04:18:00.042199: step 42690, loss = 0.1240, acc = 0.9620 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 04:18:09.129670: step 42700, loss = 0.1378, acc = 0.9520 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 04:18:18.253455: step 42710, loss = 0.1343, acc = 0.9560 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 04:18:27.618025: step 42720, loss = 0.1081, acc = 0.9740 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 04:18:36.900715: step 42730, loss = 0.1399, acc = 0.9540 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 04:18:45.970616: step 42740, loss = 0.1355, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 04:18:55.139798: step 42750, loss = 0.1254, acc = 0.9680 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 04:19:04.131105: step 42760, loss = 0.1536, acc = 0.9480 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 04:19:13.232983: step 42770, loss = 0.1201, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 04:19:22.319148: step 42780, loss = 0.1313, acc = 0.9580 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 04:19:31.407956: step 42790, loss = 0.1363, acc = 0.9640 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 04:19:40.460478: step 42800, loss = 0.1279, acc = 0.9540 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 04:19:49.599781: step 42810, loss = 0.1293, acc = 0.9600 (266.8 examples/sec; 0.240 sec/batch)
2017-05-03 04:19:59.212880: step 42820, loss = 0.1650, acc = 0.9460 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 04:20:08.548922: step 42830, loss = 0.1478, acc = 0.9480 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 04:20:17.792820: step 42840, loss = 0.1229, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 04:20:26.849631: step 42850, loss = 0.1326, acc = 0.9480 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 04:20:36.144932: step 42860, loss = 0.0958, acc = 0.9720 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 04:20:45.251241: step 42870, loss = 0.1349, acc = 0.9580 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 04:20:54.418209: step 42880, loss = 0.1324, acc = 0.9520 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 04:21:03.441488: step 42890, loss = 0.1222, acc = 0.9680 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 04:21:12.461998: step 42900, loss = 0.1475, acc = 0.9480 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 04:21:21.640693: step 42910, loss = 0.1288, acc = 0.9640 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 04:21:30.548320: step 42920, loss = 0.1384, acc = 0.9500 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 04:21:39.772701: step 42930, loss = 0.1144, acc = 0.9640 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 04:21:49.233448: step 42940, loss = 0.1360, acc = 0.9600 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 04:21:58.374946: step 42950, loss = 0.1278, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 04:22:07.440201: step 42960, loss = 0.1403, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 04:22:16.478620: step 42970, loss = 0.1152, acc = 0.9720 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 04:22:25.597450: step 42980, loss = 0.1396, acc = 0.9540 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 04:22:35.061612: step 42990, loss = 0.1244, acc = 0.9640 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 04:22:44.190732: step 43000, loss = 0.1411, acc = 0.9520 (272.3 examples/sec; 0.235 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 04:22:44.698973: step 43000, loss = 0.1153, acc = 0.9565, f1neg = 0.9521, f1pos = 0.9601, f1 = 0.9561
[Eval_batch(1)(2000,4000)] 2017-05-03 04:22:45.196324: step 43000, loss = 0.1163, acc = 0.9650, f1neg = 0.9591, f1pos = 0.9694, f1 = 0.9643
[Eval_batch(2)(2000,6000)] 2017-05-03 04:22:45.670252: step 43000, loss = 0.1292, acc = 0.9575, f1neg = 0.9547, f1pos = 0.9600, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-03 04:22:46.150157: step 43000, loss = 0.1340, acc = 0.9565, f1neg = 0.9564, f1pos = 0.9566, f1 = 0.9565
[Eval_batch(4)(2000,10000)] 2017-05-03 04:22:46.630422: step 43000, loss = 0.1362, acc = 0.9580, f1neg = 0.9590, f1pos = 0.9569, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 04:22:47.098191: step 43000, loss = 0.1314, acc = 0.9510, f1neg = 0.9472, f1pos = 0.9543, f1 = 0.9507
[Eval_batch(6)(2000,14000)] 2017-05-03 04:22:47.563680: step 43000, loss = 0.1230, acc = 0.9615, f1neg = 0.9607, f1pos = 0.9623, f1 = 0.9615
[Eval_batch(7)(2000,16000)] 2017-05-03 04:22:48.039791: step 43000, loss = 0.1203, acc = 0.9595, f1neg = 0.9514, f1pos = 0.9653, f1 = 0.9583
[Eval_batch(8)(2000,18000)] 2017-05-03 04:22:48.502390: step 43000, loss = 0.1160, acc = 0.9605, f1neg = 0.9547, f1pos = 0.9650, f1 = 0.9598
[Eval_batch(9)(2000,20000)] 2017-05-03 04:22:48.976347: step 43000, loss = 0.1204, acc = 0.9640, f1neg = 0.9614, f1pos = 0.9663, f1 = 0.9638
[Eval_batch(10)(2000,22000)] 2017-05-03 04:22:49.488365: step 43000, loss = 0.1287, acc = 0.9565, f1neg = 0.9532, f1pos = 0.9594, f1 = 0.9563
[Eval_batch(11)(2000,24000)] 2017-05-03 04:22:49.984900: step 43000, loss = 0.1280, acc = 0.9590, f1neg = 0.9525, f1pos = 0.9639, f1 = 0.9582
[Eval_batch(12)(2000,26000)] 2017-05-03 04:22:50.492897: step 43000, loss = 0.1258, acc = 0.9550, f1neg = 0.9536, f1pos = 0.9564, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 04:22:50.976683: step 43000, loss = 0.1143, acc = 0.9645, f1neg = 0.9555, f1pos = 0.9705, f1 = 0.9630
[Eval_batch(14)(2000,30000)] 2017-05-03 04:22:51.440548: step 43000, loss = 0.1418, acc = 0.9530, f1neg = 0.9437, f1pos = 0.9597, f1 = 0.9517
[Eval_batch(15)(2000,32000)] 2017-05-03 04:22:51.940290: step 43000, loss = 0.1176, acc = 0.9670, f1neg = 0.9642, f1pos = 0.9694, f1 = 0.9668
[Eval_batch(16)(2000,34000)] 2017-05-03 04:22:52.419637: step 43000, loss = 0.1139, acc = 0.9650, f1neg = 0.9635, f1pos = 0.9664, f1 = 0.9649
[Eval_batch(17)(2000,36000)] 2017-05-03 04:22:52.884448: step 43000, loss = 0.1391, acc = 0.9540, f1neg = 0.9541, f1pos = 0.9539, f1 = 0.9540
[Eval_batch(18)(2000,38000)] 2017-05-03 04:22:53.408350: step 43000, loss = 0.1367, acc = 0.9540, f1neg = 0.9556, f1pos = 0.9522, f1 = 0.9539
[Eval_batch(19)(2000,40000)] 2017-05-03 04:22:53.874392: step 43000, loss = 0.1164, acc = 0.9675, f1neg = 0.9624, f1pos = 0.9714, f1 = 0.9669
[Eval_batch(20)(2000,42000)] 2017-05-03 04:22:54.334600: step 43000, loss = 0.1204, acc = 0.9610, f1neg = 0.9654, f1pos = 0.9554, f1 = 0.9604
[Eval_batch(21)(2000,44000)] 2017-05-03 04:22:54.832820: step 43000, loss = 0.1102, acc = 0.9655, f1neg = 0.9624, f1pos = 0.9682, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 04:22:55.308003: step 43000, loss = 0.1188, acc = 0.9590, f1neg = 0.9592, f1pos = 0.9588, f1 = 0.9590
[Eval_batch(23)(2000,48000)] 2017-05-03 04:22:55.813959: step 43000, loss = 0.1239, acc = 0.9585, f1neg = 0.9529, f1pos = 0.9629, f1 = 0.9579
[Eval_batch(24)(2000,50000)] 2017-05-03 04:22:56.288368: step 43000, loss = 0.1104, acc = 0.9645, f1neg = 0.9592, f1pos = 0.9686, f1 = 0.9639
[Eval_batch(25)(2000,52000)] 2017-05-03 04:22:56.794450: step 43000, loss = 0.1030, acc = 0.9705, f1neg = 0.9675, f1pos = 0.9730, f1 = 0.9703
[Eval_batch(26)(2000,54000)] 2017-05-03 04:22:57.282685: step 43000, loss = 0.1180, acc = 0.9565, f1neg = 0.9587, f1pos = 0.9540, f1 = 0.9564
[Eval_batch(27)(2000,56000)] 2017-05-03 04:22:57.905863: step 43000, loss = 0.1040, acc = 0.9665, f1neg = 0.9646, f1pos = 0.9682, f1 = 0.9664
[Eval] 2017-05-03 04:22:57.905950: step 43000, acc = 0.9603, f1 = 0.9599
[Test_batch(0)(2000,2000)] 2017-05-03 04:22:58.419458: step 43000, loss = 0.1524, acc = 0.9505, f1neg = 0.9528, f1pos = 0.9479, f1 = 0.9504
[Test_batch(1)(2000,4000)] 2017-05-03 04:22:58.898541: step 43000, loss = 0.1336, acc = 0.9550, f1neg = 0.9598, f1pos = 0.9489, f1 = 0.9543
[Test_batch(2)(2000,6000)] 2017-05-03 04:22:59.373857: step 43000, loss = 0.1506, acc = 0.9525, f1neg = 0.9562, f1pos = 0.9481, f1 = 0.9522
[Test_batch(3)(2000,8000)] 2017-05-03 04:22:59.877861: step 43000, loss = 0.1564, acc = 0.9480, f1neg = 0.9511, f1pos = 0.9445, f1 = 0.9478
[Test_batch(4)(2000,10000)] 2017-05-03 04:23:00.353090: step 43000, loss = 0.1532, acc = 0.9460, f1neg = 0.9459, f1pos = 0.9461, f1 = 0.9460
[Test_batch(5)(2000,12000)] 2017-05-03 04:23:00.831971: step 43000, loss = 0.1646, acc = 0.9395, f1neg = 0.9399, f1pos = 0.9391, f1 = 0.9395
[Test_batch(6)(2000,14000)] 2017-05-03 04:23:01.309990: step 43000, loss = 0.1503, acc = 0.9455, f1neg = 0.9434, f1pos = 0.9475, f1 = 0.9454
[Test_batch(7)(2000,16000)] 2017-05-03 04:23:01.774639: step 43000, loss = 0.1418, acc = 0.9510, f1neg = 0.9554, f1pos = 0.9457, f1 = 0.9505
[Test_batch(8)(2000,18000)] 2017-05-03 04:23:02.254872: step 43000, loss = 0.1417, acc = 0.9470, f1neg = 0.9464, f1pos = 0.9476, f1 = 0.9470
[Test_batch(9)(2000,20000)] 2017-05-03 04:23:02.724367: step 43000, loss = 0.1711, acc = 0.9385, f1neg = 0.9350, f1pos = 0.9417, f1 = 0.9383
[Test_batch(10)(2000,22000)] 2017-05-03 04:23:03.195911: step 43000, loss = 0.1467, acc = 0.9520, f1neg = 0.9458, f1pos = 0.9570, f1 = 0.9514
[Test_batch(11)(2000,24000)] 2017-05-03 04:23:03.677503: step 43000, loss = 0.1496, acc = 0.9485, f1neg = 0.9497, f1pos = 0.9472, f1 = 0.9485
[Test_batch(12)(2000,26000)] 2017-05-03 04:23:04.164533: step 43000, loss = 0.1250, acc = 0.9600, f1neg = 0.9615, f1pos = 0.9584, f1 = 0.9599
[Test_batch(13)(2000,28000)] 2017-05-03 04:23:04.635560: step 43000, loss = 0.1503, acc = 0.9465, f1neg = 0.9403, f1pos = 0.9515, f1 = 0.9459
[Test_batch(14)(2000,30000)] 2017-05-03 04:23:05.109762: step 43000, loss = 0.1242, acc = 0.9620, f1neg = 0.9562, f1pos = 0.9664, f1 = 0.9613
[Test_batch(15)(2000,32000)] 2017-05-03 04:23:05.582506: step 43000, loss = 0.1483, acc = 0.9535, f1neg = 0.9528, f1pos = 0.9542, f1 = 0.9535
[Test_batch(16)(2000,34000)] 2017-05-03 04:23:06.069879: step 43000, loss = 0.1310, acc = 0.9565, f1neg = 0.9553, f1pos = 0.9577, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 04:23:06.517035: step 43000, loss = 0.1229, acc = 0.9650, f1neg = 0.9617, f1pos = 0.9678, f1 = 0.9647
[Test_batch(18)(2000,38000)] 2017-05-03 04:23:07.092386: step 43000, loss = 0.1167, acc = 0.9620, f1neg = 0.9611, f1pos = 0.9629, f1 = 0.9620
[Test] 2017-05-03 04:23:07.092490: step 43000, acc = 0.9516, f1 = 0.9513
[Status] 2017-05-03 04:23:07.092516: step 43000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 04:23:16.057053: step 43010, loss = 0.1400, acc = 0.9620 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 04:23:25.332092: step 43020, loss = 0.1497, acc = 0.9560 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 04:23:34.490602: step 43030, loss = 0.1179, acc = 0.9600 (263.7 examples/sec; 0.243 sec/batch)
2017-05-03 04:23:43.562894: step 43040, loss = 0.1513, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 04:23:52.674280: step 43050, loss = 0.1232, acc = 0.9620 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 04:24:01.926841: step 43060, loss = 0.1237, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:24:11.263366: step 43070, loss = 0.1539, acc = 0.9500 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 04:24:20.582974: step 43080, loss = 0.1282, acc = 0.9620 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 04:24:29.724370: step 43090, loss = 0.1361, acc = 0.9580 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 04:24:38.954233: step 43100, loss = 0.1254, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 04:24:48.094432: step 43110, loss = 0.1285, acc = 0.9660 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 04:24:57.204479: step 43120, loss = 0.1740, acc = 0.9400 (273.4 examples/sec; 0.234 sec/batch)
2017-05-03 04:25:06.352709: step 43130, loss = 0.1333, acc = 0.9580 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 04:25:15.905319: step 43140, loss = 0.1434, acc = 0.9600 (201.7 examples/sec; 0.317 sec/batch)
2017-05-03 04:25:24.966183: step 43150, loss = 0.1426, acc = 0.9480 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 04:25:34.042122: step 43160, loss = 0.1342, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 04:25:43.186202: step 43170, loss = 0.1601, acc = 0.9520 (260.5 examples/sec; 0.246 sec/batch)
2017-05-03 04:25:52.188117: step 43180, loss = 0.1269, acc = 0.9500 (263.3 examples/sec; 0.243 sec/batch)
2017-05-03 04:26:01.064002: step 43190, loss = 0.1759, acc = 0.9480 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 04:26:10.101958: step 43200, loss = 0.1474, acc = 0.9500 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 04:26:19.058622: step 43210, loss = 0.1504, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 04:26:28.427888: step 43220, loss = 0.1346, acc = 0.9520 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 04:26:37.326014: step 43230, loss = 0.1482, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 04:26:46.423793: step 43240, loss = 0.1354, acc = 0.9620 (298.5 examples/sec; 0.214 sec/batch)
2017-05-03 04:26:55.476239: step 43250, loss = 0.1245, acc = 0.9680 (298.2 examples/sec; 0.215 sec/batch)
2017-05-03 04:27:04.517411: step 43260, loss = 0.1404, acc = 0.9600 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 04:27:13.607594: step 43270, loss = 0.1165, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 04:27:22.667708: step 43280, loss = 0.1197, acc = 0.9680 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 04:27:31.718546: step 43290, loss = 0.1379, acc = 0.9520 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 04:27:40.819555: step 43300, loss = 0.1519, acc = 0.9480 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:27:49.822987: step 43310, loss = 0.1250, acc = 0.9580 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 04:27:58.768914: step 43320, loss = 0.1528, acc = 0.9560 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 04:28:07.860517: step 43330, loss = 0.1580, acc = 0.9460 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 04:28:16.769739: step 43340, loss = 0.1128, acc = 0.9700 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 04:28:27.001098: step 43350, loss = 0.1457, acc = 0.9540 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 04:28:36.384815: step 43360, loss = 0.1360, acc = 0.9560 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 04:28:45.324901: step 43370, loss = 0.1292, acc = 0.9480 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 04:28:54.357438: step 43380, loss = 0.1217, acc = 0.9600 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 04:29:03.563077: step 43390, loss = 0.1550, acc = 0.9360 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 04:29:12.664430: step 43400, loss = 0.1200, acc = 0.9620 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 04:29:21.775250: step 43410, loss = 0.1223, acc = 0.9540 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 04:29:30.776245: step 43420, loss = 0.1308, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 04:29:39.863413: step 43430, loss = 0.1606, acc = 0.9380 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 04:29:48.962396: step 43440, loss = 0.1299, acc = 0.9540 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 04:29:58.186338: step 43450, loss = 0.1324, acc = 0.9500 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 04:30:07.385387: step 43460, loss = 0.1375, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 04:30:16.577138: step 43470, loss = 0.1277, acc = 0.9700 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:30:25.772784: step 43480, loss = 0.1312, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 04:30:34.885229: step 43490, loss = 0.1241, acc = 0.9580 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 04:30:43.811684: step 43500, loss = 0.1469, acc = 0.9460 (299.0 examples/sec; 0.214 sec/batch)
2017-05-03 04:30:52.803328: step 43510, loss = 0.1389, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 04:31:01.941858: step 43520, loss = 0.1662, acc = 0.9520 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 04:31:10.871736: step 43530, loss = 0.1074, acc = 0.9720 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 04:31:19.998856: step 43540, loss = 0.1312, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:31:29.055348: step 43550, loss = 0.1193, acc = 0.9700 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 04:31:38.083782: step 43560, loss = 0.1205, acc = 0.9640 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 04:31:47.476978: step 43570, loss = 0.1200, acc = 0.9640 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 04:31:56.628376: step 43580, loss = 0.1379, acc = 0.9540 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 04:32:05.757874: step 43590, loss = 0.1429, acc = 0.9420 (270.5 examples/sec; 0.237 sec/batch)
2017-05-03 04:32:14.917581: step 43600, loss = 0.1319, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 04:32:24.061673: step 43610, loss = 0.1266, acc = 0.9640 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 04:32:33.046598: step 43620, loss = 0.1130, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 04:32:42.133552: step 43630, loss = 0.1417, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 04:32:51.173470: step 43640, loss = 0.1235, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 04:33:00.396178: step 43650, loss = 0.1162, acc = 0.9620 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 04:33:09.526940: step 43660, loss = 0.1522, acc = 0.9460 (265.4 examples/sec; 0.241 sec/batch)
2017-05-03 04:33:18.579731: step 43670, loss = 0.1229, acc = 0.9620 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 04:33:27.775596: step 43680, loss = 0.1195, acc = 0.9680 (266.2 examples/sec; 0.240 sec/batch)
2017-05-03 04:33:37.019513: step 43690, loss = 0.1296, acc = 0.9560 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 04:33:46.113341: step 43700, loss = 0.1272, acc = 0.9640 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 04:33:55.176045: step 43710, loss = 0.1223, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 04:34:04.196699: step 43720, loss = 0.1150, acc = 0.9600 (266.1 examples/sec; 0.240 sec/batch)
2017-05-03 04:34:13.331200: step 43730, loss = 0.1026, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:34:22.366686: step 43740, loss = 0.1431, acc = 0.9560 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 04:34:31.451019: step 43750, loss = 0.1357, acc = 0.9460 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 04:34:40.629968: step 43760, loss = 0.1405, acc = 0.9660 (267.3 examples/sec; 0.239 sec/batch)
2017-05-03 04:34:49.664320: step 43770, loss = 0.1199, acc = 0.9600 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 04:34:58.855841: step 43780, loss = 0.1375, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 04:35:08.162879: step 43790, loss = 0.1345, acc = 0.9500 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 04:35:17.376174: step 43800, loss = 0.1726, acc = 0.9480 (258.1 examples/sec; 0.248 sec/batch)
2017-05-03 04:35:26.796578: step 43810, loss = 0.0976, acc = 0.9800 (212.7 examples/sec; 0.301 sec/batch)
2017-05-03 04:35:35.868824: step 43820, loss = 0.1478, acc = 0.9480 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 04:35:44.887256: step 43830, loss = 0.1516, acc = 0.9500 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 04:35:53.967561: step 43840, loss = 0.1301, acc = 0.9620 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 04:36:03.107537: step 43850, loss = 0.1276, acc = 0.9620 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 04:36:12.295997: step 43860, loss = 0.1277, acc = 0.9560 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 04:36:21.294493: step 43870, loss = 0.1418, acc = 0.9440 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 04:36:31.046622: step 43880, loss = 0.1131, acc = 0.9660 (239.1 examples/sec; 0.268 sec/batch)
2017-05-03 04:36:40.358834: step 43890, loss = 0.1118, acc = 0.9740 (261.1 examples/sec; 0.245 sec/batch)
2017-05-03 04:36:49.457092: step 43900, loss = 0.1314, acc = 0.9620 (299.9 examples/sec; 0.213 sec/batch)
2017-05-03 04:36:58.498021: step 43910, loss = 0.1382, acc = 0.9500 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 04:37:07.562820: step 43920, loss = 0.1442, acc = 0.9480 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 04:37:16.681271: step 43930, loss = 0.1585, acc = 0.9500 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 04:37:26.187200: step 43940, loss = 0.1324, acc = 0.9500 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 04:37:35.537063: step 43950, loss = 0.1419, acc = 0.9560 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 04:37:44.650524: step 43960, loss = 0.1321, acc = 0.9560 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 04:37:53.739356: step 43970, loss = 0.1224, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 04:38:02.746290: step 43980, loss = 0.1106, acc = 0.9720 (299.0 examples/sec; 0.214 sec/batch)
2017-05-03 04:38:12.189689: step 43990, loss = 0.1421, acc = 0.9440 (263.2 examples/sec; 0.243 sec/batch)
2017-05-03 04:38:21.374659: step 44000, loss = 0.1381, acc = 0.9620 (276.8 examples/sec; 0.231 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 04:38:21.879984: step 44000, loss = 0.1126, acc = 0.9585, f1neg = 0.9546, f1pos = 0.9618, f1 = 0.9582
[Eval_batch(1)(2000,4000)] 2017-05-03 04:38:22.354177: step 44000, loss = 0.1177, acc = 0.9645, f1neg = 0.9588, f1pos = 0.9688, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 04:38:22.862864: step 44000, loss = 0.1270, acc = 0.9565, f1neg = 0.9541, f1pos = 0.9587, f1 = 0.9564
[Eval_batch(3)(2000,8000)] 2017-05-03 04:38:23.367210: step 44000, loss = 0.1307, acc = 0.9580, f1neg = 0.9581, f1pos = 0.9579, f1 = 0.9580
[Eval_batch(4)(2000,10000)] 2017-05-03 04:38:23.842812: step 44000, loss = 0.1329, acc = 0.9580, f1neg = 0.9594, f1pos = 0.9565, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 04:38:24.359126: step 44000, loss = 0.1303, acc = 0.9525, f1neg = 0.9494, f1pos = 0.9553, f1 = 0.9523
[Eval_batch(6)(2000,14000)] 2017-05-03 04:38:24.839009: step 44000, loss = 0.1197, acc = 0.9645, f1neg = 0.9639, f1pos = 0.9650, f1 = 0.9645
[Eval_batch(7)(2000,16000)] 2017-05-03 04:38:25.301608: step 44000, loss = 0.1198, acc = 0.9610, f1neg = 0.9535, f1pos = 0.9664, f1 = 0.9599
[Eval_batch(8)(2000,18000)] 2017-05-03 04:38:25.805290: step 44000, loss = 0.1140, acc = 0.9625, f1neg = 0.9574, f1pos = 0.9665, f1 = 0.9619
[Eval_batch(9)(2000,20000)] 2017-05-03 04:38:26.275095: step 44000, loss = 0.1200, acc = 0.9630, f1neg = 0.9606, f1pos = 0.9652, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-03 04:38:26.776805: step 44000, loss = 0.1266, acc = 0.9590, f1neg = 0.9564, f1pos = 0.9613, f1 = 0.9589
[Eval_batch(11)(2000,24000)] 2017-05-03 04:38:27.250006: step 44000, loss = 0.1278, acc = 0.9605, f1neg = 0.9546, f1pos = 0.9650, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-03 04:38:27.730466: step 44000, loss = 0.1247, acc = 0.9505, f1neg = 0.9493, f1pos = 0.9517, f1 = 0.9505
[Eval_batch(13)(2000,28000)] 2017-05-03 04:38:28.195921: step 44000, loss = 0.1134, acc = 0.9665, f1neg = 0.9582, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 04:38:28.664880: step 44000, loss = 0.1409, acc = 0.9540, f1neg = 0.9455, f1pos = 0.9602, f1 = 0.9529
[Eval_batch(15)(2000,32000)] 2017-05-03 04:38:29.167988: step 44000, loss = 0.1175, acc = 0.9665, f1neg = 0.9638, f1pos = 0.9688, f1 = 0.9663
[Eval_batch(16)(2000,34000)] 2017-05-03 04:38:29.665949: step 44000, loss = 0.1139, acc = 0.9655, f1neg = 0.9642, f1pos = 0.9667, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 04:38:30.142514: step 44000, loss = 0.1387, acc = 0.9550, f1neg = 0.9553, f1pos = 0.9547, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 04:38:30.636283: step 44000, loss = 0.1325, acc = 0.9535, f1neg = 0.9555, f1pos = 0.9513, f1 = 0.9534
[Eval_batch(19)(2000,40000)] 2017-05-03 04:38:31.105059: step 44000, loss = 0.1148, acc = 0.9680, f1neg = 0.9630, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 04:38:31.607863: step 44000, loss = 0.1169, acc = 0.9625, f1neg = 0.9669, f1pos = 0.9568, f1 = 0.9618
[Eval_batch(21)(2000,44000)] 2017-05-03 04:38:32.067090: step 44000, loss = 0.1122, acc = 0.9625, f1neg = 0.9593, f1pos = 0.9652, f1 = 0.9623
[Eval_batch(22)(2000,46000)] 2017-05-03 04:38:32.550078: step 44000, loss = 0.1185, acc = 0.9605, f1neg = 0.9611, f1pos = 0.9599, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 04:38:33.011908: step 44000, loss = 0.1244, acc = 0.9610, f1neg = 0.9562, f1pos = 0.9649, f1 = 0.9605
[Eval_batch(24)(2000,50000)] 2017-05-03 04:38:33.513437: step 44000, loss = 0.1101, acc = 0.9645, f1neg = 0.9595, f1pos = 0.9684, f1 = 0.9639
[Eval_batch(25)(2000,52000)] 2017-05-03 04:38:34.015812: step 44000, loss = 0.1012, acc = 0.9705, f1neg = 0.9677, f1pos = 0.9728, f1 = 0.9703
[Eval_batch(26)(2000,54000)] 2017-05-03 04:38:34.485661: step 44000, loss = 0.1142, acc = 0.9610, f1neg = 0.9633, f1pos = 0.9584, f1 = 0.9608
[Eval_batch(27)(2000,56000)] 2017-05-03 04:38:35.076293: step 44000, loss = 0.1013, acc = 0.9695, f1neg = 0.9681, f1pos = 0.9708, f1 = 0.9694
[Eval] 2017-05-03 04:38:35.076383: step 44000, acc = 0.9611, f1 = 0.9607
[Test_batch(0)(2000,2000)] 2017-05-03 04:38:35.558922: step 44000, loss = 0.1498, acc = 0.9525, f1neg = 0.9551, f1pos = 0.9495, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 04:38:36.030254: step 44000, loss = 0.1315, acc = 0.9595, f1neg = 0.9642, f1pos = 0.9534, f1 = 0.9588
[Test_batch(2)(2000,6000)] 2017-05-03 04:38:36.498213: step 44000, loss = 0.1471, acc = 0.9555, f1neg = 0.9593, f1pos = 0.9510, f1 = 0.9551
[Test_batch(3)(2000,8000)] 2017-05-03 04:38:36.939325: step 44000, loss = 0.1548, acc = 0.9470, f1neg = 0.9505, f1pos = 0.9429, f1 = 0.9467
[Test_batch(4)(2000,10000)] 2017-05-03 04:38:37.411891: step 44000, loss = 0.1523, acc = 0.9490, f1neg = 0.9493, f1pos = 0.9487, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 04:38:37.877687: step 44000, loss = 0.1637, acc = 0.9410, f1neg = 0.9418, f1pos = 0.9402, f1 = 0.9410
[Test_batch(6)(2000,14000)] 2017-05-03 04:38:38.354079: step 44000, loss = 0.1480, acc = 0.9470, f1neg = 0.9452, f1pos = 0.9486, f1 = 0.9469
[Test_batch(7)(2000,16000)] 2017-05-03 04:38:38.825256: step 44000, loss = 0.1394, acc = 0.9525, f1neg = 0.9570, f1pos = 0.9470, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 04:38:39.297040: step 44000, loss = 0.1413, acc = 0.9475, f1neg = 0.9470, f1pos = 0.9479, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 04:38:39.771272: step 44000, loss = 0.1706, acc = 0.9385, f1neg = 0.9356, f1pos = 0.9412, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 04:38:40.254709: step 44000, loss = 0.1483, acc = 0.9505, f1neg = 0.9445, f1pos = 0.9553, f1 = 0.9499
[Test_batch(11)(2000,24000)] 2017-05-03 04:38:40.738656: step 44000, loss = 0.1478, acc = 0.9510, f1neg = 0.9526, f1pos = 0.9493, f1 = 0.9509
[Test_batch(12)(2000,26000)] 2017-05-03 04:38:41.218060: step 44000, loss = 0.1245, acc = 0.9610, f1neg = 0.9626, f1pos = 0.9592, f1 = 0.9609
[Test_batch(13)(2000,28000)] 2017-05-03 04:38:41.700994: step 44000, loss = 0.1499, acc = 0.9495, f1neg = 0.9445, f1pos = 0.9536, f1 = 0.9491
[Test_batch(14)(2000,30000)] 2017-05-03 04:38:42.214212: step 44000, loss = 0.1245, acc = 0.9605, f1neg = 0.9549, f1pos = 0.9649, f1 = 0.9599
[Test_batch(15)(2000,32000)] 2017-05-03 04:38:42.712498: step 44000, loss = 0.1475, acc = 0.9550, f1neg = 0.9546, f1pos = 0.9554, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 04:38:43.219637: step 44000, loss = 0.1296, acc = 0.9580, f1neg = 0.9571, f1pos = 0.9589, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 04:38:43.726132: step 44000, loss = 0.1212, acc = 0.9635, f1neg = 0.9603, f1pos = 0.9662, f1 = 0.9633
[Test_batch(18)(2000,38000)] 2017-05-03 04:38:44.289028: step 44000, loss = 0.1135, acc = 0.9645, f1neg = 0.9639, f1pos = 0.9651, f1 = 0.9645
[Test] 2017-05-03 04:38:44.289104: step 44000, acc = 0.9528, f1 = 0.9526
[Status] 2017-05-03 04:38:44.289130: step 44000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 04:38:53.474862: step 44010, loss = 0.1406, acc = 0.9540 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 04:39:02.438061: step 44020, loss = 0.1399, acc = 0.9700 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 04:39:11.602846: step 44030, loss = 0.1067, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 04:39:20.604469: step 44040, loss = 0.1537, acc = 0.9480 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 04:39:29.819981: step 44050, loss = 0.1478, acc = 0.9500 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 04:39:38.892082: step 44060, loss = 0.1377, acc = 0.9540 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 04:39:48.135094: step 44070, loss = 0.1375, acc = 0.9500 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 04:39:57.358874: step 44080, loss = 0.1643, acc = 0.9400 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 04:40:06.399510: step 44090, loss = 0.1273, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 04:40:15.464375: step 44100, loss = 0.1380, acc = 0.9460 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 04:40:24.726603: step 44110, loss = 0.1336, acc = 0.9540 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 04:40:33.648685: step 44120, loss = 0.1121, acc = 0.9620 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 04:40:42.807678: step 44130, loss = 0.1230, acc = 0.9540 (265.4 examples/sec; 0.241 sec/batch)
2017-05-03 04:40:51.821549: step 44140, loss = 0.1337, acc = 0.9520 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 04:41:00.964915: step 44150, loss = 0.1243, acc = 0.9640 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 04:41:10.122415: step 44160, loss = 0.1385, acc = 0.9600 (297.0 examples/sec; 0.216 sec/batch)
2017-05-03 04:41:19.858418: step 44170, loss = 0.1135, acc = 0.9660 (205.8 examples/sec; 0.311 sec/batch)
2017-05-03 04:41:28.943425: step 44180, loss = 0.1321, acc = 0.9580 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 04:41:38.421224: step 44190, loss = 0.1376, acc = 0.9580 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 04:41:47.723083: step 44200, loss = 0.1118, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:41:56.853915: step 44210, loss = 0.1375, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 04:42:06.101915: step 44220, loss = 0.1408, acc = 0.9480 (267.4 examples/sec; 0.239 sec/batch)
2017-05-03 04:42:15.291678: step 44230, loss = 0.1268, acc = 0.9440 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 04:42:24.455186: step 44240, loss = 0.1457, acc = 0.9500 (267.7 examples/sec; 0.239 sec/batch)
2017-05-03 04:42:33.598123: step 44250, loss = 0.1118, acc = 0.9640 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 04:42:42.742878: step 44260, loss = 0.1405, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 04:42:51.853216: step 44270, loss = 0.0992, acc = 0.9780 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 04:43:01.080603: step 44280, loss = 0.1613, acc = 0.9440 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 04:43:10.096372: step 44290, loss = 0.1339, acc = 0.9520 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 04:43:19.092733: step 44300, loss = 0.1216, acc = 0.9620 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 04:43:28.683214: step 44310, loss = 0.1450, acc = 0.9540 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 04:43:38.117286: step 44320, loss = 0.1173, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 04:43:47.435501: step 44330, loss = 0.1322, acc = 0.9540 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 04:43:56.534010: step 44340, loss = 0.1376, acc = 0.9560 (260.4 examples/sec; 0.246 sec/batch)
2017-05-03 04:44:05.538796: step 44350, loss = 0.1187, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 04:44:15.418030: step 44360, loss = 0.1373, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 04:44:24.452203: step 44370, loss = 0.1434, acc = 0.9500 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 04:44:33.463243: step 44380, loss = 0.1291, acc = 0.9580 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 04:44:42.472749: step 44390, loss = 0.1278, acc = 0.9600 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 04:44:51.445103: step 44400, loss = 0.1500, acc = 0.9500 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 04:45:00.617103: step 44410, loss = 0.1192, acc = 0.9520 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:45:09.671791: step 44420, loss = 0.1257, acc = 0.9640 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 04:45:18.756361: step 44430, loss = 0.1280, acc = 0.9640 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 04:45:27.852749: step 44440, loss = 0.1154, acc = 0.9640 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 04:45:37.525253: step 44450, loss = 0.1240, acc = 0.9760 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 04:45:46.509226: step 44460, loss = 0.1400, acc = 0.9420 (254.8 examples/sec; 0.251 sec/batch)
2017-05-03 04:45:55.479366: step 44470, loss = 0.1176, acc = 0.9620 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 04:46:04.423258: step 44480, loss = 0.1185, acc = 0.9560 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 04:46:13.384049: step 44490, loss = 0.1382, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 04:46:22.377542: step 44500, loss = 0.1037, acc = 0.9680 (266.3 examples/sec; 0.240 sec/batch)
2017-05-03 04:46:31.521980: step 44510, loss = 0.1427, acc = 0.9520 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:46:40.407350: step 44520, loss = 0.1403, acc = 0.9480 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 04:46:49.414125: step 44530, loss = 0.1599, acc = 0.9540 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 04:46:58.446917: step 44540, loss = 0.1421, acc = 0.9520 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 04:47:07.473115: step 44550, loss = 0.1500, acc = 0.9440 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 04:47:16.475487: step 44560, loss = 0.1367, acc = 0.9520 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 04:47:25.722618: step 44570, loss = 0.1358, acc = 0.9580 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 04:47:34.688034: step 44580, loss = 0.1386, acc = 0.9500 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 04:47:43.696456: step 44590, loss = 0.1572, acc = 0.9460 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 04:47:52.629664: step 44600, loss = 0.1260, acc = 0.9660 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 04:48:01.716569: step 44610, loss = 0.1209, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 04:48:10.720535: step 44620, loss = 0.1152, acc = 0.9700 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 04:48:19.898858: step 44630, loss = 0.1424, acc = 0.9500 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:48:29.208282: step 44640, loss = 0.1423, acc = 0.9540 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 04:48:38.600672: step 44650, loss = 0.1065, acc = 0.9720 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 04:48:47.693355: step 44660, loss = 0.1577, acc = 0.9380 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 04:48:56.701023: step 44670, loss = 0.0897, acc = 0.9820 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 04:49:05.957144: step 44680, loss = 0.1326, acc = 0.9540 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 04:49:14.949368: step 44690, loss = 0.1255, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 04:49:23.979322: step 44700, loss = 0.1202, acc = 0.9680 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 04:49:32.972840: step 44710, loss = 0.1504, acc = 0.9500 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 04:49:41.912169: step 44720, loss = 0.1157, acc = 0.9580 (299.6 examples/sec; 0.214 sec/batch)
2017-05-03 04:49:51.502900: step 44730, loss = 0.1076, acc = 0.9680 (229.0 examples/sec; 0.280 sec/batch)
2017-05-03 04:50:00.698535: step 44740, loss = 0.1452, acc = 0.9520 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 04:50:09.726713: step 44750, loss = 0.1207, acc = 0.9640 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 04:50:18.718398: step 44760, loss = 0.1404, acc = 0.9520 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 04:50:27.760483: step 44770, loss = 0.1078, acc = 0.9580 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 04:50:36.995311: step 44780, loss = 0.1304, acc = 0.9560 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 04:50:46.144686: step 44790, loss = 0.1185, acc = 0.9680 (269.9 examples/sec; 0.237 sec/batch)
2017-05-03 04:50:55.174879: step 44800, loss = 0.1514, acc = 0.9540 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 04:51:04.305792: step 44810, loss = 0.1441, acc = 0.9520 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 04:51:13.276979: step 44820, loss = 0.1698, acc = 0.9400 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 04:51:22.433206: step 44830, loss = 0.1139, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 04:51:31.809508: step 44840, loss = 0.1666, acc = 0.9420 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 04:51:40.958962: step 44850, loss = 0.1047, acc = 0.9680 (266.8 examples/sec; 0.240 sec/batch)
2017-05-03 04:51:50.078013: step 44860, loss = 0.1283, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 04:51:59.120037: step 44870, loss = 0.1510, acc = 0.9580 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 04:52:08.113747: step 44880, loss = 0.1569, acc = 0.9460 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 04:52:17.132689: step 44890, loss = 0.1274, acc = 0.9600 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 04:52:26.201830: step 44900, loss = 0.1072, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 04:52:35.609748: step 44910, loss = 0.1783, acc = 0.9460 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 04:52:44.796159: step 44920, loss = 0.1410, acc = 0.9500 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 04:52:53.862036: step 44930, loss = 0.1121, acc = 0.9680 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 04:53:02.926473: step 44940, loss = 0.1322, acc = 0.9620 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 04:53:11.856069: step 44950, loss = 0.1337, acc = 0.9620 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 04:53:20.975030: step 44960, loss = 0.1287, acc = 0.9540 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 04:53:30.107050: step 44970, loss = 0.1191, acc = 0.9580 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 04:53:39.438701: step 44980, loss = 0.1255, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 04:53:48.392125: step 44990, loss = 0.1104, acc = 0.9600 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 04:53:57.293134: step 45000, loss = 0.1278, acc = 0.9580 (287.9 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 04:53:57.777134: step 45000, loss = 0.1126, acc = 0.9590, f1neg = 0.9551, f1pos = 0.9623, f1 = 0.9587
[Eval_batch(1)(2000,4000)] 2017-05-03 04:53:58.298176: step 45000, loss = 0.1167, acc = 0.9655, f1neg = 0.9600, f1pos = 0.9697, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 04:53:58.782203: step 45000, loss = 0.1271, acc = 0.9560, f1neg = 0.9534, f1pos = 0.9583, f1 = 0.9559
[Eval_batch(3)(2000,8000)] 2017-05-03 04:53:59.282483: step 45000, loss = 0.1305, acc = 0.9585, f1neg = 0.9586, f1pos = 0.9584, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 04:53:59.721821: step 45000, loss = 0.1323, acc = 0.9565, f1neg = 0.9579, f1pos = 0.9550, f1 = 0.9565
[Eval_batch(5)(2000,12000)] 2017-05-03 04:54:00.194208: step 45000, loss = 0.1303, acc = 0.9550, f1neg = 0.9519, f1pos = 0.9577, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 04:54:00.709214: step 45000, loss = 0.1194, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Eval_batch(7)(2000,16000)] 2017-05-03 04:54:01.230595: step 45000, loss = 0.1195, acc = 0.9605, f1neg = 0.9529, f1pos = 0.9660, f1 = 0.9594
[Eval_batch(8)(2000,18000)] 2017-05-03 04:54:01.696647: step 45000, loss = 0.1136, acc = 0.9630, f1neg = 0.9579, f1pos = 0.9670, f1 = 0.9625
[Eval_batch(9)(2000,20000)] 2017-05-03 04:54:02.166566: step 45000, loss = 0.1191, acc = 0.9650, f1neg = 0.9626, f1pos = 0.9671, f1 = 0.9649
[Eval_batch(10)(2000,22000)] 2017-05-03 04:54:02.679049: step 45000, loss = 0.1267, acc = 0.9580, f1neg = 0.9552, f1pos = 0.9605, f1 = 0.9578
[Eval_batch(11)(2000,24000)] 2017-05-03 04:54:03.193118: step 45000, loss = 0.1272, acc = 0.9605, f1neg = 0.9546, f1pos = 0.9651, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-03 04:54:03.669297: step 45000, loss = 0.1242, acc = 0.9520, f1neg = 0.9508, f1pos = 0.9532, f1 = 0.9520
[Eval_batch(13)(2000,28000)] 2017-05-03 04:54:04.167006: step 45000, loss = 0.1134, acc = 0.9650, f1neg = 0.9563, f1pos = 0.9708, f1 = 0.9636
[Eval_batch(14)(2000,30000)] 2017-05-03 04:54:04.670010: step 45000, loss = 0.1401, acc = 0.9550, f1neg = 0.9466, f1pos = 0.9611, f1 = 0.9538
[Eval_batch(15)(2000,32000)] 2017-05-03 04:54:05.146581: step 45000, loss = 0.1170, acc = 0.9675, f1neg = 0.9650, f1pos = 0.9697, f1 = 0.9673
[Eval_batch(16)(2000,34000)] 2017-05-03 04:54:05.619139: step 45000, loss = 0.1129, acc = 0.9655, f1neg = 0.9642, f1pos = 0.9667, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 04:54:06.124239: step 45000, loss = 0.1385, acc = 0.9550, f1neg = 0.9553, f1pos = 0.9547, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 04:54:06.597860: step 45000, loss = 0.1329, acc = 0.9545, f1neg = 0.9564, f1pos = 0.9524, f1 = 0.9544
[Eval_batch(19)(2000,40000)] 2017-05-03 04:54:07.063917: step 45000, loss = 0.1153, acc = 0.9680, f1neg = 0.9630, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 04:54:07.536103: step 45000, loss = 0.1169, acc = 0.9620, f1neg = 0.9664, f1pos = 0.9563, f1 = 0.9613
[Eval_batch(21)(2000,44000)] 2017-05-03 04:54:07.991827: step 45000, loss = 0.1117, acc = 0.9620, f1neg = 0.9587, f1pos = 0.9648, f1 = 0.9618
[Eval_batch(22)(2000,46000)] 2017-05-03 04:54:08.465232: step 45000, loss = 0.1182, acc = 0.9600, f1neg = 0.9606, f1pos = 0.9594, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 04:54:08.925323: step 45000, loss = 0.1244, acc = 0.9595, f1neg = 0.9544, f1pos = 0.9636, f1 = 0.9590
[Eval_batch(24)(2000,50000)] 2017-05-03 04:54:09.426183: step 45000, loss = 0.1094, acc = 0.9660, f1neg = 0.9611, f1pos = 0.9698, f1 = 0.9654
[Eval_batch(25)(2000,52000)] 2017-05-03 04:54:09.939495: step 45000, loss = 0.1009, acc = 0.9710, f1neg = 0.9683, f1pos = 0.9733, f1 = 0.9708
[Eval_batch(26)(2000,54000)] 2017-05-03 04:54:10.456641: step 45000, loss = 0.1146, acc = 0.9595, f1neg = 0.9618, f1pos = 0.9568, f1 = 0.9593
[Eval_batch(27)(2000,56000)] 2017-05-03 04:54:11.080839: step 45000, loss = 0.1013, acc = 0.9690, f1neg = 0.9675, f1pos = 0.9704, f1 = 0.9689
[Eval] 2017-05-03 04:54:11.080929: step 45000, acc = 0.9612, f1 = 0.9609
[Test_batch(0)(2000,2000)] 2017-05-03 04:54:11.563128: step 45000, loss = 0.1492, acc = 0.9525, f1neg = 0.9550, f1pos = 0.9497, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 04:54:12.072375: step 45000, loss = 0.1311, acc = 0.9610, f1neg = 0.9655, f1pos = 0.9552, f1 = 0.9603
[Test_batch(2)(2000,6000)] 2017-05-03 04:54:12.575243: step 45000, loss = 0.1473, acc = 0.9550, f1neg = 0.9588, f1pos = 0.9504, f1 = 0.9546
[Test_batch(3)(2000,8000)] 2017-05-03 04:54:13.091550: step 45000, loss = 0.1542, acc = 0.9495, f1neg = 0.9529, f1pos = 0.9456, f1 = 0.9492
[Test_batch(4)(2000,10000)] 2017-05-03 04:54:13.574377: step 45000, loss = 0.1520, acc = 0.9480, f1neg = 0.9482, f1pos = 0.9478, f1 = 0.9480
[Test_batch(5)(2000,12000)] 2017-05-03 04:54:14.025274: step 45000, loss = 0.1637, acc = 0.9405, f1neg = 0.9415, f1pos = 0.9395, f1 = 0.9405
[Test_batch(6)(2000,14000)] 2017-05-03 04:54:14.504086: step 45000, loss = 0.1474, acc = 0.9460, f1neg = 0.9442, f1pos = 0.9477, f1 = 0.9459
[Test_batch(7)(2000,16000)] 2017-05-03 04:54:14.966715: step 45000, loss = 0.1387, acc = 0.9535, f1neg = 0.9578, f1pos = 0.9482, f1 = 0.9530
[Test_batch(8)(2000,18000)] 2017-05-03 04:54:15.440531: step 45000, loss = 0.1406, acc = 0.9485, f1neg = 0.9480, f1pos = 0.9490, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 04:54:15.913271: step 45000, loss = 0.1705, acc = 0.9390, f1neg = 0.9360, f1pos = 0.9417, f1 = 0.9389
[Test_batch(10)(2000,22000)] 2017-05-03 04:54:16.417387: step 45000, loss = 0.1464, acc = 0.9500, f1neg = 0.9439, f1pos = 0.9549, f1 = 0.9494
[Test_batch(11)(2000,24000)] 2017-05-03 04:54:16.930036: step 45000, loss = 0.1475, acc = 0.9500, f1neg = 0.9516, f1pos = 0.9483, f1 = 0.9499
[Test_batch(12)(2000,26000)] 2017-05-03 04:54:17.392409: step 45000, loss = 0.1244, acc = 0.9610, f1neg = 0.9626, f1pos = 0.9593, f1 = 0.9609
[Test_batch(13)(2000,28000)] 2017-05-03 04:54:17.865272: step 45000, loss = 0.1492, acc = 0.9490, f1neg = 0.9437, f1pos = 0.9534, f1 = 0.9485
[Test_batch(14)(2000,30000)] 2017-05-03 04:54:18.378525: step 45000, loss = 0.1241, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Test_batch(15)(2000,32000)] 2017-05-03 04:54:18.841268: step 45000, loss = 0.1478, acc = 0.9540, f1neg = 0.9535, f1pos = 0.9545, f1 = 0.9540
[Test_batch(16)(2000,34000)] 2017-05-03 04:54:19.343313: step 45000, loss = 0.1286, acc = 0.9590, f1neg = 0.9581, f1pos = 0.9598, f1 = 0.9590
[Test_batch(17)(2000,36000)] 2017-05-03 04:54:19.818821: step 45000, loss = 0.1209, acc = 0.9630, f1neg = 0.9597, f1pos = 0.9658, f1 = 0.9627
[Test_batch(18)(2000,38000)] 2017-05-03 04:54:20.400689: step 45000, loss = 0.1137, acc = 0.9645, f1neg = 0.9639, f1pos = 0.9651, f1 = 0.9645
[Test] 2017-05-03 04:54:20.400777: step 45000, acc = 0.9529, f1 = 0.9527
[Status] 2017-05-03 04:54:20.400802: step 45000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 04:54:29.567510: step 45010, loss = 0.1295, acc = 0.9580 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:54:38.679222: step 45020, loss = 0.1136, acc = 0.9640 (240.5 examples/sec; 0.266 sec/batch)
2017-05-03 04:54:47.811952: step 45030, loss = 0.1312, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 04:54:56.826841: step 45040, loss = 0.1127, acc = 0.9640 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 04:55:05.908071: step 45050, loss = 0.1246, acc = 0.9620 (270.6 examples/sec; 0.236 sec/batch)
2017-05-03 04:55:14.910893: step 45060, loss = 0.1299, acc = 0.9600 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 04:55:24.177358: step 45070, loss = 0.1015, acc = 0.9660 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 04:55:33.170322: step 45080, loss = 0.1206, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 04:55:42.333672: step 45090, loss = 0.1567, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 04:55:51.404121: step 45100, loss = 0.1214, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 04:56:00.458270: step 45110, loss = 0.1315, acc = 0.9520 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 04:56:09.574380: step 45120, loss = 0.1259, acc = 0.9540 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 04:56:18.799321: step 45130, loss = 0.0968, acc = 0.9720 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 04:56:27.771452: step 45140, loss = 0.1588, acc = 0.9520 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 04:56:37.632658: step 45150, loss = 0.1394, acc = 0.9520 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 04:56:46.567406: step 45160, loss = 0.0929, acc = 0.9660 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 04:56:55.835495: step 45170, loss = 0.1270, acc = 0.9540 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 04:57:04.946999: step 45180, loss = 0.1225, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 04:57:14.528892: step 45190, loss = 0.1200, acc = 0.9680 (198.1 examples/sec; 0.323 sec/batch)
2017-05-03 04:57:23.874428: step 45200, loss = 0.1562, acc = 0.9420 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 04:57:33.227438: step 45210, loss = 0.1159, acc = 0.9660 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 04:57:42.273450: step 45220, loss = 0.1517, acc = 0.9560 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 04:57:51.457148: step 45230, loss = 0.1505, acc = 0.9460 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 04:58:00.584570: step 45240, loss = 0.1452, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 04:58:09.547986: step 45250, loss = 0.1189, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 04:58:18.698592: step 45260, loss = 0.1428, acc = 0.9460 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 04:58:27.640526: step 45270, loss = 0.1098, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 04:58:36.673517: step 45280, loss = 0.1393, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 04:58:45.922688: step 45290, loss = 0.1088, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 04:58:54.925597: step 45300, loss = 0.1090, acc = 0.9760 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 04:59:03.961322: step 45310, loss = 0.1338, acc = 0.9620 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 04:59:13.055138: step 45320, loss = 0.1161, acc = 0.9600 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 04:59:22.363026: step 45330, loss = 0.1277, acc = 0.9620 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 04:59:31.442684: step 45340, loss = 0.1314, acc = 0.9600 (263.5 examples/sec; 0.243 sec/batch)
2017-05-03 04:59:40.697687: step 45350, loss = 0.1490, acc = 0.9560 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 04:59:50.677787: step 45360, loss = 0.1225, acc = 0.9680 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 04:59:59.697829: step 45370, loss = 0.1225, acc = 0.9620 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 05:00:08.659857: step 45380, loss = 0.1420, acc = 0.9520 (296.9 examples/sec; 0.216 sec/batch)
2017-05-03 05:00:17.685233: step 45390, loss = 0.1428, acc = 0.9460 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 05:00:26.884945: step 45400, loss = 0.1132, acc = 0.9660 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 05:00:36.228223: step 45410, loss = 0.1121, acc = 0.9680 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 05:00:45.222779: step 45420, loss = 0.1272, acc = 0.9620 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 05:00:54.345173: step 45430, loss = 0.1278, acc = 0.9580 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 05:01:03.467684: step 45440, loss = 0.1394, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 05:01:12.471512: step 45450, loss = 0.1431, acc = 0.9520 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 05:01:21.888678: step 45460, loss = 0.1441, acc = 0.9600 (257.1 examples/sec; 0.249 sec/batch)
2017-05-03 05:01:30.965432: step 45470, loss = 0.1122, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 05:01:40.134729: step 45480, loss = 0.1310, acc = 0.9600 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 05:01:49.397233: step 45490, loss = 0.1300, acc = 0.9620 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 05:01:58.322310: step 45500, loss = 0.1060, acc = 0.9660 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 05:02:07.524320: step 45510, loss = 0.1182, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 05:02:16.652060: step 45520, loss = 0.1302, acc = 0.9560 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 05:02:25.796121: step 45530, loss = 0.1239, acc = 0.9680 (255.0 examples/sec; 0.251 sec/batch)
2017-05-03 05:02:35.104556: step 45540, loss = 0.1157, acc = 0.9680 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 05:02:44.432835: step 45550, loss = 0.1462, acc = 0.9460 (269.2 examples/sec; 0.238 sec/batch)
2017-05-03 05:02:53.736908: step 45560, loss = 0.1606, acc = 0.9500 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 05:03:02.866959: step 45570, loss = 0.1361, acc = 0.9580 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 05:03:11.740295: step 45580, loss = 0.1285, acc = 0.9580 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:03:20.822316: step 45590, loss = 0.1412, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 05:03:29.872093: step 45600, loss = 0.1649, acc = 0.9440 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 05:03:38.937680: step 45610, loss = 0.1060, acc = 0.9700 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 05:03:47.983545: step 45620, loss = 0.1387, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 05:03:57.250003: step 45630, loss = 0.1499, acc = 0.9540 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 05:04:06.264557: step 45640, loss = 0.1496, acc = 0.9400 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 05:04:15.384718: step 45650, loss = 0.1147, acc = 0.9620 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 05:04:24.451498: step 45660, loss = 0.1760, acc = 0.9480 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:04:33.841623: step 45670, loss = 0.1345, acc = 0.9520 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 05:04:43.095471: step 45680, loss = 0.1060, acc = 0.9640 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 05:04:52.186159: step 45690, loss = 0.1258, acc = 0.9640 (269.9 examples/sec; 0.237 sec/batch)
2017-05-03 05:05:01.237548: step 45700, loss = 0.1409, acc = 0.9580 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 05:05:10.307808: step 45710, loss = 0.1251, acc = 0.9620 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 05:05:19.940508: step 45720, loss = 0.1308, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 05:05:29.045533: step 45730, loss = 0.1041, acc = 0.9700 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 05:05:38.188060: step 45740, loss = 0.1235, acc = 0.9580 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 05:05:47.205109: step 45750, loss = 0.1435, acc = 0.9420 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 05:05:56.383247: step 45760, loss = 0.1223, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 05:06:05.390190: step 45770, loss = 0.1071, acc = 0.9680 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 05:06:14.475442: step 45780, loss = 0.1410, acc = 0.9460 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 05:06:23.656740: step 45790, loss = 0.1361, acc = 0.9660 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 05:06:32.813662: step 45800, loss = 0.1218, acc = 0.9580 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 05:06:41.973251: step 45810, loss = 0.1354, acc = 0.9540 (262.8 examples/sec; 0.244 sec/batch)
2017-05-03 05:06:51.211817: step 45820, loss = 0.1300, acc = 0.9640 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 05:07:00.557144: step 45830, loss = 0.1475, acc = 0.9600 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 05:07:09.646091: step 45840, loss = 0.1494, acc = 0.9540 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 05:07:18.862858: step 45850, loss = 0.1333, acc = 0.9600 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 05:07:28.038760: step 45860, loss = 0.1763, acc = 0.9320 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 05:07:36.969249: step 45870, loss = 0.1076, acc = 0.9720 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 05:07:46.650862: step 45880, loss = 0.1463, acc = 0.9600 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 05:07:55.909608: step 45890, loss = 0.1295, acc = 0.9520 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 05:08:04.991502: step 45900, loss = 0.1094, acc = 0.9620 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 05:08:14.129975: step 45910, loss = 0.1192, acc = 0.9660 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 05:08:23.224254: step 45920, loss = 0.1378, acc = 0.9520 (304.3 examples/sec; 0.210 sec/batch)
2017-05-03 05:08:32.347207: step 45930, loss = 0.1261, acc = 0.9600 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 05:08:41.314726: step 45940, loss = 0.1155, acc = 0.9640 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:08:50.430378: step 45950, loss = 0.1070, acc = 0.9660 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 05:08:59.533350: step 45960, loss = 0.2018, acc = 0.9220 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:09:08.842106: step 45970, loss = 0.1417, acc = 0.9480 (246.7 examples/sec; 0.259 sec/batch)
2017-05-03 05:09:18.049008: step 45980, loss = 0.1322, acc = 0.9620 (264.2 examples/sec; 0.242 sec/batch)
2017-05-03 05:09:27.205104: step 45990, loss = 0.1242, acc = 0.9660 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 05:09:36.310925: step 46000, loss = 0.1374, acc = 0.9560 (285.9 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 05:09:36.794436: step 46000, loss = 0.1122, acc = 0.9645, f1neg = 0.9617, f1pos = 0.9669, f1 = 0.9643
[Eval_batch(1)(2000,4000)] 2017-05-03 05:09:37.269531: step 46000, loss = 0.1233, acc = 0.9630, f1neg = 0.9576, f1pos = 0.9672, f1 = 0.9624
[Eval_batch(2)(2000,6000)] 2017-05-03 05:09:37.774120: step 46000, loss = 0.1292, acc = 0.9570, f1neg = 0.9551, f1pos = 0.9587, f1 = 0.9569
[Eval_batch(3)(2000,8000)] 2017-05-03 05:09:38.244553: step 46000, loss = 0.1321, acc = 0.9550, f1neg = 0.9558, f1pos = 0.9542, f1 = 0.9550
[Eval_batch(4)(2000,10000)] 2017-05-03 05:09:38.717289: step 46000, loss = 0.1342, acc = 0.9525, f1neg = 0.9548, f1pos = 0.9500, f1 = 0.9524
[Eval_batch(5)(2000,12000)] 2017-05-03 05:09:39.223504: step 46000, loss = 0.1332, acc = 0.9505, f1neg = 0.9480, f1pos = 0.9527, f1 = 0.9504
[Eval_batch(6)(2000,14000)] 2017-05-03 05:09:39.692046: step 46000, loss = 0.1182, acc = 0.9650, f1neg = 0.9648, f1pos = 0.9652, f1 = 0.9650
[Eval_batch(7)(2000,16000)] 2017-05-03 05:09:40.197132: step 46000, loss = 0.1240, acc = 0.9590, f1neg = 0.9519, f1pos = 0.9643, f1 = 0.9581
[Eval_batch(8)(2000,18000)] 2017-05-03 05:09:40.708308: step 46000, loss = 0.1175, acc = 0.9615, f1neg = 0.9571, f1pos = 0.9651, f1 = 0.9611
[Eval_batch(9)(2000,20000)] 2017-05-03 05:09:41.216032: step 46000, loss = 0.1234, acc = 0.9620, f1neg = 0.9599, f1pos = 0.9639, f1 = 0.9619
[Eval_batch(10)(2000,22000)] 2017-05-03 05:09:41.726236: step 46000, loss = 0.1306, acc = 0.9625, f1neg = 0.9607, f1pos = 0.9642, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 05:09:42.202001: step 46000, loss = 0.1326, acc = 0.9575, f1neg = 0.9516, f1pos = 0.9621, f1 = 0.9569
[Eval_batch(12)(2000,26000)] 2017-05-03 05:09:42.679769: step 46000, loss = 0.1263, acc = 0.9525, f1neg = 0.9519, f1pos = 0.9530, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-03 05:09:43.160259: step 46000, loss = 0.1167, acc = 0.9675, f1neg = 0.9599, f1pos = 0.9727, f1 = 0.9663
[Eval_batch(14)(2000,30000)] 2017-05-03 05:09:43.669500: step 46000, loss = 0.1427, acc = 0.9535, f1neg = 0.9456, f1pos = 0.9594, f1 = 0.9525
[Eval_batch(15)(2000,32000)] 2017-05-03 05:09:44.142903: step 46000, loss = 0.1215, acc = 0.9630, f1neg = 0.9607, f1pos = 0.9650, f1 = 0.9629
[Eval_batch(16)(2000,34000)] 2017-05-03 05:09:44.662768: step 46000, loss = 0.1170, acc = 0.9640, f1neg = 0.9630, f1pos = 0.9650, f1 = 0.9640
[Eval_batch(17)(2000,36000)] 2017-05-03 05:09:45.163992: step 46000, loss = 0.1420, acc = 0.9565, f1neg = 0.9573, f1pos = 0.9557, f1 = 0.9565
[Eval_batch(18)(2000,38000)] 2017-05-03 05:09:45.623634: step 46000, loss = 0.1328, acc = 0.9585, f1neg = 0.9608, f1pos = 0.9560, f1 = 0.9584
[Eval_batch(19)(2000,40000)] 2017-05-03 05:09:46.129971: step 46000, loss = 0.1181, acc = 0.9665, f1neg = 0.9618, f1pos = 0.9702, f1 = 0.9660
[Eval_batch(20)(2000,42000)] 2017-05-03 05:09:46.643534: step 46000, loss = 0.1153, acc = 0.9625, f1neg = 0.9671, f1pos = 0.9564, f1 = 0.9618
[Eval_batch(21)(2000,44000)] 2017-05-03 05:09:47.150787: step 46000, loss = 0.1198, acc = 0.9580, f1neg = 0.9550, f1pos = 0.9606, f1 = 0.9578
[Eval_batch(22)(2000,46000)] 2017-05-03 05:09:47.655609: step 46000, loss = 0.1207, acc = 0.9620, f1neg = 0.9628, f1pos = 0.9611, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 05:09:48.161667: step 46000, loss = 0.1299, acc = 0.9595, f1neg = 0.9551, f1pos = 0.9631, f1 = 0.9591
[Eval_batch(24)(2000,50000)] 2017-05-03 05:09:48.646146: step 46000, loss = 0.1150, acc = 0.9640, f1neg = 0.9594, f1pos = 0.9677, f1 = 0.9635
[Eval_batch(25)(2000,52000)] 2017-05-03 05:09:49.147986: step 46000, loss = 0.1027, acc = 0.9685, f1neg = 0.9659, f1pos = 0.9707, f1 = 0.9683
[Eval_batch(26)(2000,54000)] 2017-05-03 05:09:49.618369: step 46000, loss = 0.1139, acc = 0.9635, f1neg = 0.9661, f1pos = 0.9605, f1 = 0.9633
[Eval_batch(27)(2000,56000)] 2017-05-03 05:09:50.190826: step 46000, loss = 0.1011, acc = 0.9695, f1neg = 0.9684, f1pos = 0.9705, f1 = 0.9695
[Eval] 2017-05-03 05:09:50.190889: step 46000, acc = 0.9607, f1 = 0.9604
[Test_batch(0)(2000,2000)] 2017-05-03 05:09:50.665210: step 46000, loss = 0.1523, acc = 0.9525, f1neg = 0.9555, f1pos = 0.9490, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 05:09:51.146384: step 46000, loss = 0.1322, acc = 0.9550, f1neg = 0.9607, f1pos = 0.9474, f1 = 0.9540
[Test_batch(2)(2000,6000)] 2017-05-03 05:09:51.617660: step 46000, loss = 0.1465, acc = 0.9570, f1neg = 0.9611, f1pos = 0.9520, f1 = 0.9565
[Test_batch(3)(2000,8000)] 2017-05-03 05:09:52.087013: step 46000, loss = 0.1567, acc = 0.9420, f1neg = 0.9468, f1pos = 0.9363, f1 = 0.9415
[Test_batch(4)(2000,10000)] 2017-05-03 05:09:52.557394: step 46000, loss = 0.1579, acc = 0.9460, f1neg = 0.9473, f1pos = 0.9447, f1 = 0.9460
[Test_batch(5)(2000,12000)] 2017-05-03 05:09:53.012004: step 46000, loss = 0.1688, acc = 0.9430, f1neg = 0.9449, f1pos = 0.9410, f1 = 0.9429
[Test_batch(6)(2000,14000)] 2017-05-03 05:09:53.487532: step 46000, loss = 0.1497, acc = 0.9525, f1neg = 0.9518, f1pos = 0.9531, f1 = 0.9525
[Test_batch(7)(2000,16000)] 2017-05-03 05:09:53.955321: step 46000, loss = 0.1407, acc = 0.9530, f1neg = 0.9581, f1pos = 0.9465, f1 = 0.9523
[Test_batch(8)(2000,18000)] 2017-05-03 05:09:54.434864: step 46000, loss = 0.1453, acc = 0.9505, f1neg = 0.9507, f1pos = 0.9503, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 05:09:54.918231: step 46000, loss = 0.1775, acc = 0.9325, f1neg = 0.9305, f1pos = 0.9344, f1 = 0.9324
[Test_batch(10)(2000,22000)] 2017-05-03 05:09:55.392192: step 46000, loss = 0.1579, acc = 0.9480, f1neg = 0.9427, f1pos = 0.9524, f1 = 0.9475
[Test_batch(11)(2000,24000)] 2017-05-03 05:09:55.871388: step 46000, loss = 0.1508, acc = 0.9530, f1neg = 0.9550, f1pos = 0.9508, f1 = 0.9529
[Test_batch(12)(2000,26000)] 2017-05-03 05:09:56.347194: step 46000, loss = 0.1303, acc = 0.9560, f1neg = 0.9582, f1pos = 0.9535, f1 = 0.9559
[Test_batch(13)(2000,28000)] 2017-05-03 05:09:56.832963: step 46000, loss = 0.1561, acc = 0.9530, f1neg = 0.9489, f1pos = 0.9565, f1 = 0.9527
[Test_batch(14)(2000,30000)] 2017-05-03 05:09:57.344067: step 46000, loss = 0.1329, acc = 0.9555, f1neg = 0.9500, f1pos = 0.9599, f1 = 0.9550
[Test_batch(15)(2000,32000)] 2017-05-03 05:09:57.793948: step 46000, loss = 0.1526, acc = 0.9510, f1neg = 0.9511, f1pos = 0.9509, f1 = 0.9510
[Test_batch(16)(2000,34000)] 2017-05-03 05:09:58.302466: step 46000, loss = 0.1342, acc = 0.9570, f1neg = 0.9567, f1pos = 0.9573, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 05:09:58.758623: step 46000, loss = 0.1225, acc = 0.9595, f1neg = 0.9564, f1pos = 0.9622, f1 = 0.9593
[Test_batch(18)(2000,38000)] 2017-05-03 05:09:59.339801: step 46000, loss = 0.1131, acc = 0.9665, f1neg = 0.9663, f1pos = 0.9667, f1 = 0.9665
[Test] 2017-05-03 05:09:59.339864: step 46000, acc = 0.9518, f1 = 0.9515
[Status] 2017-05-03 05:09:59.339877: step 46000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 05:10:08.648121: step 46010, loss = 0.1137, acc = 0.9780 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 05:10:17.686554: step 46020, loss = 0.1214, acc = 0.9660 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 05:10:26.829388: step 46030, loss = 0.1496, acc = 0.9460 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 05:10:36.234810: step 46040, loss = 0.1207, acc = 0.9560 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 05:10:45.158889: step 46050, loss = 0.1526, acc = 0.9400 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 05:10:54.251079: step 46060, loss = 0.1420, acc = 0.9560 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 05:11:03.463417: step 46070, loss = 0.1267, acc = 0.9640 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 05:11:12.571069: step 46080, loss = 0.1495, acc = 0.9400 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 05:11:21.572131: step 46090, loss = 0.1383, acc = 0.9500 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 05:11:30.965245: step 46100, loss = 0.1536, acc = 0.9440 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 05:11:39.866656: step 46110, loss = 0.1103, acc = 0.9680 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 05:11:48.974648: step 46120, loss = 0.1025, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 05:11:57.954651: step 46130, loss = 0.1376, acc = 0.9540 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 05:12:07.334140: step 46140, loss = 0.1323, acc = 0.9660 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 05:12:16.482352: step 46150, loss = 0.1525, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 05:12:25.444147: step 46160, loss = 0.1449, acc = 0.9480 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 05:12:34.440717: step 46170, loss = 0.1416, acc = 0.9640 (297.4 examples/sec; 0.215 sec/batch)
2017-05-03 05:12:43.578871: step 46180, loss = 0.1242, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 05:12:52.585252: step 46190, loss = 0.1469, acc = 0.9440 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 05:13:01.588740: step 46200, loss = 0.1235, acc = 0.9580 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 05:13:10.559049: step 46210, loss = 0.1253, acc = 0.9640 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 05:13:19.627580: step 46220, loss = 0.1228, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 05:13:28.830546: step 46230, loss = 0.1240, acc = 0.9640 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 05:13:38.287808: step 46240, loss = 0.1280, acc = 0.9520 (203.7 examples/sec; 0.314 sec/batch)
2017-05-03 05:13:47.387563: step 46250, loss = 0.1404, acc = 0.9460 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 05:13:56.428273: step 46260, loss = 0.1184, acc = 0.9640 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 05:14:05.377751: step 46270, loss = 0.1086, acc = 0.9720 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 05:14:14.488194: step 46280, loss = 0.1674, acc = 0.9400 (268.8 examples/sec; 0.238 sec/batch)
2017-05-03 05:14:23.570259: step 46290, loss = 0.1171, acc = 0.9580 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 05:14:32.648064: step 46300, loss = 0.1399, acc = 0.9440 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 05:14:41.601245: step 46310, loss = 0.1426, acc = 0.9520 (298.1 examples/sec; 0.215 sec/batch)
2017-05-03 05:14:50.536928: step 46320, loss = 0.1423, acc = 0.9560 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 05:14:59.571544: step 46330, loss = 0.1265, acc = 0.9600 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 05:15:08.556622: step 46340, loss = 0.1347, acc = 0.9520 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 05:15:17.616471: step 46350, loss = 0.1456, acc = 0.9440 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 05:15:26.785537: step 46360, loss = 0.1517, acc = 0.9480 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 05:15:36.982808: step 46370, loss = 0.1282, acc = 0.9520 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 05:15:46.075090: step 46380, loss = 0.1527, acc = 0.9520 (266.1 examples/sec; 0.240 sec/batch)
2017-05-03 05:15:55.320433: step 46390, loss = 0.1284, acc = 0.9560 (260.0 examples/sec; 0.246 sec/batch)
2017-05-03 05:16:04.342271: step 46400, loss = 0.1335, acc = 0.9640 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 05:16:13.386360: step 46410, loss = 0.1444, acc = 0.9400 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 05:16:22.473005: step 46420, loss = 0.1181, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 05:16:31.611958: step 46430, loss = 0.1372, acc = 0.9540 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 05:16:40.970118: step 46440, loss = 0.1247, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 05:16:50.202210: step 46450, loss = 0.1261, acc = 0.9700 (264.9 examples/sec; 0.242 sec/batch)
2017-05-03 05:16:59.160926: step 46460, loss = 0.1253, acc = 0.9660 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 05:17:08.288407: step 46470, loss = 0.1169, acc = 0.9560 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 05:17:17.421392: step 46480, loss = 0.1218, acc = 0.9640 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 05:17:26.478482: step 46490, loss = 0.1600, acc = 0.9400 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 05:17:36.575595: step 46500, loss = 0.1082, acc = 0.9720 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 05:17:45.682728: step 46510, loss = 0.1347, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 05:17:54.899273: step 46520, loss = 0.1243, acc = 0.9560 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 05:18:04.327395: step 46530, loss = 0.1530, acc = 0.9500 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 05:18:13.383758: step 46540, loss = 0.1446, acc = 0.9460 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 05:18:22.577686: step 46550, loss = 0.0991, acc = 0.9760 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 05:18:31.641443: step 46560, loss = 0.1450, acc = 0.9520 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:18:40.589892: step 46570, loss = 0.1645, acc = 0.9360 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 05:18:49.861219: step 46580, loss = 0.1439, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 05:18:58.807282: step 46590, loss = 0.1021, acc = 0.9620 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 05:19:07.935473: step 46600, loss = 0.1380, acc = 0.9440 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 05:19:16.958225: step 46610, loss = 0.1373, acc = 0.9520 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 05:19:26.004453: step 46620, loss = 0.1350, acc = 0.9580 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 05:19:35.128429: step 46630, loss = 0.1235, acc = 0.9560 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 05:19:44.553328: step 46640, loss = 0.1356, acc = 0.9520 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 05:19:53.924557: step 46650, loss = 0.1408, acc = 0.9520 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 05:20:02.967512: step 46660, loss = 0.1495, acc = 0.9700 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 05:20:12.047604: step 46670, loss = 0.1316, acc = 0.9580 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 05:20:21.135574: step 46680, loss = 0.1251, acc = 0.9560 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 05:20:30.726437: step 46690, loss = 0.1093, acc = 0.9680 (180.1 examples/sec; 0.355 sec/batch)
2017-05-03 05:20:39.805755: step 46700, loss = 0.1050, acc = 0.9760 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 05:20:48.899796: step 46710, loss = 0.1213, acc = 0.9620 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 05:20:57.862946: step 46720, loss = 0.1229, acc = 0.9620 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 05:21:06.977329: step 46730, loss = 0.1282, acc = 0.9560 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 05:21:16.176507: step 46740, loss = 0.1366, acc = 0.9600 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 05:21:25.325868: step 46750, loss = 0.1494, acc = 0.9660 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 05:21:34.345321: step 46760, loss = 0.1256, acc = 0.9680 (243.6 examples/sec; 0.263 sec/batch)
2017-05-03 05:21:43.724634: step 46770, loss = 0.1411, acc = 0.9460 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 05:21:52.820883: step 46780, loss = 0.1776, acc = 0.9460 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 05:22:01.852350: step 46790, loss = 0.1508, acc = 0.9580 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 05:22:10.774736: step 46800, loss = 0.1175, acc = 0.9620 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 05:22:19.679721: step 46810, loss = 0.1649, acc = 0.9420 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:22:28.674109: step 46820, loss = 0.1266, acc = 0.9640 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 05:22:37.835457: step 46830, loss = 0.1297, acc = 0.9600 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 05:22:46.840252: step 46840, loss = 0.1407, acc = 0.9500 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 05:22:56.010336: step 46850, loss = 0.1421, acc = 0.9480 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 05:23:05.086073: step 46860, loss = 0.1476, acc = 0.9560 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 05:23:14.268752: step 46870, loss = 0.1246, acc = 0.9540 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 05:23:23.566898: step 46880, loss = 0.1149, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 05:23:32.781486: step 46890, loss = 0.1367, acc = 0.9540 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 05:23:41.932166: step 46900, loss = 0.1128, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 05:23:51.079908: step 46910, loss = 0.1297, acc = 0.9540 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 05:24:00.236916: step 46920, loss = 0.1107, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 05:24:09.385903: step 46930, loss = 0.1301, acc = 0.9620 (300.3 examples/sec; 0.213 sec/batch)
2017-05-03 05:24:18.415877: step 46940, loss = 0.1196, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 05:24:27.485329: step 46950, loss = 0.1302, acc = 0.9600 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 05:24:36.530459: step 46960, loss = 0.1357, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:24:45.562924: step 46970, loss = 0.1192, acc = 0.9620 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 05:24:54.579959: step 46980, loss = 0.1112, acc = 0.9640 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 05:25:03.681801: step 46990, loss = 0.1281, acc = 0.9540 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 05:25:13.001793: step 47000, loss = 0.1036, acc = 0.9720 (302.1 examples/sec; 0.212 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 05:25:13.469359: step 47000, loss = 0.1111, acc = 0.9605, f1neg = 0.9571, f1pos = 0.9634, f1 = 0.9603
[Eval_batch(1)(2000,4000)] 2017-05-03 05:25:13.929014: step 47000, loss = 0.1206, acc = 0.9620, f1neg = 0.9563, f1pos = 0.9664, f1 = 0.9613
[Eval_batch(2)(2000,6000)] 2017-05-03 05:25:14.435136: step 47000, loss = 0.1271, acc = 0.9570, f1neg = 0.9549, f1pos = 0.9589, f1 = 0.9569
[Eval_batch(3)(2000,8000)] 2017-05-03 05:25:14.911061: step 47000, loss = 0.1304, acc = 0.9560, f1neg = 0.9566, f1pos = 0.9554, f1 = 0.9560
[Eval_batch(4)(2000,10000)] 2017-05-03 05:25:15.359221: step 47000, loss = 0.1326, acc = 0.9535, f1neg = 0.9555, f1pos = 0.9513, f1 = 0.9534
[Eval_batch(5)(2000,12000)] 2017-05-03 05:25:15.838654: step 47000, loss = 0.1312, acc = 0.9545, f1neg = 0.9520, f1pos = 0.9568, f1 = 0.9544
[Eval_batch(6)(2000,14000)] 2017-05-03 05:25:16.347641: step 47000, loss = 0.1183, acc = 0.9655, f1neg = 0.9652, f1pos = 0.9658, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 05:25:16.853408: step 47000, loss = 0.1222, acc = 0.9570, f1neg = 0.9494, f1pos = 0.9626, f1 = 0.9560
[Eval_batch(8)(2000,18000)] 2017-05-03 05:25:17.325500: step 47000, loss = 0.1158, acc = 0.9625, f1neg = 0.9578, f1pos = 0.9662, f1 = 0.9620
[Eval_batch(9)(2000,20000)] 2017-05-03 05:25:17.832158: step 47000, loss = 0.1207, acc = 0.9605, f1neg = 0.9583, f1pos = 0.9625, f1 = 0.9604
[Eval_batch(10)(2000,22000)] 2017-05-03 05:25:18.341436: step 47000, loss = 0.1286, acc = 0.9610, f1neg = 0.9590, f1pos = 0.9628, f1 = 0.9609
[Eval_batch(11)(2000,24000)] 2017-05-03 05:25:18.844222: step 47000, loss = 0.1302, acc = 0.9590, f1neg = 0.9532, f1pos = 0.9635, f1 = 0.9584
[Eval_batch(12)(2000,26000)] 2017-05-03 05:25:19.318061: step 47000, loss = 0.1261, acc = 0.9515, f1neg = 0.9507, f1pos = 0.9522, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-03 05:25:19.791736: step 47000, loss = 0.1147, acc = 0.9650, f1neg = 0.9567, f1pos = 0.9706, f1 = 0.9637
[Eval_batch(14)(2000,30000)] 2017-05-03 05:25:20.289017: step 47000, loss = 0.1416, acc = 0.9525, f1neg = 0.9443, f1pos = 0.9586, f1 = 0.9515
[Eval_batch(15)(2000,32000)] 2017-05-03 05:25:20.761736: step 47000, loss = 0.1197, acc = 0.9630, f1neg = 0.9606, f1pos = 0.9652, f1 = 0.9629
[Eval_batch(16)(2000,34000)] 2017-05-03 05:25:21.236363: step 47000, loss = 0.1148, acc = 0.9650, f1neg = 0.9639, f1pos = 0.9660, f1 = 0.9650
[Eval_batch(17)(2000,36000)] 2017-05-03 05:25:21.717986: step 47000, loss = 0.1404, acc = 0.9580, f1neg = 0.9587, f1pos = 0.9573, f1 = 0.9580
[Eval_batch(18)(2000,38000)] 2017-05-03 05:25:22.222741: step 47000, loss = 0.1323, acc = 0.9565, f1neg = 0.9587, f1pos = 0.9541, f1 = 0.9564
[Eval_batch(19)(2000,40000)] 2017-05-03 05:25:22.731882: step 47000, loss = 0.1165, acc = 0.9660, f1neg = 0.9611, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(20)(2000,42000)] 2017-05-03 05:25:23.237921: step 47000, loss = 0.1151, acc = 0.9630, f1neg = 0.9674, f1pos = 0.9572, f1 = 0.9623
[Eval_batch(21)(2000,44000)] 2017-05-03 05:25:23.746856: step 47000, loss = 0.1168, acc = 0.9615, f1neg = 0.9585, f1pos = 0.9641, f1 = 0.9613
[Eval_batch(22)(2000,46000)] 2017-05-03 05:25:24.239519: step 47000, loss = 0.1191, acc = 0.9630, f1neg = 0.9637, f1pos = 0.9623, f1 = 0.9630
[Eval_batch(23)(2000,48000)] 2017-05-03 05:25:24.712440: step 47000, loss = 0.1274, acc = 0.9600, f1neg = 0.9555, f1pos = 0.9637, f1 = 0.9596
[Eval_batch(24)(2000,50000)] 2017-05-03 05:25:25.188471: step 47000, loss = 0.1129, acc = 0.9650, f1neg = 0.9603, f1pos = 0.9687, f1 = 0.9645
[Eval_batch(25)(2000,52000)] 2017-05-03 05:25:25.669963: step 47000, loss = 0.1019, acc = 0.9720, f1neg = 0.9696, f1pos = 0.9741, f1 = 0.9718
[Eval_batch(26)(2000,54000)] 2017-05-03 05:25:26.171756: step 47000, loss = 0.1136, acc = 0.9620, f1neg = 0.9645, f1pos = 0.9591, f1 = 0.9618
[Eval_batch(27)(2000,56000)] 2017-05-03 05:25:26.760341: step 47000, loss = 0.1004, acc = 0.9700, f1neg = 0.9688, f1pos = 0.9711, f1 = 0.9700
[Eval] 2017-05-03 05:25:26.760450: step 47000, acc = 0.9608, f1 = 0.9605
[Test_batch(0)(2000,2000)] 2017-05-03 05:25:27.245866: step 47000, loss = 0.1501, acc = 0.9530, f1neg = 0.9559, f1pos = 0.9497, f1 = 0.9528
[Test_batch(1)(2000,4000)] 2017-05-03 05:25:27.696960: step 47000, loss = 0.1306, acc = 0.9570, f1neg = 0.9622, f1pos = 0.9502, f1 = 0.9562
[Test_batch(2)(2000,6000)] 2017-05-03 05:25:28.171991: step 47000, loss = 0.1451, acc = 0.9575, f1neg = 0.9614, f1pos = 0.9528, f1 = 0.9571
[Test_batch(3)(2000,8000)] 2017-05-03 05:25:28.644375: step 47000, loss = 0.1541, acc = 0.9440, f1neg = 0.9484, f1pos = 0.9388, f1 = 0.9436
[Test_batch(4)(2000,10000)] 2017-05-03 05:25:29.112336: step 47000, loss = 0.1535, acc = 0.9475, f1neg = 0.9484, f1pos = 0.9466, f1 = 0.9475
[Test_batch(5)(2000,12000)] 2017-05-03 05:25:29.555988: step 47000, loss = 0.1658, acc = 0.9435, f1neg = 0.9450, f1pos = 0.9419, f1 = 0.9435
[Test_batch(6)(2000,14000)] 2017-05-03 05:25:30.029886: step 47000, loss = 0.1471, acc = 0.9500, f1neg = 0.9490, f1pos = 0.9509, f1 = 0.9500
[Test_batch(7)(2000,16000)] 2017-05-03 05:25:30.526803: step 47000, loss = 0.1391, acc = 0.9500, f1neg = 0.9552, f1pos = 0.9435, f1 = 0.9493
[Test_batch(8)(2000,18000)] 2017-05-03 05:25:30.996480: step 47000, loss = 0.1422, acc = 0.9490, f1neg = 0.9488, f1pos = 0.9492, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 05:25:31.474589: step 47000, loss = 0.1747, acc = 0.9350, f1neg = 0.9326, f1pos = 0.9372, f1 = 0.9349
[Test_batch(10)(2000,22000)] 2017-05-03 05:25:31.980295: step 47000, loss = 0.1530, acc = 0.9490, f1neg = 0.9435, f1pos = 0.9536, f1 = 0.9485
[Test_batch(11)(2000,24000)] 2017-05-03 05:25:32.486492: step 47000, loss = 0.1481, acc = 0.9525, f1neg = 0.9544, f1pos = 0.9504, f1 = 0.9524
[Test_batch(12)(2000,26000)] 2017-05-03 05:25:32.985362: step 47000, loss = 0.1266, acc = 0.9595, f1neg = 0.9614, f1pos = 0.9574, f1 = 0.9594
[Test_batch(13)(2000,28000)] 2017-05-03 05:25:33.468038: step 47000, loss = 0.1521, acc = 0.9520, f1neg = 0.9477, f1pos = 0.9557, f1 = 0.9517
[Test_batch(14)(2000,30000)] 2017-05-03 05:25:33.930447: step 47000, loss = 0.1284, acc = 0.9580, f1neg = 0.9526, f1pos = 0.9623, f1 = 0.9574
[Test_batch(15)(2000,32000)] 2017-05-03 05:25:34.402454: step 47000, loss = 0.1501, acc = 0.9550, f1neg = 0.9549, f1pos = 0.9551, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 05:25:34.888942: step 47000, loss = 0.1308, acc = 0.9585, f1neg = 0.9580, f1pos = 0.9590, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 05:25:35.365024: step 47000, loss = 0.1217, acc = 0.9625, f1neg = 0.9594, f1pos = 0.9651, f1 = 0.9623
[Test_batch(18)(2000,38000)] 2017-05-03 05:25:35.900399: step 47000, loss = 0.1116, acc = 0.9640, f1neg = 0.9637, f1pos = 0.9643, f1 = 0.9640
[Test] 2017-05-03 05:25:35.900491: step 47000, acc = 0.9525, f1 = 0.9523
[Status] 2017-05-03 05:25:35.900517: step 47000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 05:25:44.955701: step 47010, loss = 0.1534, acc = 0.9480 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 05:25:53.907399: step 47020, loss = 0.1267, acc = 0.9620 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 05:26:03.051203: step 47030, loss = 0.1227, acc = 0.9580 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 05:26:11.981123: step 47040, loss = 0.1127, acc = 0.9720 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 05:26:20.954005: step 47050, loss = 0.1455, acc = 0.9500 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 05:26:29.869156: step 47060, loss = 0.1142, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 05:26:39.799976: step 47070, loss = 0.1141, acc = 0.9600 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 05:26:48.909350: step 47080, loss = 0.1221, acc = 0.9560 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 05:26:57.979894: step 47090, loss = 0.1275, acc = 0.9560 (272.9 examples/sec; 0.235 sec/batch)
2017-05-03 05:27:07.167439: step 47100, loss = 0.1144, acc = 0.9720 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 05:27:16.337524: step 47110, loss = 0.1230, acc = 0.9640 (261.3 examples/sec; 0.245 sec/batch)
2017-05-03 05:27:25.673958: step 47120, loss = 0.1297, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 05:27:34.962962: step 47130, loss = 0.1474, acc = 0.9520 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 05:27:43.973035: step 47140, loss = 0.1405, acc = 0.9500 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 05:27:53.040102: step 47150, loss = 0.1496, acc = 0.9600 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 05:28:02.038264: step 47160, loss = 0.1119, acc = 0.9680 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:28:11.045593: step 47170, loss = 0.1331, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 05:28:20.123237: step 47180, loss = 0.1402, acc = 0.9520 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 05:28:29.083039: step 47190, loss = 0.1330, acc = 0.9540 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 05:28:38.254425: step 47200, loss = 0.1479, acc = 0.9540 (264.3 examples/sec; 0.242 sec/batch)
2017-05-03 05:28:47.568750: step 47210, loss = 0.1313, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 05:28:56.746910: step 47220, loss = 0.1330, acc = 0.9580 (263.0 examples/sec; 0.243 sec/batch)
2017-05-03 05:29:05.982957: step 47230, loss = 0.1397, acc = 0.9460 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 05:29:15.239264: step 47240, loss = 0.1388, acc = 0.9440 (262.3 examples/sec; 0.244 sec/batch)
2017-05-03 05:29:24.269455: step 47250, loss = 0.1245, acc = 0.9620 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 05:29:33.339915: step 47260, loss = 0.1135, acc = 0.9580 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 05:29:42.339773: step 47270, loss = 0.1526, acc = 0.9480 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:29:51.582696: step 47280, loss = 0.1850, acc = 0.9260 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 05:30:00.685024: step 47290, loss = 0.1444, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 05:30:09.835776: step 47300, loss = 0.1303, acc = 0.9600 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 05:30:18.960768: step 47310, loss = 0.1243, acc = 0.9640 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 05:30:27.978324: step 47320, loss = 0.1042, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 05:30:37.044756: step 47330, loss = 0.1144, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 05:30:45.943502: step 47340, loss = 0.1110, acc = 0.9740 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 05:30:54.848236: step 47350, loss = 0.1031, acc = 0.9800 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 05:31:03.893457: step 47360, loss = 0.1405, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 05:31:12.885619: step 47370, loss = 0.1286, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 05:31:23.206483: step 47380, loss = 0.1339, acc = 0.9560 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 05:31:32.187630: step 47390, loss = 0.1315, acc = 0.9640 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 05:31:41.421782: step 47400, loss = 0.1395, acc = 0.9520 (259.6 examples/sec; 0.247 sec/batch)
2017-05-03 05:31:50.726505: step 47410, loss = 0.1144, acc = 0.9580 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 05:32:00.039476: step 47420, loss = 0.1306, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 05:32:09.169999: step 47430, loss = 0.1486, acc = 0.9480 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 05:32:18.262692: step 47440, loss = 0.1524, acc = 0.9500 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 05:32:27.268975: step 47450, loss = 0.1266, acc = 0.9620 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 05:32:36.985634: step 47460, loss = 0.1382, acc = 0.9620 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 05:32:45.964337: step 47470, loss = 0.1236, acc = 0.9580 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 05:32:55.177628: step 47480, loss = 0.1284, acc = 0.9720 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 05:33:04.351300: step 47490, loss = 0.1167, acc = 0.9680 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 05:33:13.262931: step 47500, loss = 0.1218, acc = 0.9600 (300.8 examples/sec; 0.213 sec/batch)
2017-05-03 05:33:22.366323: step 47510, loss = 0.1358, acc = 0.9620 (263.6 examples/sec; 0.243 sec/batch)
2017-05-03 05:33:31.449947: step 47520, loss = 0.1158, acc = 0.9600 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 05:33:40.318833: step 47530, loss = 0.1380, acc = 0.9420 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 05:33:49.645212: step 47540, loss = 0.1208, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 05:33:58.681836: step 47550, loss = 0.1334, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 05:34:08.016233: step 47560, loss = 0.1089, acc = 0.9660 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 05:34:17.131699: step 47570, loss = 0.1215, acc = 0.9600 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 05:34:26.174580: step 47580, loss = 0.1232, acc = 0.9500 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 05:34:35.426854: step 47590, loss = 0.1329, acc = 0.9600 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 05:34:44.440122: step 47600, loss = 0.1359, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 05:34:53.566764: step 47610, loss = 0.1415, acc = 0.9560 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 05:35:02.630213: step 47620, loss = 0.1345, acc = 0.9480 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 05:35:11.554286: step 47630, loss = 0.1112, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 05:35:20.950611: step 47640, loss = 0.1370, acc = 0.9500 (216.0 examples/sec; 0.296 sec/batch)
2017-05-03 05:35:30.215922: step 47650, loss = 0.1132, acc = 0.9660 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 05:35:39.398323: step 47660, loss = 0.1338, acc = 0.9540 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 05:35:48.775468: step 47670, loss = 0.1225, acc = 0.9600 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 05:35:57.890508: step 47680, loss = 0.0965, acc = 0.9740 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:36:07.231158: step 47690, loss = 0.1612, acc = 0.9580 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 05:36:16.420667: step 47700, loss = 0.1490, acc = 0.9500 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 05:36:25.559941: step 47710, loss = 0.1256, acc = 0.9600 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 05:36:34.640071: step 47720, loss = 0.1266, acc = 0.9640 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 05:36:43.779013: step 47730, loss = 0.1225, acc = 0.9600 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 05:36:52.808401: step 47740, loss = 0.0994, acc = 0.9760 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 05:37:02.082256: step 47750, loss = 0.1123, acc = 0.9700 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 05:37:10.961938: step 47760, loss = 0.1251, acc = 0.9660 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 05:37:20.048705: step 47770, loss = 0.1254, acc = 0.9620 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:37:29.118748: step 47780, loss = 0.1158, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 05:37:38.251627: step 47790, loss = 0.1170, acc = 0.9700 (250.7 examples/sec; 0.255 sec/batch)
2017-05-03 05:37:47.333826: step 47800, loss = 0.1222, acc = 0.9580 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 05:37:56.342835: step 47810, loss = 0.1316, acc = 0.9560 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 05:38:05.474082: step 47820, loss = 0.1448, acc = 0.9520 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:38:14.450586: step 47830, loss = 0.1229, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 05:38:23.443288: step 47840, loss = 0.1017, acc = 0.9700 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 05:38:32.613496: step 47850, loss = 0.1410, acc = 0.9500 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 05:38:41.667184: step 47860, loss = 0.1369, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 05:38:50.756509: step 47870, loss = 0.1645, acc = 0.9380 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 05:39:00.543569: step 47880, loss = 0.1140, acc = 0.9680 (236.0 examples/sec; 0.271 sec/batch)
2017-05-03 05:39:09.453045: step 47890, loss = 0.1343, acc = 0.9660 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 05:39:18.648048: step 47900, loss = 0.1259, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 05:39:27.647002: step 47910, loss = 0.1119, acc = 0.9660 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 05:39:36.644793: step 47920, loss = 0.1358, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 05:39:45.686335: step 47930, loss = 0.1121, acc = 0.9760 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 05:39:54.688286: step 47940, loss = 0.1350, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 05:40:03.607399: step 47950, loss = 0.1009, acc = 0.9760 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 05:40:12.603980: step 47960, loss = 0.1284, acc = 0.9580 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 05:40:21.507546: step 47970, loss = 0.1190, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 05:40:30.620385: step 47980, loss = 0.1128, acc = 0.9520 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 05:40:39.654907: step 47990, loss = 0.1160, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 05:40:48.745535: step 48000, loss = 0.1194, acc = 0.9620 (285.1 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 05:40:49.216219: step 48000, loss = 0.1138, acc = 0.9575, f1neg = 0.9532, f1pos = 0.9611, f1 = 0.9571
[Eval_batch(1)(2000,4000)] 2017-05-03 05:40:49.676100: step 48000, loss = 0.1145, acc = 0.9655, f1neg = 0.9597, f1pos = 0.9699, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 05:40:50.151563: step 48000, loss = 0.1264, acc = 0.9580, f1neg = 0.9552, f1pos = 0.9605, f1 = 0.9578
[Eval_batch(3)(2000,8000)] 2017-05-03 05:40:50.622032: step 48000, loss = 0.1329, acc = 0.9560, f1neg = 0.9558, f1pos = 0.9562, f1 = 0.9560
[Eval_batch(4)(2000,10000)] 2017-05-03 05:40:51.085462: step 48000, loss = 0.1343, acc = 0.9580, f1neg = 0.9590, f1pos = 0.9570, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 05:40:51.542015: step 48000, loss = 0.1305, acc = 0.9540, f1neg = 0.9504, f1pos = 0.9571, f1 = 0.9538
[Eval_batch(6)(2000,14000)] 2017-05-03 05:40:52.048244: step 48000, loss = 0.1218, acc = 0.9625, f1neg = 0.9617, f1pos = 0.9633, f1 = 0.9625
[Eval_batch(7)(2000,16000)] 2017-05-03 05:40:52.517914: step 48000, loss = 0.1197, acc = 0.9600, f1neg = 0.9520, f1pos = 0.9657, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 05:40:53.012066: step 48000, loss = 0.1147, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 05:40:53.482673: step 48000, loss = 0.1191, acc = 0.9635, f1neg = 0.9608, f1pos = 0.9659, f1 = 0.9633
[Eval_batch(10)(2000,22000)] 2017-05-03 05:40:53.949916: step 48000, loss = 0.1281, acc = 0.9560, f1neg = 0.9527, f1pos = 0.9588, f1 = 0.9558
[Eval_batch(11)(2000,24000)] 2017-05-03 05:40:54.424373: step 48000, loss = 0.1260, acc = 0.9580, f1neg = 0.9514, f1pos = 0.9630, f1 = 0.9572
[Eval_batch(12)(2000,26000)] 2017-05-03 05:40:54.933655: step 48000, loss = 0.1243, acc = 0.9540, f1neg = 0.9524, f1pos = 0.9555, f1 = 0.9539
[Eval_batch(13)(2000,28000)] 2017-05-03 05:40:55.400513: step 48000, loss = 0.1132, acc = 0.9645, f1neg = 0.9555, f1pos = 0.9705, f1 = 0.9630
[Eval_batch(14)(2000,30000)] 2017-05-03 05:40:55.863756: step 48000, loss = 0.1401, acc = 0.9565, f1neg = 0.9479, f1pos = 0.9626, f1 = 0.9553
[Eval_batch(15)(2000,32000)] 2017-05-03 05:40:56.367542: step 48000, loss = 0.1165, acc = 0.9680, f1neg = 0.9653, f1pos = 0.9703, f1 = 0.9678
[Eval_batch(16)(2000,34000)] 2017-05-03 05:40:56.842981: step 48000, loss = 0.1125, acc = 0.9670, f1neg = 0.9656, f1pos = 0.9683, f1 = 0.9669
[Eval_batch(17)(2000,36000)] 2017-05-03 05:40:57.327661: step 48000, loss = 0.1385, acc = 0.9550, f1neg = 0.9551, f1pos = 0.9549, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 05:40:57.805461: step 48000, loss = 0.1347, acc = 0.9545, f1neg = 0.9561, f1pos = 0.9527, f1 = 0.9544
[Eval_batch(19)(2000,40000)] 2017-05-03 05:40:58.269223: step 48000, loss = 0.1151, acc = 0.9695, f1neg = 0.9647, f1pos = 0.9731, f1 = 0.9689
[Eval_batch(20)(2000,42000)] 2017-05-03 05:40:58.748651: step 48000, loss = 0.1202, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9565, f1 = 0.9614
[Eval_batch(21)(2000,44000)] 2017-05-03 05:40:59.254224: step 48000, loss = 0.1094, acc = 0.9640, f1neg = 0.9607, f1pos = 0.9668, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 05:40:59.752947: step 48000, loss = 0.1178, acc = 0.9610, f1neg = 0.9612, f1pos = 0.9608, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 05:41:00.219502: step 48000, loss = 0.1236, acc = 0.9580, f1neg = 0.9524, f1pos = 0.9624, f1 = 0.9574
[Eval_batch(24)(2000,50000)] 2017-05-03 05:41:00.683566: step 48000, loss = 0.1095, acc = 0.9655, f1neg = 0.9603, f1pos = 0.9695, f1 = 0.9649
[Eval_batch(25)(2000,52000)] 2017-05-03 05:41:01.141701: step 48000, loss = 0.1011, acc = 0.9710, f1neg = 0.9681, f1pos = 0.9734, f1 = 0.9708
[Eval_batch(26)(2000,54000)] 2017-05-03 05:41:01.611543: step 48000, loss = 0.1162, acc = 0.9600, f1neg = 0.9621, f1pos = 0.9577, f1 = 0.9599
[Eval_batch(27)(2000,56000)] 2017-05-03 05:41:02.136524: step 48000, loss = 0.1031, acc = 0.9665, f1neg = 0.9647, f1pos = 0.9681, f1 = 0.9664
[Eval] 2017-05-03 05:41:02.136579: step 48000, acc = 0.9610, f1 = 0.9606
[Test_batch(0)(2000,2000)] 2017-05-03 05:41:02.611939: step 48000, loss = 0.1511, acc = 0.9515, f1neg = 0.9537, f1pos = 0.9490, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 05:41:03.119563: step 48000, loss = 0.1334, acc = 0.9555, f1neg = 0.9603, f1pos = 0.9494, f1 = 0.9548
[Test_batch(2)(2000,6000)] 2017-05-03 05:41:03.628186: step 48000, loss = 0.1484, acc = 0.9525, f1neg = 0.9562, f1pos = 0.9481, f1 = 0.9522
[Test_batch(3)(2000,8000)] 2017-05-03 05:41:04.101807: step 48000, loss = 0.1544, acc = 0.9470, f1neg = 0.9502, f1pos = 0.9433, f1 = 0.9468
[Test_batch(4)(2000,10000)] 2017-05-03 05:41:04.562844: step 48000, loss = 0.1514, acc = 0.9460, f1neg = 0.9459, f1pos = 0.9461, f1 = 0.9460
[Test_batch(5)(2000,12000)] 2017-05-03 05:41:05.043372: step 48000, loss = 0.1630, acc = 0.9400, f1neg = 0.9404, f1pos = 0.9396, f1 = 0.9400
[Test_batch(6)(2000,14000)] 2017-05-03 05:41:05.518458: step 48000, loss = 0.1492, acc = 0.9450, f1neg = 0.9428, f1pos = 0.9470, f1 = 0.9449
[Test_batch(7)(2000,16000)] 2017-05-03 05:41:06.021747: step 48000, loss = 0.1398, acc = 0.9525, f1neg = 0.9567, f1pos = 0.9474, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 05:41:06.528145: step 48000, loss = 0.1405, acc = 0.9490, f1neg = 0.9483, f1pos = 0.9497, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 05:41:07.021103: step 48000, loss = 0.1694, acc = 0.9365, f1neg = 0.9328, f1pos = 0.9398, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-03 05:41:07.514299: step 48000, loss = 0.1440, acc = 0.9525, f1neg = 0.9464, f1pos = 0.9574, f1 = 0.9519
[Test_batch(11)(2000,24000)] 2017-05-03 05:41:07.995382: step 48000, loss = 0.1484, acc = 0.9490, f1neg = 0.9503, f1pos = 0.9476, f1 = 0.9490
[Test_batch(12)(2000,26000)] 2017-05-03 05:41:08.444683: step 48000, loss = 0.1232, acc = 0.9610, f1neg = 0.9624, f1pos = 0.9595, f1 = 0.9609
[Test_batch(13)(2000,28000)] 2017-05-03 05:41:08.930946: step 48000, loss = 0.1478, acc = 0.9475, f1neg = 0.9414, f1pos = 0.9524, f1 = 0.9469
[Test_batch(14)(2000,30000)] 2017-05-03 05:41:09.440204: step 48000, loss = 0.1229, acc = 0.9615, f1neg = 0.9556, f1pos = 0.9660, f1 = 0.9608
[Test_batch(15)(2000,32000)] 2017-05-03 05:41:09.949290: step 48000, loss = 0.1468, acc = 0.9530, f1neg = 0.9522, f1pos = 0.9537, f1 = 0.9530
[Test_batch(16)(2000,34000)] 2017-05-03 05:41:10.457308: step 48000, loss = 0.1287, acc = 0.9580, f1neg = 0.9567, f1pos = 0.9592, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 05:41:10.960992: step 48000, loss = 0.1221, acc = 0.9660, f1neg = 0.9627, f1pos = 0.9688, f1 = 0.9657
[Test_batch(18)(2000,38000)] 2017-05-03 05:41:11.507717: step 48000, loss = 0.1150, acc = 0.9625, f1neg = 0.9616, f1pos = 0.9633, f1 = 0.9625
[Test] 2017-05-03 05:41:11.507798: step 48000, acc = 0.9519, f1 = 0.9517
[Status] 2017-05-03 05:41:11.507824: step 48000, maxindex = 42000, maxdev = 0.9613, maxtst = 0.9524
2017-05-03 05:41:20.586449: step 48010, loss = 0.1299, acc = 0.9600 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 05:41:29.634273: step 48020, loss = 0.1410, acc = 0.9540 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 05:41:38.817349: step 48030, loss = 0.1329, acc = 0.9580 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 05:41:48.148487: step 48040, loss = 0.1490, acc = 0.9420 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 05:41:57.469686: step 48050, loss = 0.1892, acc = 0.9320 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 05:42:06.776654: step 48060, loss = 0.1546, acc = 0.9500 (206.5 examples/sec; 0.310 sec/batch)
2017-05-03 05:42:15.691117: step 48070, loss = 0.1338, acc = 0.9560 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 05:42:24.600311: step 48080, loss = 0.1417, acc = 0.9600 (300.1 examples/sec; 0.213 sec/batch)
2017-05-03 05:42:33.550606: step 48090, loss = 0.1591, acc = 0.9400 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 05:42:42.614258: step 48100, loss = 0.1440, acc = 0.9520 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 05:42:51.638958: step 48110, loss = 0.1043, acc = 0.9760 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 05:43:00.665007: step 48120, loss = 0.1053, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 05:43:09.699484: step 48130, loss = 0.1398, acc = 0.9560 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 05:43:18.788915: step 48140, loss = 0.1820, acc = 0.9460 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 05:43:27.741925: step 48150, loss = 0.1148, acc = 0.9640 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 05:43:36.674502: step 48160, loss = 0.1380, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 05:43:45.794726: step 48170, loss = 0.1434, acc = 0.9520 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 05:43:54.822967: step 48180, loss = 0.1409, acc = 0.9520 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 05:44:03.922581: step 48190, loss = 0.1333, acc = 0.9480 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 05:44:13.068487: step 48200, loss = 0.1303, acc = 0.9560 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 05:44:22.236535: step 48210, loss = 0.1026, acc = 0.9680 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 05:44:31.455113: step 48220, loss = 0.1344, acc = 0.9460 (227.0 examples/sec; 0.282 sec/batch)
2017-05-03 05:44:40.544831: step 48230, loss = 0.1174, acc = 0.9560 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 05:44:49.630538: step 48240, loss = 0.1240, acc = 0.9640 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 05:44:58.631351: step 48250, loss = 0.1690, acc = 0.9420 (268.5 examples/sec; 0.238 sec/batch)
2017-05-03 05:45:07.761248: step 48260, loss = 0.1086, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 05:45:16.700667: step 48270, loss = 0.1440, acc = 0.9480 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 05:45:25.838294: step 48280, loss = 0.1463, acc = 0.9560 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 05:45:34.772430: step 48290, loss = 0.1135, acc = 0.9700 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 05:45:43.907076: step 48300, loss = 0.1262, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 05:45:53.247167: step 48310, loss = 0.1241, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 05:46:02.253954: step 48320, loss = 0.1196, acc = 0.9620 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 05:46:11.640576: step 48330, loss = 0.1311, acc = 0.9440 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 05:46:20.844596: step 48340, loss = 0.1151, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 05:46:29.999942: step 48350, loss = 0.1213, acc = 0.9620 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 05:46:39.127463: step 48360, loss = 0.1271, acc = 0.9520 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 05:46:48.168545: step 48370, loss = 0.1223, acc = 0.9680 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 05:46:57.389652: step 48380, loss = 0.1400, acc = 0.9480 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 05:47:07.349700: step 48390, loss = 0.1413, acc = 0.9560 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 05:47:16.373951: step 48400, loss = 0.1135, acc = 0.9680 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 05:47:25.404371: step 48410, loss = 0.1134, acc = 0.9600 (299.4 examples/sec; 0.214 sec/batch)
2017-05-03 05:47:34.565838: step 48420, loss = 0.1432, acc = 0.9480 (302.7 examples/sec; 0.211 sec/batch)
2017-05-03 05:47:43.394026: step 48430, loss = 0.1398, acc = 0.9540 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 05:47:52.625040: step 48440, loss = 0.1090, acc = 0.9660 (262.2 examples/sec; 0.244 sec/batch)
2017-05-03 05:48:01.649480: step 48450, loss = 0.1184, acc = 0.9580 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 05:48:10.542327: step 48460, loss = 0.1330, acc = 0.9480 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 05:48:19.555153: step 48470, loss = 0.1062, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 05:48:28.725142: step 48480, loss = 0.1203, acc = 0.9620 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 05:48:37.802287: step 48490, loss = 0.1071, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 05:48:46.907243: step 48500, loss = 0.1451, acc = 0.9640 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 05:48:56.031425: step 48510, loss = 0.1320, acc = 0.9560 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 05:49:05.102999: step 48520, loss = 0.1207, acc = 0.9680 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 05:49:13.989392: step 48530, loss = 0.1477, acc = 0.9520 (300.1 examples/sec; 0.213 sec/batch)
2017-05-03 05:49:23.313957: step 48540, loss = 0.1191, acc = 0.9580 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 05:49:32.481151: step 48550, loss = 0.1599, acc = 0.9540 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 05:49:41.492308: step 48560, loss = 0.1373, acc = 0.9560 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 05:49:50.667640: step 48570, loss = 0.1168, acc = 0.9700 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 05:49:59.707787: step 48580, loss = 0.1305, acc = 0.9580 (265.9 examples/sec; 0.241 sec/batch)
2017-05-03 05:50:08.738198: step 48590, loss = 0.1266, acc = 0.9620 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 05:50:17.886857: step 48600, loss = 0.1516, acc = 0.9460 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 05:50:26.944793: step 48610, loss = 0.1309, acc = 0.9640 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 05:50:36.182932: step 48620, loss = 0.1357, acc = 0.9520 (265.8 examples/sec; 0.241 sec/batch)
2017-05-03 05:50:45.273537: step 48630, loss = 0.1073, acc = 0.9700 (288.9 examples/sec; 0.221 sec/batch)
2017-05-03 05:50:54.405060: step 48640, loss = 0.1134, acc = 0.9540 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 05:51:03.475762: step 48650, loss = 0.1216, acc = 0.9580 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:51:12.479282: step 48660, loss = 0.1311, acc = 0.9700 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 05:51:21.607501: step 48670, loss = 0.1171, acc = 0.9620 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 05:51:30.725158: step 48680, loss = 0.1275, acc = 0.9600 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 05:51:39.850206: step 48690, loss = 0.1241, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 05:51:49.506057: step 48700, loss = 0.1295, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 05:51:58.663119: step 48710, loss = 0.1231, acc = 0.9660 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 05:52:07.687078: step 48720, loss = 0.1192, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 05:52:16.994522: step 48730, loss = 0.1449, acc = 0.9480 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 05:52:26.110287: step 48740, loss = 0.1194, acc = 0.9620 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 05:52:35.492583: step 48750, loss = 0.1421, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 05:52:44.681341: step 48760, loss = 0.1335, acc = 0.9680 (260.9 examples/sec; 0.245 sec/batch)
2017-05-03 05:52:53.887553: step 48770, loss = 0.1122, acc = 0.9700 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 05:53:03.189634: step 48780, loss = 0.1466, acc = 0.9500 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 05:53:12.258558: step 48790, loss = 0.1417, acc = 0.9480 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 05:53:21.313383: step 48800, loss = 0.1004, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 05:53:30.338687: step 48810, loss = 0.1240, acc = 0.9580 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 05:53:39.443815: step 48820, loss = 0.1608, acc = 0.9400 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 05:53:48.618191: step 48830, loss = 0.1297, acc = 0.9580 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 05:53:57.814601: step 48840, loss = 0.1172, acc = 0.9600 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 05:54:06.995582: step 48850, loss = 0.1628, acc = 0.9380 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 05:54:16.092127: step 48860, loss = 0.1418, acc = 0.9520 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 05:54:25.287593: step 48870, loss = 0.1140, acc = 0.9560 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 05:54:34.475173: step 48880, loss = 0.1606, acc = 0.9460 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 05:54:43.555246: step 48890, loss = 0.1201, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 05:54:52.693066: step 48900, loss = 0.1238, acc = 0.9620 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 05:55:01.856426: step 48910, loss = 0.1476, acc = 0.9540 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 05:55:11.042414: step 48920, loss = 0.1293, acc = 0.9600 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 05:55:20.147721: step 48930, loss = 0.1335, acc = 0.9580 (264.0 examples/sec; 0.242 sec/batch)
2017-05-03 05:55:29.170551: step 48940, loss = 0.1338, acc = 0.9500 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 05:55:38.295989: step 48950, loss = 0.1148, acc = 0.9640 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 05:55:47.465613: step 48960, loss = 0.1201, acc = 0.9580 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 05:55:56.603649: step 48970, loss = 0.1721, acc = 0.9480 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 05:56:05.643662: step 48980, loss = 0.1250, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 05:56:14.726437: step 48990, loss = 0.1292, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 05:56:23.785413: step 49000, loss = 0.1611, acc = 0.9340 (286.0 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 05:56:24.258824: step 49000, loss = 0.1112, acc = 0.9595, f1neg = 0.9557, f1pos = 0.9627, f1 = 0.9592
[Eval_batch(1)(2000,4000)] 2017-05-03 05:56:24.731568: step 49000, loss = 0.1166, acc = 0.9660, f1neg = 0.9606, f1pos = 0.9701, f1 = 0.9654
[Eval_batch(2)(2000,6000)] 2017-05-03 05:56:25.210069: step 49000, loss = 0.1253, acc = 0.9565, f1neg = 0.9541, f1pos = 0.9587, f1 = 0.9564
[Eval_batch(3)(2000,8000)] 2017-05-03 05:56:25.684253: step 49000, loss = 0.1284, acc = 0.9605, f1neg = 0.9607, f1pos = 0.9603, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 05:56:26.194693: step 49000, loss = 0.1305, acc = 0.9570, f1neg = 0.9585, f1pos = 0.9553, f1 = 0.9569
[Eval_batch(5)(2000,12000)] 2017-05-03 05:56:26.663745: step 49000, loss = 0.1290, acc = 0.9555, f1neg = 0.9527, f1pos = 0.9580, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 05:56:27.131844: step 49000, loss = 0.1188, acc = 0.9655, f1neg = 0.9650, f1pos = 0.9660, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 05:56:27.633611: step 49000, loss = 0.1191, acc = 0.9610, f1neg = 0.9536, f1pos = 0.9664, f1 = 0.9600
[Eval_batch(8)(2000,18000)] 2017-05-03 05:56:28.144810: step 49000, loss = 0.1131, acc = 0.9645, f1neg = 0.9598, f1pos = 0.9682, f1 = 0.9640
[Eval_batch(9)(2000,20000)] 2017-05-03 05:56:28.652520: step 49000, loss = 0.1184, acc = 0.9625, f1neg = 0.9601, f1pos = 0.9646, f1 = 0.9624
[Eval_batch(10)(2000,22000)] 2017-05-03 05:56:29.118954: step 49000, loss = 0.1257, acc = 0.9595, f1neg = 0.9570, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(11)(2000,24000)] 2017-05-03 05:56:29.635334: step 49000, loss = 0.1264, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Eval_batch(12)(2000,26000)] 2017-05-03 05:56:30.115495: step 49000, loss = 0.1230, acc = 0.9535, f1neg = 0.9523, f1pos = 0.9546, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 05:56:30.588111: step 49000, loss = 0.1124, acc = 0.9665, f1neg = 0.9583, f1pos = 0.9720, f1 = 0.9652
[Eval_batch(14)(2000,30000)] 2017-05-03 05:56:31.060085: step 49000, loss = 0.1395, acc = 0.9555, f1neg = 0.9473, f1pos = 0.9615, f1 = 0.9544
[Eval_batch(15)(2000,32000)] 2017-05-03 05:56:31.522283: step 49000, loss = 0.1162, acc = 0.9675, f1neg = 0.9651, f1pos = 0.9696, f1 = 0.9673
[Eval_batch(16)(2000,34000)] 2017-05-03 05:56:32.019174: step 49000, loss = 0.1120, acc = 0.9655, f1neg = 0.9643, f1pos = 0.9666, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 05:56:32.498092: step 49000, loss = 0.1385, acc = 0.9565, f1neg = 0.9570, f1pos = 0.9560, f1 = 0.9565
[Eval_batch(18)(2000,38000)] 2017-05-03 05:56:32.935835: step 49000, loss = 0.1315, acc = 0.9550, f1neg = 0.9570, f1pos = 0.9528, f1 = 0.9549
[Eval_batch(19)(2000,40000)] 2017-05-03 05:56:33.409545: step 49000, loss = 0.1142, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9717, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 05:56:33.882325: step 49000, loss = 0.1149, acc = 0.9615, f1neg = 0.9660, f1pos = 0.9557, f1 = 0.9608
[Eval_batch(21)(2000,44000)] 2017-05-03 05:56:34.368732: step 49000, loss = 0.1113, acc = 0.9635, f1neg = 0.9604, f1pos = 0.9662, f1 = 0.9633
[Eval_batch(22)(2000,46000)] 2017-05-03 05:56:34.831947: step 49000, loss = 0.1168, acc = 0.9625, f1neg = 0.9631, f1pos = 0.9619, f1 = 0.9625
[Eval_batch(23)(2000,48000)] 2017-05-03 05:56:35.304472: step 49000, loss = 0.1241, acc = 0.9615, f1neg = 0.9568, f1pos = 0.9653, f1 = 0.9610
[Eval_batch(24)(2000,50000)] 2017-05-03 05:56:35.776444: step 49000, loss = 0.1097, acc = 0.9635, f1neg = 0.9584, f1pos = 0.9675, f1 = 0.9629
[Eval_batch(25)(2000,52000)] 2017-05-03 05:56:36.241683: step 49000, loss = 0.0999, acc = 0.9705, f1neg = 0.9678, f1pos = 0.9728, f1 = 0.9703
[Eval_batch(26)(2000,54000)] 2017-05-03 05:56:36.714375: step 49000, loss = 0.1136, acc = 0.9605, f1neg = 0.9629, f1pos = 0.9578, f1 = 0.9603
[Eval_batch(27)(2000,56000)] 2017-05-03 05:56:37.278795: step 49000, loss = 0.0994, acc = 0.9710, f1neg = 0.9697, f1pos = 0.9722, f1 = 0.9709
[Eval] 2017-05-03 05:56:37.278896: step 49000, acc = 0.9618, f1 = 0.9615
[Test_batch(0)(2000,2000)] 2017-05-03 05:56:37.748644: step 49000, loss = 0.1491, acc = 0.9530, f1neg = 0.9556, f1pos = 0.9501, f1 = 0.9528
[Test_batch(1)(2000,4000)] 2017-05-03 05:56:38.209691: step 49000, loss = 0.1288, acc = 0.9590, f1neg = 0.9637, f1pos = 0.9528, f1 = 0.9583
[Test_batch(2)(2000,6000)] 2017-05-03 05:56:38.715471: step 49000, loss = 0.1458, acc = 0.9560, f1neg = 0.9597, f1pos = 0.9515, f1 = 0.9556
[Test_batch(3)(2000,8000)] 2017-05-03 05:56:39.180138: step 49000, loss = 0.1524, acc = 0.9470, f1neg = 0.9508, f1pos = 0.9426, f1 = 0.9467
[Test_batch(4)(2000,10000)] 2017-05-03 05:56:39.684319: step 49000, loss = 0.1507, acc = 0.9490, f1neg = 0.9495, f1pos = 0.9485, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 05:56:40.169605: step 49000, loss = 0.1630, acc = 0.9420, f1neg = 0.9432, f1pos = 0.9407, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 05:56:40.641743: step 49000, loss = 0.1463, acc = 0.9525, f1neg = 0.9512, f1pos = 0.9537, f1 = 0.9525
[Test_batch(7)(2000,16000)] 2017-05-03 05:56:41.107992: step 49000, loss = 0.1373, acc = 0.9525, f1neg = 0.9571, f1pos = 0.9468, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 05:56:41.581910: step 49000, loss = 0.1395, acc = 0.9485, f1neg = 0.9481, f1pos = 0.9489, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 05:56:42.091410: step 49000, loss = 0.1698, acc = 0.9375, f1neg = 0.9347, f1pos = 0.9401, f1 = 0.9374
[Test_batch(10)(2000,22000)] 2017-05-03 05:56:42.597858: step 49000, loss = 0.1477, acc = 0.9510, f1neg = 0.9452, f1pos = 0.9557, f1 = 0.9504
[Test_batch(11)(2000,24000)] 2017-05-03 05:56:43.070949: step 49000, loss = 0.1467, acc = 0.9500, f1neg = 0.9517, f1pos = 0.9481, f1 = 0.9499
[Test_batch(12)(2000,26000)] 2017-05-03 05:56:43.568069: step 49000, loss = 0.1236, acc = 0.9615, f1neg = 0.9631, f1pos = 0.9597, f1 = 0.9614
[Test_batch(13)(2000,28000)] 2017-05-03 05:56:44.036154: step 49000, loss = 0.1489, acc = 0.9510, f1neg = 0.9462, f1pos = 0.9550, f1 = 0.9506
[Test_batch(14)(2000,30000)] 2017-05-03 05:56:44.545701: step 49000, loss = 0.1240, acc = 0.9585, f1neg = 0.9527, f1pos = 0.9631, f1 = 0.9579
[Test_batch(15)(2000,32000)] 2017-05-03 05:56:45.022947: step 49000, loss = 0.1467, acc = 0.9560, f1neg = 0.9557, f1pos = 0.9563, f1 = 0.9560
[Test_batch(16)(2000,34000)] 2017-05-03 05:56:45.500583: step 49000, loss = 0.1285, acc = 0.9565, f1neg = 0.9557, f1pos = 0.9573, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 05:56:46.005282: step 49000, loss = 0.1194, acc = 0.9665, f1neg = 0.9635, f1pos = 0.9690, f1 = 0.9663
[Test_batch(18)(2000,38000)] 2017-05-03 05:56:46.544347: step 49000, loss = 0.1121, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 05:56:46.544437: step 49000, acc = 0.9533, f1 = 0.9531
[Status] 2017-05-03 05:56:46.544464: step 49000, maxindex = 49000, maxdev = 0.9618, maxtst = 0.9533
2017-05-03 05:56:58.839822: step 49010, loss = 0.1529, acc = 0.9420 (234.0 examples/sec; 0.273 sec/batch)
2017-05-03 05:57:08.041074: step 49020, loss = 0.1087, acc = 0.9600 (256.3 examples/sec; 0.250 sec/batch)
2017-05-03 05:57:17.029369: step 49030, loss = 0.1582, acc = 0.9540 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 05:57:26.132285: step 49040, loss = 0.1013, acc = 0.9820 (265.1 examples/sec; 0.241 sec/batch)
2017-05-03 05:57:35.116217: step 49050, loss = 0.1259, acc = 0.9700 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 05:57:44.093908: step 49060, loss = 0.1391, acc = 0.9580 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 05:57:53.043564: step 49070, loss = 0.1166, acc = 0.9660 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 05:58:02.027474: step 49080, loss = 0.1309, acc = 0.9600 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 05:58:11.278772: step 49090, loss = 0.1259, acc = 0.9600 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 05:58:20.300934: step 49100, loss = 0.1102, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 05:58:29.269922: step 49110, loss = 0.1404, acc = 0.9460 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 05:58:38.311720: step 49120, loss = 0.1337, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 05:58:47.358107: step 49130, loss = 0.1471, acc = 0.9500 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 05:58:56.443442: step 49140, loss = 0.1254, acc = 0.9660 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 05:59:05.529417: step 49150, loss = 0.1545, acc = 0.9380 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 05:59:14.541263: step 49160, loss = 0.1222, acc = 0.9620 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 05:59:23.650915: step 49170, loss = 0.1239, acc = 0.9520 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 05:59:32.728570: step 49180, loss = 0.1387, acc = 0.9520 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 05:59:41.796551: step 49190, loss = 0.1311, acc = 0.9600 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 05:59:50.846320: step 49200, loss = 0.1433, acc = 0.9500 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 05:59:59.896230: step 49210, loss = 0.1236, acc = 0.9560 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 06:00:09.032450: step 49220, loss = 0.1123, acc = 0.9620 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 06:00:17.992376: step 49230, loss = 0.1184, acc = 0.9700 (299.8 examples/sec; 0.213 sec/batch)
2017-05-03 06:00:27.631227: step 49240, loss = 0.1466, acc = 0.9480 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 06:00:36.751534: step 49250, loss = 0.1091, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 06:00:45.910702: step 49260, loss = 0.1413, acc = 0.9580 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 06:00:54.936186: step 49270, loss = 0.1140, acc = 0.9680 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 06:01:04.012066: step 49280, loss = 0.1351, acc = 0.9580 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 06:01:13.085304: step 49290, loss = 0.1448, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 06:01:22.112821: step 49300, loss = 0.1352, acc = 0.9560 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:01:31.072702: step 49310, loss = 0.1586, acc = 0.9480 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 06:01:40.132571: step 49320, loss = 0.1111, acc = 0.9640 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 06:01:49.417828: step 49330, loss = 0.1166, acc = 0.9580 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 06:01:58.398969: step 49340, loss = 0.1270, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 06:02:07.614368: step 49350, loss = 0.1445, acc = 0.9480 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 06:02:16.571883: step 49360, loss = 0.1405, acc = 0.9580 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 06:02:25.712284: step 49370, loss = 0.0967, acc = 0.9780 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 06:02:34.796522: step 49380, loss = 0.1297, acc = 0.9500 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 06:02:43.827958: step 49390, loss = 0.1156, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:02:54.056097: step 49400, loss = 0.1326, acc = 0.9560 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 06:03:03.035139: step 49410, loss = 0.1360, acc = 0.9540 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 06:03:11.995798: step 49420, loss = 0.1062, acc = 0.9740 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 06:03:20.990621: step 49430, loss = 0.1582, acc = 0.9480 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 06:03:30.040810: step 49440, loss = 0.1413, acc = 0.9560 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 06:03:39.064216: step 49450, loss = 0.1419, acc = 0.9660 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 06:03:48.263516: step 49460, loss = 0.1343, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 06:03:57.335774: step 49470, loss = 0.1196, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 06:04:06.459858: step 49480, loss = 0.1314, acc = 0.9600 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 06:04:15.446365: step 49490, loss = 0.1343, acc = 0.9540 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 06:04:24.812410: step 49500, loss = 0.1204, acc = 0.9680 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 06:04:34.168909: step 49510, loss = 0.1154, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 06:04:43.264538: step 49520, loss = 0.1114, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 06:04:52.318862: step 49530, loss = 0.1230, acc = 0.9620 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 06:05:01.425442: step 49540, loss = 0.1386, acc = 0.9580 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 06:05:10.302161: step 49550, loss = 0.1583, acc = 0.9600 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 06:05:19.501505: step 49560, loss = 0.1473, acc = 0.9500 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 06:05:28.446095: step 49570, loss = 0.1399, acc = 0.9600 (301.6 examples/sec; 0.212 sec/batch)
2017-05-03 06:05:37.465145: step 49580, loss = 0.0991, acc = 0.9740 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 06:05:46.587066: step 49590, loss = 0.1474, acc = 0.9620 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:05:55.598429: step 49600, loss = 0.1129, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 06:06:04.693445: step 49610, loss = 0.1327, acc = 0.9540 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 06:06:14.102778: step 49620, loss = 0.1093, acc = 0.9580 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 06:06:23.112948: step 49630, loss = 0.1214, acc = 0.9620 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 06:06:32.246157: step 49640, loss = 0.1096, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 06:06:41.229265: step 49650, loss = 0.1198, acc = 0.9600 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 06:06:50.240431: step 49660, loss = 0.1223, acc = 0.9580 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 06:06:59.335061: step 49670, loss = 0.1463, acc = 0.9420 (249.6 examples/sec; 0.256 sec/batch)
2017-05-03 06:07:08.396676: step 49680, loss = 0.1113, acc = 0.9700 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 06:07:17.421008: step 49690, loss = 0.1510, acc = 0.9440 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 06:07:26.321766: step 49700, loss = 0.1601, acc = 0.9580 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 06:07:35.329207: step 49710, loss = 0.1328, acc = 0.9580 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 06:07:44.512851: step 49720, loss = 0.1303, acc = 0.9580 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 06:07:53.573881: step 49730, loss = 0.1461, acc = 0.9480 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 06:08:02.541428: step 49740, loss = 0.1545, acc = 0.9500 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 06:08:11.734213: step 49750, loss = 0.1391, acc = 0.9520 (261.0 examples/sec; 0.245 sec/batch)
2017-05-03 06:08:20.862838: step 49760, loss = 0.1546, acc = 0.9420 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 06:08:30.067032: step 49770, loss = 0.1156, acc = 0.9620 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 06:08:39.149615: step 49780, loss = 0.1245, acc = 0.9580 (267.4 examples/sec; 0.239 sec/batch)
2017-05-03 06:08:48.235770: step 49790, loss = 0.1229, acc = 0.9580 (299.7 examples/sec; 0.214 sec/batch)
2017-05-03 06:08:57.243537: step 49800, loss = 0.1232, acc = 0.9580 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 06:09:06.453700: step 49810, loss = 0.1332, acc = 0.9600 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 06:09:15.583972: step 49820, loss = 0.1197, acc = 0.9680 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 06:09:24.618690: step 49830, loss = 0.1078, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 06:09:33.749785: step 49840, loss = 0.1167, acc = 0.9620 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 06:09:42.687671: step 49850, loss = 0.1473, acc = 0.9620 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 06:09:51.796293: step 49860, loss = 0.1331, acc = 0.9540 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 06:10:00.732891: step 49870, loss = 0.1466, acc = 0.9540 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 06:10:09.790028: step 49880, loss = 0.1146, acc = 0.9580 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 06:10:19.210889: step 49890, loss = 0.1362, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 06:10:28.462303: step 49900, loss = 0.1316, acc = 0.9620 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 06:10:37.482609: step 49910, loss = 0.1487, acc = 0.9360 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 06:10:46.443727: step 49920, loss = 0.1424, acc = 0.9580 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 06:10:55.635931: step 49930, loss = 0.1290, acc = 0.9580 (214.2 examples/sec; 0.299 sec/batch)
2017-05-03 06:11:04.510357: step 49940, loss = 0.1276, acc = 0.9500 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 06:11:13.649890: step 49950, loss = 0.1247, acc = 0.9540 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 06:11:22.600284: step 49960, loss = 0.1248, acc = 0.9620 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 06:11:31.775907: step 49970, loss = 0.1123, acc = 0.9680 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 06:11:41.046778: step 49980, loss = 0.1104, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 06:11:50.166332: step 49990, loss = 0.1224, acc = 0.9540 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 06:11:59.402602: step 50000, loss = 0.1305, acc = 0.9580 (287.6 examples/sec; 0.223 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 06:11:59.909976: step 50000, loss = 0.1103, acc = 0.9600, f1neg = 0.9562, f1pos = 0.9632, f1 = 0.9597
[Eval_batch(1)(2000,4000)] 2017-05-03 06:12:00.393588: step 50000, loss = 0.1157, acc = 0.9665, f1neg = 0.9612, f1pos = 0.9705, f1 = 0.9659
[Eval_batch(2)(2000,6000)] 2017-05-03 06:12:00.896410: step 50000, loss = 0.1251, acc = 0.9575, f1neg = 0.9550, f1pos = 0.9598, f1 = 0.9574
[Eval_batch(3)(2000,8000)] 2017-05-03 06:12:01.371835: step 50000, loss = 0.1295, acc = 0.9580, f1neg = 0.9581, f1pos = 0.9579, f1 = 0.9580
[Eval_batch(4)(2000,10000)] 2017-05-03 06:12:01.842777: step 50000, loss = 0.1314, acc = 0.9560, f1neg = 0.9574, f1pos = 0.9545, f1 = 0.9559
[Eval_batch(5)(2000,12000)] 2017-05-03 06:12:02.353327: step 50000, loss = 0.1287, acc = 0.9550, f1neg = 0.9520, f1pos = 0.9576, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 06:12:02.866619: step 50000, loss = 0.1187, acc = 0.9665, f1neg = 0.9660, f1pos = 0.9670, f1 = 0.9665
[Eval_batch(7)(2000,16000)] 2017-05-03 06:12:03.341637: step 50000, loss = 0.1186, acc = 0.9605, f1neg = 0.9528, f1pos = 0.9660, f1 = 0.9594
[Eval_batch(8)(2000,18000)] 2017-05-03 06:12:03.815905: step 50000, loss = 0.1129, acc = 0.9625, f1neg = 0.9573, f1pos = 0.9666, f1 = 0.9619
[Eval_batch(9)(2000,20000)] 2017-05-03 06:12:04.289991: step 50000, loss = 0.1178, acc = 0.9630, f1neg = 0.9605, f1pos = 0.9652, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-03 06:12:04.768187: step 50000, loss = 0.1256, acc = 0.9580, f1neg = 0.9553, f1pos = 0.9604, f1 = 0.9578
[Eval_batch(11)(2000,24000)] 2017-05-03 06:12:05.282184: step 50000, loss = 0.1265, acc = 0.9605, f1neg = 0.9545, f1pos = 0.9651, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-03 06:12:05.758490: step 50000, loss = 0.1232, acc = 0.9530, f1neg = 0.9517, f1pos = 0.9542, f1 = 0.9530
[Eval_batch(13)(2000,28000)] 2017-05-03 06:12:06.225475: step 50000, loss = 0.1123, acc = 0.9650, f1neg = 0.9564, f1pos = 0.9708, f1 = 0.9636
[Eval_batch(14)(2000,30000)] 2017-05-03 06:12:06.701082: step 50000, loss = 0.1392, acc = 0.9545, f1neg = 0.9461, f1pos = 0.9607, f1 = 0.9534
[Eval_batch(15)(2000,32000)] 2017-05-03 06:12:07.201115: step 50000, loss = 0.1161, acc = 0.9675, f1neg = 0.9650, f1pos = 0.9697, f1 = 0.9673
[Eval_batch(16)(2000,34000)] 2017-05-03 06:12:07.706918: step 50000, loss = 0.1116, acc = 0.9660, f1neg = 0.9647, f1pos = 0.9672, f1 = 0.9660
[Eval_batch(17)(2000,36000)] 2017-05-03 06:12:08.215458: step 50000, loss = 0.1378, acc = 0.9555, f1neg = 0.9559, f1pos = 0.9551, f1 = 0.9555
[Eval_batch(18)(2000,38000)] 2017-05-03 06:12:08.716546: step 50000, loss = 0.1311, acc = 0.9570, f1neg = 0.9588, f1pos = 0.9551, f1 = 0.9569
[Eval_batch(19)(2000,40000)] 2017-05-03 06:12:09.213236: step 50000, loss = 0.1138, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 06:12:09.690835: step 50000, loss = 0.1166, acc = 0.9610, f1neg = 0.9655, f1pos = 0.9551, f1 = 0.9603
[Eval_batch(21)(2000,44000)] 2017-05-03 06:12:10.155789: step 50000, loss = 0.1104, acc = 0.9635, f1neg = 0.9605, f1pos = 0.9661, f1 = 0.9633
[Eval_batch(22)(2000,46000)] 2017-05-03 06:12:10.670717: step 50000, loss = 0.1164, acc = 0.9630, f1neg = 0.9636, f1pos = 0.9624, f1 = 0.9630
[Eval_batch(23)(2000,48000)] 2017-05-03 06:12:11.171473: step 50000, loss = 0.1235, acc = 0.9615, f1neg = 0.9567, f1pos = 0.9654, f1 = 0.9610
[Eval_batch(24)(2000,50000)] 2017-05-03 06:12:11.643585: step 50000, loss = 0.1086, acc = 0.9660, f1neg = 0.9612, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 06:12:12.120682: step 50000, loss = 0.0997, acc = 0.9715, f1neg = 0.9689, f1pos = 0.9737, f1 = 0.9713
[Eval_batch(26)(2000,54000)] 2017-05-03 06:12:12.601677: step 50000, loss = 0.1131, acc = 0.9610, f1neg = 0.9632, f1pos = 0.9585, f1 = 0.9609
[Eval_batch(27)(2000,56000)] 2017-05-03 06:12:13.209066: step 50000, loss = 0.1000, acc = 0.9710, f1neg = 0.9696, f1pos = 0.9722, f1 = 0.9709
[Eval] 2017-05-03 06:12:13.209158: step 50000, acc = 0.9618, f1 = 0.9614
[Test_batch(0)(2000,2000)] 2017-05-03 06:12:13.720476: step 50000, loss = 0.1483, acc = 0.9510, f1neg = 0.9536, f1pos = 0.9481, f1 = 0.9509
[Test_batch(1)(2000,4000)] 2017-05-03 06:12:14.161887: step 50000, loss = 0.1297, acc = 0.9595, f1neg = 0.9641, f1pos = 0.9535, f1 = 0.9588
[Test_batch(2)(2000,6000)] 2017-05-03 06:12:14.636613: step 50000, loss = 0.1452, acc = 0.9555, f1neg = 0.9592, f1pos = 0.9510, f1 = 0.9551
[Test_batch(3)(2000,8000)] 2017-05-03 06:12:15.150804: step 50000, loss = 0.1519, acc = 0.9485, f1neg = 0.9520, f1pos = 0.9445, f1 = 0.9482
[Test_batch(4)(2000,10000)] 2017-05-03 06:12:15.657883: step 50000, loss = 0.1495, acc = 0.9490, f1neg = 0.9494, f1pos = 0.9486, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 06:12:16.157702: step 50000, loss = 0.1620, acc = 0.9430, f1neg = 0.9440, f1pos = 0.9420, f1 = 0.9430
[Test_batch(6)(2000,14000)] 2017-05-03 06:12:16.661620: step 50000, loss = 0.1457, acc = 0.9470, f1neg = 0.9454, f1pos = 0.9485, f1 = 0.9470
[Test_batch(7)(2000,16000)] 2017-05-03 06:12:17.159383: step 50000, loss = 0.1376, acc = 0.9530, f1neg = 0.9574, f1pos = 0.9477, f1 = 0.9525
[Test_batch(8)(2000,18000)] 2017-05-03 06:12:17.659896: step 50000, loss = 0.1389, acc = 0.9485, f1neg = 0.9480, f1pos = 0.9490, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 06:12:18.162965: step 50000, loss = 0.1686, acc = 0.9385, f1neg = 0.9355, f1pos = 0.9412, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 06:12:18.674413: step 50000, loss = 0.1459, acc = 0.9520, f1neg = 0.9462, f1pos = 0.9567, f1 = 0.9514
[Test_batch(11)(2000,24000)] 2017-05-03 06:12:19.182018: step 50000, loss = 0.1470, acc = 0.9505, f1neg = 0.9522, f1pos = 0.9487, f1 = 0.9504
[Test_batch(12)(2000,26000)] 2017-05-03 06:12:19.687070: step 50000, loss = 0.1226, acc = 0.9615, f1neg = 0.9631, f1pos = 0.9598, f1 = 0.9614
[Test_batch(13)(2000,28000)] 2017-05-03 06:12:20.182672: step 50000, loss = 0.1472, acc = 0.9505, f1neg = 0.9456, f1pos = 0.9546, f1 = 0.9501
[Test_batch(14)(2000,30000)] 2017-05-03 06:12:20.686159: step 50000, loss = 0.1222, acc = 0.9610, f1neg = 0.9554, f1pos = 0.9653, f1 = 0.9604
[Test_batch(15)(2000,32000)] 2017-05-03 06:12:21.192848: step 50000, loss = 0.1465, acc = 0.9565, f1neg = 0.9562, f1pos = 0.9568, f1 = 0.9565
[Test_batch(16)(2000,34000)] 2017-05-03 06:12:21.698672: step 50000, loss = 0.1275, acc = 0.9580, f1neg = 0.9571, f1pos = 0.9589, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 06:12:22.203772: step 50000, loss = 0.1193, acc = 0.9635, f1neg = 0.9603, f1pos = 0.9663, f1 = 0.9633
[Test_batch(18)(2000,38000)] 2017-05-03 06:12:22.776171: step 50000, loss = 0.1117, acc = 0.9645, f1neg = 0.9639, f1pos = 0.9651, f1 = 0.9645
[Test] 2017-05-03 06:12:22.776236: step 50000, acc = 0.9532, f1 = 0.9530
[Status] 2017-05-03 06:12:22.776250: step 50000, maxindex = 49000, maxdev = 0.9618, maxtst = 0.9533
2017-05-03 06:12:32.004106: step 50010, loss = 0.0957, acc = 0.9760 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 06:12:41.103816: step 50020, loss = 0.1091, acc = 0.9680 (251.4 examples/sec; 0.255 sec/batch)
2017-05-03 06:12:50.005436: step 50030, loss = 0.1536, acc = 0.9480 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 06:12:59.586682: step 50040, loss = 0.1587, acc = 0.9400 (215.1 examples/sec; 0.298 sec/batch)
2017-05-03 06:13:08.735867: step 50050, loss = 0.1209, acc = 0.9580 (266.3 examples/sec; 0.240 sec/batch)
2017-05-03 06:13:18.025644: step 50060, loss = 0.1339, acc = 0.9540 (239.6 examples/sec; 0.267 sec/batch)
2017-05-03 06:13:27.122761: step 50070, loss = 0.1490, acc = 0.9500 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 06:13:36.107488: step 50080, loss = 0.1426, acc = 0.9560 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 06:13:45.330427: step 50090, loss = 0.1175, acc = 0.9680 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 06:13:54.300340: step 50100, loss = 0.1418, acc = 0.9480 (297.6 examples/sec; 0.215 sec/batch)
2017-05-03 06:14:03.409050: step 50110, loss = 0.1394, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 06:14:12.431257: step 50120, loss = 0.1143, acc = 0.9680 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 06:14:21.467340: step 50130, loss = 0.1154, acc = 0.9600 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 06:14:30.569598: step 50140, loss = 0.1441, acc = 0.9480 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 06:14:39.670203: step 50150, loss = 0.1152, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 06:14:48.731455: step 50160, loss = 0.1230, acc = 0.9640 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 06:14:57.847837: step 50170, loss = 0.1277, acc = 0.9520 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 06:15:06.819060: step 50180, loss = 0.1173, acc = 0.9680 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 06:15:15.700930: step 50190, loss = 0.0961, acc = 0.9700 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 06:15:24.777721: step 50200, loss = 0.1448, acc = 0.9540 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 06:15:33.792443: step 50210, loss = 0.1286, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 06:15:42.793091: step 50220, loss = 0.1305, acc = 0.9580 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 06:15:51.877927: step 50230, loss = 0.1430, acc = 0.9520 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 06:16:00.858031: step 50240, loss = 0.1220, acc = 0.9580 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 06:16:09.974396: step 50250, loss = 0.1156, acc = 0.9620 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 06:16:19.110452: step 50260, loss = 0.1436, acc = 0.9540 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 06:16:28.516523: step 50270, loss = 0.1565, acc = 0.9460 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 06:16:37.577792: step 50280, loss = 0.1154, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 06:16:46.494685: step 50290, loss = 0.1440, acc = 0.9440 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 06:16:55.450814: step 50300, loss = 0.1387, acc = 0.9480 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 06:17:04.497886: step 50310, loss = 0.1364, acc = 0.9540 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 06:17:13.502546: step 50320, loss = 0.1296, acc = 0.9660 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 06:17:22.794300: step 50330, loss = 0.1213, acc = 0.9680 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 06:17:32.018197: step 50340, loss = 0.1201, acc = 0.9640 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 06:17:40.918467: step 50350, loss = 0.1450, acc = 0.9540 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 06:17:50.172620: step 50360, loss = 0.1160, acc = 0.9560 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 06:17:59.256198: step 50370, loss = 0.1448, acc = 0.9480 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 06:18:08.189612: step 50380, loss = 0.1169, acc = 0.9620 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 06:18:17.321400: step 50390, loss = 0.1536, acc = 0.9500 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 06:18:27.171601: step 50400, loss = 0.1271, acc = 0.9640 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 06:18:36.384724: step 50410, loss = 0.1467, acc = 0.9620 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 06:18:45.366580: step 50420, loss = 0.1258, acc = 0.9540 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 06:18:54.118484: step 50430, loss = 0.1236, acc = 0.9620 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 06:19:03.248561: step 50440, loss = 0.1288, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 06:19:12.118292: step 50450, loss = 0.1382, acc = 0.9580 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 06:19:21.191094: step 50460, loss = 0.1237, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 06:19:30.117203: step 50470, loss = 0.1316, acc = 0.9600 (272.9 examples/sec; 0.235 sec/batch)
2017-05-03 06:19:39.280387: step 50480, loss = 0.1147, acc = 0.9640 (245.1 examples/sec; 0.261 sec/batch)
2017-05-03 06:19:48.358864: step 50490, loss = 0.1297, acc = 0.9560 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 06:19:57.335882: step 50500, loss = 0.1357, acc = 0.9520 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 06:20:06.567106: step 50510, loss = 0.1214, acc = 0.9640 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 06:20:15.724735: step 50520, loss = 0.1416, acc = 0.9540 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 06:20:24.744827: step 50530, loss = 0.1230, acc = 0.9640 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 06:20:33.910199: step 50540, loss = 0.1721, acc = 0.9280 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 06:20:43.283697: step 50550, loss = 0.1446, acc = 0.9620 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 06:20:52.348107: step 50560, loss = 0.1358, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 06:21:01.724418: step 50570, loss = 0.1430, acc = 0.9580 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 06:21:10.692683: step 50580, loss = 0.1335, acc = 0.9620 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 06:21:19.609620: step 50590, loss = 0.1367, acc = 0.9660 (262.5 examples/sec; 0.244 sec/batch)
2017-05-03 06:21:28.670415: step 50600, loss = 0.1558, acc = 0.9400 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 06:21:37.691829: step 50610, loss = 0.1412, acc = 0.9540 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 06:21:46.713782: step 50620, loss = 0.1287, acc = 0.9600 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 06:21:55.658845: step 50630, loss = 0.1233, acc = 0.9660 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 06:22:04.732270: step 50640, loss = 0.1212, acc = 0.9560 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 06:22:14.207778: step 50650, loss = 0.1020, acc = 0.9680 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 06:22:23.233701: step 50660, loss = 0.1321, acc = 0.9620 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 06:22:32.245212: step 50670, loss = 0.1325, acc = 0.9560 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 06:22:41.328570: step 50680, loss = 0.1134, acc = 0.9560 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 06:22:50.531734: step 50690, loss = 0.1252, acc = 0.9580 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 06:22:59.612171: step 50700, loss = 0.1334, acc = 0.9620 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 06:23:08.709527: step 50710, loss = 0.1504, acc = 0.9500 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 06:23:17.692306: step 50720, loss = 0.1220, acc = 0.9640 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:23:26.766632: step 50730, loss = 0.1208, acc = 0.9660 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 06:23:35.828802: step 50740, loss = 0.1146, acc = 0.9680 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 06:23:44.757537: step 50750, loss = 0.1386, acc = 0.9460 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 06:23:53.820080: step 50760, loss = 0.1562, acc = 0.9480 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 06:24:02.917414: step 50770, loss = 0.1393, acc = 0.9420 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 06:24:11.936400: step 50780, loss = 0.1450, acc = 0.9520 (297.8 examples/sec; 0.215 sec/batch)
2017-05-03 06:24:20.960067: step 50790, loss = 0.1532, acc = 0.9460 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 06:24:30.104689: step 50800, loss = 0.1303, acc = 0.9600 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 06:24:39.016468: step 50810, loss = 0.1151, acc = 0.9600 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 06:24:47.933949: step 50820, loss = 0.1295, acc = 0.9640 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 06:24:56.998773: step 50830, loss = 0.1610, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 06:25:05.963959: step 50840, loss = 0.1490, acc = 0.9520 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 06:25:14.935631: step 50850, loss = 0.1214, acc = 0.9600 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 06:25:23.910871: step 50860, loss = 0.1216, acc = 0.9580 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 06:25:32.903236: step 50870, loss = 0.1225, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 06:25:41.838964: step 50880, loss = 0.1280, acc = 0.9560 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 06:25:50.913266: step 50890, loss = 0.1406, acc = 0.9500 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 06:26:00.073610: step 50900, loss = 0.1245, acc = 0.9580 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 06:26:09.274099: step 50910, loss = 0.1076, acc = 0.9780 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 06:26:18.187076: step 50920, loss = 0.1267, acc = 0.9580 (296.2 examples/sec; 0.216 sec/batch)
2017-05-03 06:26:27.277489: step 50930, loss = 0.1248, acc = 0.9600 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 06:26:36.339368: step 50940, loss = 0.1233, acc = 0.9500 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 06:26:45.325278: step 50950, loss = 0.1330, acc = 0.9520 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 06:26:54.329752: step 50960, loss = 0.1324, acc = 0.9500 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 06:27:03.407807: step 50970, loss = 0.1186, acc = 0.9620 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 06:27:12.541475: step 50980, loss = 0.1466, acc = 0.9440 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 06:27:21.475778: step 50990, loss = 0.1208, acc = 0.9600 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 06:27:30.496017: step 51000, loss = 0.0997, acc = 0.9760 (280.1 examples/sec; 0.229 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 06:27:30.990794: step 51000, loss = 0.1137, acc = 0.9655, f1neg = 0.9629, f1pos = 0.9677, f1 = 0.9653
[Eval_batch(1)(2000,4000)] 2017-05-03 06:27:31.495013: step 51000, loss = 0.1287, acc = 0.9595, f1neg = 0.9540, f1pos = 0.9638, f1 = 0.9589
[Eval_batch(2)(2000,6000)] 2017-05-03 06:27:32.007230: step 51000, loss = 0.1316, acc = 0.9535, f1neg = 0.9518, f1pos = 0.9551, f1 = 0.9534
[Eval_batch(3)(2000,8000)] 2017-05-03 06:27:32.514376: step 51000, loss = 0.1349, acc = 0.9535, f1neg = 0.9545, f1pos = 0.9524, f1 = 0.9535
[Eval_batch(4)(2000,10000)] 2017-05-03 06:27:33.021798: step 51000, loss = 0.1364, acc = 0.9500, f1neg = 0.9527, f1pos = 0.9470, f1 = 0.9498
[Eval_batch(5)(2000,12000)] 2017-05-03 06:27:33.495013: step 51000, loss = 0.1348, acc = 0.9490, f1neg = 0.9469, f1pos = 0.9509, f1 = 0.9489
[Eval_batch(6)(2000,14000)] 2017-05-03 06:27:33.997197: step 51000, loss = 0.1194, acc = 0.9640, f1neg = 0.9639, f1pos = 0.9641, f1 = 0.9640
[Eval_batch(7)(2000,16000)] 2017-05-03 06:27:34.471049: step 51000, loss = 0.1282, acc = 0.9580, f1neg = 0.9512, f1pos = 0.9632, f1 = 0.9572
[Eval_batch(8)(2000,18000)] 2017-05-03 06:27:34.936901: step 51000, loss = 0.1206, acc = 0.9595, f1neg = 0.9552, f1pos = 0.9631, f1 = 0.9591
[Eval_batch(9)(2000,20000)] 2017-05-03 06:27:35.441795: step 51000, loss = 0.1261, acc = 0.9605, f1neg = 0.9586, f1pos = 0.9623, f1 = 0.9604
[Eval_batch(10)(2000,22000)] 2017-05-03 06:27:35.946567: step 51000, loss = 0.1333, acc = 0.9595, f1neg = 0.9579, f1pos = 0.9610, f1 = 0.9594
[Eval_batch(11)(2000,24000)] 2017-05-03 06:27:36.452781: step 51000, loss = 0.1361, acc = 0.9540, f1neg = 0.9480, f1pos = 0.9587, f1 = 0.9534
[Eval_batch(12)(2000,26000)] 2017-05-03 06:27:36.965447: step 51000, loss = 0.1286, acc = 0.9525, f1neg = 0.9522, f1pos = 0.9528, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-03 06:27:37.472507: step 51000, loss = 0.1196, acc = 0.9670, f1neg = 0.9595, f1pos = 0.9722, f1 = 0.9658
[Eval_batch(14)(2000,30000)] 2017-05-03 06:27:37.948204: step 51000, loss = 0.1463, acc = 0.9490, f1neg = 0.9408, f1pos = 0.9552, f1 = 0.9480
[Eval_batch(15)(2000,32000)] 2017-05-03 06:27:38.431144: step 51000, loss = 0.1258, acc = 0.9605, f1neg = 0.9583, f1pos = 0.9625, f1 = 0.9604
[Eval_batch(16)(2000,34000)] 2017-05-03 06:27:38.909859: step 51000, loss = 0.1191, acc = 0.9630, f1neg = 0.9622, f1pos = 0.9638, f1 = 0.9630
[Eval_batch(17)(2000,36000)] 2017-05-03 06:27:39.405874: step 51000, loss = 0.1459, acc = 0.9565, f1neg = 0.9576, f1pos = 0.9553, f1 = 0.9565
[Eval_batch(18)(2000,38000)] 2017-05-03 06:27:39.869020: step 51000, loss = 0.1351, acc = 0.9575, f1neg = 0.9602, f1pos = 0.9545, f1 = 0.9573
[Eval_batch(19)(2000,40000)] 2017-05-03 06:27:40.363358: step 51000, loss = 0.1212, acc = 0.9655, f1neg = 0.9610, f1pos = 0.9691, f1 = 0.9650
[Eval_batch(20)(2000,42000)] 2017-05-03 06:27:40.840551: step 51000, loss = 0.1159, acc = 0.9610, f1neg = 0.9659, f1pos = 0.9545, f1 = 0.9602
[Eval_batch(21)(2000,44000)] 2017-05-03 06:27:41.333423: step 51000, loss = 0.1240, acc = 0.9565, f1neg = 0.9537, f1pos = 0.9590, f1 = 0.9563
[Eval_batch(22)(2000,46000)] 2017-05-03 06:27:41.806104: step 51000, loss = 0.1234, acc = 0.9610, f1neg = 0.9620, f1pos = 0.9600, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 06:27:42.309076: step 51000, loss = 0.1350, acc = 0.9560, f1neg = 0.9517, f1pos = 0.9596, f1 = 0.9556
[Eval_batch(24)(2000,50000)] 2017-05-03 06:27:42.770621: step 51000, loss = 0.1187, acc = 0.9625, f1neg = 0.9580, f1pos = 0.9661, f1 = 0.9621
[Eval_batch(25)(2000,52000)] 2017-05-03 06:27:43.265857: step 51000, loss = 0.1054, acc = 0.9680, f1neg = 0.9655, f1pos = 0.9702, f1 = 0.9678
[Eval_batch(26)(2000,54000)] 2017-05-03 06:27:43.747457: step 51000, loss = 0.1159, acc = 0.9640, f1neg = 0.9666, f1pos = 0.9610, f1 = 0.9638
[Eval_batch(27)(2000,56000)] 2017-05-03 06:27:44.327716: step 51000, loss = 0.1012, acc = 0.9725, f1neg = 0.9716, f1pos = 0.9733, f1 = 0.9725
[Eval] 2017-05-03 06:27:44.327788: step 51000, acc = 0.9593, f1 = 0.9590
[Test_batch(0)(2000,2000)] 2017-05-03 06:27:44.804093: step 51000, loss = 0.1558, acc = 0.9515, f1neg = 0.9550, f1pos = 0.9474, f1 = 0.9512
[Test_batch(1)(2000,4000)] 2017-05-03 06:27:45.275774: step 51000, loss = 0.1346, acc = 0.9540, f1neg = 0.9600, f1pos = 0.9459, f1 = 0.9529
[Test_batch(2)(2000,6000)] 2017-05-03 06:27:45.748243: step 51000, loss = 0.1489, acc = 0.9545, f1neg = 0.9591, f1pos = 0.9488, f1 = 0.9539
[Test_batch(3)(2000,8000)] 2017-05-03 06:27:46.230887: step 51000, loss = 0.1597, acc = 0.9410, f1neg = 0.9462, f1pos = 0.9347, f1 = 0.9405
[Test_batch(4)(2000,10000)] 2017-05-03 06:27:46.735194: step 51000, loss = 0.1601, acc = 0.9450, f1neg = 0.9467, f1pos = 0.9432, f1 = 0.9449
[Test_batch(5)(2000,12000)] 2017-05-03 06:27:47.242772: step 51000, loss = 0.1727, acc = 0.9395, f1neg = 0.9420, f1pos = 0.9368, f1 = 0.9394
[Test_batch(6)(2000,14000)] 2017-05-03 06:27:47.686053: step 51000, loss = 0.1518, acc = 0.9475, f1neg = 0.9471, f1pos = 0.9479, f1 = 0.9475
[Test_batch(7)(2000,16000)] 2017-05-03 06:27:48.181361: step 51000, loss = 0.1447, acc = 0.9510, f1neg = 0.9565, f1pos = 0.9439, f1 = 0.9502
[Test_batch(8)(2000,18000)] 2017-05-03 06:27:48.650826: step 51000, loss = 0.1479, acc = 0.9490, f1neg = 0.9496, f1pos = 0.9484, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 06:27:49.131396: step 51000, loss = 0.1830, acc = 0.9285, f1neg = 0.9273, f1pos = 0.9297, f1 = 0.9285
[Test_batch(10)(2000,22000)] 2017-05-03 06:27:49.641615: step 51000, loss = 0.1631, acc = 0.9440, f1neg = 0.9387, f1pos = 0.9485, f1 = 0.9436
[Test_batch(11)(2000,24000)] 2017-05-03 06:27:50.146587: step 51000, loss = 0.1536, acc = 0.9500, f1neg = 0.9525, f1pos = 0.9473, f1 = 0.9499
[Test_batch(12)(2000,26000)] 2017-05-03 06:27:50.620336: step 51000, loss = 0.1336, acc = 0.9510, f1neg = 0.9538, f1pos = 0.9478, f1 = 0.9508
[Test_batch(13)(2000,28000)] 2017-05-03 06:27:51.104264: step 51000, loss = 0.1610, acc = 0.9500, f1neg = 0.9462, f1pos = 0.9533, f1 = 0.9497
[Test_batch(14)(2000,30000)] 2017-05-03 06:27:51.614996: step 51000, loss = 0.1365, acc = 0.9545, f1neg = 0.9492, f1pos = 0.9588, f1 = 0.9540
[Test_batch(15)(2000,32000)] 2017-05-03 06:27:52.079276: step 51000, loss = 0.1571, acc = 0.9520, f1neg = 0.9524, f1pos = 0.9516, f1 = 0.9520
[Test_batch(16)(2000,34000)] 2017-05-03 06:27:52.558686: step 51000, loss = 0.1375, acc = 0.9535, f1neg = 0.9535, f1pos = 0.9535, f1 = 0.9535
[Test_batch(17)(2000,36000)] 2017-05-03 06:27:53.061186: step 51000, loss = 0.1254, acc = 0.9595, f1neg = 0.9566, f1pos = 0.9620, f1 = 0.9593
[Test_batch(18)(2000,38000)] 2017-05-03 06:27:53.602176: step 51000, loss = 0.1145, acc = 0.9620, f1neg = 0.9621, f1pos = 0.9619, f1 = 0.9620
[Test] 2017-05-03 06:27:53.602265: step 51000, acc = 0.9494, f1 = 0.9491
[Status] 2017-05-03 06:27:53.602290: step 51000, maxindex = 49000, maxdev = 0.9618, maxtst = 0.9533
2017-05-03 06:28:02.749991: step 51010, loss = 0.1333, acc = 0.9560 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 06:28:11.727954: step 51020, loss = 0.1414, acc = 0.9540 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 06:28:20.988819: step 51030, loss = 0.1166, acc = 0.9580 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 06:28:30.023746: step 51040, loss = 0.1015, acc = 0.9720 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 06:28:39.024931: step 51050, loss = 0.1025, acc = 0.9740 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 06:28:48.134939: step 51060, loss = 0.1223, acc = 0.9560 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 06:28:57.262890: step 51070, loss = 0.1434, acc = 0.9420 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 06:29:06.301810: step 51080, loss = 0.1500, acc = 0.9560 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 06:29:15.370725: step 51090, loss = 0.1367, acc = 0.9560 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 06:29:24.322738: step 51100, loss = 0.1244, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 06:29:33.351083: step 51110, loss = 0.1228, acc = 0.9500 (295.7 examples/sec; 0.216 sec/batch)
2017-05-03 06:29:42.496276: step 51120, loss = 0.1053, acc = 0.9660 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 06:29:51.685989: step 51130, loss = 0.1306, acc = 0.9520 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 06:30:00.646915: step 51140, loss = 0.1377, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 06:30:09.930883: step 51150, loss = 0.1192, acc = 0.9580 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 06:30:18.974280: step 51160, loss = 0.1226, acc = 0.9600 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 06:30:27.897611: step 51170, loss = 0.1343, acc = 0.9620 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 06:30:36.994366: step 51180, loss = 0.1342, acc = 0.9440 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 06:30:45.993603: step 51190, loss = 0.1238, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 06:30:55.257085: step 51200, loss = 0.1798, acc = 0.9320 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 06:31:04.316585: step 51210, loss = 0.1227, acc = 0.9600 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 06:31:13.374548: step 51220, loss = 0.1324, acc = 0.9560 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 06:31:22.323820: step 51230, loss = 0.1424, acc = 0.9600 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 06:31:31.326579: step 51240, loss = 0.1403, acc = 0.9520 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 06:31:40.514967: step 51250, loss = 0.1379, acc = 0.9520 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 06:31:49.650092: step 51260, loss = 0.1145, acc = 0.9680 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 06:31:58.717021: step 51270, loss = 0.1187, acc = 0.9560 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 06:32:07.904618: step 51280, loss = 0.1166, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 06:32:16.968460: step 51290, loss = 0.1644, acc = 0.9380 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 06:32:26.065526: step 51300, loss = 0.1420, acc = 0.9580 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 06:32:35.256264: step 51310, loss = 0.1245, acc = 0.9600 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 06:32:44.249911: step 51320, loss = 0.1230, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 06:32:53.434362: step 51330, loss = 0.1148, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 06:33:02.460515: step 51340, loss = 0.1625, acc = 0.9300 (268.3 examples/sec; 0.239 sec/batch)
2017-05-03 06:33:11.681296: step 51350, loss = 0.1319, acc = 0.9560 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 06:33:20.586009: step 51360, loss = 0.1088, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 06:33:29.563918: step 51370, loss = 0.1581, acc = 0.9380 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 06:33:38.631051: step 51380, loss = 0.1181, acc = 0.9680 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 06:33:47.897250: step 51390, loss = 0.1018, acc = 0.9680 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 06:33:56.902253: step 51400, loss = 0.1539, acc = 0.9500 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 06:34:06.919209: step 51410, loss = 0.1364, acc = 0.9620 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 06:34:16.044066: step 51420, loss = 0.1146, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 06:34:24.890229: step 51430, loss = 0.1319, acc = 0.9560 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 06:34:34.343177: step 51440, loss = 0.1132, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 06:34:43.460737: step 51450, loss = 0.1350, acc = 0.9580 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 06:34:52.460587: step 51460, loss = 0.1240, acc = 0.9660 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:35:01.486551: step 51470, loss = 0.1507, acc = 0.9540 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 06:35:10.732482: step 51480, loss = 0.1400, acc = 0.9600 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 06:35:19.663015: step 51490, loss = 0.1462, acc = 0.9500 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 06:35:28.878132: step 51500, loss = 0.1071, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 06:35:37.889364: step 51510, loss = 0.1488, acc = 0.9540 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 06:35:46.945040: step 51520, loss = 0.1247, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 06:35:56.046764: step 51530, loss = 0.1401, acc = 0.9480 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 06:36:05.315338: step 51540, loss = 0.1169, acc = 0.9640 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 06:36:14.457879: step 51550, loss = 0.1467, acc = 0.9560 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 06:36:23.567943: step 51560, loss = 0.1352, acc = 0.9560 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 06:36:32.634896: step 51570, loss = 0.1367, acc = 0.9560 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 06:36:41.686753: step 51580, loss = 0.1310, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 06:36:50.663108: step 51590, loss = 0.0977, acc = 0.9720 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 06:36:59.618288: step 51600, loss = 0.1376, acc = 0.9520 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 06:37:08.630768: step 51610, loss = 0.1253, acc = 0.9700 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 06:37:17.788811: step 51620, loss = 0.1288, acc = 0.9660 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 06:37:26.757330: step 51630, loss = 0.1604, acc = 0.9380 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 06:37:35.787677: step 51640, loss = 0.1216, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 06:37:45.088227: step 51650, loss = 0.1282, acc = 0.9640 (260.9 examples/sec; 0.245 sec/batch)
2017-05-03 06:37:54.368548: step 51660, loss = 0.1265, acc = 0.9640 (263.2 examples/sec; 0.243 sec/batch)
2017-05-03 06:38:03.519614: step 51670, loss = 0.1383, acc = 0.9580 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 06:38:12.593057: step 51680, loss = 0.1495, acc = 0.9480 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 06:38:21.640673: step 51690, loss = 0.1193, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 06:38:30.783810: step 51700, loss = 0.1258, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 06:38:39.914461: step 51710, loss = 0.1296, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 06:38:48.978834: step 51720, loss = 0.1351, acc = 0.9560 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 06:38:58.055264: step 51730, loss = 0.0969, acc = 0.9780 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 06:39:07.052152: step 51740, loss = 0.1344, acc = 0.9500 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 06:39:16.221625: step 51750, loss = 0.1255, acc = 0.9580 (291.6 examples/sec; 0.220 sec/batch)
2017-05-03 06:39:25.233842: step 51760, loss = 0.1210, acc = 0.9680 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 06:39:34.333809: step 51770, loss = 0.1136, acc = 0.9580 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 06:39:43.509062: step 51780, loss = 0.1550, acc = 0.9440 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 06:39:52.502222: step 51790, loss = 0.1296, acc = 0.9640 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 06:40:01.688467: step 51800, loss = 0.1543, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 06:40:10.800309: step 51810, loss = 0.1516, acc = 0.9420 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 06:40:19.788771: step 51820, loss = 0.1158, acc = 0.9680 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 06:40:28.995694: step 51830, loss = 0.1218, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 06:40:38.073190: step 51840, loss = 0.1104, acc = 0.9700 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 06:40:47.006593: step 51850, loss = 0.1266, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 06:40:56.156424: step 51860, loss = 0.1216, acc = 0.9700 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 06:41:05.168018: step 51870, loss = 0.1160, acc = 0.9620 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 06:41:14.210731: step 51880, loss = 0.1077, acc = 0.9660 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 06:41:23.743252: step 51890, loss = 0.1312, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 06:41:32.792902: step 51900, loss = 0.1489, acc = 0.9500 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 06:41:41.732502: step 51910, loss = 0.1241, acc = 0.9600 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 06:41:50.812448: step 51920, loss = 0.1314, acc = 0.9480 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:41:59.909650: step 51930, loss = 0.1070, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 06:42:09.072396: step 51940, loss = 0.1119, acc = 0.9700 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 06:42:18.024101: step 51950, loss = 0.1271, acc = 0.9580 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 06:42:27.106647: step 51960, loss = 0.1378, acc = 0.9580 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 06:42:36.229590: step 51970, loss = 0.1430, acc = 0.9460 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 06:42:45.209349: step 51980, loss = 0.0953, acc = 0.9720 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 06:42:54.386775: step 51990, loss = 0.1420, acc = 0.9580 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 06:43:03.588652: step 52000, loss = 0.1386, acc = 0.9560 (282.7 examples/sec; 0.226 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 06:43:04.096732: step 52000, loss = 0.1119, acc = 0.9565, f1neg = 0.9521, f1pos = 0.9601, f1 = 0.9561
[Eval_batch(1)(2000,4000)] 2017-05-03 06:43:04.611806: step 52000, loss = 0.1135, acc = 0.9655, f1neg = 0.9597, f1pos = 0.9699, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 06:43:05.087264: step 52000, loss = 0.1261, acc = 0.9585, f1neg = 0.9558, f1pos = 0.9609, f1 = 0.9583
[Eval_batch(3)(2000,8000)] 2017-05-03 06:43:05.600717: step 52000, loss = 0.1303, acc = 0.9575, f1neg = 0.9574, f1pos = 0.9576, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 06:43:06.075313: step 52000, loss = 0.1320, acc = 0.9575, f1neg = 0.9586, f1pos = 0.9563, f1 = 0.9575
[Eval_batch(5)(2000,12000)] 2017-05-03 06:43:06.541841: step 52000, loss = 0.1287, acc = 0.9545, f1neg = 0.9511, f1pos = 0.9575, f1 = 0.9543
[Eval_batch(6)(2000,14000)] 2017-05-03 06:43:06.983016: step 52000, loss = 0.1201, acc = 0.9640, f1neg = 0.9633, f1pos = 0.9647, f1 = 0.9640
[Eval_batch(7)(2000,16000)] 2017-05-03 06:43:07.442128: step 52000, loss = 0.1176, acc = 0.9610, f1neg = 0.9532, f1pos = 0.9666, f1 = 0.9599
[Eval_batch(8)(2000,18000)] 2017-05-03 06:43:07.904465: step 52000, loss = 0.1126, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 06:43:08.380756: step 52000, loss = 0.1176, acc = 0.9660, f1neg = 0.9636, f1pos = 0.9681, f1 = 0.9658
[Eval_batch(10)(2000,22000)] 2017-05-03 06:43:08.844205: step 52000, loss = 0.1254, acc = 0.9575, f1neg = 0.9544, f1pos = 0.9602, f1 = 0.9573
[Eval_batch(11)(2000,24000)] 2017-05-03 06:43:09.317428: step 52000, loss = 0.1260, acc = 0.9595, f1neg = 0.9531, f1pos = 0.9644, f1 = 0.9587
[Eval_batch(12)(2000,26000)] 2017-05-03 06:43:09.789936: step 52000, loss = 0.1230, acc = 0.9540, f1neg = 0.9525, f1pos = 0.9554, f1 = 0.9540
[Eval_batch(13)(2000,28000)] 2017-05-03 06:43:10.252483: step 52000, loss = 0.1118, acc = 0.9650, f1neg = 0.9561, f1pos = 0.9709, f1 = 0.9635
[Eval_batch(14)(2000,30000)] 2017-05-03 06:43:10.728699: step 52000, loss = 0.1390, acc = 0.9550, f1neg = 0.9463, f1pos = 0.9613, f1 = 0.9538
[Eval_batch(15)(2000,32000)] 2017-05-03 06:43:11.195771: step 52000, loss = 0.1143, acc = 0.9685, f1neg = 0.9659, f1pos = 0.9707, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 06:43:11.671799: step 52000, loss = 0.1111, acc = 0.9665, f1neg = 0.9650, f1pos = 0.9678, f1 = 0.9664
[Eval_batch(17)(2000,36000)] 2017-05-03 06:43:12.145970: step 52000, loss = 0.1375, acc = 0.9550, f1neg = 0.9551, f1pos = 0.9549, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 06:43:12.583642: step 52000, loss = 0.1329, acc = 0.9560, f1neg = 0.9577, f1pos = 0.9542, f1 = 0.9559
[Eval_batch(19)(2000,40000)] 2017-05-03 06:43:13.048904: step 52000, loss = 0.1143, acc = 0.9675, f1neg = 0.9624, f1pos = 0.9714, f1 = 0.9669
[Eval_batch(20)(2000,42000)] 2017-05-03 06:43:13.507485: step 52000, loss = 0.1173, acc = 0.9630, f1neg = 0.9671, f1pos = 0.9577, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 06:43:13.975121: step 52000, loss = 0.1085, acc = 0.9635, f1neg = 0.9602, f1pos = 0.9663, f1 = 0.9632
[Eval_batch(22)(2000,46000)] 2017-05-03 06:43:14.439635: step 52000, loss = 0.1167, acc = 0.9595, f1neg = 0.9598, f1pos = 0.9592, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 06:43:14.901750: step 52000, loss = 0.1215, acc = 0.9585, f1neg = 0.9529, f1pos = 0.9629, f1 = 0.9579
[Eval_batch(24)(2000,50000)] 2017-05-03 06:43:15.337459: step 52000, loss = 0.1076, acc = 0.9665, f1neg = 0.9615, f1pos = 0.9704, f1 = 0.9659
[Eval_batch(25)(2000,52000)] 2017-05-03 06:43:15.801682: step 52000, loss = 0.0994, acc = 0.9725, f1neg = 0.9698, f1pos = 0.9748, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 06:43:16.267219: step 52000, loss = 0.1148, acc = 0.9590, f1neg = 0.9612, f1pos = 0.9566, f1 = 0.9589
[Eval_batch(27)(2000,56000)] 2017-05-03 06:43:16.839241: step 52000, loss = 0.1006, acc = 0.9685, f1neg = 0.9668, f1pos = 0.9700, f1 = 0.9684
[Eval] 2017-05-03 06:43:16.839328: step 52000, acc = 0.9614, f1 = 0.9610
[Test_batch(0)(2000,2000)] 2017-05-03 06:43:17.313112: step 52000, loss = 0.1494, acc = 0.9520, f1neg = 0.9542, f1pos = 0.9495, f1 = 0.9519
[Test_batch(1)(2000,4000)] 2017-05-03 06:43:17.779037: step 52000, loss = 0.1300, acc = 0.9585, f1neg = 0.9631, f1pos = 0.9527, f1 = 0.9579
[Test_batch(2)(2000,6000)] 2017-05-03 06:43:18.232241: step 52000, loss = 0.1471, acc = 0.9545, f1neg = 0.9581, f1pos = 0.9502, f1 = 0.9542
[Test_batch(3)(2000,8000)] 2017-05-03 06:43:18.709368: step 52000, loss = 0.1528, acc = 0.9460, f1neg = 0.9492, f1pos = 0.9424, f1 = 0.9458
[Test_batch(4)(2000,10000)] 2017-05-03 06:43:19.180695: step 52000, loss = 0.1501, acc = 0.9470, f1neg = 0.9471, f1pos = 0.9469, f1 = 0.9470
[Test_batch(5)(2000,12000)] 2017-05-03 06:43:19.656850: step 52000, loss = 0.1624, acc = 0.9375, f1neg = 0.9378, f1pos = 0.9372, f1 = 0.9375
[Test_batch(6)(2000,14000)] 2017-05-03 06:43:20.135824: step 52000, loss = 0.1481, acc = 0.9455, f1neg = 0.9433, f1pos = 0.9475, f1 = 0.9454
[Test_batch(7)(2000,16000)] 2017-05-03 06:43:20.644285: step 52000, loss = 0.1383, acc = 0.9530, f1neg = 0.9573, f1pos = 0.9478, f1 = 0.9525
[Test_batch(8)(2000,18000)] 2017-05-03 06:43:21.104980: step 52000, loss = 0.1394, acc = 0.9490, f1neg = 0.9483, f1pos = 0.9497, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 06:43:21.575781: step 52000, loss = 0.1677, acc = 0.9375, f1neg = 0.9340, f1pos = 0.9407, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 06:43:22.047586: step 52000, loss = 0.1444, acc = 0.9520, f1neg = 0.9458, f1pos = 0.9569, f1 = 0.9514
[Test_batch(11)(2000,24000)] 2017-05-03 06:43:22.525629: step 52000, loss = 0.1469, acc = 0.9500, f1neg = 0.9513, f1pos = 0.9487, f1 = 0.9500
[Test_batch(12)(2000,26000)] 2017-05-03 06:43:23.031044: step 52000, loss = 0.1220, acc = 0.9620, f1neg = 0.9635, f1pos = 0.9604, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 06:43:23.475895: step 52000, loss = 0.1460, acc = 0.9485, f1neg = 0.9425, f1pos = 0.9534, f1 = 0.9479
[Test_batch(14)(2000,30000)] 2017-05-03 06:43:23.986102: step 52000, loss = 0.1207, acc = 0.9605, f1neg = 0.9545, f1pos = 0.9651, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 06:43:24.497129: step 52000, loss = 0.1458, acc = 0.9550, f1neg = 0.9544, f1pos = 0.9556, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 06:43:25.003871: step 52000, loss = 0.1275, acc = 0.9585, f1neg = 0.9573, f1pos = 0.9596, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 06:43:25.510003: step 52000, loss = 0.1195, acc = 0.9660, f1neg = 0.9627, f1pos = 0.9688, f1 = 0.9657
[Test_batch(18)(2000,38000)] 2017-05-03 06:43:26.077982: step 52000, loss = 0.1135, acc = 0.9615, f1neg = 0.9607, f1pos = 0.9623, f1 = 0.9615
[Test] 2017-05-03 06:43:26.078073: step 52000, acc = 0.9523, f1 = 0.9521
[Status] 2017-05-03 06:43:26.078101: step 52000, maxindex = 49000, maxdev = 0.9618, maxtst = 0.9533
2017-05-03 06:43:35.105212: step 52010, loss = 0.1294, acc = 0.9600 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 06:43:44.451481: step 52020, loss = 0.1249, acc = 0.9520 (216.3 examples/sec; 0.296 sec/batch)
2017-05-03 06:43:53.639364: step 52030, loss = 0.1335, acc = 0.9460 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 06:44:02.635221: step 52040, loss = 0.1590, acc = 0.9560 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 06:44:11.722454: step 52050, loss = 0.1435, acc = 0.9440 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 06:44:21.106083: step 52060, loss = 0.1161, acc = 0.9620 (267.3 examples/sec; 0.239 sec/batch)
2017-05-03 06:44:30.280332: step 52070, loss = 0.1218, acc = 0.9560 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 06:44:39.389090: step 52080, loss = 0.1175, acc = 0.9740 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 06:44:48.592551: step 52090, loss = 0.1157, acc = 0.9680 (258.6 examples/sec; 0.247 sec/batch)
2017-05-03 06:44:57.575345: step 52100, loss = 0.1308, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 06:45:06.597738: step 52110, loss = 0.1033, acc = 0.9720 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 06:45:15.624065: step 52120, loss = 0.1210, acc = 0.9580 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 06:45:24.710472: step 52130, loss = 0.1226, acc = 0.9660 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 06:45:33.759006: step 52140, loss = 0.1293, acc = 0.9520 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 06:45:42.938947: step 52150, loss = 0.1390, acc = 0.9540 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 06:45:52.169898: step 52160, loss = 0.1422, acc = 0.9700 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 06:46:01.314760: step 52170, loss = 0.1446, acc = 0.9540 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 06:46:10.393065: step 52180, loss = 0.1099, acc = 0.9680 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 06:46:19.510884: step 52190, loss = 0.1432, acc = 0.9560 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 06:46:28.588883: step 52200, loss = 0.1206, acc = 0.9660 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 06:46:37.594569: step 52210, loss = 0.1417, acc = 0.9520 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 06:46:46.535213: step 52220, loss = 0.1346, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 06:46:55.932728: step 52230, loss = 0.1153, acc = 0.9620 (248.8 examples/sec; 0.257 sec/batch)
2017-05-03 06:47:05.171159: step 52240, loss = 0.1551, acc = 0.9480 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 06:47:14.199463: step 52250, loss = 0.1124, acc = 0.9620 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 06:47:23.364943: step 52260, loss = 0.1131, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 06:47:32.519109: step 52270, loss = 0.0916, acc = 0.9720 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 06:47:41.562441: step 52280, loss = 0.1135, acc = 0.9620 (295.1 examples/sec; 0.217 sec/batch)
2017-05-03 06:47:51.074638: step 52290, loss = 0.1208, acc = 0.9640 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 06:48:00.224431: step 52300, loss = 0.1200, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:48:09.345779: step 52310, loss = 0.1463, acc = 0.9500 (246.1 examples/sec; 0.260 sec/batch)
2017-05-03 06:48:18.403967: step 52320, loss = 0.1347, acc = 0.9540 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 06:48:27.408013: step 52330, loss = 0.1349, acc = 0.9660 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 06:48:36.523185: step 52340, loss = 0.1368, acc = 0.9500 (259.7 examples/sec; 0.246 sec/batch)
2017-05-03 06:48:46.069674: step 52350, loss = 0.1513, acc = 0.9580 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 06:48:55.115293: step 52360, loss = 0.1238, acc = 0.9580 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 06:49:04.178326: step 52370, loss = 0.1131, acc = 0.9620 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 06:49:13.243994: step 52380, loss = 0.1050, acc = 0.9680 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 06:49:22.434610: step 52390, loss = 0.1100, acc = 0.9640 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 06:49:31.437868: step 52400, loss = 0.1465, acc = 0.9580 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 06:49:40.464998: step 52410, loss = 0.1368, acc = 0.9600 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 06:49:50.325445: step 52420, loss = 0.1403, acc = 0.9600 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 06:49:59.237371: step 52430, loss = 0.1249, acc = 0.9580 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 06:50:08.296552: step 52440, loss = 0.1415, acc = 0.9500 (298.0 examples/sec; 0.215 sec/batch)
2017-05-03 06:50:17.323997: step 52450, loss = 0.1418, acc = 0.9520 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 06:50:26.395786: step 52460, loss = 0.1411, acc = 0.9540 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 06:50:35.534669: step 52470, loss = 0.1042, acc = 0.9700 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 06:50:44.452167: step 52480, loss = 0.1076, acc = 0.9660 (297.0 examples/sec; 0.216 sec/batch)
2017-05-03 06:50:53.572830: step 52490, loss = 0.1175, acc = 0.9620 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 06:51:02.487539: step 52500, loss = 0.1184, acc = 0.9620 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 06:51:11.586331: step 52510, loss = 0.1194, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 06:51:20.737816: step 52520, loss = 0.1114, acc = 0.9760 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 06:51:29.849648: step 52530, loss = 0.1248, acc = 0.9640 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 06:51:38.974054: step 52540, loss = 0.1382, acc = 0.9480 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 06:51:48.313505: step 52550, loss = 0.1070, acc = 0.9680 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 06:51:57.649381: step 52560, loss = 0.1335, acc = 0.9520 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 06:52:06.634673: step 52570, loss = 0.1204, acc = 0.9660 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 06:52:15.903131: step 52580, loss = 0.1295, acc = 0.9540 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 06:52:25.108648: step 52590, loss = 0.1062, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 06:52:34.181606: step 52600, loss = 0.1297, acc = 0.9580 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 06:52:43.163426: step 52610, loss = 0.1377, acc = 0.9580 (300.5 examples/sec; 0.213 sec/batch)
2017-05-03 06:52:52.270208: step 52620, loss = 0.1145, acc = 0.9640 (301.8 examples/sec; 0.212 sec/batch)
2017-05-03 06:53:01.518195: step 52630, loss = 0.1255, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 06:53:10.760020: step 52640, loss = 0.1592, acc = 0.9480 (259.9 examples/sec; 0.246 sec/batch)
2017-05-03 06:53:19.823235: step 52650, loss = 0.1194, acc = 0.9600 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 06:53:28.969784: step 52660, loss = 0.1275, acc = 0.9620 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 06:53:37.959691: step 52670, loss = 0.1538, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 06:53:47.272958: step 52680, loss = 0.1256, acc = 0.9620 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 06:53:56.426310: step 52690, loss = 0.1206, acc = 0.9560 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 06:54:05.889442: step 52700, loss = 0.1329, acc = 0.9620 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 06:54:15.134810: step 52710, loss = 0.1298, acc = 0.9500 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 06:54:24.041535: step 52720, loss = 0.1537, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 06:54:33.088972: step 52730, loss = 0.1382, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 06:54:42.491702: step 52740, loss = 0.1224, acc = 0.9640 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 06:54:51.600963: step 52750, loss = 0.1192, acc = 0.9560 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 06:55:00.751125: step 52760, loss = 0.1177, acc = 0.9600 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 06:55:09.866012: step 52770, loss = 0.1457, acc = 0.9540 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 06:55:19.099315: step 52780, loss = 0.1181, acc = 0.9640 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 06:55:28.097799: step 52790, loss = 0.1083, acc = 0.9740 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 06:55:37.059339: step 52800, loss = 0.1239, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 06:55:46.375646: step 52810, loss = 0.0909, acc = 0.9740 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 06:55:55.651667: step 52820, loss = 0.1136, acc = 0.9700 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 06:56:04.716065: step 52830, loss = 0.0828, acc = 0.9880 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 06:56:13.978866: step 52840, loss = 0.1371, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 06:56:22.980878: step 52850, loss = 0.1395, acc = 0.9520 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 06:56:32.182189: step 52860, loss = 0.1198, acc = 0.9540 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 06:56:41.177811: step 52870, loss = 0.1307, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 06:56:50.221608: step 52880, loss = 0.1029, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 06:56:59.571179: step 52890, loss = 0.1411, acc = 0.9400 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 06:57:08.809850: step 52900, loss = 0.1165, acc = 0.9600 (258.5 examples/sec; 0.248 sec/batch)
2017-05-03 06:57:17.858725: step 52910, loss = 0.1065, acc = 0.9620 (265.1 examples/sec; 0.241 sec/batch)
2017-05-03 06:57:26.886194: step 52920, loss = 0.1260, acc = 0.9680 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 06:57:36.005106: step 52930, loss = 0.1313, acc = 0.9620 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 06:57:45.096915: step 52940, loss = 0.1237, acc = 0.9660 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 06:57:54.281169: step 52950, loss = 0.1151, acc = 0.9620 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 06:58:03.482665: step 52960, loss = 0.1294, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 06:58:12.602193: step 52970, loss = 0.1170, acc = 0.9620 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 06:58:21.569876: step 52980, loss = 0.1291, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 06:58:30.549127: step 52990, loss = 0.1054, acc = 0.9600 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 06:58:39.469831: step 53000, loss = 0.1323, acc = 0.9560 (284.1 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 06:58:39.941764: step 53000, loss = 0.1093, acc = 0.9605, f1neg = 0.9569, f1pos = 0.9635, f1 = 0.9602
[Eval_batch(1)(2000,4000)] 2017-05-03 06:58:40.404803: step 53000, loss = 0.1163, acc = 0.9660, f1neg = 0.9607, f1pos = 0.9700, f1 = 0.9654
[Eval_batch(2)(2000,6000)] 2017-05-03 06:58:40.888015: step 53000, loss = 0.1251, acc = 0.9570, f1neg = 0.9547, f1pos = 0.9591, f1 = 0.9569
[Eval_batch(3)(2000,8000)] 2017-05-03 06:58:41.357913: step 53000, loss = 0.1278, acc = 0.9600, f1neg = 0.9602, f1pos = 0.9598, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 06:58:41.825645: step 53000, loss = 0.1307, acc = 0.9555, f1neg = 0.9571, f1pos = 0.9537, f1 = 0.9554
[Eval_batch(5)(2000,12000)] 2017-05-03 06:58:42.290017: step 53000, loss = 0.1280, acc = 0.9545, f1neg = 0.9517, f1pos = 0.9570, f1 = 0.9543
[Eval_batch(6)(2000,14000)] 2017-05-03 06:58:42.757741: step 53000, loss = 0.1169, acc = 0.9670, f1neg = 0.9665, f1pos = 0.9675, f1 = 0.9670
[Eval_batch(7)(2000,16000)] 2017-05-03 06:58:43.232900: step 53000, loss = 0.1189, acc = 0.9605, f1neg = 0.9532, f1pos = 0.9658, f1 = 0.9595
[Eval_batch(8)(2000,18000)] 2017-05-03 06:58:43.706380: step 53000, loss = 0.1120, acc = 0.9640, f1neg = 0.9593, f1pos = 0.9677, f1 = 0.9635
[Eval_batch(9)(2000,20000)] 2017-05-03 06:58:44.171677: step 53000, loss = 0.1178, acc = 0.9620, f1neg = 0.9595, f1pos = 0.9642, f1 = 0.9619
[Eval_batch(10)(2000,22000)] 2017-05-03 06:58:44.643060: step 53000, loss = 0.1246, acc = 0.9595, f1neg = 0.9571, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(11)(2000,24000)] 2017-05-03 06:58:45.147270: step 53000, loss = 0.1267, acc = 0.9600, f1neg = 0.9542, f1pos = 0.9645, f1 = 0.9593
[Eval_batch(12)(2000,26000)] 2017-05-03 06:58:45.618058: step 53000, loss = 0.1225, acc = 0.9535, f1neg = 0.9525, f1pos = 0.9545, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 06:58:46.092358: step 53000, loss = 0.1122, acc = 0.9655, f1neg = 0.9571, f1pos = 0.9712, f1 = 0.9641
[Eval_batch(14)(2000,30000)] 2017-05-03 06:58:46.590813: step 53000, loss = 0.1396, acc = 0.9540, f1neg = 0.9457, f1pos = 0.9601, f1 = 0.9529
[Eval_batch(15)(2000,32000)] 2017-05-03 06:58:47.089848: step 53000, loss = 0.1152, acc = 0.9670, f1neg = 0.9646, f1pos = 0.9691, f1 = 0.9668
[Eval_batch(16)(2000,34000)] 2017-05-03 06:58:47.594732: step 53000, loss = 0.1115, acc = 0.9670, f1neg = 0.9659, f1pos = 0.9680, f1 = 0.9670
[Eval_batch(17)(2000,36000)] 2017-05-03 06:58:48.044288: step 53000, loss = 0.1383, acc = 0.9580, f1neg = 0.9585, f1pos = 0.9575, f1 = 0.9580
[Eval_batch(18)(2000,38000)] 2017-05-03 06:58:48.541060: step 53000, loss = 0.1306, acc = 0.9540, f1neg = 0.9561, f1pos = 0.9517, f1 = 0.9539
[Eval_batch(19)(2000,40000)] 2017-05-03 06:58:49.009627: step 53000, loss = 0.1140, acc = 0.9685, f1neg = 0.9637, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 06:58:49.484581: step 53000, loss = 0.1147, acc = 0.9615, f1neg = 0.9660, f1pos = 0.9556, f1 = 0.9608
[Eval_batch(21)(2000,44000)] 2017-05-03 06:58:49.988596: step 53000, loss = 0.1116, acc = 0.9635, f1neg = 0.9605, f1pos = 0.9661, f1 = 0.9633
[Eval_batch(22)(2000,46000)] 2017-05-03 06:58:50.468352: step 53000, loss = 0.1172, acc = 0.9615, f1neg = 0.9621, f1pos = 0.9609, f1 = 0.9615
[Eval_batch(23)(2000,48000)] 2017-05-03 06:58:50.953860: step 53000, loss = 0.1244, acc = 0.9610, f1neg = 0.9563, f1pos = 0.9648, f1 = 0.9605
[Eval_batch(24)(2000,50000)] 2017-05-03 06:58:51.429432: step 53000, loss = 0.1088, acc = 0.9650, f1neg = 0.9601, f1pos = 0.9688, f1 = 0.9645
[Eval_batch(25)(2000,52000)] 2017-05-03 06:58:51.895121: step 53000, loss = 0.0993, acc = 0.9725, f1neg = 0.9700, f1pos = 0.9746, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 06:58:52.373400: step 53000, loss = 0.1123, acc = 0.9630, f1neg = 0.9653, f1pos = 0.9604, f1 = 0.9628
[Eval_batch(27)(2000,56000)] 2017-05-03 06:58:52.962924: step 53000, loss = 0.0986, acc = 0.9695, f1neg = 0.9682, f1pos = 0.9707, f1 = 0.9694
[Eval] 2017-05-03 06:58:52.962981: step 53000, acc = 0.9618, f1 = 0.9615
[Test_batch(0)(2000,2000)] 2017-05-03 06:58:53.473725: step 53000, loss = 0.1479, acc = 0.9530, f1neg = 0.9557, f1pos = 0.9499, f1 = 0.9528
[Test_batch(1)(2000,4000)] 2017-05-03 06:58:53.967584: step 53000, loss = 0.1276, acc = 0.9600, f1neg = 0.9647, f1pos = 0.9539, f1 = 0.9593
[Test_batch(2)(2000,6000)] 2017-05-03 06:58:54.431717: step 53000, loss = 0.1438, acc = 0.9580, f1neg = 0.9616, f1pos = 0.9536, f1 = 0.9576
[Test_batch(3)(2000,8000)] 2017-05-03 06:58:54.907417: step 53000, loss = 0.1519, acc = 0.9465, f1neg = 0.9503, f1pos = 0.9420, f1 = 0.9462
[Test_batch(4)(2000,10000)] 2017-05-03 06:58:55.374440: step 53000, loss = 0.1503, acc = 0.9485, f1neg = 0.9491, f1pos = 0.9478, f1 = 0.9485
[Test_batch(5)(2000,12000)] 2017-05-03 06:58:55.846883: step 53000, loss = 0.1625, acc = 0.9415, f1neg = 0.9426, f1pos = 0.9403, f1 = 0.9415
[Test_batch(6)(2000,14000)] 2017-05-03 06:58:56.307084: step 53000, loss = 0.1456, acc = 0.9510, f1neg = 0.9496, f1pos = 0.9523, f1 = 0.9510
[Test_batch(7)(2000,16000)] 2017-05-03 06:58:56.789866: step 53000, loss = 0.1363, acc = 0.9530, f1neg = 0.9575, f1pos = 0.9474, f1 = 0.9525
[Test_batch(8)(2000,18000)] 2017-05-03 06:58:57.268358: step 53000, loss = 0.1394, acc = 0.9485, f1neg = 0.9481, f1pos = 0.9489, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 06:58:57.739050: step 53000, loss = 0.1700, acc = 0.9385, f1neg = 0.9358, f1pos = 0.9410, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 06:58:58.221280: step 53000, loss = 0.1481, acc = 0.9495, f1neg = 0.9437, f1pos = 0.9542, f1 = 0.9490
[Test_batch(11)(2000,24000)] 2017-05-03 06:58:58.686597: step 53000, loss = 0.1468, acc = 0.9510, f1neg = 0.9527, f1pos = 0.9492, f1 = 0.9509
[Test_batch(12)(2000,26000)] 2017-05-03 06:58:59.165059: step 53000, loss = 0.1228, acc = 0.9620, f1neg = 0.9636, f1pos = 0.9602, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 06:58:59.642674: step 53000, loss = 0.1480, acc = 0.9525, f1neg = 0.9479, f1pos = 0.9563, f1 = 0.9521
[Test_batch(14)(2000,30000)] 2017-05-03 06:59:00.109684: step 53000, loss = 0.1237, acc = 0.9590, f1neg = 0.9532, f1pos = 0.9635, f1 = 0.9584
[Test_batch(15)(2000,32000)] 2017-05-03 06:59:00.616161: step 53000, loss = 0.1476, acc = 0.9565, f1neg = 0.9563, f1pos = 0.9567, f1 = 0.9565
[Test_batch(16)(2000,34000)] 2017-05-03 06:59:01.093928: step 53000, loss = 0.1282, acc = 0.9570, f1neg = 0.9563, f1pos = 0.9577, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 06:59:01.575777: step 53000, loss = 0.1184, acc = 0.9640, f1neg = 0.9609, f1pos = 0.9667, f1 = 0.9638
[Test_batch(18)(2000,38000)] 2017-05-03 06:59:02.114755: step 53000, loss = 0.1106, acc = 0.9660, f1neg = 0.9655, f1pos = 0.9665, f1 = 0.9660
[Test] 2017-05-03 06:59:02.114843: step 53000, acc = 0.9535, f1 = 0.9533
[Status] 2017-05-03 06:59:02.114870: step 53000, maxindex = 49000, maxdev = 0.9618, maxtst = 0.9533
2017-05-03 06:59:11.191343: step 53010, loss = 0.1341, acc = 0.9540 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 06:59:20.434626: step 53020, loss = 0.1432, acc = 0.9520 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 06:59:30.064200: step 53030, loss = 0.1422, acc = 0.9580 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 06:59:39.281046: step 53040, loss = 0.1232, acc = 0.9600 (263.6 examples/sec; 0.243 sec/batch)
2017-05-03 06:59:48.385669: step 53050, loss = 0.1222, acc = 0.9660 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 06:59:57.413886: step 53060, loss = 0.1488, acc = 0.9580 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 07:00:06.297398: step 53070, loss = 0.1285, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 07:00:15.361373: step 53080, loss = 0.1310, acc = 0.9480 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 07:00:24.181549: step 53090, loss = 0.1135, acc = 0.9640 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 07:00:33.428825: step 53100, loss = 0.1363, acc = 0.9480 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 07:00:42.570962: step 53110, loss = 0.1293, acc = 0.9560 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 07:00:51.585949: step 53120, loss = 0.1194, acc = 0.9660 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 07:01:01.040909: step 53130, loss = 0.0910, acc = 0.9840 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 07:01:10.166434: step 53140, loss = 0.1150, acc = 0.9560 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 07:01:19.127705: step 53150, loss = 0.1114, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 07:01:28.024226: step 53160, loss = 0.1359, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 07:01:36.975913: step 53170, loss = 0.1397, acc = 0.9480 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:01:46.020984: step 53180, loss = 0.1435, acc = 0.9580 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 07:01:55.150759: step 53190, loss = 0.1100, acc = 0.9600 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 07:02:04.160242: step 53200, loss = 0.1121, acc = 0.9760 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 07:02:13.390738: step 53210, loss = 0.1399, acc = 0.9580 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 07:02:22.377000: step 53220, loss = 0.1318, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 07:02:31.351061: step 53230, loss = 0.1149, acc = 0.9660 (288.9 examples/sec; 0.221 sec/batch)
2017-05-03 07:02:40.338546: step 53240, loss = 0.1288, acc = 0.9540 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 07:02:49.170155: step 53250, loss = 0.1280, acc = 0.9540 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 07:02:58.404372: step 53260, loss = 0.1296, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:03:07.632977: step 53270, loss = 0.1235, acc = 0.9640 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 07:03:16.840852: step 53280, loss = 0.1250, acc = 0.9580 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 07:03:25.918218: step 53290, loss = 0.0992, acc = 0.9700 (307.2 examples/sec; 0.208 sec/batch)
2017-05-03 07:03:35.226746: step 53300, loss = 0.1360, acc = 0.9560 (263.2 examples/sec; 0.243 sec/batch)
2017-05-03 07:03:44.301131: step 53310, loss = 0.1022, acc = 0.9660 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:03:53.372150: step 53320, loss = 0.1178, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 07:04:02.405539: step 53330, loss = 0.1351, acc = 0.9540 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 07:04:11.477465: step 53340, loss = 0.1025, acc = 0.9760 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 07:04:20.587751: step 53350, loss = 0.1273, acc = 0.9660 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:04:29.767249: step 53360, loss = 0.1196, acc = 0.9620 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 07:04:38.880307: step 53370, loss = 0.1092, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 07:04:47.912743: step 53380, loss = 0.1400, acc = 0.9600 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 07:04:56.923671: step 53390, loss = 0.1236, acc = 0.9520 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 07:05:05.878278: step 53400, loss = 0.1396, acc = 0.9600 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 07:05:14.848978: step 53410, loss = 0.1273, acc = 0.9420 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 07:05:23.882779: step 53420, loss = 0.1234, acc = 0.9560 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 07:05:34.122644: step 53430, loss = 0.1312, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 07:05:43.230664: step 53440, loss = 0.1206, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 07:05:52.519492: step 53450, loss = 0.1236, acc = 0.9620 (261.1 examples/sec; 0.245 sec/batch)
2017-05-03 07:06:01.606210: step 53460, loss = 0.1319, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 07:06:10.676696: step 53470, loss = 0.1292, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 07:06:19.806267: step 53480, loss = 0.1330, acc = 0.9500 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 07:06:29.001800: step 53490, loss = 0.1398, acc = 0.9540 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 07:06:38.632001: step 53500, loss = 0.1289, acc = 0.9560 (263.5 examples/sec; 0.243 sec/batch)
2017-05-03 07:06:47.741905: step 53510, loss = 0.1091, acc = 0.9700 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 07:06:57.269164: step 53520, loss = 0.1355, acc = 0.9520 (183.6 examples/sec; 0.349 sec/batch)
2017-05-03 07:07:06.384311: step 53530, loss = 0.1336, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 07:07:15.664734: step 53540, loss = 0.1296, acc = 0.9660 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 07:07:24.673075: step 53550, loss = 0.1401, acc = 0.9460 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 07:07:33.766749: step 53560, loss = 0.1419, acc = 0.9480 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 07:07:42.828729: step 53570, loss = 0.1090, acc = 0.9680 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 07:07:51.773716: step 53580, loss = 0.1031, acc = 0.9720 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 07:08:00.956307: step 53590, loss = 0.1250, acc = 0.9620 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 07:08:10.009492: step 53600, loss = 0.1220, acc = 0.9680 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 07:08:18.948787: step 53610, loss = 0.1454, acc = 0.9500 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 07:08:28.023538: step 53620, loss = 0.1133, acc = 0.9800 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 07:08:36.994299: step 53630, loss = 0.1325, acc = 0.9620 (300.6 examples/sec; 0.213 sec/batch)
2017-05-03 07:08:46.046726: step 53640, loss = 0.1159, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 07:08:55.417585: step 53650, loss = 0.1228, acc = 0.9560 (255.5 examples/sec; 0.250 sec/batch)
2017-05-03 07:09:04.376285: step 53660, loss = 0.1078, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 07:09:13.548350: step 53670, loss = 0.1117, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 07:09:22.638850: step 53680, loss = 0.1259, acc = 0.9460 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 07:09:31.656395: step 53690, loss = 0.1355, acc = 0.9580 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 07:09:40.654561: step 53700, loss = 0.1326, acc = 0.9600 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 07:09:49.700847: step 53710, loss = 0.1200, acc = 0.9600 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 07:09:58.878001: step 53720, loss = 0.1578, acc = 0.9460 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 07:10:07.848900: step 53730, loss = 0.1334, acc = 0.9520 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 07:10:17.139688: step 53740, loss = 0.1043, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 07:10:26.145007: step 53750, loss = 0.1474, acc = 0.9540 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 07:10:35.222513: step 53760, loss = 0.1180, acc = 0.9700 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 07:10:44.146622: step 53770, loss = 0.1169, acc = 0.9620 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 07:10:53.149044: step 53780, loss = 0.1267, acc = 0.9620 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 07:11:02.203188: step 53790, loss = 0.0958, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 07:11:11.351404: step 53800, loss = 0.1431, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 07:11:20.321051: step 53810, loss = 0.1347, acc = 0.9600 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 07:11:29.436121: step 53820, loss = 0.1327, acc = 0.9620 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 07:11:38.655573: step 53830, loss = 0.1021, acc = 0.9740 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 07:11:47.932361: step 53840, loss = 0.1234, acc = 0.9700 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 07:11:57.127695: step 53850, loss = 0.1217, acc = 0.9640 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 07:12:06.176121: step 53860, loss = 0.1117, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 07:12:15.228850: step 53870, loss = 0.1185, acc = 0.9680 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 07:12:24.388224: step 53880, loss = 0.1322, acc = 0.9600 (268.7 examples/sec; 0.238 sec/batch)
2017-05-03 07:12:33.416931: step 53890, loss = 0.1138, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 07:12:42.513115: step 53900, loss = 0.1499, acc = 0.9480 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 07:12:51.650925: step 53910, loss = 0.1196, acc = 0.9680 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 07:13:00.682402: step 53920, loss = 0.1562, acc = 0.9500 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 07:13:09.774444: step 53930, loss = 0.1103, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 07:13:18.850313: step 53940, loss = 0.1289, acc = 0.9560 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 07:13:27.815127: step 53950, loss = 0.1325, acc = 0.9600 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 07:13:36.801671: step 53960, loss = 0.1276, acc = 0.9620 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 07:13:45.826531: step 53970, loss = 0.1492, acc = 0.9520 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 07:13:54.883648: step 53980, loss = 0.1664, acc = 0.9400 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 07:14:03.883798: step 53990, loss = 0.1069, acc = 0.9680 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 07:14:13.373853: step 54000, loss = 0.1273, acc = 0.9580 (274.9 examples/sec; 0.233 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 07:14:13.824582: step 54000, loss = 0.1100, acc = 0.9600, f1neg = 0.9561, f1pos = 0.9632, f1 = 0.9597
[Eval_batch(1)(2000,4000)] 2017-05-03 07:14:14.286127: step 54000, loss = 0.1136, acc = 0.9660, f1neg = 0.9605, f1pos = 0.9701, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 07:14:14.756594: step 54000, loss = 0.1239, acc = 0.9585, f1neg = 0.9560, f1pos = 0.9607, f1 = 0.9584
[Eval_batch(3)(2000,8000)] 2017-05-03 07:14:15.236376: step 54000, loss = 0.1291, acc = 0.9585, f1neg = 0.9586, f1pos = 0.9584, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 07:14:15.716200: step 54000, loss = 0.1303, acc = 0.9585, f1neg = 0.9598, f1pos = 0.9571, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 07:14:16.192167: step 54000, loss = 0.1276, acc = 0.9535, f1neg = 0.9503, f1pos = 0.9563, f1 = 0.9533
[Eval_batch(6)(2000,14000)] 2017-05-03 07:14:16.659663: step 54000, loss = 0.1175, acc = 0.9655, f1neg = 0.9650, f1pos = 0.9660, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 07:14:17.129714: step 54000, loss = 0.1182, acc = 0.9600, f1neg = 0.9522, f1pos = 0.9656, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 07:14:17.626269: step 54000, loss = 0.1113, acc = 0.9645, f1neg = 0.9596, f1pos = 0.9683, f1 = 0.9640
[Eval_batch(9)(2000,20000)] 2017-05-03 07:14:18.086383: step 54000, loss = 0.1167, acc = 0.9640, f1neg = 0.9615, f1pos = 0.9662, f1 = 0.9639
[Eval_batch(10)(2000,22000)] 2017-05-03 07:14:18.591526: step 54000, loss = 0.1242, acc = 0.9595, f1neg = 0.9568, f1pos = 0.9619, f1 = 0.9593
[Eval_batch(11)(2000,24000)] 2017-05-03 07:14:19.065937: step 54000, loss = 0.1251, acc = 0.9610, f1neg = 0.9552, f1pos = 0.9655, f1 = 0.9603
[Eval_batch(12)(2000,26000)] 2017-05-03 07:14:19.538608: step 54000, loss = 0.1214, acc = 0.9515, f1neg = 0.9501, f1pos = 0.9528, f1 = 0.9515
[Eval_batch(13)(2000,28000)] 2017-05-03 07:14:20.043309: step 54000, loss = 0.1110, acc = 0.9660, f1neg = 0.9577, f1pos = 0.9716, f1 = 0.9646
[Eval_batch(14)(2000,30000)] 2017-05-03 07:14:20.544836: step 54000, loss = 0.1383, acc = 0.9545, f1neg = 0.9461, f1pos = 0.9606, f1 = 0.9534
[Eval_batch(15)(2000,32000)] 2017-05-03 07:14:20.987991: step 54000, loss = 0.1134, acc = 0.9700, f1neg = 0.9676, f1pos = 0.9720, f1 = 0.9698
[Eval_batch(16)(2000,34000)] 2017-05-03 07:14:21.495627: step 54000, loss = 0.1098, acc = 0.9675, f1neg = 0.9662, f1pos = 0.9687, f1 = 0.9675
[Eval_batch(17)(2000,36000)] 2017-05-03 07:14:21.998289: step 54000, loss = 0.1362, acc = 0.9560, f1neg = 0.9563, f1pos = 0.9557, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 07:14:22.470910: step 54000, loss = 0.1296, acc = 0.9585, f1neg = 0.9602, f1pos = 0.9567, f1 = 0.9584
[Eval_batch(19)(2000,40000)] 2017-05-03 07:14:22.974168: step 54000, loss = 0.1133, acc = 0.9680, f1neg = 0.9630, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 07:14:23.484195: step 54000, loss = 0.1149, acc = 0.9610, f1neg = 0.9655, f1pos = 0.9552, f1 = 0.9603
[Eval_batch(21)(2000,44000)] 2017-05-03 07:14:23.989361: step 54000, loss = 0.1083, acc = 0.9640, f1neg = 0.9609, f1pos = 0.9666, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 07:14:24.497748: step 54000, loss = 0.1154, acc = 0.9620, f1neg = 0.9625, f1pos = 0.9615, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 07:14:24.978354: step 54000, loss = 0.1216, acc = 0.9610, f1neg = 0.9561, f1pos = 0.9649, f1 = 0.9605
[Eval_batch(24)(2000,50000)] 2017-05-03 07:14:25.481907: step 54000, loss = 0.1069, acc = 0.9645, f1neg = 0.9595, f1pos = 0.9684, f1 = 0.9639
[Eval_batch(25)(2000,52000)] 2017-05-03 07:14:25.987244: step 54000, loss = 0.0983, acc = 0.9725, f1neg = 0.9699, f1pos = 0.9747, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 07:14:26.469610: step 54000, loss = 0.1124, acc = 0.9600, f1neg = 0.9623, f1pos = 0.9574, f1 = 0.9599
[Eval_batch(27)(2000,56000)] 2017-05-03 07:14:27.002853: step 54000, loss = 0.0996, acc = 0.9705, f1neg = 0.9691, f1pos = 0.9718, f1 = 0.9704
[Eval] 2017-05-03 07:14:27.002929: step 54000, acc = 0.9620, f1 = 0.9617
[Test_batch(0)(2000,2000)] 2017-05-03 07:14:27.506957: step 54000, loss = 0.1485, acc = 0.9515, f1neg = 0.9541, f1pos = 0.9487, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 07:14:27.952524: step 54000, loss = 0.1279, acc = 0.9600, f1neg = 0.9646, f1pos = 0.9541, f1 = 0.9593
[Test_batch(2)(2000,6000)] 2017-05-03 07:14:28.430015: step 54000, loss = 0.1447, acc = 0.9560, f1neg = 0.9597, f1pos = 0.9516, f1 = 0.9556
[Test_batch(3)(2000,8000)] 2017-05-03 07:14:28.906615: step 54000, loss = 0.1521, acc = 0.9475, f1neg = 0.9510, f1pos = 0.9435, f1 = 0.9472
[Test_batch(4)(2000,10000)] 2017-05-03 07:14:29.356488: step 54000, loss = 0.1481, acc = 0.9490, f1neg = 0.9493, f1pos = 0.9487, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 07:14:29.809250: step 54000, loss = 0.1603, acc = 0.9420, f1neg = 0.9429, f1pos = 0.9411, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 07:14:30.287375: step 54000, loss = 0.1451, acc = 0.9475, f1neg = 0.9457, f1pos = 0.9492, f1 = 0.9474
[Test_batch(7)(2000,16000)] 2017-05-03 07:14:30.757428: step 54000, loss = 0.1361, acc = 0.9535, f1neg = 0.9578, f1pos = 0.9482, f1 = 0.9530
[Test_batch(8)(2000,18000)] 2017-05-03 07:14:31.226535: step 54000, loss = 0.1377, acc = 0.9475, f1neg = 0.9470, f1pos = 0.9480, f1 = 0.9475
[Test_batch(9)(2000,20000)] 2017-05-03 07:14:31.701116: step 54000, loss = 0.1668, acc = 0.9380, f1neg = 0.9349, f1pos = 0.9408, f1 = 0.9379
[Test_batch(10)(2000,22000)] 2017-05-03 07:14:32.167384: step 54000, loss = 0.1448, acc = 0.9530, f1neg = 0.9472, f1pos = 0.9577, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 07:14:32.634591: step 54000, loss = 0.1453, acc = 0.9495, f1neg = 0.9510, f1pos = 0.9479, f1 = 0.9494
[Test_batch(12)(2000,26000)] 2017-05-03 07:14:33.103021: step 54000, loss = 0.1222, acc = 0.9615, f1neg = 0.9631, f1pos = 0.9597, f1 = 0.9614
[Test_batch(13)(2000,28000)] 2017-05-03 07:14:33.586835: step 54000, loss = 0.1455, acc = 0.9515, f1neg = 0.9464, f1pos = 0.9557, f1 = 0.9511
[Test_batch(14)(2000,30000)] 2017-05-03 07:14:34.061331: step 54000, loss = 0.1204, acc = 0.9625, f1neg = 0.9570, f1pos = 0.9668, f1 = 0.9619
[Test_batch(15)(2000,32000)] 2017-05-03 07:14:34.528568: step 54000, loss = 0.1455, acc = 0.9545, f1neg = 0.9540, f1pos = 0.9550, f1 = 0.9545
[Test_batch(16)(2000,34000)] 2017-05-03 07:14:35.020264: step 54000, loss = 0.1264, acc = 0.9585, f1neg = 0.9575, f1pos = 0.9595, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 07:14:35.496955: step 54000, loss = 0.1174, acc = 0.9660, f1neg = 0.9628, f1pos = 0.9687, f1 = 0.9657
[Test_batch(18)(2000,38000)] 2017-05-03 07:14:36.023705: step 54000, loss = 0.1110, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 07:14:36.023795: step 54000, acc = 0.9534, f1 = 0.9532
[Status] 2017-05-03 07:14:36.023820: step 54000, maxindex = 54000, maxdev = 0.9620, maxtst = 0.9534
2017-05-03 07:14:48.347922: step 54010, loss = 0.1052, acc = 0.9720 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 07:14:57.484992: step 54020, loss = 0.1215, acc = 0.9660 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 07:15:06.681908: step 54030, loss = 0.1302, acc = 0.9580 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 07:15:15.801186: step 54040, loss = 0.1006, acc = 0.9700 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 07:15:24.838182: step 54050, loss = 0.1284, acc = 0.9560 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 07:15:34.151640: step 54060, loss = 0.1494, acc = 0.9460 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 07:15:43.230621: step 54070, loss = 0.1178, acc = 0.9540 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 07:15:52.309202: step 54080, loss = 0.1186, acc = 0.9640 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 07:16:01.286372: step 54090, loss = 0.1386, acc = 0.9560 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 07:16:10.499062: step 54100, loss = 0.1384, acc = 0.9540 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 07:16:19.394887: step 54110, loss = 0.0948, acc = 0.9700 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 07:16:28.593859: step 54120, loss = 0.1865, acc = 0.9380 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 07:16:37.706839: step 54130, loss = 0.1343, acc = 0.9460 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 07:16:46.803350: step 54140, loss = 0.1372, acc = 0.9580 (265.2 examples/sec; 0.241 sec/batch)
2017-05-03 07:16:55.825436: step 54150, loss = 0.1256, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 07:17:05.181116: step 54160, loss = 0.1273, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 07:17:14.418774: step 54170, loss = 0.0975, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 07:17:23.613347: step 54180, loss = 0.1344, acc = 0.9560 (263.8 examples/sec; 0.243 sec/batch)
2017-05-03 07:17:32.812289: step 54190, loss = 0.1105, acc = 0.9640 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 07:17:41.903056: step 54200, loss = 0.1258, acc = 0.9560 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 07:17:51.274247: step 54210, loss = 0.1341, acc = 0.9600 (247.8 examples/sec; 0.258 sec/batch)
2017-05-03 07:18:00.445987: step 54220, loss = 0.1295, acc = 0.9560 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 07:18:09.569501: step 54230, loss = 0.1129, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 07:18:18.596169: step 54240, loss = 0.1372, acc = 0.9500 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 07:18:27.651055: step 54250, loss = 0.1123, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 07:18:36.705548: step 54260, loss = 0.1516, acc = 0.9460 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 07:18:45.706376: step 54270, loss = 0.1472, acc = 0.9520 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 07:18:54.824576: step 54280, loss = 0.1279, acc = 0.9680 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 07:19:03.957265: step 54290, loss = 0.1199, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:19:12.999904: step 54300, loss = 0.1224, acc = 0.9660 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 07:19:22.106704: step 54310, loss = 0.1448, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 07:19:31.100441: step 54320, loss = 0.1114, acc = 0.9640 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 07:19:40.911325: step 54330, loss = 0.1308, acc = 0.9680 (188.5 examples/sec; 0.340 sec/batch)
2017-05-03 07:19:50.096140: step 54340, loss = 0.1269, acc = 0.9600 (263.3 examples/sec; 0.243 sec/batch)
2017-05-03 07:19:59.092194: step 54350, loss = 0.1565, acc = 0.9520 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 07:20:08.138756: step 54360, loss = 0.1037, acc = 0.9720 (261.8 examples/sec; 0.244 sec/batch)
2017-05-03 07:20:17.237445: step 54370, loss = 0.1279, acc = 0.9620 (267.8 examples/sec; 0.239 sec/batch)
2017-05-03 07:20:26.269684: step 54380, loss = 0.1091, acc = 0.9660 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 07:20:35.235562: step 54390, loss = 0.1311, acc = 0.9640 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 07:20:44.287038: step 54400, loss = 0.1192, acc = 0.9580 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 07:20:53.579320: step 54410, loss = 0.1382, acc = 0.9500 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 07:21:02.683531: step 54420, loss = 0.1296, acc = 0.9520 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 07:21:11.767402: step 54430, loss = 0.1269, acc = 0.9600 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 07:21:21.442903: step 54440, loss = 0.1079, acc = 0.9660 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 07:21:30.537977: step 54450, loss = 0.1429, acc = 0.9460 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 07:21:39.520496: step 54460, loss = 0.1140, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 07:21:48.631994: step 54470, loss = 0.1502, acc = 0.9560 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 07:21:57.580090: step 54480, loss = 0.1148, acc = 0.9780 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 07:22:06.626280: step 54490, loss = 0.1298, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 07:22:15.769337: step 54500, loss = 0.1437, acc = 0.9520 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 07:22:24.761574: step 54510, loss = 0.1448, acc = 0.9440 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 07:22:33.838558: step 54520, loss = 0.1211, acc = 0.9680 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 07:22:43.036280: step 54530, loss = 0.1268, acc = 0.9540 (234.8 examples/sec; 0.273 sec/batch)
2017-05-03 07:22:52.522361: step 54540, loss = 0.1174, acc = 0.9600 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:23:01.502622: step 54550, loss = 0.1529, acc = 0.9460 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 07:23:10.538523: step 54560, loss = 0.1292, acc = 0.9540 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 07:23:19.751588: step 54570, loss = 0.1232, acc = 0.9640 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 07:23:28.788749: step 54580, loss = 0.1588, acc = 0.9300 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 07:23:37.989541: step 54590, loss = 0.1095, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 07:23:47.117707: step 54600, loss = 0.1114, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 07:23:56.072630: step 54610, loss = 0.1002, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 07:24:05.103141: step 54620, loss = 0.1173, acc = 0.9540 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 07:24:14.407619: step 54630, loss = 0.1421, acc = 0.9560 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 07:24:23.474881: step 54640, loss = 0.0996, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 07:24:32.673460: step 54650, loss = 0.1111, acc = 0.9660 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:24:41.790885: step 54660, loss = 0.1048, acc = 0.9780 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 07:24:50.991904: step 54670, loss = 0.1196, acc = 0.9620 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 07:25:00.050368: step 54680, loss = 0.1034, acc = 0.9680 (267.5 examples/sec; 0.239 sec/batch)
2017-05-03 07:25:09.064360: step 54690, loss = 0.1397, acc = 0.9520 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 07:25:18.454462: step 54700, loss = 0.1227, acc = 0.9620 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 07:25:27.816670: step 54710, loss = 0.1162, acc = 0.9640 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 07:25:36.873791: step 54720, loss = 0.1229, acc = 0.9580 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 07:25:45.956401: step 54730, loss = 0.1059, acc = 0.9620 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 07:25:54.929862: step 54740, loss = 0.1050, acc = 0.9700 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 07:26:03.965451: step 54750, loss = 0.0981, acc = 0.9720 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 07:26:13.004308: step 54760, loss = 0.1304, acc = 0.9580 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 07:26:22.074613: step 54770, loss = 0.1037, acc = 0.9680 (270.4 examples/sec; 0.237 sec/batch)
2017-05-03 07:26:31.137734: step 54780, loss = 0.1274, acc = 0.9580 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 07:26:40.146734: step 54790, loss = 0.1227, acc = 0.9480 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 07:26:49.152479: step 54800, loss = 0.1281, acc = 0.9560 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 07:26:58.637040: step 54810, loss = 0.1203, acc = 0.9580 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 07:27:07.703978: step 54820, loss = 0.1332, acc = 0.9500 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 07:27:16.626554: step 54830, loss = 0.1557, acc = 0.9440 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 07:27:25.816041: step 54840, loss = 0.1033, acc = 0.9620 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 07:27:34.887897: step 54850, loss = 0.1088, acc = 0.9700 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 07:27:44.045393: step 54860, loss = 0.1340, acc = 0.9540 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 07:27:53.055144: step 54870, loss = 0.1131, acc = 0.9700 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 07:28:02.098460: step 54880, loss = 0.1299, acc = 0.9520 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 07:28:11.345944: step 54890, loss = 0.1469, acc = 0.9540 (237.3 examples/sec; 0.270 sec/batch)
2017-05-03 07:28:20.539335: step 54900, loss = 0.1143, acc = 0.9580 (270.5 examples/sec; 0.237 sec/batch)
2017-05-03 07:28:29.641832: step 54910, loss = 0.0857, acc = 0.9840 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 07:28:38.646912: step 54920, loss = 0.1215, acc = 0.9720 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 07:28:47.608905: step 54930, loss = 0.1309, acc = 0.9620 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 07:28:56.494724: step 54940, loss = 0.1014, acc = 0.9720 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 07:29:05.528821: step 54950, loss = 0.1041, acc = 0.9740 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 07:29:14.516065: step 54960, loss = 0.1799, acc = 0.9280 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 07:29:23.614761: step 54970, loss = 0.1639, acc = 0.9360 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 07:29:32.805150: step 54980, loss = 0.1274, acc = 0.9620 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 07:29:41.852239: step 54990, loss = 0.1554, acc = 0.9520 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 07:29:50.895114: step 55000, loss = 0.1390, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 07:29:51.378678: step 55000, loss = 0.1099, acc = 0.9595, f1neg = 0.9556, f1pos = 0.9628, f1 = 0.9592
[Eval_batch(1)(2000,4000)] 2017-05-03 07:29:51.852828: step 55000, loss = 0.1129, acc = 0.9655, f1neg = 0.9598, f1pos = 0.9698, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 07:29:52.349800: step 55000, loss = 0.1230, acc = 0.9595, f1neg = 0.9571, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 07:29:52.818511: step 55000, loss = 0.1283, acc = 0.9595, f1neg = 0.9595, f1pos = 0.9595, f1 = 0.9595
[Eval_batch(4)(2000,10000)] 2017-05-03 07:29:53.292865: step 55000, loss = 0.1296, acc = 0.9580, f1neg = 0.9593, f1pos = 0.9566, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 07:29:53.768906: step 55000, loss = 0.1268, acc = 0.9555, f1neg = 0.9524, f1pos = 0.9582, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 07:29:54.244696: step 55000, loss = 0.1178, acc = 0.9660, f1neg = 0.9654, f1pos = 0.9665, f1 = 0.9660
[Eval_batch(7)(2000,16000)] 2017-05-03 07:29:54.720646: step 55000, loss = 0.1171, acc = 0.9605, f1neg = 0.9528, f1pos = 0.9660, f1 = 0.9594
[Eval_batch(8)(2000,18000)] 2017-05-03 07:29:55.191205: step 55000, loss = 0.1106, acc = 0.9635, f1neg = 0.9585, f1pos = 0.9675, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 07:29:55.695673: step 55000, loss = 0.1171, acc = 0.9650, f1neg = 0.9626, f1pos = 0.9671, f1 = 0.9649
[Eval_batch(10)(2000,22000)] 2017-05-03 07:29:56.167332: step 55000, loss = 0.1244, acc = 0.9570, f1neg = 0.9541, f1pos = 0.9595, f1 = 0.9568
[Eval_batch(11)(2000,24000)] 2017-05-03 07:29:56.620641: step 55000, loss = 0.1247, acc = 0.9610, f1neg = 0.9551, f1pos = 0.9655, f1 = 0.9603
[Eval_batch(12)(2000,26000)] 2017-05-03 07:29:57.097663: step 55000, loss = 0.1218, acc = 0.9535, f1neg = 0.9521, f1pos = 0.9548, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 07:29:57.568478: step 55000, loss = 0.1107, acc = 0.9655, f1neg = 0.9568, f1pos = 0.9713, f1 = 0.9641
[Eval_batch(14)(2000,30000)] 2017-05-03 07:29:58.026107: step 55000, loss = 0.1376, acc = 0.9555, f1neg = 0.9471, f1pos = 0.9616, f1 = 0.9544
[Eval_batch(15)(2000,32000)] 2017-05-03 07:29:58.490946: step 55000, loss = 0.1139, acc = 0.9690, f1neg = 0.9666, f1pos = 0.9711, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 07:29:58.955387: step 55000, loss = 0.1101, acc = 0.9655, f1neg = 0.9642, f1pos = 0.9667, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 07:29:59.431065: step 55000, loss = 0.1377, acc = 0.9560, f1neg = 0.9563, f1pos = 0.9557, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 07:29:59.892507: step 55000, loss = 0.1307, acc = 0.9575, f1neg = 0.9592, f1pos = 0.9557, f1 = 0.9574
[Eval_batch(19)(2000,40000)] 2017-05-03 07:30:00.396410: step 55000, loss = 0.1131, acc = 0.9685, f1neg = 0.9636, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 07:30:00.876093: step 55000, loss = 0.1155, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9564, f1 = 0.9614
[Eval_batch(21)(2000,44000)] 2017-05-03 07:30:01.344959: step 55000, loss = 0.1081, acc = 0.9640, f1neg = 0.9609, f1pos = 0.9667, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 07:30:01.818853: step 55000, loss = 0.1156, acc = 0.9620, f1neg = 0.9624, f1pos = 0.9616, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 07:30:02.278417: step 55000, loss = 0.1215, acc = 0.9605, f1neg = 0.9555, f1pos = 0.9645, f1 = 0.9600
[Eval_batch(24)(2000,50000)] 2017-05-03 07:30:02.728308: step 55000, loss = 0.1066, acc = 0.9655, f1neg = 0.9605, f1pos = 0.9694, f1 = 0.9649
[Eval_batch(25)(2000,52000)] 2017-05-03 07:30:03.194723: step 55000, loss = 0.0983, acc = 0.9725, f1neg = 0.9699, f1pos = 0.9747, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 07:30:03.670406: step 55000, loss = 0.1131, acc = 0.9610, f1neg = 0.9632, f1pos = 0.9585, f1 = 0.9609
[Eval_batch(27)(2000,56000)] 2017-05-03 07:30:04.261390: step 55000, loss = 0.0989, acc = 0.9705, f1neg = 0.9690, f1pos = 0.9718, f1 = 0.9704
[Eval] 2017-05-03 07:30:04.261479: step 55000, acc = 0.9621, f1 = 0.9618
[Test_batch(0)(2000,2000)] 2017-05-03 07:30:04.733804: step 55000, loss = 0.1484, acc = 0.9515, f1neg = 0.9539, f1pos = 0.9488, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 07:30:05.209156: step 55000, loss = 0.1281, acc = 0.9615, f1neg = 0.9659, f1pos = 0.9559, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 07:30:05.680343: step 55000, loss = 0.1446, acc = 0.9550, f1neg = 0.9587, f1pos = 0.9506, f1 = 0.9546
[Test_batch(3)(2000,8000)] 2017-05-03 07:30:06.145900: step 55000, loss = 0.1509, acc = 0.9495, f1neg = 0.9527, f1pos = 0.9458, f1 = 0.9493
[Test_batch(4)(2000,10000)] 2017-05-03 07:30:06.618485: step 55000, loss = 0.1481, acc = 0.9505, f1neg = 0.9508, f1pos = 0.9502, f1 = 0.9505
[Test_batch(5)(2000,12000)] 2017-05-03 07:30:07.058521: step 55000, loss = 0.1613, acc = 0.9420, f1neg = 0.9426, f1pos = 0.9414, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 07:30:07.524499: step 55000, loss = 0.1463, acc = 0.9485, f1neg = 0.9468, f1pos = 0.9501, f1 = 0.9484
[Test_batch(7)(2000,16000)] 2017-05-03 07:30:08.003827: step 55000, loss = 0.1371, acc = 0.9520, f1neg = 0.9565, f1pos = 0.9465, f1 = 0.9515
[Test_batch(8)(2000,18000)] 2017-05-03 07:30:08.479496: step 55000, loss = 0.1378, acc = 0.9495, f1neg = 0.9490, f1pos = 0.9500, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 07:30:08.958431: step 55000, loss = 0.1656, acc = 0.9390, f1neg = 0.9359, f1pos = 0.9418, f1 = 0.9389
[Test_batch(10)(2000,22000)] 2017-05-03 07:30:09.425510: step 55000, loss = 0.1444, acc = 0.9535, f1neg = 0.9477, f1pos = 0.9581, f1 = 0.9529
[Test_batch(11)(2000,24000)] 2017-05-03 07:30:09.890481: step 55000, loss = 0.1459, acc = 0.9500, f1neg = 0.9515, f1pos = 0.9484, f1 = 0.9500
[Test_batch(12)(2000,26000)] 2017-05-03 07:30:10.385887: step 55000, loss = 0.1213, acc = 0.9625, f1neg = 0.9640, f1pos = 0.9609, f1 = 0.9624
[Test_batch(13)(2000,28000)] 2017-05-03 07:30:10.847794: step 55000, loss = 0.1454, acc = 0.9520, f1neg = 0.9469, f1pos = 0.9562, f1 = 0.9516
[Test_batch(14)(2000,30000)] 2017-05-03 07:30:11.324440: step 55000, loss = 0.1206, acc = 0.9600, f1neg = 0.9541, f1pos = 0.9646, f1 = 0.9593
[Test_batch(15)(2000,32000)] 2017-05-03 07:30:11.825998: step 55000, loss = 0.1447, acc = 0.9550, f1neg = 0.9545, f1pos = 0.9555, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 07:30:12.329445: step 55000, loss = 0.1265, acc = 0.9580, f1neg = 0.9569, f1pos = 0.9590, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 07:30:12.808546: step 55000, loss = 0.1173, acc = 0.9655, f1neg = 0.9623, f1pos = 0.9682, f1 = 0.9652
[Test_batch(18)(2000,38000)] 2017-05-03 07:30:13.342849: step 55000, loss = 0.1117, acc = 0.9640, f1neg = 0.9633, f1pos = 0.9647, f1 = 0.9640
[Test] 2017-05-03 07:30:13.342939: step 55000, acc = 0.9537, f1 = 0.9534
[Status] 2017-05-03 07:30:13.342965: step 55000, maxindex = 55000, maxdev = 0.9621, maxtst = 0.9537
2017-05-03 07:30:25.514080: step 55010, loss = 0.1500, acc = 0.9480 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 07:30:34.533494: step 55020, loss = 0.1208, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 07:30:43.701047: step 55030, loss = 0.1230, acc = 0.9620 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 07:30:52.838520: step 55040, loss = 0.0899, acc = 0.9800 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 07:31:01.955286: step 55050, loss = 0.1072, acc = 0.9680 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 07:31:11.100176: step 55060, loss = 0.1230, acc = 0.9700 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 07:31:20.271399: step 55070, loss = 0.1373, acc = 0.9560 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 07:31:29.284165: step 55080, loss = 0.1209, acc = 0.9620 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 07:31:38.436762: step 55090, loss = 0.1178, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 07:31:47.555163: step 55100, loss = 0.1324, acc = 0.9640 (251.1 examples/sec; 0.255 sec/batch)
2017-05-03 07:31:56.605986: step 55110, loss = 0.1585, acc = 0.9460 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:32:05.810424: step 55120, loss = 0.1404, acc = 0.9540 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 07:32:15.131099: step 55130, loss = 0.0981, acc = 0.9660 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 07:32:24.113389: step 55140, loss = 0.1241, acc = 0.9580 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 07:32:33.268137: step 55150, loss = 0.1168, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 07:32:42.557631: step 55160, loss = 0.1380, acc = 0.9580 (263.6 examples/sec; 0.243 sec/batch)
2017-05-03 07:32:51.654879: step 55170, loss = 0.1178, acc = 0.9580 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:33:00.719343: step 55180, loss = 0.1408, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 07:33:09.711610: step 55190, loss = 0.1174, acc = 0.9740 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 07:33:18.893043: step 55200, loss = 0.1256, acc = 0.9580 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 07:33:28.371632: step 55210, loss = 0.1173, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 07:33:37.697962: step 55220, loss = 0.1202, acc = 0.9720 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 07:33:46.745647: step 55230, loss = 0.1042, acc = 0.9700 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 07:33:55.965516: step 55240, loss = 0.1383, acc = 0.9540 (270.4 examples/sec; 0.237 sec/batch)
2017-05-03 07:34:05.113739: step 55250, loss = 0.1118, acc = 0.9640 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 07:34:14.217185: step 55260, loss = 0.1124, acc = 0.9660 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 07:34:23.152464: step 55270, loss = 0.0948, acc = 0.9740 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 07:34:32.241303: step 55280, loss = 0.1072, acc = 0.9720 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 07:34:41.294301: step 55290, loss = 0.1252, acc = 0.9620 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 07:34:50.307208: step 55300, loss = 0.1287, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 07:34:59.239934: step 55310, loss = 0.1314, acc = 0.9540 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 07:35:08.375912: step 55320, loss = 0.1409, acc = 0.9560 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 07:35:17.402886: step 55330, loss = 0.1186, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 07:35:26.453163: step 55340, loss = 0.1042, acc = 0.9700 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 07:35:35.790284: step 55350, loss = 0.1241, acc = 0.9540 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 07:35:44.842669: step 55360, loss = 0.1298, acc = 0.9620 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 07:35:53.915311: step 55370, loss = 0.1244, acc = 0.9580 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 07:36:02.853401: step 55380, loss = 0.1132, acc = 0.9660 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 07:36:12.269734: step 55390, loss = 0.1141, acc = 0.9740 (272.9 examples/sec; 0.234 sec/batch)
2017-05-03 07:36:21.394311: step 55400, loss = 0.1280, acc = 0.9500 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 07:36:30.479180: step 55410, loss = 0.1136, acc = 0.9700 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 07:36:39.798758: step 55420, loss = 0.1317, acc = 0.9640 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 07:36:48.877336: step 55430, loss = 0.1222, acc = 0.9620 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 07:36:58.899907: step 55440, loss = 0.1483, acc = 0.9500 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 07:37:08.007885: step 55450, loss = 0.1204, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 07:37:17.043409: step 55460, loss = 0.1434, acc = 0.9480 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 07:37:26.077737: step 55470, loss = 0.1401, acc = 0.9580 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 07:37:35.296806: step 55480, loss = 0.1168, acc = 0.9660 (263.7 examples/sec; 0.243 sec/batch)
2017-05-03 07:37:44.382477: step 55490, loss = 0.1366, acc = 0.9580 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 07:37:53.695358: step 55500, loss = 0.1174, acc = 0.9680 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 07:38:02.926183: step 55510, loss = 0.1541, acc = 0.9480 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 07:38:12.235014: step 55520, loss = 0.1559, acc = 0.9580 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 07:38:21.234174: step 55530, loss = 0.1536, acc = 0.9480 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 07:38:30.470725: step 55540, loss = 0.1029, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 07:38:39.820403: step 55550, loss = 0.1265, acc = 0.9640 (257.2 examples/sec; 0.249 sec/batch)
2017-05-03 07:38:48.952670: step 55560, loss = 0.1229, acc = 0.9620 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 07:38:58.045221: step 55570, loss = 0.1367, acc = 0.9600 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 07:39:07.187136: step 55580, loss = 0.1151, acc = 0.9720 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 07:39:16.224655: step 55590, loss = 0.1102, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 07:39:25.345035: step 55600, loss = 0.1280, acc = 0.9580 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 07:39:34.442625: step 55610, loss = 0.1371, acc = 0.9580 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 07:39:43.644726: step 55620, loss = 0.1240, acc = 0.9600 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 07:39:52.964041: step 55630, loss = 0.1350, acc = 0.9580 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 07:40:02.194797: step 55640, loss = 0.1101, acc = 0.9660 (244.7 examples/sec; 0.262 sec/batch)
2017-05-03 07:40:11.205176: step 55650, loss = 0.1496, acc = 0.9540 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 07:40:20.777473: step 55660, loss = 0.1364, acc = 0.9580 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 07:40:29.843265: step 55670, loss = 0.1208, acc = 0.9620 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 07:40:38.809905: step 55680, loss = 0.1156, acc = 0.9700 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 07:40:47.971977: step 55690, loss = 0.1565, acc = 0.9360 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 07:40:58.071624: step 55700, loss = 0.1132, acc = 0.9620 (176.5 examples/sec; 0.363 sec/batch)
2017-05-03 07:41:07.090499: step 55710, loss = 0.1194, acc = 0.9660 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 07:41:16.070558: step 55720, loss = 0.1497, acc = 0.9460 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 07:41:25.159060: step 55730, loss = 0.1456, acc = 0.9520 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 07:41:34.306023: step 55740, loss = 0.1238, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 07:41:43.520570: step 55750, loss = 0.1392, acc = 0.9560 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 07:41:52.604965: step 55760, loss = 0.0976, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 07:42:01.703550: step 55770, loss = 0.1393, acc = 0.9560 (275.3 examples/sec; 0.233 sec/batch)
2017-05-03 07:42:11.089212: step 55780, loss = 0.1310, acc = 0.9600 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 07:42:20.270037: step 55790, loss = 0.1206, acc = 0.9640 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 07:42:29.285911: step 55800, loss = 0.1048, acc = 0.9760 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 07:42:38.408942: step 55810, loss = 0.1136, acc = 0.9720 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 07:42:47.406204: step 55820, loss = 0.1087, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 07:42:56.457144: step 55830, loss = 0.1110, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 07:43:05.555532: step 55840, loss = 0.1240, acc = 0.9620 (266.0 examples/sec; 0.241 sec/batch)
2017-05-03 07:43:14.498499: step 55850, loss = 0.0951, acc = 0.9760 (304.7 examples/sec; 0.210 sec/batch)
2017-05-03 07:43:23.579039: step 55860, loss = 0.1258, acc = 0.9640 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 07:43:32.672460: step 55870, loss = 0.1487, acc = 0.9500 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 07:43:41.690566: step 55880, loss = 0.1343, acc = 0.9580 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 07:43:50.717583: step 55890, loss = 0.1058, acc = 0.9680 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 07:43:59.737695: step 55900, loss = 0.1199, acc = 0.9640 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 07:44:08.836544: step 55910, loss = 0.1530, acc = 0.9460 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 07:44:17.814329: step 55920, loss = 0.1117, acc = 0.9560 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 07:44:26.936882: step 55930, loss = 0.1241, acc = 0.9580 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 07:44:36.124291: step 55940, loss = 0.1388, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 07:44:45.214899: step 55950, loss = 0.1418, acc = 0.9420 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 07:44:54.364093: step 55960, loss = 0.1211, acc = 0.9580 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 07:45:03.509895: step 55970, loss = 0.1135, acc = 0.9700 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 07:45:12.515012: step 55980, loss = 0.1046, acc = 0.9640 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 07:45:21.475191: step 55990, loss = 0.1299, acc = 0.9560 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 07:45:30.687658: step 56000, loss = 0.1030, acc = 0.9680 (280.6 examples/sec; 0.228 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 07:45:31.159182: step 56000, loss = 0.1184, acc = 0.9565, f1neg = 0.9519, f1pos = 0.9603, f1 = 0.9561
[Eval_batch(1)(2000,4000)] 2017-05-03 07:45:31.632573: step 56000, loss = 0.1145, acc = 0.9630, f1neg = 0.9563, f1pos = 0.9679, f1 = 0.9621
[Eval_batch(2)(2000,6000)] 2017-05-03 07:45:32.094976: step 56000, loss = 0.1316, acc = 0.9570, f1neg = 0.9536, f1pos = 0.9600, f1 = 0.9568
[Eval_batch(3)(2000,8000)] 2017-05-03 07:45:32.556687: step 56000, loss = 0.1400, acc = 0.9515, f1neg = 0.9508, f1pos = 0.9522, f1 = 0.9515
[Eval_batch(4)(2000,10000)] 2017-05-03 07:45:33.022702: step 56000, loss = 0.1406, acc = 0.9550, f1neg = 0.9557, f1pos = 0.9543, f1 = 0.9550
[Eval_batch(5)(2000,12000)] 2017-05-03 07:45:33.460518: step 56000, loss = 0.1351, acc = 0.9505, f1neg = 0.9458, f1pos = 0.9544, f1 = 0.9501
[Eval_batch(6)(2000,14000)] 2017-05-03 07:45:33.906826: step 56000, loss = 0.1277, acc = 0.9625, f1neg = 0.9614, f1pos = 0.9636, f1 = 0.9625
[Eval_batch(7)(2000,16000)] 2017-05-03 07:45:34.379419: step 56000, loss = 0.1218, acc = 0.9585, f1neg = 0.9497, f1pos = 0.9647, f1 = 0.9572
[Eval_batch(8)(2000,18000)] 2017-05-03 07:45:34.825498: step 56000, loss = 0.1170, acc = 0.9605, f1neg = 0.9544, f1pos = 0.9652, f1 = 0.9598
[Eval_batch(9)(2000,20000)] 2017-05-03 07:45:35.288457: step 56000, loss = 0.1217, acc = 0.9605, f1neg = 0.9571, f1pos = 0.9634, f1 = 0.9602
[Eval_batch(10)(2000,22000)] 2017-05-03 07:45:35.728333: step 56000, loss = 0.1322, acc = 0.9540, f1neg = 0.9500, f1pos = 0.9574, f1 = 0.9537
[Eval_batch(11)(2000,24000)] 2017-05-03 07:45:36.188211: step 56000, loss = 0.1283, acc = 0.9565, f1neg = 0.9491, f1pos = 0.9620, f1 = 0.9556
[Eval_batch(12)(2000,26000)] 2017-05-03 07:45:36.643632: step 56000, loss = 0.1275, acc = 0.9545, f1neg = 0.9523, f1pos = 0.9565, f1 = 0.9544
[Eval_batch(13)(2000,28000)] 2017-05-03 07:45:37.105939: step 56000, loss = 0.1159, acc = 0.9615, f1neg = 0.9512, f1pos = 0.9682, f1 = 0.9597
[Eval_batch(14)(2000,30000)] 2017-05-03 07:45:37.543556: step 56000, loss = 0.1440, acc = 0.9500, f1neg = 0.9390, f1pos = 0.9576, f1 = 0.9483
[Eval_batch(15)(2000,32000)] 2017-05-03 07:45:37.986890: step 56000, loss = 0.1183, acc = 0.9655, f1neg = 0.9623, f1pos = 0.9682, f1 = 0.9652
[Eval_batch(16)(2000,34000)] 2017-05-03 07:45:38.445441: step 56000, loss = 0.1137, acc = 0.9645, f1neg = 0.9627, f1pos = 0.9662, f1 = 0.9644
[Eval_batch(17)(2000,36000)] 2017-05-03 07:45:38.920334: step 56000, loss = 0.1424, acc = 0.9525, f1neg = 0.9524, f1pos = 0.9526, f1 = 0.9525
[Eval_batch(18)(2000,38000)] 2017-05-03 07:45:39.386363: step 56000, loss = 0.1409, acc = 0.9545, f1neg = 0.9558, f1pos = 0.9531, f1 = 0.9545
[Eval_batch(19)(2000,40000)] 2017-05-03 07:45:39.847850: step 56000, loss = 0.1191, acc = 0.9655, f1neg = 0.9598, f1pos = 0.9698, f1 = 0.9648
[Eval_batch(20)(2000,42000)] 2017-05-03 07:45:40.322133: step 56000, loss = 0.1271, acc = 0.9600, f1neg = 0.9642, f1pos = 0.9547, f1 = 0.9594
[Eval_batch(21)(2000,44000)] 2017-05-03 07:45:40.796687: step 56000, loss = 0.1076, acc = 0.9650, f1neg = 0.9614, f1pos = 0.9680, f1 = 0.9647
[Eval_batch(22)(2000,46000)] 2017-05-03 07:45:41.273998: step 56000, loss = 0.1210, acc = 0.9580, f1neg = 0.9580, f1pos = 0.9580, f1 = 0.9580
[Eval_batch(23)(2000,48000)] 2017-05-03 07:45:41.727792: step 56000, loss = 0.1249, acc = 0.9575, f1neg = 0.9515, f1pos = 0.9622, f1 = 0.9568
[Eval_batch(24)(2000,50000)] 2017-05-03 07:45:42.204447: step 56000, loss = 0.1112, acc = 0.9660, f1neg = 0.9606, f1pos = 0.9701, f1 = 0.9653
[Eval_batch(25)(2000,52000)] 2017-05-03 07:45:42.667873: step 56000, loss = 0.1043, acc = 0.9690, f1neg = 0.9655, f1pos = 0.9718, f1 = 0.9687
[Eval_batch(26)(2000,54000)] 2017-05-03 07:45:43.122253: step 56000, loss = 0.1223, acc = 0.9595, f1neg = 0.9613, f1pos = 0.9575, f1 = 0.9594
[Eval_batch(27)(2000,56000)] 2017-05-03 07:45:43.622780: step 56000, loss = 0.1070, acc = 0.9645, f1neg = 0.9623, f1pos = 0.9664, f1 = 0.9644
[Eval] 2017-05-03 07:45:43.622863: step 56000, acc = 0.9591, f1 = 0.9586
[Test_batch(0)(2000,2000)] 2017-05-03 07:45:44.104320: step 56000, loss = 0.1570, acc = 0.9445, f1neg = 0.9465, f1pos = 0.9423, f1 = 0.9444
[Test_batch(1)(2000,4000)] 2017-05-03 07:45:44.554300: step 56000, loss = 0.1386, acc = 0.9515, f1neg = 0.9562, f1pos = 0.9456, f1 = 0.9509
[Test_batch(2)(2000,6000)] 2017-05-03 07:45:45.033590: step 56000, loss = 0.1562, acc = 0.9505, f1neg = 0.9539, f1pos = 0.9465, f1 = 0.9502
[Test_batch(3)(2000,8000)] 2017-05-03 07:45:45.510177: step 56000, loss = 0.1601, acc = 0.9465, f1neg = 0.9491, f1pos = 0.9436, f1 = 0.9464
[Test_batch(4)(2000,10000)] 2017-05-03 07:45:45.991735: step 56000, loss = 0.1557, acc = 0.9440, f1neg = 0.9430, f1pos = 0.9449, f1 = 0.9440
[Test_batch(5)(2000,12000)] 2017-05-03 07:45:46.493893: step 56000, loss = 0.1677, acc = 0.9365, f1neg = 0.9360, f1pos = 0.9370, f1 = 0.9365
[Test_batch(6)(2000,14000)] 2017-05-03 07:45:46.969033: step 56000, loss = 0.1559, acc = 0.9415, f1neg = 0.9385, f1pos = 0.9443, f1 = 0.9414
[Test_batch(7)(2000,16000)] 2017-05-03 07:45:47.459950: step 56000, loss = 0.1458, acc = 0.9505, f1neg = 0.9544, f1pos = 0.9459, f1 = 0.9501
[Test_batch(8)(2000,18000)] 2017-05-03 07:45:47.942525: step 56000, loss = 0.1436, acc = 0.9485, f1neg = 0.9474, f1pos = 0.9496, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 07:45:48.422337: step 56000, loss = 0.1704, acc = 0.9375, f1neg = 0.9332, f1pos = 0.9413, f1 = 0.9372
[Test_batch(10)(2000,22000)] 2017-05-03 07:45:48.906108: step 56000, loss = 0.1447, acc = 0.9485, f1neg = 0.9412, f1pos = 0.9542, f1 = 0.9477
[Test_batch(11)(2000,24000)] 2017-05-03 07:45:49.389698: step 56000, loss = 0.1542, acc = 0.9465, f1neg = 0.9472, f1pos = 0.9458, f1 = 0.9465
[Test_batch(12)(2000,26000)] 2017-05-03 07:45:49.851717: step 56000, loss = 0.1253, acc = 0.9600, f1neg = 0.9612, f1pos = 0.9587, f1 = 0.9600
[Test_batch(13)(2000,28000)] 2017-05-03 07:45:50.292675: step 56000, loss = 0.1499, acc = 0.9435, f1neg = 0.9356, f1pos = 0.9497, f1 = 0.9426
[Test_batch(14)(2000,30000)] 2017-05-03 07:45:50.769649: step 56000, loss = 0.1229, acc = 0.9605, f1neg = 0.9540, f1pos = 0.9654, f1 = 0.9597
[Test_batch(15)(2000,32000)] 2017-05-03 07:45:51.223376: step 56000, loss = 0.1502, acc = 0.9460, f1neg = 0.9444, f1pos = 0.9475, f1 = 0.9460
[Test_batch(16)(2000,34000)] 2017-05-03 07:45:51.692512: step 56000, loss = 0.1320, acc = 0.9540, f1neg = 0.9521, f1pos = 0.9557, f1 = 0.9539
[Test_batch(17)(2000,36000)] 2017-05-03 07:45:52.167906: step 56000, loss = 0.1251, acc = 0.9650, f1neg = 0.9613, f1pos = 0.9681, f1 = 0.9647
[Test_batch(18)(2000,38000)] 2017-05-03 07:45:52.685836: step 56000, loss = 0.1213, acc = 0.9575, f1neg = 0.9561, f1pos = 0.9588, f1 = 0.9575
[Test] 2017-05-03 07:45:52.685900: step 56000, acc = 0.9491, f1 = 0.9488
[Status] 2017-05-03 07:45:52.685912: step 56000, maxindex = 55000, maxdev = 0.9621, maxtst = 0.9537
2017-05-03 07:46:01.739081: step 56010, loss = 0.1565, acc = 0.9480 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 07:46:10.685408: step 56020, loss = 0.1249, acc = 0.9520 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 07:46:19.784663: step 56030, loss = 0.1434, acc = 0.9500 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 07:46:28.898555: step 56040, loss = 0.1001, acc = 0.9780 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 07:46:37.856331: step 56050, loss = 0.1161, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:46:46.796191: step 56060, loss = 0.1213, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 07:46:55.938365: step 56070, loss = 0.1448, acc = 0.9480 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 07:47:05.079538: step 56080, loss = 0.1386, acc = 0.9520 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 07:47:14.103019: step 56090, loss = 0.1268, acc = 0.9540 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 07:47:23.245533: step 56100, loss = 0.1240, acc = 0.9580 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 07:47:32.248261: step 56110, loss = 0.1203, acc = 0.9580 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 07:47:41.425010: step 56120, loss = 0.1352, acc = 0.9640 (264.0 examples/sec; 0.242 sec/batch)
2017-05-03 07:47:50.631263: step 56130, loss = 0.1383, acc = 0.9480 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 07:47:59.724437: step 56140, loss = 0.1063, acc = 0.9800 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 07:48:08.737578: step 56150, loss = 0.1145, acc = 0.9640 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 07:48:17.838749: step 56160, loss = 0.1130, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 07:48:26.943052: step 56170, loss = 0.1226, acc = 0.9560 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 07:48:36.028152: step 56180, loss = 0.1138, acc = 0.9620 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 07:48:45.385405: step 56190, loss = 0.1162, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 07:48:54.446312: step 56200, loss = 0.1553, acc = 0.9500 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 07:49:03.496797: step 56210, loss = 0.1467, acc = 0.9440 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 07:49:12.543939: step 56220, loss = 0.1492, acc = 0.9520 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:49:21.604772: step 56230, loss = 0.1444, acc = 0.9460 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 07:49:30.550370: step 56240, loss = 0.1212, acc = 0.9600 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 07:49:39.576828: step 56250, loss = 0.1290, acc = 0.9560 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 07:49:48.710625: step 56260, loss = 0.1020, acc = 0.9740 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 07:49:57.827405: step 56270, loss = 0.1358, acc = 0.9520 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 07:50:06.856803: step 56280, loss = 0.1288, acc = 0.9520 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 07:50:15.735022: step 56290, loss = 0.1267, acc = 0.9560 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 07:50:24.819349: step 56300, loss = 0.1128, acc = 0.9740 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 07:50:33.852184: step 56310, loss = 0.1071, acc = 0.9580 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 07:50:42.883196: step 56320, loss = 0.1223, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 07:50:51.926708: step 56330, loss = 0.1386, acc = 0.9620 (266.2 examples/sec; 0.240 sec/batch)
2017-05-03 07:51:00.912583: step 56340, loss = 0.1501, acc = 0.9420 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 07:51:09.863203: step 56350, loss = 0.1370, acc = 0.9600 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 07:51:18.903041: step 56360, loss = 0.1406, acc = 0.9560 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 07:51:27.875322: step 56370, loss = 0.0985, acc = 0.9680 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:51:36.854006: step 56380, loss = 0.1380, acc = 0.9540 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 07:51:45.986420: step 56390, loss = 0.1237, acc = 0.9540 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 07:51:55.030076: step 56400, loss = 0.1101, acc = 0.9680 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 07:52:04.253916: step 56410, loss = 0.1603, acc = 0.9420 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 07:52:13.325834: step 56420, loss = 0.1230, acc = 0.9640 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 07:52:22.385505: step 56430, loss = 0.1308, acc = 0.9520 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 07:52:31.561251: step 56440, loss = 0.0975, acc = 0.9720 (262.7 examples/sec; 0.244 sec/batch)
2017-05-03 07:52:41.290977: step 56450, loss = 0.1479, acc = 0.9460 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 07:52:50.283602: step 56460, loss = 0.1375, acc = 0.9500 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 07:52:59.352706: step 56470, loss = 0.1319, acc = 0.9480 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 07:53:08.430524: step 56480, loss = 0.1656, acc = 0.9400 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 07:53:17.520177: step 56490, loss = 0.1429, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 07:53:26.532423: step 56500, loss = 0.1288, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 07:53:35.582979: step 56510, loss = 0.1076, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 07:53:44.541100: step 56520, loss = 0.1435, acc = 0.9580 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 07:53:53.554296: step 56530, loss = 0.1204, acc = 0.9640 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 07:54:02.432904: step 56540, loss = 0.1466, acc = 0.9620 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 07:54:11.475455: step 56550, loss = 0.2055, acc = 0.9300 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 07:54:20.713357: step 56560, loss = 0.1119, acc = 0.9640 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 07:54:29.868457: step 56570, loss = 0.1198, acc = 0.9700 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 07:54:38.996848: step 56580, loss = 0.1140, acc = 0.9620 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 07:54:47.942081: step 56590, loss = 0.1404, acc = 0.9460 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 07:54:56.864451: step 56600, loss = 0.1301, acc = 0.9540 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 07:55:05.998674: step 56610, loss = 0.1096, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 07:55:15.127792: step 56620, loss = 0.1002, acc = 0.9740 (300.5 examples/sec; 0.213 sec/batch)
2017-05-03 07:55:24.194265: step 56630, loss = 0.1490, acc = 0.9480 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 07:55:33.221093: step 56640, loss = 0.1164, acc = 0.9620 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 07:55:42.257206: step 56650, loss = 0.1033, acc = 0.9680 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 07:55:51.189185: step 56660, loss = 0.1306, acc = 0.9580 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 07:56:00.230248: step 56670, loss = 0.1603, acc = 0.9420 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 07:56:09.217013: step 56680, loss = 0.1183, acc = 0.9620 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 07:56:18.355436: step 56690, loss = 0.1067, acc = 0.9720 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 07:56:27.426992: step 56700, loss = 0.1154, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 07:56:36.542045: step 56710, loss = 0.1225, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 07:56:45.626704: step 56720, loss = 0.1366, acc = 0.9620 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 07:56:54.784110: step 56730, loss = 0.1056, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 07:57:03.813962: step 56740, loss = 0.1158, acc = 0.9600 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 07:57:12.836253: step 56750, loss = 0.1190, acc = 0.9640 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 07:57:21.933889: step 56760, loss = 0.1535, acc = 0.9500 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 07:57:31.162890: step 56770, loss = 0.1620, acc = 0.9500 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 07:57:40.363972: step 56780, loss = 0.1096, acc = 0.9640 (265.3 examples/sec; 0.241 sec/batch)
2017-05-03 07:57:49.491730: step 56790, loss = 0.1377, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 07:57:58.494893: step 56800, loss = 0.1269, acc = 0.9540 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 07:58:07.763432: step 56810, loss = 0.1260, acc = 0.9520 (264.7 examples/sec; 0.242 sec/batch)
2017-05-03 07:58:16.738169: step 56820, loss = 0.1030, acc = 0.9760 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 07:58:26.276394: step 56830, loss = 0.1163, acc = 0.9640 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 07:58:35.357089: step 56840, loss = 0.1520, acc = 0.9560 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 07:58:44.529356: step 56850, loss = 0.1317, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 07:58:53.518581: step 56860, loss = 0.1308, acc = 0.9560 (314.9 examples/sec; 0.203 sec/batch)
2017-05-03 07:59:02.778147: step 56870, loss = 0.1114, acc = 0.9640 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 07:59:12.009147: step 56880, loss = 0.1465, acc = 0.9460 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 07:59:21.048640: step 56890, loss = 0.1604, acc = 0.9380 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 07:59:30.167150: step 56900, loss = 0.1227, acc = 0.9640 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 07:59:39.245863: step 56910, loss = 0.0945, acc = 0.9780 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 07:59:48.792805: step 56920, loss = 0.1247, acc = 0.9660 (264.6 examples/sec; 0.242 sec/batch)
2017-05-03 07:59:58.128687: step 56930, loss = 0.1027, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:00:07.058652: step 56940, loss = 0.1336, acc = 0.9480 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 08:00:16.128680: step 56950, loss = 0.1457, acc = 0.9640 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 08:00:25.241034: step 56960, loss = 0.1350, acc = 0.9560 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 08:00:34.545902: step 56970, loss = 0.1573, acc = 0.9540 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:00:43.833594: step 56980, loss = 0.1176, acc = 0.9620 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 08:00:52.854645: step 56990, loss = 0.1067, acc = 0.9740 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 08:01:01.803983: step 57000, loss = 0.1491, acc = 0.9520 (293.2 examples/sec; 0.218 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 08:01:02.280175: step 57000, loss = 0.1102, acc = 0.9590, f1neg = 0.9549, f1pos = 0.9624, f1 = 0.9587
[Eval_batch(1)(2000,4000)] 2017-05-03 08:01:02.744732: step 57000, loss = 0.1122, acc = 0.9670, f1neg = 0.9615, f1pos = 0.9711, f1 = 0.9663
[Eval_batch(2)(2000,6000)] 2017-05-03 08:01:03.248460: step 57000, loss = 0.1247, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9599
[Eval_batch(3)(2000,8000)] 2017-05-03 08:01:03.722763: step 57000, loss = 0.1290, acc = 0.9590, f1neg = 0.9590, f1pos = 0.9590, f1 = 0.9590
[Eval_batch(4)(2000,10000)] 2017-05-03 08:01:04.190639: step 57000, loss = 0.1303, acc = 0.9595, f1neg = 0.9607, f1pos = 0.9583, f1 = 0.9595
[Eval_batch(5)(2000,12000)] 2017-05-03 08:01:04.661707: step 57000, loss = 0.1262, acc = 0.9555, f1neg = 0.9522, f1pos = 0.9584, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 08:01:05.158120: step 57000, loss = 0.1182, acc = 0.9645, f1neg = 0.9638, f1pos = 0.9651, f1 = 0.9645
[Eval_batch(7)(2000,16000)] 2017-05-03 08:01:05.631697: step 57000, loss = 0.1168, acc = 0.9600, f1neg = 0.9521, f1pos = 0.9657, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 08:01:06.084168: step 57000, loss = 0.1120, acc = 0.9620, f1neg = 0.9566, f1pos = 0.9662, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 08:01:06.563535: step 57000, loss = 0.1168, acc = 0.9650, f1neg = 0.9624, f1pos = 0.9673, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 08:01:07.040359: step 57000, loss = 0.1229, acc = 0.9570, f1neg = 0.9539, f1pos = 0.9597, f1 = 0.9568
[Eval_batch(11)(2000,24000)] 2017-05-03 08:01:07.555438: step 57000, loss = 0.1239, acc = 0.9590, f1neg = 0.9526, f1pos = 0.9639, f1 = 0.9582
[Eval_batch(12)(2000,26000)] 2017-05-03 08:01:08.063909: step 57000, loss = 0.1209, acc = 0.9535, f1neg = 0.9519, f1pos = 0.9550, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 08:01:08.568089: step 57000, loss = 0.1109, acc = 0.9665, f1neg = 0.9580, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 08:01:09.073724: step 57000, loss = 0.1378, acc = 0.9545, f1neg = 0.9457, f1pos = 0.9609, f1 = 0.9533
[Eval_batch(15)(2000,32000)] 2017-05-03 08:01:09.576018: step 57000, loss = 0.1132, acc = 0.9685, f1neg = 0.9659, f1pos = 0.9707, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 08:01:10.083386: step 57000, loss = 0.1089, acc = 0.9670, f1neg = 0.9657, f1pos = 0.9682, f1 = 0.9670
[Eval_batch(17)(2000,36000)] 2017-05-03 08:01:10.576490: step 57000, loss = 0.1366, acc = 0.9560, f1neg = 0.9562, f1pos = 0.9558, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 08:01:11.084436: step 57000, loss = 0.1306, acc = 0.9565, f1neg = 0.9582, f1pos = 0.9547, f1 = 0.9564
[Eval_batch(19)(2000,40000)] 2017-05-03 08:01:11.588929: step 57000, loss = 0.1129, acc = 0.9670, f1neg = 0.9618, f1pos = 0.9709, f1 = 0.9664
[Eval_batch(20)(2000,42000)] 2017-05-03 08:01:12.067137: step 57000, loss = 0.1160, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9564, f1 = 0.9614
[Eval_batch(21)(2000,44000)] 2017-05-03 08:01:12.546513: step 57000, loss = 0.1064, acc = 0.9645, f1neg = 0.9614, f1pos = 0.9672, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 08:01:13.021730: step 57000, loss = 0.1156, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 08:01:13.506438: step 57000, loss = 0.1204, acc = 0.9590, f1neg = 0.9536, f1pos = 0.9633, f1 = 0.9584
[Eval_batch(24)(2000,50000)] 2017-05-03 08:01:13.971950: step 57000, loss = 0.1067, acc = 0.9650, f1neg = 0.9598, f1pos = 0.9690, f1 = 0.9644
[Eval_batch(25)(2000,52000)] 2017-05-03 08:01:14.440053: step 57000, loss = 0.0981, acc = 0.9730, f1neg = 0.9704, f1pos = 0.9752, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 08:01:14.952470: step 57000, loss = 0.1138, acc = 0.9600, f1neg = 0.9622, f1pos = 0.9576, f1 = 0.9599
[Eval_batch(27)(2000,56000)] 2017-05-03 08:01:15.504837: step 57000, loss = 0.0990, acc = 0.9700, f1neg = 0.9685, f1pos = 0.9714, f1 = 0.9699
[Eval] 2017-05-03 08:01:15.504923: step 57000, acc = 0.9618, f1 = 0.9614
[Test_batch(0)(2000,2000)] 2017-05-03 08:01:16.001352: step 57000, loss = 0.1487, acc = 0.9520, f1neg = 0.9542, f1pos = 0.9495, f1 = 0.9519
[Test_batch(1)(2000,4000)] 2017-05-03 08:01:16.471130: step 57000, loss = 0.1290, acc = 0.9615, f1neg = 0.9658, f1pos = 0.9560, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 08:01:16.950613: step 57000, loss = 0.1455, acc = 0.9550, f1neg = 0.9586, f1pos = 0.9507, f1 = 0.9547
[Test_batch(3)(2000,8000)] 2017-05-03 08:01:17.457400: step 57000, loss = 0.1510, acc = 0.9510, f1neg = 0.9540, f1pos = 0.9475, f1 = 0.9508
[Test_batch(4)(2000,10000)] 2017-05-03 08:01:17.967749: step 57000, loss = 0.1465, acc = 0.9480, f1neg = 0.9481, f1pos = 0.9479, f1 = 0.9480
[Test_batch(5)(2000,12000)] 2017-05-03 08:01:18.412552: step 57000, loss = 0.1604, acc = 0.9400, f1neg = 0.9404, f1pos = 0.9396, f1 = 0.9400
[Test_batch(6)(2000,14000)] 2017-05-03 08:01:18.896068: step 57000, loss = 0.1459, acc = 0.9455, f1neg = 0.9435, f1pos = 0.9474, f1 = 0.9454
[Test_batch(7)(2000,16000)] 2017-05-03 08:01:19.375452: step 57000, loss = 0.1372, acc = 0.9525, f1neg = 0.9568, f1pos = 0.9472, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 08:01:19.849320: step 57000, loss = 0.1377, acc = 0.9495, f1neg = 0.9489, f1pos = 0.9501, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 08:01:20.358169: step 57000, loss = 0.1654, acc = 0.9360, f1neg = 0.9324, f1pos = 0.9392, f1 = 0.9358
[Test_batch(10)(2000,22000)] 2017-05-03 08:01:20.853855: step 57000, loss = 0.1425, acc = 0.9525, f1neg = 0.9464, f1pos = 0.9573, f1 = 0.9519
[Test_batch(11)(2000,24000)] 2017-05-03 08:01:21.349948: step 57000, loss = 0.1457, acc = 0.9510, f1neg = 0.9522, f1pos = 0.9497, f1 = 0.9510
[Test_batch(12)(2000,26000)] 2017-05-03 08:01:21.826281: step 57000, loss = 0.1203, acc = 0.9620, f1neg = 0.9635, f1pos = 0.9604, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 08:01:22.326380: step 57000, loss = 0.1447, acc = 0.9490, f1neg = 0.9432, f1pos = 0.9537, f1 = 0.9485
[Test_batch(14)(2000,30000)] 2017-05-03 08:01:22.799905: step 57000, loss = 0.1192, acc = 0.9615, f1neg = 0.9556, f1pos = 0.9660, f1 = 0.9608
[Test_batch(15)(2000,32000)] 2017-05-03 08:01:23.278545: step 57000, loss = 0.1441, acc = 0.9565, f1neg = 0.9559, f1pos = 0.9571, f1 = 0.9565
[Test_batch(16)(2000,34000)] 2017-05-03 08:01:23.780149: step 57000, loss = 0.1258, acc = 0.9585, f1neg = 0.9573, f1pos = 0.9596, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 08:01:24.245610: step 57000, loss = 0.1176, acc = 0.9670, f1neg = 0.9638, f1pos = 0.9697, f1 = 0.9667
[Test_batch(18)(2000,38000)] 2017-05-03 08:01:24.786406: step 57000, loss = 0.1124, acc = 0.9625, f1neg = 0.9617, f1pos = 0.9633, f1 = 0.9625
[Test] 2017-05-03 08:01:24.786503: step 57000, acc = 0.9532, f1 = 0.9530
[Status] 2017-05-03 08:01:24.786531: step 57000, maxindex = 55000, maxdev = 0.9621, maxtst = 0.9537
2017-05-03 08:01:33.733829: step 57010, loss = 0.1211, acc = 0.9600 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 08:01:42.790131: step 57020, loss = 0.1310, acc = 0.9600 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 08:01:51.730435: step 57030, loss = 0.1085, acc = 0.9680 (300.2 examples/sec; 0.213 sec/batch)
2017-05-03 08:02:00.870519: step 57040, loss = 0.1200, acc = 0.9680 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 08:02:10.215779: step 57050, loss = 0.1016, acc = 0.9660 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 08:02:19.217082: step 57060, loss = 0.1263, acc = 0.9620 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:02:28.204399: step 57070, loss = 0.1244, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:02:37.184336: step 57080, loss = 0.1218, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 08:02:46.347779: step 57090, loss = 0.1069, acc = 0.9740 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 08:02:55.321216: step 57100, loss = 0.1311, acc = 0.9620 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 08:03:04.400214: step 57110, loss = 0.1409, acc = 0.9540 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 08:03:13.812591: step 57120, loss = 0.1126, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 08:03:23.033084: step 57130, loss = 0.1273, acc = 0.9520 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 08:03:31.940625: step 57140, loss = 0.1088, acc = 0.9680 (296.7 examples/sec; 0.216 sec/batch)
2017-05-03 08:03:41.094273: step 57150, loss = 0.1274, acc = 0.9480 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 08:03:50.138681: step 57160, loss = 0.1342, acc = 0.9620 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 08:03:59.488242: step 57170, loss = 0.1249, acc = 0.9560 (262.0 examples/sec; 0.244 sec/batch)
2017-05-03 08:04:08.469177: step 57180, loss = 0.1151, acc = 0.9700 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 08:04:17.641121: step 57190, loss = 0.1188, acc = 0.9560 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 08:04:26.673175: step 57200, loss = 0.1570, acc = 0.9480 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 08:04:35.667471: step 57210, loss = 0.1375, acc = 0.9520 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 08:04:44.695453: step 57220, loss = 0.1098, acc = 0.9680 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 08:04:53.795890: step 57230, loss = 0.1101, acc = 0.9640 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 08:05:02.891747: step 57240, loss = 0.0994, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 08:05:11.801704: step 57250, loss = 0.1420, acc = 0.9560 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 08:05:20.783105: step 57260, loss = 0.1175, acc = 0.9640 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 08:05:29.873753: step 57270, loss = 0.1179, acc = 0.9660 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 08:05:38.787290: step 57280, loss = 0.1300, acc = 0.9660 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 08:05:47.791198: step 57290, loss = 0.1287, acc = 0.9660 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 08:05:56.805788: step 57300, loss = 0.1240, acc = 0.9680 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 08:06:05.891168: step 57310, loss = 0.1248, acc = 0.9540 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 08:06:14.973020: step 57320, loss = 0.1325, acc = 0.9580 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 08:06:24.011996: step 57330, loss = 0.1204, acc = 0.9540 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 08:06:33.080751: step 57340, loss = 0.1168, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 08:06:42.136249: step 57350, loss = 0.1499, acc = 0.9560 (267.3 examples/sec; 0.239 sec/batch)
2017-05-03 08:06:51.798901: step 57360, loss = 0.1452, acc = 0.9600 (216.5 examples/sec; 0.296 sec/batch)
2017-05-03 08:07:01.016117: step 57370, loss = 0.1071, acc = 0.9760 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 08:07:10.130993: step 57380, loss = 0.1312, acc = 0.9500 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 08:07:19.178825: step 57390, loss = 0.1236, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 08:07:28.421916: step 57400, loss = 0.1167, acc = 0.9720 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 08:07:37.443107: step 57410, loss = 0.1104, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
2017-05-03 08:07:46.644402: step 57420, loss = 0.1493, acc = 0.9420 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 08:07:55.652802: step 57430, loss = 0.1278, acc = 0.9620 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 08:08:04.852801: step 57440, loss = 0.1331, acc = 0.9580 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 08:08:13.835829: step 57450, loss = 0.0991, acc = 0.9720 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 08:08:23.938467: step 57460, loss = 0.1368, acc = 0.9580 (295.7 examples/sec; 0.216 sec/batch)
2017-05-03 08:08:33.144592: step 57470, loss = 0.1121, acc = 0.9720 (270.6 examples/sec; 0.236 sec/batch)
2017-05-03 08:08:42.117086: step 57480, loss = 0.1293, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 08:08:51.654499: step 57490, loss = 0.1026, acc = 0.9780 (294.9 examples/sec; 0.217 sec/batch)
2017-05-03 08:09:01.041401: step 57500, loss = 0.1336, acc = 0.9680 (228.9 examples/sec; 0.280 sec/batch)
2017-05-03 08:09:09.983020: step 57510, loss = 0.1351, acc = 0.9620 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 08:09:19.067693: step 57520, loss = 0.1316, acc = 0.9540 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 08:09:28.209082: step 57530, loss = 0.1194, acc = 0.9620 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 08:09:37.325427: step 57540, loss = 0.1518, acc = 0.9520 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:09:46.641745: step 57550, loss = 0.1249, acc = 0.9600 (258.2 examples/sec; 0.248 sec/batch)
2017-05-03 08:09:55.807483: step 57560, loss = 0.1492, acc = 0.9520 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 08:10:04.975013: step 57570, loss = 0.1313, acc = 0.9660 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 08:10:14.078140: step 57580, loss = 0.1169, acc = 0.9640 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 08:10:23.050255: step 57590, loss = 0.1237, acc = 0.9600 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 08:10:32.177004: step 57600, loss = 0.1122, acc = 0.9620 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 08:10:41.295395: step 57610, loss = 0.1436, acc = 0.9480 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 08:10:50.357636: step 57620, loss = 0.1368, acc = 0.9560 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 08:10:59.457867: step 57630, loss = 0.1172, acc = 0.9660 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 08:11:08.614204: step 57640, loss = 0.1217, acc = 0.9580 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:11:17.622051: step 57650, loss = 0.1239, acc = 0.9660 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 08:11:26.813219: step 57660, loss = 0.1259, acc = 0.9580 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 08:11:35.899021: step 57670, loss = 0.1257, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 08:11:45.071966: step 57680, loss = 0.1474, acc = 0.9480 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 08:11:54.047158: step 57690, loss = 0.1649, acc = 0.9440 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:12:02.992665: step 57700, loss = 0.1379, acc = 0.9560 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 08:12:12.077489: step 57710, loss = 0.1421, acc = 0.9460 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 08:12:21.028329: step 57720, loss = 0.1277, acc = 0.9540 (297.8 examples/sec; 0.215 sec/batch)
2017-05-03 08:12:30.123736: step 57730, loss = 0.1114, acc = 0.9680 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 08:12:39.150029: step 57740, loss = 0.1113, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 08:12:48.401299: step 57750, loss = 0.1262, acc = 0.9580 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 08:12:57.630650: step 57760, loss = 0.1009, acc = 0.9720 (262.7 examples/sec; 0.244 sec/batch)
2017-05-03 08:13:06.712020: step 57770, loss = 0.1067, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 08:13:15.708343: step 57780, loss = 0.1283, acc = 0.9580 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:13:24.720947: step 57790, loss = 0.1309, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 08:13:34.021330: step 57800, loss = 0.1289, acc = 0.9560 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 08:13:43.237366: step 57810, loss = 0.1205, acc = 0.9540 (295.4 examples/sec; 0.217 sec/batch)
2017-05-03 08:13:52.177811: step 57820, loss = 0.1263, acc = 0.9520 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 08:14:01.062665: step 57830, loss = 0.0999, acc = 0.9720 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 08:14:10.142197: step 57840, loss = 0.1181, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 08:14:19.841944: step 57850, loss = 0.1456, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 08:14:28.955178: step 57860, loss = 0.1184, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:14:38.003431: step 57870, loss = 0.1173, acc = 0.9580 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 08:14:47.156907: step 57880, loss = 0.1362, acc = 0.9520 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 08:14:56.335182: step 57890, loss = 0.1243, acc = 0.9540 (275.3 examples/sec; 0.233 sec/batch)
2017-05-03 08:15:05.476215: step 57900, loss = 0.1264, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 08:15:14.523536: step 57910, loss = 0.1026, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 08:15:23.886577: step 57920, loss = 0.1307, acc = 0.9600 (257.1 examples/sec; 0.249 sec/batch)
2017-05-03 08:15:32.968342: step 57930, loss = 0.1477, acc = 0.9460 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 08:15:42.186847: step 57940, loss = 0.1340, acc = 0.9600 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 08:15:51.241599: step 57950, loss = 0.1122, acc = 0.9760 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 08:16:00.563264: step 57960, loss = 0.1197, acc = 0.9640 (232.2 examples/sec; 0.276 sec/batch)
2017-05-03 08:16:09.516054: step 57970, loss = 0.1542, acc = 0.9540 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 08:16:18.498696: step 57980, loss = 0.1269, acc = 0.9560 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 08:16:27.525013: step 57990, loss = 0.1276, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 08:16:36.610600: step 58000, loss = 0.1357, acc = 0.9680 (290.5 examples/sec; 0.220 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 08:16:37.125505: step 58000, loss = 0.1126, acc = 0.9590, f1neg = 0.9548, f1pos = 0.9625, f1 = 0.9587
[Eval_batch(1)(2000,4000)] 2017-05-03 08:16:37.599165: step 58000, loss = 0.1126, acc = 0.9640, f1neg = 0.9578, f1pos = 0.9686, f1 = 0.9632
[Eval_batch(2)(2000,6000)] 2017-05-03 08:16:38.036817: step 58000, loss = 0.1252, acc = 0.9570, f1neg = 0.9543, f1pos = 0.9594, f1 = 0.9568
[Eval_batch(3)(2000,8000)] 2017-05-03 08:16:38.548695: step 58000, loss = 0.1298, acc = 0.9585, f1neg = 0.9584, f1pos = 0.9586, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 08:16:39.057578: step 58000, loss = 0.1312, acc = 0.9580, f1neg = 0.9591, f1pos = 0.9568, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 08:16:39.531024: step 58000, loss = 0.1280, acc = 0.9550, f1neg = 0.9515, f1pos = 0.9580, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 08:16:40.002797: step 58000, loss = 0.1208, acc = 0.9640, f1neg = 0.9633, f1pos = 0.9647, f1 = 0.9640
[Eval_batch(7)(2000,16000)] 2017-05-03 08:16:40.509453: step 58000, loss = 0.1180, acc = 0.9600, f1neg = 0.9520, f1pos = 0.9657, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 08:16:40.969069: step 58000, loss = 0.1120, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 08:16:41.441076: step 58000, loss = 0.1170, acc = 0.9650, f1neg = 0.9624, f1pos = 0.9673, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 08:16:41.955635: step 58000, loss = 0.1242, acc = 0.9600, f1neg = 0.9570, f1pos = 0.9626, f1 = 0.9598
[Eval_batch(11)(2000,24000)] 2017-05-03 08:16:42.416338: step 58000, loss = 0.1249, acc = 0.9585, f1neg = 0.9519, f1pos = 0.9635, f1 = 0.9577
[Eval_batch(12)(2000,26000)] 2017-05-03 08:16:42.919131: step 58000, loss = 0.1215, acc = 0.9560, f1neg = 0.9545, f1pos = 0.9574, f1 = 0.9560
[Eval_batch(13)(2000,28000)] 2017-05-03 08:16:43.385525: step 58000, loss = 0.1120, acc = 0.9640, f1neg = 0.9548, f1pos = 0.9701, f1 = 0.9625
[Eval_batch(14)(2000,30000)] 2017-05-03 08:16:43.865296: step 58000, loss = 0.1389, acc = 0.9530, f1neg = 0.9438, f1pos = 0.9596, f1 = 0.9517
[Eval_batch(15)(2000,32000)] 2017-05-03 08:16:44.338780: step 58000, loss = 0.1135, acc = 0.9685, f1neg = 0.9659, f1pos = 0.9708, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 08:16:44.806188: step 58000, loss = 0.1092, acc = 0.9670, f1neg = 0.9656, f1pos = 0.9683, f1 = 0.9669
[Eval_batch(17)(2000,36000)] 2017-05-03 08:16:45.284934: step 58000, loss = 0.1378, acc = 0.9550, f1neg = 0.9552, f1pos = 0.9548, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 08:16:45.760332: step 58000, loss = 0.1323, acc = 0.9545, f1neg = 0.9561, f1pos = 0.9527, f1 = 0.9544
[Eval_batch(19)(2000,40000)] 2017-05-03 08:16:46.222503: step 58000, loss = 0.1138, acc = 0.9685, f1neg = 0.9635, f1pos = 0.9723, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 08:16:46.738201: step 58000, loss = 0.1169, acc = 0.9630, f1neg = 0.9671, f1pos = 0.9577, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 08:16:47.218752: step 58000, loss = 0.1065, acc = 0.9655, f1neg = 0.9624, f1pos = 0.9681, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 08:16:47.695672: step 58000, loss = 0.1164, acc = 0.9615, f1neg = 0.9617, f1pos = 0.9612, f1 = 0.9615
[Eval_batch(23)(2000,48000)] 2017-05-03 08:16:48.136401: step 58000, loss = 0.1214, acc = 0.9585, f1neg = 0.9529, f1pos = 0.9629, f1 = 0.9579
[Eval_batch(24)(2000,50000)] 2017-05-03 08:16:48.583561: step 58000, loss = 0.1069, acc = 0.9650, f1neg = 0.9597, f1pos = 0.9691, f1 = 0.9644
[Eval_batch(25)(2000,52000)] 2017-05-03 08:16:49.094139: step 58000, loss = 0.0990, acc = 0.9735, f1neg = 0.9709, f1pos = 0.9757, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 08:16:49.586538: step 58000, loss = 0.1148, acc = 0.9570, f1neg = 0.9592, f1pos = 0.9545, f1 = 0.9569
[Eval_batch(27)(2000,56000)] 2017-05-03 08:16:50.160391: step 58000, loss = 0.0999, acc = 0.9675, f1neg = 0.9658, f1pos = 0.9691, f1 = 0.9674
[Eval] 2017-05-03 08:16:50.160452: step 58000, acc = 0.9614, f1 = 0.9610
[Test_batch(0)(2000,2000)] 2017-05-03 08:16:50.640344: step 58000, loss = 0.1498, acc = 0.9515, f1neg = 0.9537, f1pos = 0.9491, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 08:16:51.148312: step 58000, loss = 0.1292, acc = 0.9600, f1neg = 0.9643, f1pos = 0.9544, f1 = 0.9594
[Test_batch(2)(2000,6000)] 2017-05-03 08:16:51.633370: step 58000, loss = 0.1474, acc = 0.9555, f1neg = 0.9590, f1pos = 0.9513, f1 = 0.9552
[Test_batch(3)(2000,8000)] 2017-05-03 08:16:52.139543: step 58000, loss = 0.1517, acc = 0.9485, f1neg = 0.9515, f1pos = 0.9451, f1 = 0.9483
[Test_batch(4)(2000,10000)] 2017-05-03 08:16:52.646552: step 58000, loss = 0.1492, acc = 0.9470, f1neg = 0.9471, f1pos = 0.9469, f1 = 0.9470
[Test_batch(5)(2000,12000)] 2017-05-03 08:16:53.152546: step 58000, loss = 0.1613, acc = 0.9400, f1neg = 0.9402, f1pos = 0.9398, f1 = 0.9400
[Test_batch(6)(2000,14000)] 2017-05-03 08:16:53.651040: step 58000, loss = 0.1482, acc = 0.9455, f1neg = 0.9433, f1pos = 0.9476, f1 = 0.9454
[Test_batch(7)(2000,16000)] 2017-05-03 08:16:54.093049: step 58000, loss = 0.1387, acc = 0.9520, f1neg = 0.9563, f1pos = 0.9467, f1 = 0.9515
[Test_batch(8)(2000,18000)] 2017-05-03 08:16:54.571319: step 58000, loss = 0.1368, acc = 0.9510, f1neg = 0.9503, f1pos = 0.9517, f1 = 0.9510
[Test_batch(9)(2000,20000)] 2017-05-03 08:16:55.035210: step 58000, loss = 0.1663, acc = 0.9375, f1neg = 0.9337, f1pos = 0.9409, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 08:16:55.543759: step 58000, loss = 0.1424, acc = 0.9510, f1neg = 0.9446, f1pos = 0.9561, f1 = 0.9503
[Test_batch(11)(2000,24000)] 2017-05-03 08:16:56.055149: step 58000, loss = 0.1470, acc = 0.9520, f1neg = 0.9531, f1pos = 0.9509, f1 = 0.9520
[Test_batch(12)(2000,26000)] 2017-05-03 08:16:56.521846: step 58000, loss = 0.1216, acc = 0.9600, f1neg = 0.9614, f1pos = 0.9585, f1 = 0.9599
[Test_batch(13)(2000,28000)] 2017-05-03 08:16:57.034056: step 58000, loss = 0.1457, acc = 0.9495, f1neg = 0.9437, f1pos = 0.9542, f1 = 0.9490
[Test_batch(14)(2000,30000)] 2017-05-03 08:16:57.519801: step 58000, loss = 0.1193, acc = 0.9610, f1neg = 0.9550, f1pos = 0.9656, f1 = 0.9603
[Test_batch(15)(2000,32000)] 2017-05-03 08:16:57.988712: step 58000, loss = 0.1454, acc = 0.9555, f1neg = 0.9548, f1pos = 0.9562, f1 = 0.9555
[Test_batch(16)(2000,34000)] 2017-05-03 08:16:58.493422: step 58000, loss = 0.1270, acc = 0.9595, f1neg = 0.9583, f1pos = 0.9607, f1 = 0.9595
[Test_batch(17)(2000,36000)] 2017-05-03 08:16:58.942392: step 58000, loss = 0.1190, acc = 0.9675, f1neg = 0.9643, f1pos = 0.9701, f1 = 0.9672
[Test_batch(18)(2000,38000)] 2017-05-03 08:16:59.483611: step 58000, loss = 0.1140, acc = 0.9630, f1neg = 0.9622, f1pos = 0.9638, f1 = 0.9630
[Test] 2017-05-03 08:16:59.483702: step 58000, acc = 0.9530, f1 = 0.9528
[Status] 2017-05-03 08:16:59.483726: step 58000, maxindex = 55000, maxdev = 0.9621, maxtst = 0.9537
2017-05-03 08:17:08.533541: step 58010, loss = 0.1340, acc = 0.9480 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 08:17:17.707925: step 58020, loss = 0.1391, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 08:17:26.652343: step 58030, loss = 0.1491, acc = 0.9640 (263.9 examples/sec; 0.243 sec/batch)
2017-05-03 08:17:35.947499: step 58040, loss = 0.1234, acc = 0.9680 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 08:17:44.995772: step 58050, loss = 0.1228, acc = 0.9540 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:17:54.078000: step 58060, loss = 0.1355, acc = 0.9560 (256.9 examples/sec; 0.249 sec/batch)
2017-05-03 08:18:03.014909: step 58070, loss = 0.1159, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 08:18:12.293451: step 58080, loss = 0.1396, acc = 0.9620 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:18:21.541378: step 58090, loss = 0.1271, acc = 0.9540 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 08:18:30.583882: step 58100, loss = 0.1177, acc = 0.9620 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 08:18:40.121412: step 58110, loss = 0.1087, acc = 0.9720 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 08:18:49.085359: step 58120, loss = 0.1047, acc = 0.9700 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 08:18:58.109977: step 58130, loss = 0.1009, acc = 0.9700 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 08:19:07.144026: step 58140, loss = 0.1440, acc = 0.9480 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 08:19:16.232891: step 58150, loss = 0.1352, acc = 0.9580 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 08:19:25.284946: step 58160, loss = 0.1323, acc = 0.9540 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 08:19:34.990178: step 58170, loss = 0.1232, acc = 0.9620 (233.3 examples/sec; 0.274 sec/batch)
2017-05-03 08:19:44.059404: step 58180, loss = 0.1042, acc = 0.9680 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 08:19:53.099294: step 58190, loss = 0.1142, acc = 0.9600 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 08:20:02.135241: step 58200, loss = 0.1030, acc = 0.9700 (271.8 examples/sec; 0.236 sec/batch)
2017-05-03 08:20:11.415170: step 58210, loss = 0.1114, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 08:20:20.413073: step 58220, loss = 0.1149, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 08:20:29.476208: step 58230, loss = 0.1176, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 08:20:38.648681: step 58240, loss = 0.1221, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 08:20:47.585441: step 58250, loss = 0.1377, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 08:20:56.542558: step 58260, loss = 0.1279, acc = 0.9520 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 08:21:05.551647: step 58270, loss = 0.1121, acc = 0.9640 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 08:21:14.679377: step 58280, loss = 0.1169, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 08:21:23.857073: step 58290, loss = 0.1499, acc = 0.9460 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 08:21:33.143323: step 58300, loss = 0.1267, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 08:21:42.097131: step 58310, loss = 0.1273, acc = 0.9540 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 08:21:50.978635: step 58320, loss = 0.1166, acc = 0.9760 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 08:21:59.964363: step 58330, loss = 0.1055, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 08:22:09.038143: step 58340, loss = 0.1498, acc = 0.9440 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 08:22:18.002715: step 58350, loss = 0.1306, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:22:26.992205: step 58360, loss = 0.1121, acc = 0.9580 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 08:22:36.014693: step 58370, loss = 0.1233, acc = 0.9640 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 08:22:44.925896: step 58380, loss = 0.1373, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 08:22:54.186820: step 58390, loss = 0.1479, acc = 0.9580 (268.8 examples/sec; 0.238 sec/batch)
2017-05-03 08:23:03.298871: step 58400, loss = 0.1170, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 08:23:12.382042: step 58410, loss = 0.1167, acc = 0.9700 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 08:23:21.540621: step 58420, loss = 0.0950, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 08:23:30.494101: step 58430, loss = 0.1396, acc = 0.9480 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 08:23:39.714278: step 58440, loss = 0.1074, acc = 0.9700 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 08:23:48.857930: step 58450, loss = 0.1534, acc = 0.9400 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 08:23:57.891291: step 58460, loss = 0.1271, acc = 0.9640 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 08:24:07.545875: step 58470, loss = 0.0960, acc = 0.9800 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 08:24:16.559678: step 58480, loss = 0.1479, acc = 0.9560 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 08:24:25.624413: step 58490, loss = 0.1159, acc = 0.9640 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 08:24:34.729863: step 58500, loss = 0.1431, acc = 0.9540 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 08:24:43.821584: step 58510, loss = 0.1613, acc = 0.9460 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 08:24:52.802722: step 58520, loss = 0.0911, acc = 0.9800 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 08:25:01.941360: step 58530, loss = 0.1347, acc = 0.9460 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 08:25:11.047456: step 58540, loss = 0.1389, acc = 0.9520 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 08:25:20.312745: step 58550, loss = 0.1206, acc = 0.9620 (236.1 examples/sec; 0.271 sec/batch)
2017-05-03 08:25:29.540012: step 58560, loss = 0.1004, acc = 0.9880 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 08:25:38.319073: step 58570, loss = 0.1503, acc = 0.9580 (299.4 examples/sec; 0.214 sec/batch)
2017-05-03 08:25:47.547007: step 58580, loss = 0.1420, acc = 0.9520 (238.9 examples/sec; 0.268 sec/batch)
2017-05-03 08:25:56.806808: step 58590, loss = 0.1397, acc = 0.9540 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 08:26:05.856331: step 58600, loss = 0.1245, acc = 0.9580 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 08:26:15.101844: step 58610, loss = 0.1181, acc = 0.9620 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 08:26:24.283160: step 58620, loss = 0.1259, acc = 0.9680 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 08:26:33.345180: step 58630, loss = 0.1196, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 08:26:42.488937: step 58640, loss = 0.1282, acc = 0.9620 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 08:26:51.828247: step 58650, loss = 0.1044, acc = 0.9700 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:27:00.762120: step 58660, loss = 0.1213, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 08:27:10.348930: step 58670, loss = 0.1288, acc = 0.9580 (177.9 examples/sec; 0.360 sec/batch)
2017-05-03 08:27:19.367688: step 58680, loss = 0.1094, acc = 0.9680 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 08:27:28.380984: step 58690, loss = 0.1075, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 08:27:37.480576: step 58700, loss = 0.1202, acc = 0.9620 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 08:27:46.585511: step 58710, loss = 0.1054, acc = 0.9720 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 08:27:55.695672: step 58720, loss = 0.1359, acc = 0.9480 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 08:28:04.592805: step 58730, loss = 0.1289, acc = 0.9540 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 08:28:13.798964: step 58740, loss = 0.1136, acc = 0.9600 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 08:28:22.798906: step 58750, loss = 0.1067, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 08:28:31.950732: step 58760, loss = 0.1391, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 08:28:41.424337: step 58770, loss = 0.1368, acc = 0.9520 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 08:28:50.443087: step 58780, loss = 0.1153, acc = 0.9660 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 08:28:59.481706: step 58790, loss = 0.1182, acc = 0.9560 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 08:29:08.567302: step 58800, loss = 0.1069, acc = 0.9720 (262.4 examples/sec; 0.244 sec/batch)
2017-05-03 08:29:17.631918: step 58810, loss = 0.1297, acc = 0.9540 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 08:29:26.744380: step 58820, loss = 0.1681, acc = 0.9480 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 08:29:35.689473: step 58830, loss = 0.1167, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 08:29:44.744091: step 58840, loss = 0.1176, acc = 0.9500 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 08:29:53.792076: step 58850, loss = 0.1167, acc = 0.9660 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 08:30:02.797956: step 58860, loss = 0.1134, acc = 0.9620 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 08:30:11.815984: step 58870, loss = 0.1284, acc = 0.9480 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:30:20.918870: step 58880, loss = 0.1370, acc = 0.9580 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 08:30:29.927205: step 58890, loss = 0.1112, acc = 0.9740 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:30:38.923187: step 58900, loss = 0.1303, acc = 0.9660 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 08:30:48.041895: step 58910, loss = 0.1305, acc = 0.9620 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 08:30:57.060065: step 58920, loss = 0.1239, acc = 0.9680 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 08:31:05.975335: step 58930, loss = 0.1308, acc = 0.9560 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 08:31:14.905434: step 58940, loss = 0.1231, acc = 0.9560 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 08:31:23.970999: step 58950, loss = 0.1160, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 08:31:33.007462: step 58960, loss = 0.0861, acc = 0.9800 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 08:31:41.946923: step 58970, loss = 0.1245, acc = 0.9580 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 08:31:50.952043: step 58980, loss = 0.1452, acc = 0.9560 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 08:31:59.941492: step 58990, loss = 0.1370, acc = 0.9620 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 08:32:08.949513: step 59000, loss = 0.1223, acc = 0.9680 (281.2 examples/sec; 0.228 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 08:32:09.436920: step 59000, loss = 0.1081, acc = 0.9610, f1neg = 0.9577, f1pos = 0.9639, f1 = 0.9608
[Eval_batch(1)(2000,4000)] 2017-05-03 08:32:09.911120: step 59000, loss = 0.1181, acc = 0.9620, f1neg = 0.9564, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(2)(2000,6000)] 2017-05-03 08:32:10.386469: step 59000, loss = 0.1246, acc = 0.9580, f1neg = 0.9561, f1pos = 0.9597, f1 = 0.9579
[Eval_batch(3)(2000,8000)] 2017-05-03 08:32:10.899084: step 59000, loss = 0.1269, acc = 0.9575, f1neg = 0.9580, f1pos = 0.9570, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 08:32:11.404296: step 59000, loss = 0.1296, acc = 0.9540, f1neg = 0.9559, f1pos = 0.9519, f1 = 0.9539
[Eval_batch(5)(2000,12000)] 2017-05-03 08:32:11.878984: step 59000, loss = 0.1282, acc = 0.9535, f1neg = 0.9510, f1pos = 0.9557, f1 = 0.9534
[Eval_batch(6)(2000,14000)] 2017-05-03 08:32:12.366156: step 59000, loss = 0.1148, acc = 0.9650, f1neg = 0.9647, f1pos = 0.9653, f1 = 0.9650
[Eval_batch(7)(2000,16000)] 2017-05-03 08:32:12.872658: step 59000, loss = 0.1199, acc = 0.9615, f1neg = 0.9546, f1pos = 0.9666, f1 = 0.9606
[Eval_batch(8)(2000,18000)] 2017-05-03 08:32:13.382965: step 59000, loss = 0.1130, acc = 0.9640, f1neg = 0.9596, f1pos = 0.9675, f1 = 0.9636
[Eval_batch(9)(2000,20000)] 2017-05-03 08:32:13.888979: step 59000, loss = 0.1182, acc = 0.9630, f1neg = 0.9609, f1pos = 0.9649, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-03 08:32:14.400343: step 59000, loss = 0.1261, acc = 0.9615, f1neg = 0.9595, f1pos = 0.9633, f1 = 0.9614
[Eval_batch(11)(2000,24000)] 2017-05-03 08:32:14.901729: step 59000, loss = 0.1278, acc = 0.9595, f1neg = 0.9538, f1pos = 0.9639, f1 = 0.9589
[Eval_batch(12)(2000,26000)] 2017-05-03 08:32:15.414398: step 59000, loss = 0.1216, acc = 0.9535, f1neg = 0.9529, f1pos = 0.9541, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 08:32:15.908645: step 59000, loss = 0.1126, acc = 0.9670, f1neg = 0.9592, f1pos = 0.9723, f1 = 0.9657
[Eval_batch(14)(2000,30000)] 2017-05-03 08:32:16.415489: step 59000, loss = 0.1393, acc = 0.9520, f1neg = 0.9437, f1pos = 0.9582, f1 = 0.9509
[Eval_batch(15)(2000,32000)] 2017-05-03 08:32:16.921267: step 59000, loss = 0.1170, acc = 0.9640, f1neg = 0.9616, f1pos = 0.9661, f1 = 0.9639
[Eval_batch(16)(2000,34000)] 2017-05-03 08:32:17.432550: step 59000, loss = 0.1113, acc = 0.9655, f1neg = 0.9645, f1pos = 0.9664, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 08:32:17.938285: step 59000, loss = 0.1395, acc = 0.9590, f1neg = 0.9598, f1pos = 0.9582, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 08:32:18.450474: step 59000, loss = 0.1291, acc = 0.9565, f1neg = 0.9588, f1pos = 0.9539, f1 = 0.9564
[Eval_batch(19)(2000,40000)] 2017-05-03 08:32:18.938134: step 59000, loss = 0.1135, acc = 0.9690, f1neg = 0.9647, f1pos = 0.9724, f1 = 0.9685
[Eval_batch(20)(2000,42000)] 2017-05-03 08:32:19.456365: step 59000, loss = 0.1121, acc = 0.9640, f1neg = 0.9683, f1pos = 0.9583, f1 = 0.9633
[Eval_batch(21)(2000,44000)] 2017-05-03 08:32:19.965627: step 59000, loss = 0.1134, acc = 0.9635, f1neg = 0.9606, f1pos = 0.9660, f1 = 0.9633
[Eval_batch(22)(2000,46000)] 2017-05-03 08:32:20.469333: step 59000, loss = 0.1161, acc = 0.9635, f1neg = 0.9643, f1pos = 0.9627, f1 = 0.9635
[Eval_batch(23)(2000,48000)] 2017-05-03 08:32:20.974411: step 59000, loss = 0.1250, acc = 0.9615, f1neg = 0.9572, f1pos = 0.9650, f1 = 0.9611
[Eval_batch(24)(2000,50000)] 2017-05-03 08:32:21.478906: step 59000, loss = 0.1090, acc = 0.9665, f1neg = 0.9621, f1pos = 0.9700, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 08:32:21.981630: step 59000, loss = 0.0992, acc = 0.9715, f1neg = 0.9691, f1pos = 0.9735, f1 = 0.9713
[Eval_batch(26)(2000,54000)] 2017-05-03 08:32:22.477998: step 59000, loss = 0.1107, acc = 0.9635, f1neg = 0.9660, f1pos = 0.9606, f1 = 0.9633
[Eval_batch(27)(2000,56000)] 2017-05-03 08:32:23.102120: step 59000, loss = 0.0971, acc = 0.9710, f1neg = 0.9699, f1pos = 0.9721, f1 = 0.9710
[Eval] 2017-05-03 08:32:23.102207: step 59000, acc = 0.9619, f1 = 0.9616
[Test_batch(0)(2000,2000)] 2017-05-03 08:32:23.614183: step 59000, loss = 0.1479, acc = 0.9530, f1neg = 0.9560, f1pos = 0.9496, f1 = 0.9528
[Test_batch(1)(2000,4000)] 2017-05-03 08:32:24.113245: step 59000, loss = 0.1269, acc = 0.9565, f1neg = 0.9618, f1pos = 0.9494, f1 = 0.9556
[Test_batch(2)(2000,6000)] 2017-05-03 08:32:24.617372: step 59000, loss = 0.1429, acc = 0.9555, f1neg = 0.9596, f1pos = 0.9504, f1 = 0.9550
[Test_batch(3)(2000,8000)] 2017-05-03 08:32:25.094541: step 59000, loss = 0.1515, acc = 0.9435, f1neg = 0.9480, f1pos = 0.9382, f1 = 0.9431
[Test_batch(4)(2000,10000)] 2017-05-03 08:32:25.564710: step 59000, loss = 0.1506, acc = 0.9475, f1neg = 0.9484, f1pos = 0.9466, f1 = 0.9475
[Test_batch(5)(2000,12000)] 2017-05-03 08:32:26.062661: step 59000, loss = 0.1644, acc = 0.9425, f1neg = 0.9443, f1pos = 0.9406, f1 = 0.9424
[Test_batch(6)(2000,14000)] 2017-05-03 08:32:26.533849: step 59000, loss = 0.1447, acc = 0.9500, f1neg = 0.9490, f1pos = 0.9509, f1 = 0.9500
[Test_batch(7)(2000,16000)] 2017-05-03 08:32:27.010971: step 59000, loss = 0.1367, acc = 0.9530, f1neg = 0.9579, f1pos = 0.9468, f1 = 0.9524
[Test_batch(8)(2000,18000)] 2017-05-03 08:32:27.523795: step 59000, loss = 0.1395, acc = 0.9520, f1neg = 0.9520, f1pos = 0.9520, f1 = 0.9520
[Test_batch(9)(2000,20000)] 2017-05-03 08:32:28.028191: step 59000, loss = 0.1718, acc = 0.9365, f1neg = 0.9344, f1pos = 0.9385, f1 = 0.9364
[Test_batch(10)(2000,22000)] 2017-05-03 08:32:28.516782: step 59000, loss = 0.1512, acc = 0.9490, f1neg = 0.9436, f1pos = 0.9535, f1 = 0.9485
[Test_batch(11)(2000,24000)] 2017-05-03 08:32:29.021714: step 59000, loss = 0.1467, acc = 0.9520, f1neg = 0.9539, f1pos = 0.9499, f1 = 0.9519
[Test_batch(12)(2000,26000)] 2017-05-03 08:32:29.501093: step 59000, loss = 0.1248, acc = 0.9595, f1neg = 0.9614, f1pos = 0.9573, f1 = 0.9594
[Test_batch(13)(2000,28000)] 2017-05-03 08:32:29.984191: step 59000, loss = 0.1487, acc = 0.9520, f1neg = 0.9477, f1pos = 0.9557, f1 = 0.9517
[Test_batch(14)(2000,30000)] 2017-05-03 08:32:30.462569: step 59000, loss = 0.1264, acc = 0.9580, f1neg = 0.9527, f1pos = 0.9622, f1 = 0.9575
[Test_batch(15)(2000,32000)] 2017-05-03 08:32:30.929703: step 59000, loss = 0.1478, acc = 0.9545, f1neg = 0.9545, f1pos = 0.9545, f1 = 0.9545
[Test_batch(16)(2000,34000)] 2017-05-03 08:32:31.409140: step 59000, loss = 0.1295, acc = 0.9595, f1neg = 0.9591, f1pos = 0.9599, f1 = 0.9595
[Test_batch(17)(2000,36000)] 2017-05-03 08:32:31.857329: step 59000, loss = 0.1180, acc = 0.9615, f1neg = 0.9584, f1pos = 0.9642, f1 = 0.9613
[Test_batch(18)(2000,38000)] 2017-05-03 08:32:32.369682: step 59000, loss = 0.1103, acc = 0.9655, f1neg = 0.9651, f1pos = 0.9659, f1 = 0.9655
[Test] 2017-05-03 08:32:32.369744: step 59000, acc = 0.9527, f1 = 0.9525
[Status] 2017-05-03 08:32:32.369755: step 59000, maxindex = 55000, maxdev = 0.9621, maxtst = 0.9537
2017-05-03 08:32:41.588642: step 59010, loss = 0.1281, acc = 0.9620 (257.6 examples/sec; 0.248 sec/batch)
2017-05-03 08:32:50.485157: step 59020, loss = 0.0944, acc = 0.9720 (297.6 examples/sec; 0.215 sec/batch)
2017-05-03 08:33:00.056294: step 59030, loss = 0.0974, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 08:33:09.146124: step 59040, loss = 0.1250, acc = 0.9540 (267.3 examples/sec; 0.239 sec/batch)
2017-05-03 08:33:18.222663: step 59050, loss = 0.1434, acc = 0.9560 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 08:33:27.447175: step 59060, loss = 0.0987, acc = 0.9720 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 08:33:36.356073: step 59070, loss = 0.1288, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 08:33:45.445617: step 59080, loss = 0.1150, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 08:33:54.726117: step 59090, loss = 0.1156, acc = 0.9740 (224.8 examples/sec; 0.285 sec/batch)
2017-05-03 08:34:03.764673: step 59100, loss = 0.1422, acc = 0.9560 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 08:34:13.011985: step 59110, loss = 0.1227, acc = 0.9460 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 08:34:22.167374: step 59120, loss = 0.1230, acc = 0.9720 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 08:34:31.212455: step 59130, loss = 0.1233, acc = 0.9620 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 08:34:40.213218: step 59140, loss = 0.1098, acc = 0.9640 (296.2 examples/sec; 0.216 sec/batch)
2017-05-03 08:34:49.311729: step 59150, loss = 0.0985, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:34:58.459323: step 59160, loss = 0.1251, acc = 0.9580 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 08:35:07.492832: step 59170, loss = 0.1205, acc = 0.9600 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 08:35:16.709045: step 59180, loss = 0.1273, acc = 0.9640 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 08:35:25.743417: step 59190, loss = 0.1293, acc = 0.9640 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 08:35:34.924402: step 59200, loss = 0.1130, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 08:35:44.011739: step 59210, loss = 0.1181, acc = 0.9580 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 08:35:53.335898: step 59220, loss = 0.1129, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 08:36:02.391650: step 59230, loss = 0.1284, acc = 0.9660 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:36:11.383074: step 59240, loss = 0.1185, acc = 0.9520 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 08:36:20.399601: step 59250, loss = 0.1139, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:36:29.304470: step 59260, loss = 0.1299, acc = 0.9600 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 08:36:38.566989: step 59270, loss = 0.1146, acc = 0.9720 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 08:36:47.556594: step 59280, loss = 0.1350, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 08:36:56.800453: step 59290, loss = 0.1332, acc = 0.9560 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 08:37:05.950129: step 59300, loss = 0.1107, acc = 0.9620 (244.7 examples/sec; 0.262 sec/batch)
2017-05-03 08:37:15.010179: step 59310, loss = 0.1311, acc = 0.9560 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 08:37:24.148908: step 59320, loss = 0.1403, acc = 0.9540 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 08:37:33.248617: step 59330, loss = 0.1001, acc = 0.9720 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 08:37:42.265908: step 59340, loss = 0.1109, acc = 0.9640 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 08:37:51.276034: step 59350, loss = 0.1492, acc = 0.9620 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 08:38:00.485682: step 59360, loss = 0.1115, acc = 0.9720 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 08:38:09.539865: step 59370, loss = 0.1106, acc = 0.9680 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 08:38:18.674468: step 59380, loss = 0.0987, acc = 0.9780 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 08:38:27.817480: step 59390, loss = 0.1174, acc = 0.9680 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 08:38:36.937128: step 59400, loss = 0.1249, acc = 0.9580 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 08:38:46.100820: step 59410, loss = 0.1287, acc = 0.9540 (264.9 examples/sec; 0.242 sec/batch)
2017-05-03 08:38:55.370732: step 59420, loss = 0.1029, acc = 0.9680 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 08:39:04.479320: step 59430, loss = 0.1399, acc = 0.9480 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 08:39:13.382033: step 59440, loss = 0.1287, acc = 0.9580 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:39:22.586718: step 59450, loss = 0.1214, acc = 0.9580 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 08:39:32.164552: step 59460, loss = 0.1279, acc = 0.9700 (236.4 examples/sec; 0.271 sec/batch)
2017-05-03 08:39:41.321681: step 59470, loss = 0.1348, acc = 0.9560 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:39:51.307467: step 59480, loss = 0.1236, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 08:40:00.330554: step 59490, loss = 0.1304, acc = 0.9540 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 08:40:09.344090: step 59500, loss = 0.1371, acc = 0.9620 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 08:40:18.533716: step 59510, loss = 0.1205, acc = 0.9640 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 08:40:27.595439: step 59520, loss = 0.1400, acc = 0.9580 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 08:40:36.728787: step 59530, loss = 0.0983, acc = 0.9740 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 08:40:45.864096: step 59540, loss = 0.0981, acc = 0.9720 (272.9 examples/sec; 0.234 sec/batch)
2017-05-03 08:40:55.015352: step 59550, loss = 0.1255, acc = 0.9560 (241.7 examples/sec; 0.265 sec/batch)
2017-05-03 08:41:03.990676: step 59560, loss = 0.1540, acc = 0.9500 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 08:41:12.982105: step 59570, loss = 0.1212, acc = 0.9680 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 08:41:22.006162: step 59580, loss = 0.1227, acc = 0.9700 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 08:41:30.957863: step 59590, loss = 0.1118, acc = 0.9700 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 08:41:40.006067: step 59600, loss = 0.1305, acc = 0.9560 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 08:41:49.047326: step 59610, loss = 0.1165, acc = 0.9700 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 08:41:58.091292: step 59620, loss = 0.1230, acc = 0.9500 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 08:42:07.156637: step 59630, loss = 0.1444, acc = 0.9420 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 08:42:16.268167: step 59640, loss = 0.1246, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 08:42:25.252002: step 59650, loss = 0.1102, acc = 0.9740 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 08:42:34.185349: step 59660, loss = 0.1218, acc = 0.9680 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 08:42:43.345012: step 59670, loss = 0.1086, acc = 0.9680 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 08:42:52.358290: step 59680, loss = 0.1326, acc = 0.9540 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 08:43:01.403961: step 59690, loss = 0.1187, acc = 0.9620 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 08:43:10.277717: step 59700, loss = 0.1293, acc = 0.9520 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 08:43:19.258707: step 59710, loss = 0.1423, acc = 0.9580 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 08:43:28.356252: step 59720, loss = 0.0925, acc = 0.9720 (269.5 examples/sec; 0.237 sec/batch)
2017-05-03 08:43:37.311106: step 59730, loss = 0.1151, acc = 0.9680 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 08:43:46.391543: step 59740, loss = 0.1359, acc = 0.9760 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 08:43:55.416472: step 59750, loss = 0.1390, acc = 0.9460 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 08:44:04.420300: step 59760, loss = 0.1231, acc = 0.9560 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 08:44:13.376983: step 59770, loss = 0.1132, acc = 0.9640 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 08:44:22.476596: step 59780, loss = 0.1318, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 08:44:31.491955: step 59790, loss = 0.1178, acc = 0.9660 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 08:44:40.581510: step 59800, loss = 0.1220, acc = 0.9680 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 08:44:49.636743: step 59810, loss = 0.1317, acc = 0.9560 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 08:44:58.790125: step 59820, loss = 0.1349, acc = 0.9500 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 08:45:07.803756: step 59830, loss = 0.1011, acc = 0.9720 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 08:45:16.967526: step 59840, loss = 0.1141, acc = 0.9720 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 08:45:25.937178: step 59850, loss = 0.1128, acc = 0.9560 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 08:45:34.912845: step 59860, loss = 0.1227, acc = 0.9580 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 08:45:44.002429: step 59870, loss = 0.1170, acc = 0.9720 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 08:45:53.161625: step 59880, loss = 0.1201, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 08:46:02.274711: step 59890, loss = 0.1094, acc = 0.9580 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 08:46:11.217600: step 59900, loss = 0.1364, acc = 0.9460 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 08:46:20.306539: step 59910, loss = 0.1318, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 08:46:29.545767: step 59920, loss = 0.1432, acc = 0.9500 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 08:46:38.765733: step 59930, loss = 0.1432, acc = 0.9520 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 08:46:47.809137: step 59940, loss = 0.1538, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 08:46:56.860327: step 59950, loss = 0.1365, acc = 0.9540 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 08:47:05.937291: step 59960, loss = 0.1021, acc = 0.9740 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 08:47:15.064609: step 59970, loss = 0.1260, acc = 0.9600 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 08:47:24.223405: step 59980, loss = 0.1201, acc = 0.9620 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 08:47:33.417740: step 59990, loss = 0.1732, acc = 0.9460 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 08:47:42.536932: step 60000, loss = 0.1357, acc = 0.9480 (291.0 examples/sec; 0.220 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 08:47:43.019409: step 60000, loss = 0.1081, acc = 0.9610, f1neg = 0.9575, f1pos = 0.9640, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 08:47:43.528937: step 60000, loss = 0.1146, acc = 0.9645, f1neg = 0.9590, f1pos = 0.9687, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 08:47:44.007694: step 60000, loss = 0.1231, acc = 0.9560, f1neg = 0.9536, f1pos = 0.9582, f1 = 0.9559
[Eval_batch(3)(2000,8000)] 2017-05-03 08:47:44.521254: step 60000, loss = 0.1260, acc = 0.9600, f1neg = 0.9602, f1pos = 0.9598, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 08:47:45.031734: step 60000, loss = 0.1284, acc = 0.9560, f1neg = 0.9577, f1pos = 0.9542, f1 = 0.9559
[Eval_batch(5)(2000,12000)] 2017-05-03 08:47:45.543585: step 60000, loss = 0.1259, acc = 0.9530, f1neg = 0.9502, f1pos = 0.9555, f1 = 0.9529
[Eval_batch(6)(2000,14000)] 2017-05-03 08:47:46.051510: step 60000, loss = 0.1156, acc = 0.9665, f1neg = 0.9660, f1pos = 0.9669, f1 = 0.9665
[Eval_batch(7)(2000,16000)] 2017-05-03 08:47:46.525771: step 60000, loss = 0.1168, acc = 0.9620, f1neg = 0.9550, f1pos = 0.9671, f1 = 0.9611
[Eval_batch(8)(2000,18000)] 2017-05-03 08:47:47.002526: step 60000, loss = 0.1111, acc = 0.9615, f1neg = 0.9566, f1pos = 0.9654, f1 = 0.9610
[Eval_batch(9)(2000,20000)] 2017-05-03 08:47:47.514072: step 60000, loss = 0.1169, acc = 0.9640, f1neg = 0.9618, f1pos = 0.9660, f1 = 0.9639
[Eval_batch(10)(2000,22000)] 2017-05-03 08:47:48.024759: step 60000, loss = 0.1227, acc = 0.9605, f1neg = 0.9582, f1pos = 0.9625, f1 = 0.9604
[Eval_batch(11)(2000,24000)] 2017-05-03 08:47:48.503518: step 60000, loss = 0.1249, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Eval_batch(12)(2000,26000)] 2017-05-03 08:47:48.979808: step 60000, loss = 0.1192, acc = 0.9535, f1neg = 0.9524, f1pos = 0.9545, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 08:47:49.450607: step 60000, loss = 0.1113, acc = 0.9660, f1neg = 0.9577, f1pos = 0.9716, f1 = 0.9646
[Eval_batch(14)(2000,30000)] 2017-05-03 08:47:49.928403: step 60000, loss = 0.1379, acc = 0.9550, f1neg = 0.9469, f1pos = 0.9609, f1 = 0.9539
[Eval_batch(15)(2000,32000)] 2017-05-03 08:47:50.437246: step 60000, loss = 0.1147, acc = 0.9665, f1neg = 0.9641, f1pos = 0.9686, f1 = 0.9664
[Eval_batch(16)(2000,34000)] 2017-05-03 08:47:50.945711: step 60000, loss = 0.1093, acc = 0.9660, f1neg = 0.9649, f1pos = 0.9670, f1 = 0.9660
[Eval_batch(17)(2000,36000)] 2017-05-03 08:47:51.424475: step 60000, loss = 0.1380, acc = 0.9570, f1neg = 0.9575, f1pos = 0.9565, f1 = 0.9570
[Eval_batch(18)(2000,38000)] 2017-05-03 08:47:51.935817: step 60000, loss = 0.1281, acc = 0.9595, f1neg = 0.9614, f1pos = 0.9574, f1 = 0.9594
[Eval_batch(19)(2000,40000)] 2017-05-03 08:47:52.450612: step 60000, loss = 0.1119, acc = 0.9685, f1neg = 0.9638, f1pos = 0.9721, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 08:47:52.950705: step 60000, loss = 0.1130, acc = 0.9630, f1neg = 0.9673, f1pos = 0.9573, f1 = 0.9623
[Eval_batch(21)(2000,44000)] 2017-05-03 08:47:53.456528: step 60000, loss = 0.1093, acc = 0.9635, f1neg = 0.9606, f1pos = 0.9660, f1 = 0.9633
[Eval_batch(22)(2000,46000)] 2017-05-03 08:47:53.960885: step 60000, loss = 0.1163, acc = 0.9620, f1neg = 0.9626, f1pos = 0.9613, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 08:47:54.472392: step 60000, loss = 0.1223, acc = 0.9620, f1neg = 0.9576, f1pos = 0.9655, f1 = 0.9616
[Eval_batch(24)(2000,50000)] 2017-05-03 08:47:54.978820: step 60000, loss = 0.1069, acc = 0.9670, f1neg = 0.9625, f1pos = 0.9705, f1 = 0.9665
[Eval_batch(25)(2000,52000)] 2017-05-03 08:47:55.494180: step 60000, loss = 0.0980, acc = 0.9720, f1neg = 0.9695, f1pos = 0.9741, f1 = 0.9718
[Eval_batch(26)(2000,54000)] 2017-05-03 08:47:56.010989: step 60000, loss = 0.1113, acc = 0.9615, f1neg = 0.9639, f1pos = 0.9588, f1 = 0.9613
[Eval_batch(27)(2000,56000)] 2017-05-03 08:47:56.667873: step 60000, loss = 0.0979, acc = 0.9705, f1neg = 0.9692, f1pos = 0.9717, f1 = 0.9704
[Eval] 2017-05-03 08:47:56.667937: step 60000, acc = 0.9621, f1 = 0.9618
[Test_batch(0)(2000,2000)] 2017-05-03 08:47:57.186537: step 60000, loss = 0.1464, acc = 0.9525, f1neg = 0.9553, f1pos = 0.9494, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 08:47:57.689119: step 60000, loss = 0.1257, acc = 0.9590, f1neg = 0.9638, f1pos = 0.9527, f1 = 0.9583
[Test_batch(2)(2000,6000)] 2017-05-03 08:47:58.194714: step 60000, loss = 0.1417, acc = 0.9585, f1neg = 0.9622, f1pos = 0.9541, f1 = 0.9581
[Test_batch(3)(2000,8000)] 2017-05-03 08:47:58.705892: step 60000, loss = 0.1501, acc = 0.9485, f1neg = 0.9522, f1pos = 0.9442, f1 = 0.9482
[Test_batch(4)(2000,10000)] 2017-05-03 08:47:59.194676: step 60000, loss = 0.1473, acc = 0.9510, f1neg = 0.9515, f1pos = 0.9505, f1 = 0.9510
[Test_batch(5)(2000,12000)] 2017-05-03 08:47:59.702019: step 60000, loss = 0.1615, acc = 0.9435, f1neg = 0.9445, f1pos = 0.9424, f1 = 0.9435
[Test_batch(6)(2000,14000)] 2017-05-03 08:48:00.202539: step 60000, loss = 0.1437, acc = 0.9510, f1neg = 0.9497, f1pos = 0.9522, f1 = 0.9510
[Test_batch(7)(2000,16000)] 2017-05-03 08:48:00.705411: step 60000, loss = 0.1355, acc = 0.9505, f1neg = 0.9554, f1pos = 0.9444, f1 = 0.9499
[Test_batch(8)(2000,18000)] 2017-05-03 08:48:01.197464: step 60000, loss = 0.1375, acc = 0.9495, f1neg = 0.9492, f1pos = 0.9498, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 08:48:01.706916: step 60000, loss = 0.1673, acc = 0.9395, f1neg = 0.9369, f1pos = 0.9419, f1 = 0.9394
[Test_batch(10)(2000,22000)] 2017-05-03 08:48:02.217764: step 60000, loss = 0.1457, acc = 0.9530, f1neg = 0.9474, f1pos = 0.9575, f1 = 0.9525
[Test_batch(11)(2000,24000)] 2017-05-03 08:48:02.716880: step 60000, loss = 0.1452, acc = 0.9505, f1neg = 0.9522, f1pos = 0.9486, f1 = 0.9504
[Test_batch(12)(2000,26000)] 2017-05-03 08:48:03.223744: step 60000, loss = 0.1216, acc = 0.9620, f1neg = 0.9637, f1pos = 0.9602, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 08:48:03.730000: step 60000, loss = 0.1450, acc = 0.9530, f1neg = 0.9485, f1pos = 0.9568, f1 = 0.9526
[Test_batch(14)(2000,30000)] 2017-05-03 08:48:04.238803: step 60000, loss = 0.1222, acc = 0.9585, f1neg = 0.9527, f1pos = 0.9630, f1 = 0.9579
[Test_batch(15)(2000,32000)] 2017-05-03 08:48:04.748826: step 60000, loss = 0.1445, acc = 0.9565, f1neg = 0.9563, f1pos = 0.9567, f1 = 0.9565
[Test_batch(16)(2000,34000)] 2017-05-03 08:48:05.252452: step 60000, loss = 0.1273, acc = 0.9595, f1neg = 0.9588, f1pos = 0.9602, f1 = 0.9595
[Test_batch(17)(2000,36000)] 2017-05-03 08:48:05.763143: step 60000, loss = 0.1168, acc = 0.9645, f1neg = 0.9614, f1pos = 0.9671, f1 = 0.9643
[Test_batch(18)(2000,38000)] 2017-05-03 08:48:06.346509: step 60000, loss = 0.1093, acc = 0.9670, f1neg = 0.9665, f1pos = 0.9675, f1 = 0.9670
[Test] 2017-05-03 08:48:06.346599: step 60000, acc = 0.9541, f1 = 0.9539
[Status] 2017-05-03 08:48:06.346625: step 60000, maxindex = 60000, maxdev = 0.9621, maxtst = 0.9541
2017-05-03 08:48:19.592417: step 60010, loss = 0.1420, acc = 0.9540 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 08:48:28.537513: step 60020, loss = 0.1121, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 08:48:37.379296: step 60030, loss = 0.1139, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 08:48:46.455545: step 60040, loss = 0.1665, acc = 0.9380 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 08:48:55.379945: step 60050, loss = 0.1086, acc = 0.9780 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 08:49:04.668938: step 60060, loss = 0.0937, acc = 0.9700 (257.5 examples/sec; 0.249 sec/batch)
2017-05-03 08:49:13.613931: step 60070, loss = 0.1429, acc = 0.9540 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 08:49:22.546058: step 60080, loss = 0.1184, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 08:49:31.698512: step 60090, loss = 0.1415, acc = 0.9560 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 08:49:40.675803: step 60100, loss = 0.1537, acc = 0.9460 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 08:49:50.041810: step 60110, loss = 0.1225, acc = 0.9680 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 08:49:59.007598: step 60120, loss = 0.1511, acc = 0.9500 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 08:50:08.126002: step 60130, loss = 0.1188, acc = 0.9660 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 08:50:17.195007: step 60140, loss = 0.1422, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 08:50:26.227958: step 60150, loss = 0.1297, acc = 0.9460 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 08:50:35.217706: step 60160, loss = 0.1146, acc = 0.9620 (298.4 examples/sec; 0.214 sec/batch)
2017-05-03 08:50:44.384204: step 60170, loss = 0.1167, acc = 0.9640 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 08:50:53.835117: step 60180, loss = 0.1018, acc = 0.9640 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 08:51:02.783047: step 60190, loss = 0.1199, acc = 0.9660 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 08:51:11.633235: step 60200, loss = 0.1340, acc = 0.9580 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 08:51:20.751609: step 60210, loss = 0.1677, acc = 0.9420 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 08:51:29.767533: step 60220, loss = 0.1452, acc = 0.9480 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 08:51:38.741017: step 60230, loss = 0.1319, acc = 0.9620 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 08:51:47.921089: step 60240, loss = 0.1090, acc = 0.9660 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 08:51:57.001157: step 60250, loss = 0.1165, acc = 0.9580 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 08:52:06.056313: step 60260, loss = 0.1364, acc = 0.9600 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 08:52:15.318027: step 60270, loss = 0.1536, acc = 0.9420 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 08:52:24.287929: step 60280, loss = 0.1157, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:52:33.760789: step 60290, loss = 0.1500, acc = 0.9400 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 08:52:43.020519: step 60300, loss = 0.1359, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 08:52:52.177492: step 60310, loss = 0.1226, acc = 0.9560 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 08:53:01.294996: step 60320, loss = 0.1340, acc = 0.9540 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 08:53:10.465512: step 60330, loss = 0.1379, acc = 0.9540 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 08:53:19.534818: step 60340, loss = 0.1264, acc = 0.9600 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 08:53:28.639050: step 60350, loss = 0.1415, acc = 0.9560 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 08:53:37.776150: step 60360, loss = 0.1208, acc = 0.9560 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 08:53:46.823175: step 60370, loss = 0.1175, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 08:53:55.736430: step 60380, loss = 0.1392, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 08:54:04.717235: step 60390, loss = 0.1088, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 08:54:14.171013: step 60400, loss = 0.1479, acc = 0.9620 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 08:54:23.226862: step 60410, loss = 0.1407, acc = 0.9500 (301.0 examples/sec; 0.213 sec/batch)
2017-05-03 08:54:32.189111: step 60420, loss = 0.1401, acc = 0.9540 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 08:54:41.312692: step 60430, loss = 0.1339, acc = 0.9540 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 08:54:50.249476: step 60440, loss = 0.1144, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 08:54:59.234303: step 60450, loss = 0.1287, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 08:55:08.492859: step 60460, loss = 0.1417, acc = 0.9560 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 08:55:17.578868: step 60470, loss = 0.1243, acc = 0.9620 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 08:55:27.572449: step 60480, loss = 0.1384, acc = 0.9580 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 08:55:36.673838: step 60490, loss = 0.1179, acc = 0.9640 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 08:55:45.854998: step 60500, loss = 0.1234, acc = 0.9660 (264.6 examples/sec; 0.242 sec/batch)
2017-05-03 08:55:55.118545: step 60510, loss = 0.1188, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 08:56:04.171837: step 60520, loss = 0.1138, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 08:56:13.157430: step 60530, loss = 0.1067, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 08:56:22.143078: step 60540, loss = 0.1437, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 08:56:31.034795: step 60550, loss = 0.1216, acc = 0.9680 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 08:56:40.191255: step 60560, loss = 0.1203, acc = 0.9680 (259.2 examples/sec; 0.247 sec/batch)
2017-05-03 08:56:49.215819: step 60570, loss = 0.1217, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 08:56:58.151967: step 60580, loss = 0.1275, acc = 0.9640 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 08:57:07.275227: step 60590, loss = 0.1323, acc = 0.9660 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 08:57:16.381994: step 60600, loss = 0.1087, acc = 0.9620 (265.0 examples/sec; 0.241 sec/batch)
2017-05-03 08:57:25.549200: step 60610, loss = 0.1263, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 08:57:34.716040: step 60620, loss = 0.1280, acc = 0.9600 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 08:57:43.952239: step 60630, loss = 0.1109, acc = 0.9760 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 08:57:52.956091: step 60640, loss = 0.1445, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 08:58:02.302352: step 60650, loss = 0.1180, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 08:58:11.390840: step 60660, loss = 0.1199, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 08:58:20.486237: step 60670, loss = 0.1266, acc = 0.9540 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 08:58:29.591680: step 60680, loss = 0.1330, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 08:58:38.929230: step 60690, loss = 0.1098, acc = 0.9660 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 08:58:47.793117: step 60700, loss = 0.1469, acc = 0.9520 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 08:58:56.897633: step 60710, loss = 0.1123, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 08:59:05.944478: step 60720, loss = 0.1118, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 08:59:14.909397: step 60730, loss = 0.1278, acc = 0.9580 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 08:59:23.972591: step 60740, loss = 0.1557, acc = 0.9460 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 08:59:32.964056: step 60750, loss = 0.1406, acc = 0.9540 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 08:59:41.948691: step 60760, loss = 0.1340, acc = 0.9520 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 08:59:51.115704: step 60770, loss = 0.1168, acc = 0.9580 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 09:00:00.184850: step 60780, loss = 0.1155, acc = 0.9680 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 09:00:09.743122: step 60790, loss = 0.1108, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 09:00:18.994497: step 60800, loss = 0.1177, acc = 0.9580 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 09:00:28.001061: step 60810, loss = 0.1141, acc = 0.9660 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 09:00:37.090185: step 60820, loss = 0.1286, acc = 0.9620 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 09:00:46.148007: step 60830, loss = 0.1206, acc = 0.9660 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 09:00:55.314780: step 60840, loss = 0.1305, acc = 0.9680 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 09:01:04.377465: step 60850, loss = 0.1346, acc = 0.9640 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 09:01:13.345765: step 60860, loss = 0.1152, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 09:01:22.312552: step 60870, loss = 0.0951, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 09:01:31.487385: step 60880, loss = 0.1235, acc = 0.9560 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 09:01:40.372026: step 60890, loss = 0.1406, acc = 0.9500 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 09:01:49.396839: step 60900, loss = 0.1329, acc = 0.9560 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 09:01:58.472985: step 60910, loss = 0.1212, acc = 0.9660 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 09:02:07.526406: step 60920, loss = 0.1225, acc = 0.9600 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 09:02:16.709529: step 60930, loss = 0.1048, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 09:02:25.630334: step 60940, loss = 0.1180, acc = 0.9600 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 09:02:34.735605: step 60950, loss = 0.1367, acc = 0.9640 (268.8 examples/sec; 0.238 sec/batch)
2017-05-03 09:02:43.771213: step 60960, loss = 0.1125, acc = 0.9740 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 09:02:52.859958: step 60970, loss = 0.1199, acc = 0.9580 (265.4 examples/sec; 0.241 sec/batch)
2017-05-03 09:03:02.114089: step 60980, loss = 0.1029, acc = 0.9700 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 09:03:11.135901: step 60990, loss = 0.1190, acc = 0.9640 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 09:03:20.137330: step 61000, loss = 0.1227, acc = 0.9600 (267.6 examples/sec; 0.239 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 09:03:20.598838: step 61000, loss = 0.1120, acc = 0.9570, f1neg = 0.9526, f1pos = 0.9607, f1 = 0.9566
[Eval_batch(1)(2000,4000)] 2017-05-03 09:03:21.059504: step 61000, loss = 0.1123, acc = 0.9635, f1neg = 0.9572, f1pos = 0.9682, f1 = 0.9627
[Eval_batch(2)(2000,6000)] 2017-05-03 09:03:21.525222: step 61000, loss = 0.1252, acc = 0.9590, f1neg = 0.9562, f1pos = 0.9614, f1 = 0.9588
[Eval_batch(3)(2000,8000)] 2017-05-03 09:03:22.022472: step 61000, loss = 0.1318, acc = 0.9565, f1neg = 0.9562, f1pos = 0.9568, f1 = 0.9565
[Eval_batch(4)(2000,10000)] 2017-05-03 09:03:22.505380: step 61000, loss = 0.1331, acc = 0.9570, f1neg = 0.9579, f1pos = 0.9561, f1 = 0.9570
[Eval_batch(5)(2000,12000)] 2017-05-03 09:03:22.969644: step 61000, loss = 0.1296, acc = 0.9535, f1neg = 0.9497, f1pos = 0.9568, f1 = 0.9532
[Eval_batch(6)(2000,14000)] 2017-05-03 09:03:23.442358: step 61000, loss = 0.1215, acc = 0.9640, f1neg = 0.9632, f1pos = 0.9648, f1 = 0.9640
[Eval_batch(7)(2000,16000)] 2017-05-03 09:03:23.911474: step 61000, loss = 0.1180, acc = 0.9595, f1neg = 0.9513, f1pos = 0.9653, f1 = 0.9583
[Eval_batch(8)(2000,18000)] 2017-05-03 09:03:24.420485: step 61000, loss = 0.1115, acc = 0.9615, f1neg = 0.9558, f1pos = 0.9659, f1 = 0.9608
[Eval_batch(9)(2000,20000)] 2017-05-03 09:03:24.928621: step 61000, loss = 0.1176, acc = 0.9630, f1neg = 0.9601, f1pos = 0.9655, f1 = 0.9628
[Eval_batch(10)(2000,22000)] 2017-05-03 09:03:25.439041: step 61000, loss = 0.1248, acc = 0.9590, f1neg = 0.9558, f1pos = 0.9618, f1 = 0.9588
[Eval_batch(11)(2000,24000)] 2017-05-03 09:03:25.916855: step 61000, loss = 0.1241, acc = 0.9590, f1neg = 0.9524, f1pos = 0.9640, f1 = 0.9582
[Eval_batch(12)(2000,26000)] 2017-05-03 09:03:26.391214: step 61000, loss = 0.1229, acc = 0.9545, f1neg = 0.9528, f1pos = 0.9561, f1 = 0.9544
[Eval_batch(13)(2000,28000)] 2017-05-03 09:03:26.890890: step 61000, loss = 0.1118, acc = 0.9645, f1neg = 0.9553, f1pos = 0.9706, f1 = 0.9629
[Eval_batch(14)(2000,30000)] 2017-05-03 09:03:27.391864: step 61000, loss = 0.1389, acc = 0.9535, f1neg = 0.9444, f1pos = 0.9600, f1 = 0.9522
[Eval_batch(15)(2000,32000)] 2017-05-03 09:03:27.902013: step 61000, loss = 0.1137, acc = 0.9690, f1neg = 0.9664, f1pos = 0.9712, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 09:03:28.399330: step 61000, loss = 0.1089, acc = 0.9675, f1neg = 0.9661, f1pos = 0.9688, f1 = 0.9674
[Eval_batch(17)(2000,36000)] 2017-05-03 09:03:28.903618: step 61000, loss = 0.1366, acc = 0.9550, f1neg = 0.9551, f1pos = 0.9549, f1 = 0.9550
[Eval_batch(18)(2000,38000)] 2017-05-03 09:03:29.399256: step 61000, loss = 0.1335, acc = 0.9550, f1neg = 0.9566, f1pos = 0.9533, f1 = 0.9549
[Eval_batch(19)(2000,40000)] 2017-05-03 09:03:29.903190: step 61000, loss = 0.1143, acc = 0.9670, f1neg = 0.9618, f1pos = 0.9710, f1 = 0.9664
[Eval_batch(20)(2000,42000)] 2017-05-03 09:03:30.408341: step 61000, loss = 0.1194, acc = 0.9625, f1neg = 0.9667, f1pos = 0.9572, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 09:03:30.879006: step 61000, loss = 0.1064, acc = 0.9650, f1neg = 0.9617, f1pos = 0.9678, f1 = 0.9647
[Eval_batch(22)(2000,46000)] 2017-05-03 09:03:31.385278: step 61000, loss = 0.1169, acc = 0.9595, f1neg = 0.9597, f1pos = 0.9593, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 09:03:31.827491: step 61000, loss = 0.1214, acc = 0.9570, f1neg = 0.9510, f1pos = 0.9617, f1 = 0.9564
[Eval_batch(24)(2000,50000)] 2017-05-03 09:03:32.303572: step 61000, loss = 0.1067, acc = 0.9660, f1neg = 0.9608, f1pos = 0.9700, f1 = 0.9654
[Eval_batch(25)(2000,52000)] 2017-05-03 09:03:32.777705: step 61000, loss = 0.0992, acc = 0.9690, f1neg = 0.9658, f1pos = 0.9716, f1 = 0.9687
[Eval_batch(26)(2000,54000)] 2017-05-03 09:03:33.257884: step 61000, loss = 0.1156, acc = 0.9580, f1neg = 0.9601, f1pos = 0.9557, f1 = 0.9579
[Eval_batch(27)(2000,56000)] 2017-05-03 09:03:33.872848: step 61000, loss = 0.1013, acc = 0.9665, f1neg = 0.9646, f1pos = 0.9682, f1 = 0.9664
[Eval] 2017-05-03 09:03:33.872965: step 61000, acc = 0.9608, f1 = 0.9604
[Test_batch(0)(2000,2000)] 2017-05-03 09:03:34.342729: step 61000, loss = 0.1497, acc = 0.9515, f1neg = 0.9536, f1pos = 0.9492, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 09:03:34.816744: step 61000, loss = 0.1298, acc = 0.9595, f1neg = 0.9638, f1pos = 0.9540, f1 = 0.9589
[Test_batch(2)(2000,6000)] 2017-05-03 09:03:35.281466: step 61000, loss = 0.1481, acc = 0.9525, f1neg = 0.9560, f1pos = 0.9483, f1 = 0.9522
[Test_batch(3)(2000,8000)] 2017-05-03 09:03:35.752976: step 61000, loss = 0.1535, acc = 0.9475, f1neg = 0.9504, f1pos = 0.9442, f1 = 0.9473
[Test_batch(4)(2000,10000)] 2017-05-03 09:03:36.228852: step 61000, loss = 0.1481, acc = 0.9470, f1neg = 0.9467, f1pos = 0.9473, f1 = 0.9470
[Test_batch(5)(2000,12000)] 2017-05-03 09:03:36.697269: step 61000, loss = 0.1627, acc = 0.9400, f1neg = 0.9402, f1pos = 0.9398, f1 = 0.9400
[Test_batch(6)(2000,14000)] 2017-05-03 09:03:37.156819: step 61000, loss = 0.1483, acc = 0.9435, f1neg = 0.9410, f1pos = 0.9458, f1 = 0.9434
[Test_batch(7)(2000,16000)] 2017-05-03 09:03:37.625971: step 61000, loss = 0.1384, acc = 0.9535, f1neg = 0.9575, f1pos = 0.9487, f1 = 0.9531
[Test_batch(8)(2000,18000)] 2017-05-03 09:03:38.080846: step 61000, loss = 0.1382, acc = 0.9485, f1neg = 0.9476, f1pos = 0.9493, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 09:03:38.541102: step 61000, loss = 0.1657, acc = 0.9375, f1neg = 0.9337, f1pos = 0.9409, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 09:03:38.991087: step 61000, loss = 0.1419, acc = 0.9515, f1neg = 0.9450, f1pos = 0.9566, f1 = 0.9508
[Test_batch(11)(2000,24000)] 2017-05-03 09:03:39.459268: step 61000, loss = 0.1479, acc = 0.9510, f1neg = 0.9521, f1pos = 0.9498, f1 = 0.9510
[Test_batch(12)(2000,26000)] 2017-05-03 09:03:39.922292: step 61000, loss = 0.1204, acc = 0.9620, f1neg = 0.9633, f1pos = 0.9606, f1 = 0.9620
[Test_batch(13)(2000,28000)] 2017-05-03 09:03:40.429304: step 61000, loss = 0.1443, acc = 0.9490, f1neg = 0.9429, f1pos = 0.9539, f1 = 0.9484
[Test_batch(14)(2000,30000)] 2017-05-03 09:03:40.900390: step 61000, loss = 0.1189, acc = 0.9610, f1neg = 0.9549, f1pos = 0.9657, f1 = 0.9603
[Test_batch(15)(2000,32000)] 2017-05-03 09:03:41.367635: step 61000, loss = 0.1457, acc = 0.9520, f1neg = 0.9511, f1pos = 0.9529, f1 = 0.9520
[Test_batch(16)(2000,34000)] 2017-05-03 09:03:41.824510: step 61000, loss = 0.1264, acc = 0.9580, f1neg = 0.9566, f1pos = 0.9593, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 09:03:42.300929: step 61000, loss = 0.1190, acc = 0.9670, f1neg = 0.9637, f1pos = 0.9698, f1 = 0.9667
[Test_batch(18)(2000,38000)] 2017-05-03 09:03:42.875806: step 61000, loss = 0.1147, acc = 0.9620, f1neg = 0.9611, f1pos = 0.9629, f1 = 0.9620
[Test] 2017-05-03 09:03:42.875887: step 61000, acc = 0.9523, f1 = 0.9521
[Status] 2017-05-03 09:03:42.875911: step 61000, maxindex = 60000, maxdev = 0.9621, maxtst = 0.9541
2017-05-03 09:03:52.021663: step 61010, loss = 0.1085, acc = 0.9740 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 09:04:00.915381: step 61020, loss = 0.1095, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:04:09.870892: step 61030, loss = 0.0969, acc = 0.9700 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 09:04:18.886430: step 61040, loss = 0.1394, acc = 0.9540 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 09:04:27.965378: step 61050, loss = 0.1177, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 09:04:37.002139: step 61060, loss = 0.1216, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:04:46.137292: step 61070, loss = 0.1082, acc = 0.9640 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:04:55.195332: step 61080, loss = 0.1470, acc = 0.9460 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 09:05:04.216460: step 61090, loss = 0.1319, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 09:05:13.202309: step 61100, loss = 0.1018, acc = 0.9760 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:05:22.245238: step 61110, loss = 0.1116, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 09:05:31.427224: step 61120, loss = 0.1208, acc = 0.9680 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 09:05:40.478539: step 61130, loss = 0.0885, acc = 0.9780 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 09:05:49.470793: step 61140, loss = 0.1361, acc = 0.9480 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 09:05:58.494541: step 61150, loss = 0.0909, acc = 0.9820 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:06:07.524706: step 61160, loss = 0.1318, acc = 0.9520 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:06:16.563188: step 61170, loss = 0.1176, acc = 0.9700 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 09:06:25.569754: step 61180, loss = 0.1015, acc = 0.9680 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 09:06:34.598456: step 61190, loss = 0.1283, acc = 0.9580 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 09:06:43.594916: step 61200, loss = 0.1063, acc = 0.9660 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 09:06:52.498832: step 61210, loss = 0.1089, acc = 0.9660 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 09:07:01.603240: step 61220, loss = 0.1031, acc = 0.9680 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:07:10.690957: step 61230, loss = 0.1153, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:07:19.919005: step 61240, loss = 0.1136, acc = 0.9680 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 09:07:29.016043: step 61250, loss = 0.1168, acc = 0.9600 (302.4 examples/sec; 0.212 sec/batch)
2017-05-03 09:07:38.359710: step 61260, loss = 0.1439, acc = 0.9640 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 09:07:47.427229: step 61270, loss = 0.1068, acc = 0.9680 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 09:07:56.772401: step 61280, loss = 0.1333, acc = 0.9540 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 09:08:05.815586: step 61290, loss = 0.1158, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 09:08:14.941682: step 61300, loss = 0.1254, acc = 0.9620 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 09:08:23.833342: step 61310, loss = 0.1229, acc = 0.9600 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 09:08:32.768375: step 61320, loss = 0.1270, acc = 0.9560 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 09:08:41.769175: step 61330, loss = 0.1284, acc = 0.9500 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 09:08:50.981464: step 61340, loss = 0.1257, acc = 0.9640 (251.1 examples/sec; 0.255 sec/batch)
2017-05-03 09:08:59.908247: step 61350, loss = 0.1382, acc = 0.9520 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 09:09:08.895085: step 61360, loss = 0.1095, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 09:09:18.009694: step 61370, loss = 0.1259, acc = 0.9540 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 09:09:27.118803: step 61380, loss = 0.1483, acc = 0.9500 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 09:09:36.112310: step 61390, loss = 0.0859, acc = 0.9860 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 09:09:45.265752: step 61400, loss = 0.1219, acc = 0.9660 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 09:09:54.367068: step 61410, loss = 0.1267, acc = 0.9600 (302.9 examples/sec; 0.211 sec/batch)
2017-05-03 09:10:03.364201: step 61420, loss = 0.1361, acc = 0.9580 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 09:10:12.392813: step 61430, loss = 0.1538, acc = 0.9460 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 09:10:21.463555: step 61440, loss = 0.1261, acc = 0.9700 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 09:10:30.866984: step 61450, loss = 0.1251, acc = 0.9560 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 09:10:40.107968: step 61460, loss = 0.1273, acc = 0.9540 (262.1 examples/sec; 0.244 sec/batch)
2017-05-03 09:10:49.433191: step 61470, loss = 0.1285, acc = 0.9660 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 09:10:58.504493: step 61480, loss = 0.1122, acc = 0.9620 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 09:11:08.453094: step 61490, loss = 0.1124, acc = 0.9760 (298.0 examples/sec; 0.215 sec/batch)
2017-05-03 09:11:17.680312: step 61500, loss = 0.1244, acc = 0.9560 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:11:26.732646: step 61510, loss = 0.1688, acc = 0.9440 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 09:11:35.813576: step 61520, loss = 0.1209, acc = 0.9620 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 09:11:45.217835: step 61530, loss = 0.1135, acc = 0.9600 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 09:11:54.407008: step 61540, loss = 0.0958, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 09:12:03.299911: step 61550, loss = 0.1168, acc = 0.9720 (299.4 examples/sec; 0.214 sec/batch)
2017-05-03 09:12:12.366022: step 61560, loss = 0.1117, acc = 0.9680 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 09:12:21.457195: step 61570, loss = 0.1022, acc = 0.9660 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 09:12:30.455490: step 61580, loss = 0.1298, acc = 0.9600 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 09:12:39.613739: step 61590, loss = 0.0893, acc = 0.9820 (270.4 examples/sec; 0.237 sec/batch)
2017-05-03 09:12:48.761527: step 61600, loss = 0.1378, acc = 0.9480 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 09:12:57.880652: step 61610, loss = 0.1579, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 09:13:06.912500: step 61620, loss = 0.1503, acc = 0.9500 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 09:13:15.968310: step 61630, loss = 0.1222, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 09:13:24.948520: step 61640, loss = 0.1136, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 09:13:33.972726: step 61650, loss = 0.1141, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 09:13:43.113299: step 61660, loss = 0.1188, acc = 0.9620 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 09:13:52.171886: step 61670, loss = 0.1235, acc = 0.9600 (298.6 examples/sec; 0.214 sec/batch)
2017-05-03 09:14:01.093018: step 61680, loss = 0.1318, acc = 0.9640 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 09:14:10.245615: step 61690, loss = 0.1472, acc = 0.9560 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 09:14:19.278006: step 61700, loss = 0.1110, acc = 0.9700 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 09:14:28.535114: step 61710, loss = 0.1443, acc = 0.9640 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 09:14:37.462642: step 61720, loss = 0.1369, acc = 0.9500 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 09:14:46.502485: step 61730, loss = 0.1326, acc = 0.9520 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 09:14:55.622335: step 61740, loss = 0.1212, acc = 0.9600 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 09:15:04.572323: step 61750, loss = 0.1286, acc = 0.9520 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 09:15:13.584767: step 61760, loss = 0.1303, acc = 0.9620 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 09:15:22.553467: step 61770, loss = 0.1205, acc = 0.9620 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 09:15:31.707766: step 61780, loss = 0.1167, acc = 0.9620 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 09:15:40.734654: step 61790, loss = 0.1217, acc = 0.9680 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 09:15:49.907391: step 61800, loss = 0.1129, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:15:58.994555: step 61810, loss = 0.1504, acc = 0.9500 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 09:16:07.967537: step 61820, loss = 0.0946, acc = 0.9760 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 09:16:17.207292: step 61830, loss = 0.1039, acc = 0.9640 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 09:16:26.411851: step 61840, loss = 0.0958, acc = 0.9720 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 09:16:35.365504: step 61850, loss = 0.1265, acc = 0.9620 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:16:44.217738: step 61860, loss = 0.1038, acc = 0.9720 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 09:16:53.271222: step 61870, loss = 0.1067, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 09:17:02.787555: step 61880, loss = 0.1534, acc = 0.9480 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 09:17:11.763596: step 61890, loss = 0.1158, acc = 0.9640 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 09:17:20.595863: step 61900, loss = 0.1389, acc = 0.9520 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 09:17:29.721012: step 61910, loss = 0.1225, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 09:17:38.748638: step 61920, loss = 0.1503, acc = 0.9540 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 09:17:47.759906: step 61930, loss = 0.1256, acc = 0.9600 (302.8 examples/sec; 0.211 sec/batch)
2017-05-03 09:17:56.992310: step 61940, loss = 0.1223, acc = 0.9660 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 09:18:06.099317: step 61950, loss = 0.1427, acc = 0.9600 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 09:18:15.381797: step 61960, loss = 0.1324, acc = 0.9560 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 09:18:24.429859: step 61970, loss = 0.1216, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 09:18:33.664210: step 61980, loss = 0.1369, acc = 0.9600 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 09:18:42.851686: step 61990, loss = 0.1487, acc = 0.9460 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 09:18:52.129416: step 62000, loss = 0.1466, acc = 0.9540 (251.9 examples/sec; 0.254 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 09:18:52.620070: step 62000, loss = 0.1067, acc = 0.9615, f1neg = 0.9580, f1pos = 0.9644, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 09:18:53.128601: step 62000, loss = 0.1139, acc = 0.9655, f1neg = 0.9601, f1pos = 0.9696, f1 = 0.9649
[Eval_batch(2)(2000,6000)] 2017-05-03 09:18:53.590068: step 62000, loss = 0.1219, acc = 0.9590, f1neg = 0.9568, f1pos = 0.9610, f1 = 0.9589
[Eval_batch(3)(2000,8000)] 2017-05-03 09:18:54.098897: step 62000, loss = 0.1259, acc = 0.9585, f1neg = 0.9587, f1pos = 0.9583, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 09:18:54.569386: step 62000, loss = 0.1282, acc = 0.9565, f1neg = 0.9580, f1pos = 0.9549, f1 = 0.9564
[Eval_batch(5)(2000,12000)] 2017-05-03 09:18:55.036558: step 62000, loss = 0.1265, acc = 0.9540, f1neg = 0.9513, f1pos = 0.9564, f1 = 0.9539
[Eval_batch(6)(2000,14000)] 2017-05-03 09:18:55.545978: step 62000, loss = 0.1157, acc = 0.9695, f1neg = 0.9691, f1pos = 0.9699, f1 = 0.9695
[Eval_batch(7)(2000,16000)] 2017-05-03 09:18:56.055691: step 62000, loss = 0.1170, acc = 0.9605, f1neg = 0.9532, f1pos = 0.9658, f1 = 0.9595
[Eval_batch(8)(2000,18000)] 2017-05-03 09:18:56.519916: step 62000, loss = 0.1110, acc = 0.9620, f1neg = 0.9572, f1pos = 0.9659, f1 = 0.9615
[Eval_batch(9)(2000,20000)] 2017-05-03 09:18:56.997927: step 62000, loss = 0.1158, acc = 0.9650, f1neg = 0.9627, f1pos = 0.9670, f1 = 0.9649
[Eval_batch(10)(2000,22000)] 2017-05-03 09:18:57.504417: step 62000, loss = 0.1236, acc = 0.9595, f1neg = 0.9572, f1pos = 0.9616, f1 = 0.9594
[Eval_batch(11)(2000,24000)] 2017-05-03 09:18:57.956529: step 62000, loss = 0.1243, acc = 0.9600, f1neg = 0.9542, f1pos = 0.9645, f1 = 0.9594
[Eval_batch(12)(2000,26000)] 2017-05-03 09:18:58.400548: step 62000, loss = 0.1193, acc = 0.9530, f1neg = 0.9519, f1pos = 0.9541, f1 = 0.9530
[Eval_batch(13)(2000,28000)] 2017-05-03 09:18:58.908107: step 62000, loss = 0.1113, acc = 0.9650, f1neg = 0.9565, f1pos = 0.9707, f1 = 0.9636
[Eval_batch(14)(2000,30000)] 2017-05-03 09:18:59.402783: step 62000, loss = 0.1376, acc = 0.9550, f1neg = 0.9470, f1pos = 0.9609, f1 = 0.9540
[Eval_batch(15)(2000,32000)] 2017-05-03 09:18:59.868404: step 62000, loss = 0.1137, acc = 0.9680, f1neg = 0.9656, f1pos = 0.9701, f1 = 0.9678
[Eval_batch(16)(2000,34000)] 2017-05-03 09:19:00.321265: step 62000, loss = 0.1086, acc = 0.9680, f1neg = 0.9669, f1pos = 0.9690, f1 = 0.9680
[Eval_batch(17)(2000,36000)] 2017-05-03 09:19:00.822113: step 62000, loss = 0.1370, acc = 0.9570, f1neg = 0.9575, f1pos = 0.9565, f1 = 0.9570
[Eval_batch(18)(2000,38000)] 2017-05-03 09:19:01.290444: step 62000, loss = 0.1284, acc = 0.9595, f1neg = 0.9614, f1pos = 0.9574, f1 = 0.9594
[Eval_batch(19)(2000,40000)] 2017-05-03 09:19:01.765682: step 62000, loss = 0.1117, acc = 0.9675, f1neg = 0.9626, f1pos = 0.9713, f1 = 0.9669
[Eval_batch(20)(2000,42000)] 2017-05-03 09:19:02.239103: step 62000, loss = 0.1139, acc = 0.9620, f1neg = 0.9664, f1pos = 0.9562, f1 = 0.9613
[Eval_batch(21)(2000,44000)] 2017-05-03 09:19:02.735967: step 62000, loss = 0.1093, acc = 0.9645, f1neg = 0.9616, f1pos = 0.9670, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 09:19:03.229503: step 62000, loss = 0.1155, acc = 0.9625, f1neg = 0.9631, f1pos = 0.9618, f1 = 0.9625
[Eval_batch(23)(2000,48000)] 2017-05-03 09:19:03.708531: step 62000, loss = 0.1220, acc = 0.9620, f1neg = 0.9575, f1pos = 0.9656, f1 = 0.9616
[Eval_batch(24)(2000,50000)] 2017-05-03 09:19:04.203132: step 62000, loss = 0.1078, acc = 0.9655, f1neg = 0.9608, f1pos = 0.9692, f1 = 0.9650
[Eval_batch(25)(2000,52000)] 2017-05-03 09:19:04.714979: step 62000, loss = 0.0976, acc = 0.9725, f1neg = 0.9701, f1pos = 0.9746, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 09:19:05.176016: step 62000, loss = 0.1114, acc = 0.9635, f1neg = 0.9658, f1pos = 0.9609, f1 = 0.9633
[Eval_batch(27)(2000,56000)] 2017-05-03 09:19:05.780216: step 62000, loss = 0.0969, acc = 0.9725, f1neg = 0.9713, f1pos = 0.9736, f1 = 0.9725
[Eval] 2017-05-03 09:19:05.780320: step 62000, acc = 0.9625, f1 = 0.9622
[Test_batch(0)(2000,2000)] 2017-05-03 09:19:06.266153: step 62000, loss = 0.1457, acc = 0.9525, f1neg = 0.9553, f1pos = 0.9494, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 09:19:06.736689: step 62000, loss = 0.1265, acc = 0.9595, f1neg = 0.9642, f1pos = 0.9533, f1 = 0.9588
[Test_batch(2)(2000,6000)] 2017-05-03 09:19:07.210620: step 62000, loss = 0.1417, acc = 0.9575, f1neg = 0.9612, f1pos = 0.9531, f1 = 0.9571
[Test_batch(3)(2000,8000)] 2017-05-03 09:19:07.695846: step 62000, loss = 0.1495, acc = 0.9485, f1neg = 0.9522, f1pos = 0.9441, f1 = 0.9482
[Test_batch(4)(2000,10000)] 2017-05-03 09:19:08.173005: step 62000, loss = 0.1468, acc = 0.9495, f1neg = 0.9501, f1pos = 0.9489, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 09:19:08.619861: step 62000, loss = 0.1605, acc = 0.9415, f1neg = 0.9427, f1pos = 0.9402, f1 = 0.9415
[Test_batch(6)(2000,14000)] 2017-05-03 09:19:09.070667: step 62000, loss = 0.1434, acc = 0.9505, f1neg = 0.9492, f1pos = 0.9518, f1 = 0.9505
[Test_batch(7)(2000,16000)] 2017-05-03 09:19:09.537392: step 62000, loss = 0.1355, acc = 0.9515, f1neg = 0.9562, f1pos = 0.9457, f1 = 0.9509
[Test_batch(8)(2000,18000)] 2017-05-03 09:19:10.015934: step 62000, loss = 0.1367, acc = 0.9515, f1neg = 0.9512, f1pos = 0.9518, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 09:19:10.489073: step 62000, loss = 0.1683, acc = 0.9380, f1neg = 0.9353, f1pos = 0.9404, f1 = 0.9379
[Test_batch(10)(2000,22000)] 2017-05-03 09:19:10.960692: step 62000, loss = 0.1448, acc = 0.9525, f1neg = 0.9470, f1pos = 0.9570, f1 = 0.9520
[Test_batch(11)(2000,24000)] 2017-05-03 09:19:11.429648: step 62000, loss = 0.1449, acc = 0.9515, f1neg = 0.9533, f1pos = 0.9496, f1 = 0.9514
[Test_batch(12)(2000,26000)] 2017-05-03 09:19:11.905911: step 62000, loss = 0.1210, acc = 0.9615, f1neg = 0.9632, f1pos = 0.9597, f1 = 0.9614
[Test_batch(13)(2000,28000)] 2017-05-03 09:19:12.389716: step 62000, loss = 0.1445, acc = 0.9525, f1neg = 0.9478, f1pos = 0.9564, f1 = 0.9521
[Test_batch(14)(2000,30000)] 2017-05-03 09:19:12.840115: step 62000, loss = 0.1216, acc = 0.9570, f1neg = 0.9510, f1pos = 0.9617, f1 = 0.9564
[Test_batch(15)(2000,32000)] 2017-05-03 09:19:13.312194: step 62000, loss = 0.1442, acc = 0.9575, f1neg = 0.9572, f1pos = 0.9578, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 09:19:13.792238: step 62000, loss = 0.1261, acc = 0.9590, f1neg = 0.9583, f1pos = 0.9596, f1 = 0.9590
[Test_batch(17)(2000,36000)] 2017-05-03 09:19:14.296552: step 62000, loss = 0.1172, acc = 0.9645, f1neg = 0.9614, f1pos = 0.9671, f1 = 0.9643
[Test_batch(18)(2000,38000)] 2017-05-03 09:19:14.883620: step 62000, loss = 0.1098, acc = 0.9650, f1neg = 0.9645, f1pos = 0.9655, f1 = 0.9650
[Test] 2017-05-03 09:19:14.883703: step 62000, acc = 0.9538, f1 = 0.9535
[Status] 2017-05-03 09:19:14.883729: step 62000, maxindex = 62000, maxdev = 0.9625, maxtst = 0.9538
2017-05-03 09:19:27.241613: step 62010, loss = 0.1266, acc = 0.9600 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 09:19:36.254459: step 62020, loss = 0.1239, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 09:19:45.240745: step 62030, loss = 0.1008, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 09:19:54.392463: step 62040, loss = 0.1162, acc = 0.9620 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 09:20:03.599759: step 62050, loss = 0.1320, acc = 0.9560 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:20:12.683006: step 62060, loss = 0.1254, acc = 0.9640 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:20:21.838488: step 62070, loss = 0.1745, acc = 0.9420 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 09:20:30.939480: step 62080, loss = 0.1130, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 09:20:40.005041: step 62090, loss = 0.1239, acc = 0.9600 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 09:20:48.962921: step 62100, loss = 0.1302, acc = 0.9560 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 09:20:58.040445: step 62110, loss = 0.1148, acc = 0.9680 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 09:21:07.045129: step 62120, loss = 0.1488, acc = 0.9560 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 09:21:16.187071: step 62130, loss = 0.1431, acc = 0.9540 (270.5 examples/sec; 0.237 sec/batch)
2017-05-03 09:21:25.502023: step 62140, loss = 0.1187, acc = 0.9600 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 09:21:34.636011: step 62150, loss = 0.1018, acc = 0.9700 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 09:21:43.686401: step 62160, loss = 0.1109, acc = 0.9700 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 09:21:52.737424: step 62170, loss = 0.1160, acc = 0.9700 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 09:22:01.737146: step 62180, loss = 0.1081, acc = 0.9640 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 09:22:10.802535: step 62190, loss = 0.1246, acc = 0.9540 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 09:22:20.051620: step 62200, loss = 0.1323, acc = 0.9580 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 09:22:29.180994: step 62210, loss = 0.1475, acc = 0.9600 (266.8 examples/sec; 0.240 sec/batch)
2017-05-03 09:22:38.293298: step 62220, loss = 0.1379, acc = 0.9540 (303.3 examples/sec; 0.211 sec/batch)
2017-05-03 09:22:47.198658: step 62230, loss = 0.1120, acc = 0.9600 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:22:56.328116: step 62240, loss = 0.1438, acc = 0.9540 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 09:23:05.346046: step 62250, loss = 0.1158, acc = 0.9700 (272.9 examples/sec; 0.234 sec/batch)
2017-05-03 09:23:14.464288: step 62260, loss = 0.1311, acc = 0.9640 (254.5 examples/sec; 0.252 sec/batch)
2017-05-03 09:23:23.563304: step 62270, loss = 0.1008, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 09:23:32.804966: step 62280, loss = 0.1413, acc = 0.9480 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 09:23:41.922491: step 62290, loss = 0.1245, acc = 0.9620 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 09:23:51.040267: step 62300, loss = 0.1230, acc = 0.9560 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 09:23:59.998630: step 62310, loss = 0.1164, acc = 0.9660 (298.8 examples/sec; 0.214 sec/batch)
2017-05-03 09:24:09.107397: step 62320, loss = 0.1339, acc = 0.9500 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 09:24:18.123411: step 62330, loss = 0.1101, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 09:24:27.157430: step 62340, loss = 0.1307, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 09:24:36.007258: step 62350, loss = 0.1673, acc = 0.9460 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 09:24:45.183519: step 62360, loss = 0.1113, acc = 0.9740 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 09:24:54.292892: step 62370, loss = 0.1428, acc = 0.9480 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 09:25:03.392003: step 62380, loss = 0.1261, acc = 0.9500 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 09:25:12.535028: step 62390, loss = 0.1112, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 09:25:21.413973: step 62400, loss = 0.1257, acc = 0.9580 (296.8 examples/sec; 0.216 sec/batch)
2017-05-03 09:25:30.363437: step 62410, loss = 0.1199, acc = 0.9700 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 09:25:39.433403: step 62420, loss = 0.1110, acc = 0.9660 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 09:25:48.652453: step 62430, loss = 0.1419, acc = 0.9460 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 09:25:57.779450: step 62440, loss = 0.1118, acc = 0.9700 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 09:26:06.889462: step 62450, loss = 0.1339, acc = 0.9580 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 09:26:15.864652: step 62460, loss = 0.1250, acc = 0.9580 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 09:26:25.074053: step 62470, loss = 0.1095, acc = 0.9640 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 09:26:34.348633: step 62480, loss = 0.1356, acc = 0.9540 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 09:26:43.436779: step 62490, loss = 0.1248, acc = 0.9640 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 09:26:53.230503: step 62500, loss = 0.1433, acc = 0.9480 (298.6 examples/sec; 0.214 sec/batch)
2017-05-03 09:27:02.213439: step 62510, loss = 0.1525, acc = 0.9480 (294.9 examples/sec; 0.217 sec/batch)
2017-05-03 09:27:11.126076: step 62520, loss = 0.1313, acc = 0.9500 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 09:27:20.147412: step 62530, loss = 0.1388, acc = 0.9580 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 09:27:29.125601: step 62540, loss = 0.1163, acc = 0.9540 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 09:27:38.275214: step 62550, loss = 0.1101, acc = 0.9620 (264.6 examples/sec; 0.242 sec/batch)
2017-05-03 09:27:47.582582: step 62560, loss = 0.1213, acc = 0.9640 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 09:27:56.688044: step 62570, loss = 0.1305, acc = 0.9500 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 09:28:05.798525: step 62580, loss = 0.1179, acc = 0.9720 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 09:28:14.807991: step 62590, loss = 0.1363, acc = 0.9620 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 09:28:23.825642: step 62600, loss = 0.1246, acc = 0.9600 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 09:28:32.857883: step 62610, loss = 0.1495, acc = 0.9500 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 09:28:41.865109: step 62620, loss = 0.1211, acc = 0.9680 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 09:28:50.909908: step 62630, loss = 0.1078, acc = 0.9700 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 09:29:00.011868: step 62640, loss = 0.1395, acc = 0.9620 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 09:29:09.041797: step 62650, loss = 0.1160, acc = 0.9620 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 09:29:18.075688: step 62660, loss = 0.1234, acc = 0.9620 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 09:29:27.188992: step 62670, loss = 0.1647, acc = 0.9320 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 09:29:36.232763: step 62680, loss = 0.1313, acc = 0.9560 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 09:29:45.215129: step 62690, loss = 0.1214, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 09:29:54.276690: step 62700, loss = 0.1234, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 09:30:03.302063: step 62710, loss = 0.1317, acc = 0.9560 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 09:30:12.228429: step 62720, loss = 0.1012, acc = 0.9680 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 09:30:21.274075: step 62730, loss = 0.1125, acc = 0.9640 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 09:30:30.325900: step 62740, loss = 0.1154, acc = 0.9520 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 09:30:39.472802: step 62750, loss = 0.1258, acc = 0.9620 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 09:30:48.396077: step 62760, loss = 0.1374, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 09:30:57.376601: step 62770, loss = 0.1257, acc = 0.9620 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 09:31:06.360659: step 62780, loss = 0.1094, acc = 0.9680 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 09:31:15.430244: step 62790, loss = 0.1218, acc = 0.9640 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 09:31:24.461302: step 62800, loss = 0.1237, acc = 0.9660 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 09:31:33.332837: step 62810, loss = 0.1252, acc = 0.9640 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 09:31:42.254221: step 62820, loss = 0.0953, acc = 0.9740 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:31:51.325613: step 62830, loss = 0.1237, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 09:32:00.353945: step 62840, loss = 0.1304, acc = 0.9620 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 09:32:09.280439: step 62850, loss = 0.1098, acc = 0.9640 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 09:32:18.394909: step 62860, loss = 0.1280, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:32:27.362218: step 62870, loss = 0.1320, acc = 0.9620 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 09:32:36.396377: step 62880, loss = 0.1095, acc = 0.9600 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 09:32:45.440790: step 62890, loss = 0.1155, acc = 0.9640 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 09:32:54.518137: step 62900, loss = 0.1159, acc = 0.9640 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 09:33:03.587652: step 62910, loss = 0.1266, acc = 0.9580 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 09:33:12.545793: step 62920, loss = 0.1115, acc = 0.9640 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 09:33:22.142485: step 62930, loss = 0.1151, acc = 0.9660 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 09:33:31.065290: step 62940, loss = 0.1062, acc = 0.9720 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 09:33:40.132883: step 62950, loss = 0.1231, acc = 0.9560 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 09:33:49.415055: step 62960, loss = 0.1442, acc = 0.9420 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 09:33:58.577470: step 62970, loss = 0.1164, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 09:34:07.665931: step 62980, loss = 0.1230, acc = 0.9580 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 09:34:16.867898: step 62990, loss = 0.1095, acc = 0.9700 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 09:34:26.032752: step 63000, loss = 0.1014, acc = 0.9760 (254.8 examples/sec; 0.251 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 09:34:26.505795: step 63000, loss = 0.1075, acc = 0.9630, f1neg = 0.9598, f1pos = 0.9657, f1 = 0.9628
[Eval_batch(1)(2000,4000)] 2017-05-03 09:34:26.981810: step 63000, loss = 0.1174, acc = 0.9640, f1neg = 0.9586, f1pos = 0.9681, f1 = 0.9634
[Eval_batch(2)(2000,6000)] 2017-05-03 09:34:27.482895: step 63000, loss = 0.1241, acc = 0.9580, f1neg = 0.9559, f1pos = 0.9599, f1 = 0.9579
[Eval_batch(3)(2000,8000)] 2017-05-03 09:34:27.984218: step 63000, loss = 0.1263, acc = 0.9585, f1neg = 0.9590, f1pos = 0.9580, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 09:34:28.495091: step 63000, loss = 0.1288, acc = 0.9555, f1neg = 0.9574, f1pos = 0.9534, f1 = 0.9554
[Eval_batch(5)(2000,12000)] 2017-05-03 09:34:28.995372: step 63000, loss = 0.1278, acc = 0.9530, f1neg = 0.9504, f1pos = 0.9553, f1 = 0.9529
[Eval_batch(6)(2000,14000)] 2017-05-03 09:34:29.500583: step 63000, loss = 0.1159, acc = 0.9660, f1neg = 0.9657, f1pos = 0.9663, f1 = 0.9660
[Eval_batch(7)(2000,16000)] 2017-05-03 09:34:30.006029: step 63000, loss = 0.1192, acc = 0.9590, f1neg = 0.9517, f1pos = 0.9644, f1 = 0.9580
[Eval_batch(8)(2000,18000)] 2017-05-03 09:34:30.506607: step 63000, loss = 0.1116, acc = 0.9655, f1neg = 0.9613, f1pos = 0.9689, f1 = 0.9651
[Eval_batch(9)(2000,20000)] 2017-05-03 09:34:30.995852: step 63000, loss = 0.1174, acc = 0.9610, f1neg = 0.9587, f1pos = 0.9630, f1 = 0.9609
[Eval_batch(10)(2000,22000)] 2017-05-03 09:34:31.500349: step 63000, loss = 0.1242, acc = 0.9625, f1neg = 0.9605, f1pos = 0.9643, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 09:34:32.006272: step 63000, loss = 0.1261, acc = 0.9610, f1neg = 0.9555, f1pos = 0.9653, f1 = 0.9604
[Eval_batch(12)(2000,26000)] 2017-05-03 09:34:32.506463: step 63000, loss = 0.1205, acc = 0.9535, f1neg = 0.9527, f1pos = 0.9543, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 09:34:33.007265: step 63000, loss = 0.1124, acc = 0.9650, f1neg = 0.9566, f1pos = 0.9707, f1 = 0.9636
[Eval_batch(14)(2000,30000)] 2017-05-03 09:34:33.511208: step 63000, loss = 0.1389, acc = 0.9525, f1neg = 0.9443, f1pos = 0.9586, f1 = 0.9514
[Eval_batch(15)(2000,32000)] 2017-05-03 09:34:34.006391: step 63000, loss = 0.1158, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9666, f1 = 0.9644
[Eval_batch(16)(2000,34000)] 2017-05-03 09:34:34.499175: step 63000, loss = 0.1098, acc = 0.9665, f1neg = 0.9655, f1pos = 0.9675, f1 = 0.9665
[Eval_batch(17)(2000,36000)] 2017-05-03 09:34:34.995891: step 63000, loss = 0.1386, acc = 0.9595, f1neg = 0.9601, f1pos = 0.9589, f1 = 0.9595
[Eval_batch(18)(2000,38000)] 2017-05-03 09:34:35.491567: step 63000, loss = 0.1279, acc = 0.9595, f1neg = 0.9616, f1pos = 0.9572, f1 = 0.9594
[Eval_batch(19)(2000,40000)] 2017-05-03 09:34:35.975865: step 63000, loss = 0.1130, acc = 0.9680, f1neg = 0.9635, f1pos = 0.9715, f1 = 0.9675
[Eval_batch(20)(2000,42000)] 2017-05-03 09:34:36.478858: step 63000, loss = 0.1128, acc = 0.9620, f1neg = 0.9665, f1pos = 0.9561, f1 = 0.9613
[Eval_batch(21)(2000,44000)] 2017-05-03 09:34:36.978072: step 63000, loss = 0.1126, acc = 0.9630, f1neg = 0.9602, f1pos = 0.9655, f1 = 0.9628
[Eval_batch(22)(2000,46000)] 2017-05-03 09:34:37.474714: step 63000, loss = 0.1170, acc = 0.9640, f1neg = 0.9647, f1pos = 0.9633, f1 = 0.9640
[Eval_batch(23)(2000,48000)] 2017-05-03 09:34:37.966770: step 63000, loss = 0.1255, acc = 0.9600, f1neg = 0.9555, f1pos = 0.9637, f1 = 0.9596
[Eval_batch(24)(2000,50000)] 2017-05-03 09:34:38.466919: step 63000, loss = 0.1089, acc = 0.9675, f1neg = 0.9631, f1pos = 0.9709, f1 = 0.9670
[Eval_batch(25)(2000,52000)] 2017-05-03 09:34:38.969538: step 63000, loss = 0.0988, acc = 0.9715, f1neg = 0.9690, f1pos = 0.9736, f1 = 0.9713
[Eval_batch(26)(2000,54000)] 2017-05-03 09:34:39.472449: step 63000, loss = 0.1115, acc = 0.9635, f1neg = 0.9660, f1pos = 0.9606, f1 = 0.9633
[Eval_batch(27)(2000,56000)] 2017-05-03 09:34:40.074506: step 63000, loss = 0.0963, acc = 0.9715, f1neg = 0.9704, f1pos = 0.9726, f1 = 0.9715
[Eval] 2017-05-03 09:34:40.074594: step 63000, acc = 0.9621, f1 = 0.9618
[Test_batch(0)(2000,2000)] 2017-05-03 09:34:40.579866: step 63000, loss = 0.1461, acc = 0.9525, f1neg = 0.9555, f1pos = 0.9491, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 09:34:41.089747: step 63000, loss = 0.1258, acc = 0.9605, f1neg = 0.9653, f1pos = 0.9541, f1 = 0.9597
[Test_batch(2)(2000,6000)] 2017-05-03 09:34:41.590886: step 63000, loss = 0.1420, acc = 0.9565, f1neg = 0.9604, f1pos = 0.9517, f1 = 0.9561
[Test_batch(3)(2000,8000)] 2017-05-03 09:34:42.074733: step 63000, loss = 0.1499, acc = 0.9495, f1neg = 0.9533, f1pos = 0.9451, f1 = 0.9492
[Test_batch(4)(2000,10000)] 2017-05-03 09:34:42.577633: step 63000, loss = 0.1495, acc = 0.9490, f1neg = 0.9499, f1pos = 0.9481, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 09:34:43.038219: step 63000, loss = 0.1625, acc = 0.9440, f1neg = 0.9455, f1pos = 0.9424, f1 = 0.9440
[Test_batch(6)(2000,14000)] 2017-05-03 09:34:43.512525: step 63000, loss = 0.1439, acc = 0.9510, f1neg = 0.9501, f1pos = 0.9519, f1 = 0.9510
[Test_batch(7)(2000,16000)] 2017-05-03 09:34:43.993767: step 63000, loss = 0.1355, acc = 0.9525, f1neg = 0.9575, f1pos = 0.9462, f1 = 0.9518
[Test_batch(8)(2000,18000)] 2017-05-03 09:34:44.454944: step 63000, loss = 0.1386, acc = 0.9530, f1neg = 0.9530, f1pos = 0.9530, f1 = 0.9530
[Test_batch(9)(2000,20000)] 2017-05-03 09:34:44.957596: step 63000, loss = 0.1702, acc = 0.9370, f1neg = 0.9348, f1pos = 0.9391, f1 = 0.9369
[Test_batch(10)(2000,22000)] 2017-05-03 09:34:45.435519: step 63000, loss = 0.1491, acc = 0.9510, f1neg = 0.9456, f1pos = 0.9554, f1 = 0.9505
[Test_batch(11)(2000,24000)] 2017-05-03 09:34:45.901717: step 63000, loss = 0.1463, acc = 0.9510, f1neg = 0.9529, f1pos = 0.9489, f1 = 0.9509
[Test_batch(12)(2000,26000)] 2017-05-03 09:34:46.392769: step 63000, loss = 0.1230, acc = 0.9610, f1neg = 0.9628, f1pos = 0.9590, f1 = 0.9609
[Test_batch(13)(2000,28000)] 2017-05-03 09:34:46.898563: step 63000, loss = 0.1471, acc = 0.9525, f1neg = 0.9482, f1pos = 0.9562, f1 = 0.9522
[Test_batch(14)(2000,30000)] 2017-05-03 09:34:47.412400: step 63000, loss = 0.1244, acc = 0.9580, f1neg = 0.9526, f1pos = 0.9623, f1 = 0.9574
[Test_batch(15)(2000,32000)] 2017-05-03 09:34:47.895290: step 63000, loss = 0.1470, acc = 0.9550, f1neg = 0.9550, f1pos = 0.9550, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 09:34:48.369216: step 63000, loss = 0.1277, acc = 0.9600, f1neg = 0.9596, f1pos = 0.9604, f1 = 0.9600
[Test_batch(17)(2000,36000)] 2017-05-03 09:34:48.874723: step 63000, loss = 0.1168, acc = 0.9625, f1neg = 0.9594, f1pos = 0.9651, f1 = 0.9623
[Test_batch(18)(2000,38000)] 2017-05-03 09:34:49.391845: step 63000, loss = 0.1097, acc = 0.9680, f1neg = 0.9676, f1pos = 0.9683, f1 = 0.9680
[Test] 2017-05-03 09:34:49.391934: step 63000, acc = 0.9539, f1 = 0.9537
[Status] 2017-05-03 09:34:49.391960: step 63000, maxindex = 62000, maxdev = 0.9625, maxtst = 0.9538
2017-05-03 09:34:58.741710: step 63010, loss = 0.1425, acc = 0.9500 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 09:35:07.892411: step 63020, loss = 0.1133, acc = 0.9740 (296.2 examples/sec; 0.216 sec/batch)
2017-05-03 09:35:16.837450: step 63030, loss = 0.1152, acc = 0.9680 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 09:35:25.874801: step 63040, loss = 0.1263, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 09:35:34.789917: step 63050, loss = 0.1286, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 09:35:43.889477: step 63060, loss = 0.1002, acc = 0.9760 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 09:35:52.939503: step 63070, loss = 0.1335, acc = 0.9660 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 09:36:01.994543: step 63080, loss = 0.1372, acc = 0.9560 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 09:36:11.180491: step 63090, loss = 0.1285, acc = 0.9520 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 09:36:20.291513: step 63100, loss = 0.0889, acc = 0.9780 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 09:36:29.504727: step 63110, loss = 0.1279, acc = 0.9720 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 09:36:39.103523: step 63120, loss = 0.1170, acc = 0.9640 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 09:36:48.171448: step 63130, loss = 0.1141, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 09:36:57.314571: step 63140, loss = 0.1328, acc = 0.9520 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 09:37:06.382796: step 63150, loss = 0.1334, acc = 0.9600 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 09:37:15.514380: step 63160, loss = 0.1215, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 09:37:24.756208: step 63170, loss = 0.1385, acc = 0.9500 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 09:37:33.772385: step 63180, loss = 0.1186, acc = 0.9600 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 09:37:42.668894: step 63190, loss = 0.1073, acc = 0.9700 (301.1 examples/sec; 0.213 sec/batch)
2017-05-03 09:37:51.862426: step 63200, loss = 0.1604, acc = 0.9580 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 09:38:00.835369: step 63210, loss = 0.1592, acc = 0.9400 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 09:38:09.904719: step 63220, loss = 0.1528, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 09:38:18.965304: step 63230, loss = 0.1165, acc = 0.9740 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 09:38:28.016241: step 63240, loss = 0.1073, acc = 0.9660 (262.8 examples/sec; 0.243 sec/batch)
2017-05-03 09:38:37.014937: step 63250, loss = 0.1244, acc = 0.9640 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:38:46.056837: step 63260, loss = 0.1316, acc = 0.9560 (270.6 examples/sec; 0.237 sec/batch)
2017-05-03 09:38:55.248074: step 63270, loss = 0.1357, acc = 0.9620 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 09:39:04.276890: step 63280, loss = 0.0905, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 09:39:13.301021: step 63290, loss = 0.1142, acc = 0.9660 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 09:39:22.520925: step 63300, loss = 0.1016, acc = 0.9680 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 09:39:31.427013: step 63310, loss = 0.1319, acc = 0.9560 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 09:39:40.494284: step 63320, loss = 0.1267, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 09:39:49.498900: step 63330, loss = 0.1197, acc = 0.9620 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 09:39:58.714321: step 63340, loss = 0.1090, acc = 0.9620 (266.3 examples/sec; 0.240 sec/batch)
2017-05-03 09:40:07.918401: step 63350, loss = 0.0935, acc = 0.9700 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 09:40:16.929674: step 63360, loss = 0.1343, acc = 0.9660 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 09:40:25.932217: step 63370, loss = 0.1180, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 09:40:35.071012: step 63380, loss = 0.1246, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 09:40:44.228917: step 63390, loss = 0.1449, acc = 0.9600 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 09:40:53.272834: step 63400, loss = 0.1185, acc = 0.9720 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 09:41:02.515152: step 63410, loss = 0.1292, acc = 0.9600 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 09:41:11.730534: step 63420, loss = 0.1353, acc = 0.9600 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 09:41:20.901390: step 63430, loss = 0.1347, acc = 0.9560 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 09:41:29.849894: step 63440, loss = 0.1191, acc = 0.9580 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 09:41:38.998476: step 63450, loss = 0.1246, acc = 0.9680 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 09:41:48.211102: step 63460, loss = 0.1295, acc = 0.9620 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 09:41:57.274392: step 63470, loss = 0.1204, acc = 0.9600 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 09:42:06.381105: step 63480, loss = 0.1239, acc = 0.9600 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 09:42:15.588308: step 63490, loss = 0.1315, acc = 0.9640 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 09:42:24.675738: step 63500, loss = 0.1234, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 09:42:34.707132: step 63510, loss = 0.1230, acc = 0.9600 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 09:42:43.758686: step 63520, loss = 0.1296, acc = 0.9660 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 09:42:52.928762: step 63530, loss = 0.1294, acc = 0.9520 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 09:43:01.940690: step 63540, loss = 0.1151, acc = 0.9720 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 09:43:11.095205: step 63550, loss = 0.1166, acc = 0.9700 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 09:43:20.113220: step 63560, loss = 0.1215, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 09:43:29.203040: step 63570, loss = 0.1261, acc = 0.9600 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 09:43:38.319962: step 63580, loss = 0.1233, acc = 0.9600 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 09:43:47.409417: step 63590, loss = 0.1016, acc = 0.9680 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 09:43:56.651213: step 63600, loss = 0.1331, acc = 0.9540 (255.4 examples/sec; 0.251 sec/batch)
2017-05-03 09:44:05.849063: step 63610, loss = 0.1268, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 09:44:14.910148: step 63620, loss = 0.1587, acc = 0.9480 (266.9 examples/sec; 0.240 sec/batch)
2017-05-03 09:44:23.980646: step 63630, loss = 0.1170, acc = 0.9680 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 09:44:33.068940: step 63640, loss = 0.1237, acc = 0.9620 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 09:44:42.244379: step 63650, loss = 0.1120, acc = 0.9580 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 09:44:51.386525: step 63660, loss = 0.1310, acc = 0.9580 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 09:45:00.362734: step 63670, loss = 0.1439, acc = 0.9520 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 09:45:09.358969: step 63680, loss = 0.1248, acc = 0.9600 (299.5 examples/sec; 0.214 sec/batch)
2017-05-03 09:45:18.758914: step 63690, loss = 0.0961, acc = 0.9780 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 09:45:28.013862: step 63700, loss = 0.1347, acc = 0.9520 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 09:45:37.378850: step 63710, loss = 0.1469, acc = 0.9540 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 09:45:46.784487: step 63720, loss = 0.1104, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 09:45:56.020244: step 63730, loss = 0.1467, acc = 0.9460 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 09:46:05.482224: step 63740, loss = 0.1332, acc = 0.9520 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 09:46:14.657635: step 63750, loss = 0.1328, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 09:46:23.761423: step 63760, loss = 0.1058, acc = 0.9800 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 09:46:32.837019: step 63770, loss = 0.1320, acc = 0.9600 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 09:46:41.769871: step 63780, loss = 0.1076, acc = 0.9720 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 09:46:50.792604: step 63790, loss = 0.1203, acc = 0.9740 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 09:46:59.765268: step 63800, loss = 0.1145, acc = 0.9640 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 09:47:08.867127: step 63810, loss = 0.1286, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 09:47:17.891698: step 63820, loss = 0.1045, acc = 0.9660 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 09:47:26.982622: step 63830, loss = 0.1118, acc = 0.9600 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 09:47:36.012745: step 63840, loss = 0.1036, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 09:47:44.948297: step 63850, loss = 0.1130, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 09:47:54.034541: step 63860, loss = 0.1411, acc = 0.9460 (261.0 examples/sec; 0.245 sec/batch)
2017-05-03 09:48:03.060631: step 63870, loss = 0.1243, acc = 0.9620 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 09:48:12.202011: step 63880, loss = 0.1002, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 09:48:21.258933: step 63890, loss = 0.1214, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 09:48:30.270278: step 63900, loss = 0.1245, acc = 0.9580 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 09:48:39.185277: step 63910, loss = 0.1538, acc = 0.9460 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:48:48.625032: step 63920, loss = 0.1149, acc = 0.9720 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 09:48:57.603073: step 63930, loss = 0.1410, acc = 0.9460 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 09:49:06.527750: step 63940, loss = 0.1038, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 09:49:15.834004: step 63950, loss = 0.1206, acc = 0.9600 (254.6 examples/sec; 0.251 sec/batch)
2017-05-03 09:49:24.826431: step 63960, loss = 0.1195, acc = 0.9680 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 09:49:33.840719: step 63970, loss = 0.1518, acc = 0.9460 (268.8 examples/sec; 0.238 sec/batch)
2017-05-03 09:49:42.890679: step 63980, loss = 0.1479, acc = 0.9480 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 09:49:52.049898: step 63990, loss = 0.1254, acc = 0.9560 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 09:50:01.287788: step 64000, loss = 0.1085, acc = 0.9620 (291.0 examples/sec; 0.220 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 09:50:01.788435: step 64000, loss = 0.1090, acc = 0.9580, f1neg = 0.9537, f1pos = 0.9615, f1 = 0.9576
[Eval_batch(1)(2000,4000)] 2017-05-03 09:50:02.266329: step 64000, loss = 0.1108, acc = 0.9660, f1neg = 0.9602, f1pos = 0.9703, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 09:50:02.747447: step 64000, loss = 0.1236, acc = 0.9580, f1neg = 0.9553, f1pos = 0.9604, f1 = 0.9578
[Eval_batch(3)(2000,8000)] 2017-05-03 09:50:03.250918: step 64000, loss = 0.1278, acc = 0.9575, f1neg = 0.9573, f1pos = 0.9577, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 09:50:03.709134: step 64000, loss = 0.1304, acc = 0.9575, f1neg = 0.9585, f1pos = 0.9564, f1 = 0.9575
[Eval_batch(5)(2000,12000)] 2017-05-03 09:50:04.218386: step 64000, loss = 0.1280, acc = 0.9555, f1neg = 0.9521, f1pos = 0.9585, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 09:50:04.718256: step 64000, loss = 0.1190, acc = 0.9645, f1neg = 0.9638, f1pos = 0.9652, f1 = 0.9645
[Eval_batch(7)(2000,16000)] 2017-05-03 09:50:05.211591: step 64000, loss = 0.1175, acc = 0.9610, f1neg = 0.9532, f1pos = 0.9666, f1 = 0.9599
[Eval_batch(8)(2000,18000)] 2017-05-03 09:50:05.712482: step 64000, loss = 0.1099, acc = 0.9620, f1neg = 0.9565, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 09:50:06.221324: step 64000, loss = 0.1159, acc = 0.9660, f1neg = 0.9634, f1pos = 0.9683, f1 = 0.9658
[Eval_batch(10)(2000,22000)] 2017-05-03 09:50:06.681640: step 64000, loss = 0.1237, acc = 0.9580, f1neg = 0.9549, f1pos = 0.9607, f1 = 0.9578
[Eval_batch(11)(2000,24000)] 2017-05-03 09:50:07.190006: step 64000, loss = 0.1234, acc = 0.9605, f1neg = 0.9543, f1pos = 0.9652, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-03 09:50:07.654406: step 64000, loss = 0.1212, acc = 0.9540, f1neg = 0.9524, f1pos = 0.9555, f1 = 0.9539
[Eval_batch(13)(2000,28000)] 2017-05-03 09:50:08.156223: step 64000, loss = 0.1106, acc = 0.9660, f1neg = 0.9573, f1pos = 0.9717, f1 = 0.9645
[Eval_batch(14)(2000,30000)] 2017-05-03 09:50:08.667583: step 64000, loss = 0.1377, acc = 0.9555, f1neg = 0.9469, f1pos = 0.9617, f1 = 0.9543
[Eval_batch(15)(2000,32000)] 2017-05-03 09:50:09.179543: step 64000, loss = 0.1126, acc = 0.9700, f1neg = 0.9675, f1pos = 0.9722, f1 = 0.9698
[Eval_batch(16)(2000,34000)] 2017-05-03 09:50:09.683712: step 64000, loss = 0.1085, acc = 0.9680, f1neg = 0.9666, f1pos = 0.9693, f1 = 0.9679
[Eval_batch(17)(2000,36000)] 2017-05-03 09:50:10.157772: step 64000, loss = 0.1370, acc = 0.9560, f1neg = 0.9561, f1pos = 0.9559, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 09:50:10.631239: step 64000, loss = 0.1315, acc = 0.9550, f1neg = 0.9566, f1pos = 0.9532, f1 = 0.9549
[Eval_batch(19)(2000,40000)] 2017-05-03 09:50:11.114755: step 64000, loss = 0.1133, acc = 0.9685, f1neg = 0.9635, f1pos = 0.9723, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 09:50:11.588243: step 64000, loss = 0.1174, acc = 0.9615, f1neg = 0.9658, f1pos = 0.9560, f1 = 0.9609
[Eval_batch(21)(2000,44000)] 2017-05-03 09:50:12.055216: step 64000, loss = 0.1063, acc = 0.9650, f1neg = 0.9618, f1pos = 0.9677, f1 = 0.9648
[Eval_batch(22)(2000,46000)] 2017-05-03 09:50:12.529293: step 64000, loss = 0.1160, acc = 0.9585, f1neg = 0.9588, f1pos = 0.9582, f1 = 0.9585
[Eval_batch(23)(2000,48000)] 2017-05-03 09:50:13.010356: step 64000, loss = 0.1209, acc = 0.9595, f1neg = 0.9540, f1pos = 0.9638, f1 = 0.9589
[Eval_batch(24)(2000,50000)] 2017-05-03 09:50:13.473139: step 64000, loss = 0.1066, acc = 0.9665, f1neg = 0.9614, f1pos = 0.9704, f1 = 0.9659
[Eval_batch(25)(2000,52000)] 2017-05-03 09:50:13.939290: step 64000, loss = 0.0978, acc = 0.9730, f1neg = 0.9703, f1pos = 0.9752, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 09:50:14.410036: step 64000, loss = 0.1138, acc = 0.9595, f1neg = 0.9616, f1pos = 0.9572, f1 = 0.9594
[Eval_batch(27)(2000,56000)] 2017-05-03 09:50:14.978205: step 64000, loss = 0.0994, acc = 0.9675, f1neg = 0.9657, f1pos = 0.9691, f1 = 0.9674
[Eval] 2017-05-03 09:50:14.978265: step 64000, acc = 0.9617, f1 = 0.9614
[Test_batch(0)(2000,2000)] 2017-05-03 09:50:15.456402: step 64000, loss = 0.1487, acc = 0.9530, f1neg = 0.9551, f1pos = 0.9507, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 09:50:15.920301: step 64000, loss = 0.1282, acc = 0.9635, f1neg = 0.9675, f1pos = 0.9584, f1 = 0.9629
[Test_batch(2)(2000,6000)] 2017-05-03 09:50:16.427007: step 64000, loss = 0.1456, acc = 0.9540, f1neg = 0.9575, f1pos = 0.9498, f1 = 0.9537
[Test_batch(3)(2000,8000)] 2017-05-03 09:50:16.905164: step 64000, loss = 0.1504, acc = 0.9510, f1neg = 0.9539, f1pos = 0.9478, f1 = 0.9508
[Test_batch(4)(2000,10000)] 2017-05-03 09:50:17.390712: step 64000, loss = 0.1469, acc = 0.9490, f1neg = 0.9490, f1pos = 0.9490, f1 = 0.9490
[Test_batch(5)(2000,12000)] 2017-05-03 09:50:17.858037: step 64000, loss = 0.1608, acc = 0.9415, f1neg = 0.9418, f1pos = 0.9412, f1 = 0.9415
[Test_batch(6)(2000,14000)] 2017-05-03 09:50:18.366815: step 64000, loss = 0.1465, acc = 0.9470, f1neg = 0.9449, f1pos = 0.9489, f1 = 0.9469
[Test_batch(7)(2000,16000)] 2017-05-03 09:50:18.839075: step 64000, loss = 0.1367, acc = 0.9525, f1neg = 0.9567, f1pos = 0.9474, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 09:50:19.314379: step 64000, loss = 0.1374, acc = 0.9495, f1neg = 0.9488, f1pos = 0.9502, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 09:50:19.812815: step 64000, loss = 0.1645, acc = 0.9375, f1neg = 0.9339, f1pos = 0.9407, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 09:50:20.318574: step 64000, loss = 0.1418, acc = 0.9535, f1neg = 0.9475, f1pos = 0.9582, f1 = 0.9529
[Test_batch(11)(2000,24000)] 2017-05-03 09:50:20.820997: step 64000, loss = 0.1465, acc = 0.9510, f1neg = 0.9522, f1pos = 0.9497, f1 = 0.9510
[Test_batch(12)(2000,26000)] 2017-05-03 09:50:21.329265: step 64000, loss = 0.1183, acc = 0.9640, f1neg = 0.9653, f1pos = 0.9626, f1 = 0.9640
[Test_batch(13)(2000,28000)] 2017-05-03 09:50:21.840678: step 64000, loss = 0.1433, acc = 0.9495, f1neg = 0.9435, f1pos = 0.9543, f1 = 0.9489
[Test_batch(14)(2000,30000)] 2017-05-03 09:50:22.305676: step 64000, loss = 0.1189, acc = 0.9605, f1neg = 0.9544, f1pos = 0.9652, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 09:50:22.787358: step 64000, loss = 0.1444, acc = 0.9530, f1neg = 0.9522, f1pos = 0.9538, f1 = 0.9530
[Test_batch(16)(2000,34000)] 2017-05-03 09:50:23.265741: step 64000, loss = 0.1250, acc = 0.9585, f1neg = 0.9572, f1pos = 0.9597, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 09:50:23.768649: step 64000, loss = 0.1166, acc = 0.9675, f1neg = 0.9643, f1pos = 0.9702, f1 = 0.9672
[Test_batch(18)(2000,38000)] 2017-05-03 09:50:24.298674: step 64000, loss = 0.1127, acc = 0.9620, f1neg = 0.9611, f1pos = 0.9629, f1 = 0.9620
[Test] 2017-05-03 09:50:24.298763: step 64000, acc = 0.9536, f1 = 0.9534
[Status] 2017-05-03 09:50:24.298789: step 64000, maxindex = 62000, maxdev = 0.9625, maxtst = 0.9538
2017-05-03 09:50:33.376219: step 64010, loss = 0.1439, acc = 0.9420 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 09:50:42.382744: step 64020, loss = 0.1327, acc = 0.9560 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 09:50:51.360091: step 64030, loss = 0.1171, acc = 0.9600 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 09:51:00.466002: step 64040, loss = 0.1351, acc = 0.9540 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 09:51:09.355983: step 64050, loss = 0.1066, acc = 0.9660 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 09:51:18.332881: step 64060, loss = 0.1326, acc = 0.9640 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 09:51:27.378896: step 64070, loss = 0.1334, acc = 0.9460 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 09:51:36.421155: step 64080, loss = 0.0919, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 09:51:45.489952: step 64090, loss = 0.1041, acc = 0.9720 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 09:51:54.729702: step 64100, loss = 0.1090, acc = 0.9660 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 09:52:03.863641: step 64110, loss = 0.1241, acc = 0.9660 (254.9 examples/sec; 0.251 sec/batch)
2017-05-03 09:52:12.871226: step 64120, loss = 0.1322, acc = 0.9600 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 09:52:21.876615: step 64130, loss = 0.1043, acc = 0.9800 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 09:52:30.948127: step 64140, loss = 0.1290, acc = 0.9600 (247.7 examples/sec; 0.258 sec/batch)
2017-05-03 09:52:39.968311: step 64150, loss = 0.1218, acc = 0.9560 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:52:48.955136: step 64160, loss = 0.1139, acc = 0.9640 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 09:52:58.123029: step 64170, loss = 0.1013, acc = 0.9760 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 09:53:07.263548: step 64180, loss = 0.1180, acc = 0.9680 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 09:53:16.150171: step 64190, loss = 0.1512, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 09:53:25.150036: step 64200, loss = 0.1233, acc = 0.9640 (261.1 examples/sec; 0.245 sec/batch)
2017-05-03 09:53:34.122792: step 64210, loss = 0.1328, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 09:53:43.070108: step 64220, loss = 0.1214, acc = 0.9620 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 09:53:52.100316: step 64230, loss = 0.1260, acc = 0.9620 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 09:54:01.292694: step 64240, loss = 0.1228, acc = 0.9600 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 09:54:10.460872: step 64250, loss = 0.1199, acc = 0.9620 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 09:54:19.560222: step 64260, loss = 0.0891, acc = 0.9780 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 09:54:28.676208: step 64270, loss = 0.1092, acc = 0.9620 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 09:54:38.084044: step 64280, loss = 0.0950, acc = 0.9820 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 09:54:47.102772: step 64290, loss = 0.1295, acc = 0.9580 (268.7 examples/sec; 0.238 sec/batch)
2017-05-03 09:54:56.313527: step 64300, loss = 0.1328, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 09:55:05.222409: step 64310, loss = 0.1268, acc = 0.9640 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 09:55:14.359949: step 64320, loss = 0.1294, acc = 0.9560 (272.9 examples/sec; 0.235 sec/batch)
2017-05-03 09:55:23.437065: step 64330, loss = 0.1240, acc = 0.9620 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 09:55:32.470390: step 64340, loss = 0.1032, acc = 0.9700 (297.8 examples/sec; 0.215 sec/batch)
2017-05-03 09:55:41.533831: step 64350, loss = 0.1089, acc = 0.9660 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 09:55:50.545326: step 64360, loss = 0.1190, acc = 0.9660 (267.4 examples/sec; 0.239 sec/batch)
2017-05-03 09:56:00.007503: step 64370, loss = 0.1224, acc = 0.9620 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 09:56:09.026337: step 64380, loss = 0.1083, acc = 0.9680 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 09:56:17.990241: step 64390, loss = 0.1285, acc = 0.9480 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 09:56:26.949030: step 64400, loss = 0.1256, acc = 0.9680 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 09:56:35.827584: step 64410, loss = 0.1385, acc = 0.9540 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 09:56:44.803342: step 64420, loss = 0.1119, acc = 0.9520 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 09:56:53.811540: step 64430, loss = 0.1286, acc = 0.9620 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 09:57:02.747291: step 64440, loss = 0.1028, acc = 0.9700 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 09:57:11.888237: step 64450, loss = 0.1289, acc = 0.9680 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 09:57:20.911542: step 64460, loss = 0.1339, acc = 0.9540 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 09:57:29.828808: step 64470, loss = 0.1200, acc = 0.9720 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 09:57:38.785231: step 64480, loss = 0.1058, acc = 0.9740 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 09:57:47.772375: step 64490, loss = 0.1225, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 09:57:56.792148: step 64500, loss = 0.1250, acc = 0.9600 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 09:58:05.815046: step 64510, loss = 0.1183, acc = 0.9640 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 09:58:15.928313: step 64520, loss = 0.1265, acc = 0.9520 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 09:58:24.872545: step 64530, loss = 0.0934, acc = 0.9660 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 09:58:33.894063: step 64540, loss = 0.1259, acc = 0.9640 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 09:58:42.858189: step 64550, loss = 0.1459, acc = 0.9440 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 09:58:51.888549: step 64560, loss = 0.1424, acc = 0.9560 (297.3 examples/sec; 0.215 sec/batch)
2017-05-03 09:59:01.107641: step 64570, loss = 0.1141, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 09:59:10.052168: step 64580, loss = 0.1569, acc = 0.9600 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 09:59:18.990970: step 64590, loss = 0.1439, acc = 0.9500 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 09:59:27.992078: step 64600, loss = 0.1235, acc = 0.9600 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 09:59:36.888018: step 64610, loss = 0.1222, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 09:59:45.951670: step 64620, loss = 0.1310, acc = 0.9580 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 09:59:55.102082: step 64630, loss = 0.1371, acc = 0.9460 (257.2 examples/sec; 0.249 sec/batch)
2017-05-03 10:00:04.133587: step 64640, loss = 0.1116, acc = 0.9640 (266.9 examples/sec; 0.240 sec/batch)
2017-05-03 10:00:13.129109: step 64650, loss = 0.1428, acc = 0.9480 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 10:00:22.097384: step 64660, loss = 0.1241, acc = 0.9640 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 10:00:31.154428: step 64670, loss = 0.1153, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 10:00:40.165392: step 64680, loss = 0.1001, acc = 0.9720 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 10:00:49.017674: step 64690, loss = 0.1254, acc = 0.9560 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 10:00:58.026606: step 64700, loss = 0.1245, acc = 0.9620 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 10:01:06.929509: step 64710, loss = 0.1012, acc = 0.9680 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 10:01:15.995084: step 64720, loss = 0.1145, acc = 0.9680 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 10:01:25.018830: step 64730, loss = 0.0987, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 10:01:34.221273: step 64740, loss = 0.1100, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 10:01:43.243449: step 64750, loss = 0.1278, acc = 0.9580 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 10:01:52.392625: step 64760, loss = 0.1222, acc = 0.9660 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:02:01.573254: step 64770, loss = 0.1201, acc = 0.9680 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 10:02:10.789658: step 64780, loss = 0.0772, acc = 0.9840 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 10:02:19.795318: step 64790, loss = 0.1254, acc = 0.9680 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 10:02:28.888009: step 64800, loss = 0.1275, acc = 0.9580 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 10:02:37.858271: step 64810, loss = 0.1112, acc = 0.9620 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 10:02:47.089283: step 64820, loss = 0.1258, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:02:56.743877: step 64830, loss = 0.1240, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 10:03:06.016544: step 64840, loss = 0.1353, acc = 0.9660 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 10:03:15.200521: step 64850, loss = 0.0857, acc = 0.9800 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:03:24.244268: step 64860, loss = 0.1113, acc = 0.9620 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 10:03:33.291422: step 64870, loss = 0.1426, acc = 0.9540 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 10:03:42.668361: step 64880, loss = 0.1118, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 10:03:51.614279: step 64890, loss = 0.1336, acc = 0.9540 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 10:04:00.552291: step 64900, loss = 0.1090, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 10:04:09.610269: step 64910, loss = 0.1054, acc = 0.9660 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 10:04:18.605282: step 64920, loss = 0.1485, acc = 0.9520 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 10:04:27.615613: step 64930, loss = 0.1189, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 10:04:36.599676: step 64940, loss = 0.0924, acc = 0.9820 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 10:04:45.564897: step 64950, loss = 0.1257, acc = 0.9500 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 10:04:54.677506: step 64960, loss = 0.1086, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 10:05:03.725647: step 64970, loss = 0.1289, acc = 0.9580 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 10:05:12.783763: step 64980, loss = 0.1307, acc = 0.9580 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 10:05:21.885894: step 64990, loss = 0.1070, acc = 0.9640 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 10:05:30.887476: step 65000, loss = 0.1327, acc = 0.9600 (295.3 examples/sec; 0.217 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 10:05:31.390972: step 65000, loss = 0.1091, acc = 0.9595, f1neg = 0.9555, f1pos = 0.9629, f1 = 0.9592
[Eval_batch(1)(2000,4000)] 2017-05-03 10:05:31.849306: step 65000, loss = 0.1111, acc = 0.9645, f1neg = 0.9585, f1pos = 0.9690, f1 = 0.9637
[Eval_batch(2)(2000,6000)] 2017-05-03 10:05:32.285003: step 65000, loss = 0.1229, acc = 0.9580, f1neg = 0.9552, f1pos = 0.9605, f1 = 0.9578
[Eval_batch(3)(2000,8000)] 2017-05-03 10:05:32.762046: step 65000, loss = 0.1278, acc = 0.9575, f1neg = 0.9574, f1pos = 0.9576, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 10:05:33.260626: step 65000, loss = 0.1302, acc = 0.9565, f1neg = 0.9576, f1pos = 0.9554, f1 = 0.9565
[Eval_batch(5)(2000,12000)] 2017-05-03 10:05:33.735906: step 65000, loss = 0.1256, acc = 0.9550, f1neg = 0.9516, f1pos = 0.9580, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 10:05:34.211153: step 65000, loss = 0.1184, acc = 0.9655, f1neg = 0.9648, f1pos = 0.9661, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 10:05:34.718568: step 65000, loss = 0.1173, acc = 0.9615, f1neg = 0.9538, f1pos = 0.9670, f1 = 0.9604
[Eval_batch(8)(2000,18000)] 2017-05-03 10:05:35.212610: step 65000, loss = 0.1098, acc = 0.9615, f1neg = 0.9560, f1pos = 0.9658, f1 = 0.9609
[Eval_batch(9)(2000,20000)] 2017-05-03 10:05:35.687322: step 65000, loss = 0.1150, acc = 0.9645, f1neg = 0.9618, f1pos = 0.9668, f1 = 0.9643
[Eval_batch(10)(2000,22000)] 2017-05-03 10:05:36.194333: step 65000, loss = 0.1223, acc = 0.9600, f1neg = 0.9571, f1pos = 0.9625, f1 = 0.9598
[Eval_batch(11)(2000,24000)] 2017-05-03 10:05:36.667470: step 65000, loss = 0.1227, acc = 0.9605, f1neg = 0.9544, f1pos = 0.9652, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-03 10:05:37.177205: step 65000, loss = 0.1202, acc = 0.9530, f1neg = 0.9514, f1pos = 0.9545, f1 = 0.9530
[Eval_batch(13)(2000,28000)] 2017-05-03 10:05:37.653989: step 65000, loss = 0.1105, acc = 0.9650, f1neg = 0.9561, f1pos = 0.9709, f1 = 0.9635
[Eval_batch(14)(2000,30000)] 2017-05-03 10:05:38.165333: step 65000, loss = 0.1378, acc = 0.9530, f1neg = 0.9439, f1pos = 0.9596, f1 = 0.9517
[Eval_batch(15)(2000,32000)] 2017-05-03 10:05:38.673161: step 65000, loss = 0.1114, acc = 0.9695, f1neg = 0.9670, f1pos = 0.9717, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 10:05:39.120695: step 65000, loss = 0.1072, acc = 0.9675, f1neg = 0.9661, f1pos = 0.9688, f1 = 0.9674
[Eval_batch(17)(2000,36000)] 2017-05-03 10:05:39.599038: step 65000, loss = 0.1350, acc = 0.9560, f1neg = 0.9562, f1pos = 0.9558, f1 = 0.9560
[Eval_batch(18)(2000,38000)] 2017-05-03 10:05:40.078972: step 65000, loss = 0.1304, acc = 0.9575, f1neg = 0.9591, f1pos = 0.9558, f1 = 0.9574
[Eval_batch(19)(2000,40000)] 2017-05-03 10:05:40.579755: step 65000, loss = 0.1118, acc = 0.9685, f1neg = 0.9635, f1pos = 0.9723, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 10:05:41.079497: step 65000, loss = 0.1160, acc = 0.9615, f1neg = 0.9658, f1pos = 0.9560, f1 = 0.9609
[Eval_batch(21)(2000,44000)] 2017-05-03 10:05:41.578270: step 65000, loss = 0.1052, acc = 0.9650, f1neg = 0.9618, f1pos = 0.9677, f1 = 0.9648
[Eval_batch(22)(2000,46000)] 2017-05-03 10:05:42.043911: step 65000, loss = 0.1154, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 10:05:42.527327: step 65000, loss = 0.1199, acc = 0.9595, f1neg = 0.9541, f1pos = 0.9638, f1 = 0.9589
[Eval_batch(24)(2000,50000)] 2017-05-03 10:05:42.980118: step 65000, loss = 0.1050, acc = 0.9670, f1neg = 0.9620, f1pos = 0.9708, f1 = 0.9664
[Eval_batch(25)(2000,52000)] 2017-05-03 10:05:43.458647: step 65000, loss = 0.0972, acc = 0.9730, f1neg = 0.9704, f1pos = 0.9752, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 10:05:43.976575: step 65000, loss = 0.1125, acc = 0.9595, f1neg = 0.9616, f1pos = 0.9571, f1 = 0.9594
[Eval_batch(27)(2000,56000)] 2017-05-03 10:05:44.552500: step 65000, loss = 0.0989, acc = 0.9685, f1neg = 0.9669, f1pos = 0.9700, f1 = 0.9684
[Eval] 2017-05-03 10:05:44.552592: step 65000, acc = 0.9617, f1 = 0.9614
[Test_batch(0)(2000,2000)] 2017-05-03 10:05:45.034939: step 65000, loss = 0.1479, acc = 0.9515, f1neg = 0.9537, f1pos = 0.9490, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 10:05:45.534273: step 65000, loss = 0.1278, acc = 0.9625, f1neg = 0.9666, f1pos = 0.9572, f1 = 0.9619
[Test_batch(2)(2000,6000)] 2017-05-03 10:05:46.002154: step 65000, loss = 0.1457, acc = 0.9565, f1neg = 0.9599, f1pos = 0.9525, f1 = 0.9562
[Test_batch(3)(2000,8000)] 2017-05-03 10:05:46.486125: step 65000, loss = 0.1509, acc = 0.9500, f1neg = 0.9529, f1pos = 0.9468, f1 = 0.9498
[Test_batch(4)(2000,10000)] 2017-05-03 10:05:46.991283: step 65000, loss = 0.1457, acc = 0.9495, f1neg = 0.9495, f1pos = 0.9495, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 10:05:47.469688: step 65000, loss = 0.1596, acc = 0.9415, f1neg = 0.9419, f1pos = 0.9411, f1 = 0.9415
[Test_batch(6)(2000,14000)] 2017-05-03 10:05:47.917254: step 65000, loss = 0.1460, acc = 0.9480, f1neg = 0.9460, f1pos = 0.9499, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 10:05:48.399907: step 65000, loss = 0.1376, acc = 0.9535, f1neg = 0.9576, f1pos = 0.9485, f1 = 0.9531
[Test_batch(8)(2000,18000)] 2017-05-03 10:05:48.911829: step 65000, loss = 0.1356, acc = 0.9495, f1neg = 0.9488, f1pos = 0.9502, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 10:05:49.435066: step 65000, loss = 0.1640, acc = 0.9365, f1neg = 0.9328, f1pos = 0.9398, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-03 10:05:49.911494: step 65000, loss = 0.1413, acc = 0.9525, f1neg = 0.9464, f1pos = 0.9574, f1 = 0.9519
[Test_batch(11)(2000,24000)] 2017-05-03 10:05:50.392429: step 65000, loss = 0.1452, acc = 0.9500, f1neg = 0.9512, f1pos = 0.9488, f1 = 0.9500
[Test_batch(12)(2000,26000)] 2017-05-03 10:05:50.857060: step 65000, loss = 0.1183, acc = 0.9635, f1neg = 0.9649, f1pos = 0.9620, f1 = 0.9634
[Test_batch(13)(2000,28000)] 2017-05-03 10:05:51.354114: step 65000, loss = 0.1432, acc = 0.9540, f1neg = 0.9488, f1pos = 0.9582, f1 = 0.9535
[Test_batch(14)(2000,30000)] 2017-05-03 10:05:51.827165: step 65000, loss = 0.1176, acc = 0.9610, f1neg = 0.9550, f1pos = 0.9656, f1 = 0.9603
[Test_batch(15)(2000,32000)] 2017-05-03 10:05:52.284464: step 65000, loss = 0.1443, acc = 0.9540, f1neg = 0.9533, f1pos = 0.9546, f1 = 0.9540
[Test_batch(16)(2000,34000)] 2017-05-03 10:05:52.755300: step 65000, loss = 0.1246, acc = 0.9585, f1neg = 0.9573, f1pos = 0.9596, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 10:05:53.231880: step 65000, loss = 0.1171, acc = 0.9680, f1neg = 0.9648, f1pos = 0.9706, f1 = 0.9677
[Test_batch(18)(2000,38000)] 2017-05-03 10:05:53.765007: step 65000, loss = 0.1119, acc = 0.9640, f1neg = 0.9633, f1pos = 0.9647, f1 = 0.9640
[Test] 2017-05-03 10:05:53.765080: step 65000, acc = 0.9539, f1 = 0.9537
[Status] 2017-05-03 10:05:53.765097: step 65000, maxindex = 62000, maxdev = 0.9625, maxtst = 0.9538
2017-05-03 10:06:02.664907: step 65010, loss = 0.1227, acc = 0.9600 (298.3 examples/sec; 0.215 sec/batch)
2017-05-03 10:06:11.791112: step 65020, loss = 0.1399, acc = 0.9600 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 10:06:20.882491: step 65030, loss = 0.1076, acc = 0.9800 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 10:06:30.219666: step 65040, loss = 0.1170, acc = 0.9640 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 10:06:39.266206: step 65050, loss = 0.1303, acc = 0.9540 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 10:06:48.382778: step 65060, loss = 0.1242, acc = 0.9600 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 10:06:57.305692: step 65070, loss = 0.1539, acc = 0.9460 (265.9 examples/sec; 0.241 sec/batch)
2017-05-03 10:07:06.384354: step 65080, loss = 0.1321, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:07:15.562696: step 65090, loss = 0.1199, acc = 0.9580 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 10:07:24.801618: step 65100, loss = 0.1105, acc = 0.9700 (261.9 examples/sec; 0.244 sec/batch)
2017-05-03 10:07:33.871606: step 65110, loss = 0.0987, acc = 0.9780 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 10:07:42.976292: step 65120, loss = 0.1299, acc = 0.9560 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 10:07:51.950638: step 65130, loss = 0.1378, acc = 0.9620 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 10:08:00.874465: step 65140, loss = 0.1289, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 10:08:09.856649: step 65150, loss = 0.1113, acc = 0.9660 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 10:08:18.802512: step 65160, loss = 0.1264, acc = 0.9580 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 10:08:27.770939: step 65170, loss = 0.1114, acc = 0.9680 (295.6 examples/sec; 0.217 sec/batch)
2017-05-03 10:08:36.754109: step 65180, loss = 0.0958, acc = 0.9740 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 10:08:45.691157: step 65190, loss = 0.1160, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 10:08:54.824955: step 65200, loss = 0.1313, acc = 0.9540 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 10:09:04.012564: step 65210, loss = 0.1279, acc = 0.9540 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 10:09:13.038205: step 65220, loss = 0.1190, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 10:09:21.992370: step 65230, loss = 0.1239, acc = 0.9600 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 10:09:31.029889: step 65240, loss = 0.1182, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:09:40.077435: step 65250, loss = 0.0956, acc = 0.9840 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 10:09:49.260431: step 65260, loss = 0.1022, acc = 0.9700 (267.6 examples/sec; 0.239 sec/batch)
2017-05-03 10:09:58.278337: step 65270, loss = 0.1367, acc = 0.9580 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 10:10:07.307257: step 65280, loss = 0.1082, acc = 0.9740 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 10:10:16.456666: step 65290, loss = 0.0908, acc = 0.9820 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 10:10:25.422191: step 65300, loss = 0.1297, acc = 0.9540 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:10:34.590115: step 65310, loss = 0.1319, acc = 0.9540 (254.7 examples/sec; 0.251 sec/batch)
2017-05-03 10:10:43.616383: step 65320, loss = 0.1209, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 10:10:52.552547: step 65330, loss = 0.1557, acc = 0.9420 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 10:11:01.619112: step 65340, loss = 0.1259, acc = 0.9640 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 10:11:10.744664: step 65350, loss = 0.1367, acc = 0.9540 (269.6 examples/sec; 0.237 sec/batch)
2017-05-03 10:11:19.739887: step 65360, loss = 0.0998, acc = 0.9740 (262.5 examples/sec; 0.244 sec/batch)
2017-05-03 10:11:28.843460: step 65370, loss = 0.1113, acc = 0.9680 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 10:11:37.864604: step 65380, loss = 0.1048, acc = 0.9700 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 10:11:46.777181: step 65390, loss = 0.1003, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:11:55.736742: step 65400, loss = 0.1189, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 10:12:04.680767: step 65410, loss = 0.1136, acc = 0.9620 (304.5 examples/sec; 0.210 sec/batch)
2017-05-03 10:12:13.729748: step 65420, loss = 0.0893, acc = 0.9800 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 10:12:22.737864: step 65430, loss = 0.1039, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 10:12:31.804310: step 65440, loss = 0.1219, acc = 0.9700 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 10:12:40.838172: step 65450, loss = 0.0957, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 10:12:49.804551: step 65460, loss = 0.1208, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 10:12:58.691555: step 65470, loss = 0.1098, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 10:13:07.735048: step 65480, loss = 0.1185, acc = 0.9640 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 10:13:16.820182: step 65490, loss = 0.0870, acc = 0.9800 (259.1 examples/sec; 0.247 sec/batch)
2017-05-03 10:13:25.916081: step 65500, loss = 0.1241, acc = 0.9600 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 10:13:35.117139: step 65510, loss = 0.1411, acc = 0.9560 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 10:13:45.135483: step 65520, loss = 0.1225, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 10:13:54.110994: step 65530, loss = 0.1257, acc = 0.9560 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 10:14:03.004004: step 65540, loss = 0.1301, acc = 0.9500 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 10:14:12.072314: step 65550, loss = 0.1109, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:14:20.945930: step 65560, loss = 0.1320, acc = 0.9580 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 10:14:30.231579: step 65570, loss = 0.1104, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 10:14:39.409126: step 65580, loss = 0.1319, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 10:14:48.346684: step 65590, loss = 0.1328, acc = 0.9560 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 10:14:57.277644: step 65600, loss = 0.1291, acc = 0.9560 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 10:15:06.248123: step 65610, loss = 0.1139, acc = 0.9620 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 10:15:15.206328: step 65620, loss = 0.1266, acc = 0.9600 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 10:15:24.121435: step 65630, loss = 0.1292, acc = 0.9580 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 10:15:33.136607: step 65640, loss = 0.1116, acc = 0.9760 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 10:15:42.152599: step 65650, loss = 0.1371, acc = 0.9560 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 10:15:51.144261: step 65660, loss = 0.1167, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 10:16:00.105251: step 65670, loss = 0.1238, acc = 0.9680 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 10:16:08.967188: step 65680, loss = 0.1635, acc = 0.9400 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 10:16:17.886366: step 65690, loss = 0.1227, acc = 0.9600 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 10:16:27.060116: step 65700, loss = 0.1055, acc = 0.9780 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 10:16:36.164608: step 65710, loss = 0.1005, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 10:16:45.201760: step 65720, loss = 0.1339, acc = 0.9560 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 10:16:54.324004: step 65730, loss = 0.1064, acc = 0.9700 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 10:17:03.321518: step 65740, loss = 0.1212, acc = 0.9580 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 10:17:12.204688: step 65750, loss = 0.1515, acc = 0.9500 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 10:17:21.171648: step 65760, loss = 0.1220, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 10:17:30.313472: step 65770, loss = 0.1428, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 10:17:40.040755: step 65780, loss = 0.1313, acc = 0.9540 (264.2 examples/sec; 0.242 sec/batch)
2017-05-03 10:17:49.105946: step 65790, loss = 0.1180, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 10:17:58.043454: step 65800, loss = 0.1208, acc = 0.9680 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 10:18:07.115231: step 65810, loss = 0.1519, acc = 0.9540 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 10:18:16.174948: step 65820, loss = 0.1254, acc = 0.9620 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 10:18:25.244215: step 65830, loss = 0.1423, acc = 0.9480 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 10:18:34.349142: step 65840, loss = 0.1367, acc = 0.9520 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 10:18:43.636733: step 65850, loss = 0.1168, acc = 0.9600 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 10:18:52.760311: step 65860, loss = 0.1181, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 10:19:01.900327: step 65870, loss = 0.1340, acc = 0.9580 (269.5 examples/sec; 0.238 sec/batch)
2017-05-03 10:19:11.033449: step 65880, loss = 0.0995, acc = 0.9760 (268.5 examples/sec; 0.238 sec/batch)
2017-05-03 10:19:19.981277: step 65890, loss = 0.1083, acc = 0.9640 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 10:19:29.061816: step 65900, loss = 0.0977, acc = 0.9680 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 10:19:38.154508: step 65910, loss = 0.1141, acc = 0.9740 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:19:47.241296: step 65920, loss = 0.1156, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 10:19:56.399048: step 65930, loss = 0.1409, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 10:20:05.503653: step 65940, loss = 0.1131, acc = 0.9700 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 10:20:14.389906: step 65950, loss = 0.1124, acc = 0.9640 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 10:20:23.387673: step 65960, loss = 0.1380, acc = 0.9540 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 10:20:32.552210: step 65970, loss = 0.1150, acc = 0.9720 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 10:20:41.570177: step 65980, loss = 0.1046, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 10:20:50.572680: step 65990, loss = 0.1196, acc = 0.9660 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 10:20:59.831915: step 66000, loss = 0.1186, acc = 0.9600 (280.5 examples/sec; 0.228 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 10:21:00.334066: step 66000, loss = 0.1067, acc = 0.9615, f1neg = 0.9579, f1pos = 0.9646, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 10:21:00.797652: step 66000, loss = 0.1118, acc = 0.9665, f1neg = 0.9611, f1pos = 0.9706, f1 = 0.9658
[Eval_batch(2)(2000,6000)] 2017-05-03 10:21:01.267414: step 66000, loss = 0.1219, acc = 0.9590, f1neg = 0.9566, f1pos = 0.9612, f1 = 0.9589
[Eval_batch(3)(2000,8000)] 2017-05-03 10:21:01.739218: step 66000, loss = 0.1253, acc = 0.9585, f1neg = 0.9586, f1pos = 0.9584, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 10:21:02.192661: step 66000, loss = 0.1278, acc = 0.9570, f1neg = 0.9583, f1pos = 0.9556, f1 = 0.9570
[Eval_batch(5)(2000,12000)] 2017-05-03 10:21:02.659323: step 66000, loss = 0.1252, acc = 0.9555, f1neg = 0.9524, f1pos = 0.9582, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 10:21:03.162174: step 66000, loss = 0.1153, acc = 0.9665, f1neg = 0.9659, f1pos = 0.9670, f1 = 0.9665
[Eval_batch(7)(2000,16000)] 2017-05-03 10:21:03.665605: step 66000, loss = 0.1162, acc = 0.9595, f1neg = 0.9518, f1pos = 0.9651, f1 = 0.9584
[Eval_batch(8)(2000,18000)] 2017-05-03 10:21:04.171896: step 66000, loss = 0.1089, acc = 0.9630, f1neg = 0.9580, f1pos = 0.9670, f1 = 0.9625
[Eval_batch(9)(2000,20000)] 2017-05-03 10:21:04.638156: step 66000, loss = 0.1150, acc = 0.9640, f1neg = 0.9615, f1pos = 0.9662, f1 = 0.9639
[Eval_batch(10)(2000,22000)] 2017-05-03 10:21:05.145906: step 66000, loss = 0.1219, acc = 0.9590, f1neg = 0.9565, f1pos = 0.9612, f1 = 0.9589
[Eval_batch(11)(2000,24000)] 2017-05-03 10:21:05.652290: step 66000, loss = 0.1226, acc = 0.9625, f1neg = 0.9569, f1pos = 0.9668, f1 = 0.9619
[Eval_batch(12)(2000,26000)] 2017-05-03 10:21:06.158895: step 66000, loss = 0.1183, acc = 0.9550, f1neg = 0.9538, f1pos = 0.9561, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 10:21:06.639265: step 66000, loss = 0.1096, acc = 0.9665, f1neg = 0.9582, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 10:21:07.136083: step 66000, loss = 0.1363, acc = 0.9560, f1neg = 0.9479, f1pos = 0.9619, f1 = 0.9549
[Eval_batch(15)(2000,32000)] 2017-05-03 10:21:07.609277: step 66000, loss = 0.1114, acc = 0.9690, f1neg = 0.9666, f1pos = 0.9711, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 10:21:08.116619: step 66000, loss = 0.1072, acc = 0.9680, f1neg = 0.9668, f1pos = 0.9691, f1 = 0.9680
[Eval_batch(17)(2000,36000)] 2017-05-03 10:21:08.617060: step 66000, loss = 0.1349, acc = 0.9590, f1neg = 0.9594, f1pos = 0.9586, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 10:21:09.124367: step 66000, loss = 0.1276, acc = 0.9590, f1neg = 0.9607, f1pos = 0.9572, f1 = 0.9589
[Eval_batch(19)(2000,40000)] 2017-05-03 10:21:09.630620: step 66000, loss = 0.1106, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 10:21:10.107893: step 66000, loss = 0.1131, acc = 0.9630, f1neg = 0.9673, f1pos = 0.9575, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 10:21:10.584595: step 66000, loss = 0.1066, acc = 0.9640, f1neg = 0.9609, f1pos = 0.9666, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 10:21:11.058268: step 66000, loss = 0.1146, acc = 0.9605, f1neg = 0.9610, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 10:21:11.523505: step 66000, loss = 0.1203, acc = 0.9615, f1neg = 0.9567, f1pos = 0.9654, f1 = 0.9610
[Eval_batch(24)(2000,50000)] 2017-05-03 10:21:12.003127: step 66000, loss = 0.1053, acc = 0.9655, f1neg = 0.9605, f1pos = 0.9693, f1 = 0.9649
[Eval_batch(25)(2000,52000)] 2017-05-03 10:21:12.505287: step 66000, loss = 0.0959, acc = 0.9730, f1neg = 0.9705, f1pos = 0.9751, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 10:21:12.972528: step 66000, loss = 0.1105, acc = 0.9625, f1neg = 0.9647, f1pos = 0.9600, f1 = 0.9624
[Eval_batch(27)(2000,56000)] 2017-05-03 10:21:13.568711: step 66000, loss = 0.0965, acc = 0.9710, f1neg = 0.9696, f1pos = 0.9723, f1 = 0.9709
[Eval] 2017-05-03 10:21:13.568780: step 66000, acc = 0.9626, f1 = 0.9623
[Test_batch(0)(2000,2000)] 2017-05-03 10:21:14.075796: step 66000, loss = 0.1457, acc = 0.9525, f1neg = 0.9551, f1pos = 0.9495, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 10:21:14.535021: step 66000, loss = 0.1245, acc = 0.9595, f1neg = 0.9641, f1pos = 0.9535, f1 = 0.9588
[Test_batch(2)(2000,6000)] 2017-05-03 10:21:15.042521: step 66000, loss = 0.1418, acc = 0.9590, f1neg = 0.9624, f1pos = 0.9549, f1 = 0.9587
[Test_batch(3)(2000,8000)] 2017-05-03 10:21:15.518412: step 66000, loss = 0.1484, acc = 0.9500, f1neg = 0.9534, f1pos = 0.9461, f1 = 0.9497
[Test_batch(4)(2000,10000)] 2017-05-03 10:21:15.996384: step 66000, loss = 0.1452, acc = 0.9500, f1neg = 0.9504, f1pos = 0.9495, f1 = 0.9500
[Test_batch(5)(2000,12000)] 2017-05-03 10:21:16.480064: step 66000, loss = 0.1589, acc = 0.9430, f1neg = 0.9438, f1pos = 0.9421, f1 = 0.9430
[Test_batch(6)(2000,14000)] 2017-05-03 10:21:16.981145: step 66000, loss = 0.1428, acc = 0.9480, f1neg = 0.9463, f1pos = 0.9496, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 10:21:17.477999: step 66000, loss = 0.1336, acc = 0.9525, f1neg = 0.9570, f1pos = 0.9469, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 10:21:17.983701: step 66000, loss = 0.1350, acc = 0.9505, f1neg = 0.9500, f1pos = 0.9510, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 10:21:18.455092: step 66000, loss = 0.1654, acc = 0.9375, f1neg = 0.9345, f1pos = 0.9403, f1 = 0.9374
[Test_batch(10)(2000,22000)] 2017-05-03 10:21:18.960494: step 66000, loss = 0.1430, acc = 0.9550, f1neg = 0.9494, f1pos = 0.9595, f1 = 0.9544
[Test_batch(11)(2000,24000)] 2017-05-03 10:21:19.432629: step 66000, loss = 0.1439, acc = 0.9520, f1neg = 0.9536, f1pos = 0.9503, f1 = 0.9519
[Test_batch(12)(2000,26000)] 2017-05-03 10:21:19.910269: step 66000, loss = 0.1193, acc = 0.9650, f1neg = 0.9664, f1pos = 0.9634, f1 = 0.9649
[Test_batch(13)(2000,28000)] 2017-05-03 10:21:20.392713: step 66000, loss = 0.1425, acc = 0.9530, f1neg = 0.9482, f1pos = 0.9570, f1 = 0.9526
[Test_batch(14)(2000,30000)] 2017-05-03 10:21:20.904272: step 66000, loss = 0.1192, acc = 0.9600, f1neg = 0.9542, f1pos = 0.9645, f1 = 0.9594
[Test_batch(15)(2000,32000)] 2017-05-03 10:21:21.387482: step 66000, loss = 0.1431, acc = 0.9560, f1neg = 0.9556, f1pos = 0.9564, f1 = 0.9560
[Test_batch(16)(2000,34000)] 2017-05-03 10:21:21.867698: step 66000, loss = 0.1246, acc = 0.9570, f1neg = 0.9562, f1pos = 0.9578, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 10:21:22.340277: step 66000, loss = 0.1153, acc = 0.9665, f1neg = 0.9634, f1pos = 0.9691, f1 = 0.9663
[Test_batch(18)(2000,38000)] 2017-05-03 10:21:22.902076: step 66000, loss = 0.1088, acc = 0.9655, f1neg = 0.9649, f1pos = 0.9661, f1 = 0.9655
[Test] 2017-05-03 10:21:22.902135: step 66000, acc = 0.9543, f1 = 0.9541
[Status] 2017-05-03 10:21:22.902151: step 66000, maxindex = 66000, maxdev = 0.9626, maxtst = 0.9543
2017-05-03 10:21:35.117860: step 66010, loss = 0.1423, acc = 0.9460 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 10:21:44.246146: step 66020, loss = 0.1345, acc = 0.9540 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 10:21:53.394705: step 66030, loss = 0.1222, acc = 0.9640 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 10:22:02.427991: step 66040, loss = 0.1135, acc = 0.9620 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 10:22:11.583903: step 66050, loss = 0.1402, acc = 0.9560 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 10:22:20.907135: step 66060, loss = 0.1066, acc = 0.9680 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 10:22:29.960576: step 66070, loss = 0.1015, acc = 0.9740 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:22:38.935864: step 66080, loss = 0.1231, acc = 0.9700 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 10:22:48.047499: step 66090, loss = 0.1018, acc = 0.9660 (261.6 examples/sec; 0.245 sec/batch)
2017-05-03 10:22:57.193753: step 66100, loss = 0.0960, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 10:23:06.148495: step 66110, loss = 0.1106, acc = 0.9700 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 10:23:15.145697: step 66120, loss = 0.1022, acc = 0.9720 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 10:23:24.389018: step 66130, loss = 0.1437, acc = 0.9580 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 10:23:33.656832: step 66140, loss = 0.1142, acc = 0.9600 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 10:23:42.833644: step 66150, loss = 0.1247, acc = 0.9560 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:23:51.940519: step 66160, loss = 0.1238, acc = 0.9680 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 10:24:00.987267: step 66170, loss = 0.1338, acc = 0.9600 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 10:24:09.913199: step 66180, loss = 0.1243, acc = 0.9600 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 10:24:18.866021: step 66190, loss = 0.1314, acc = 0.9520 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 10:24:27.990137: step 66200, loss = 0.1410, acc = 0.9520 (244.1 examples/sec; 0.262 sec/batch)
2017-05-03 10:24:37.707053: step 66210, loss = 0.1785, acc = 0.9460 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 10:24:46.716259: step 66220, loss = 0.0951, acc = 0.9800 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 10:24:55.764015: step 66230, loss = 0.1573, acc = 0.9400 (302.7 examples/sec; 0.211 sec/batch)
2017-05-03 10:25:04.804382: step 66240, loss = 0.1088, acc = 0.9680 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 10:25:13.864273: step 66250, loss = 0.1274, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 10:25:23.131002: step 66260, loss = 0.1219, acc = 0.9560 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 10:25:32.239667: step 66270, loss = 0.1202, acc = 0.9700 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 10:25:41.679298: step 66280, loss = 0.1456, acc = 0.9460 (251.9 examples/sec; 0.254 sec/batch)
2017-05-03 10:25:50.799911: step 66290, loss = 0.1222, acc = 0.9680 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 10:25:59.677937: step 66300, loss = 0.1619, acc = 0.9500 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:26:08.665821: step 66310, loss = 0.1276, acc = 0.9560 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 10:26:17.784072: step 66320, loss = 0.0997, acc = 0.9760 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 10:26:26.837469: step 66330, loss = 0.1492, acc = 0.9560 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 10:26:36.080868: step 66340, loss = 0.1440, acc = 0.9440 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 10:26:45.256812: step 66350, loss = 0.1098, acc = 0.9660 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 10:26:54.337272: step 66360, loss = 0.1113, acc = 0.9660 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 10:27:03.401790: step 66370, loss = 0.1351, acc = 0.9560 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 10:27:12.493479: step 66380, loss = 0.1377, acc = 0.9540 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 10:27:21.511043: step 66390, loss = 0.1203, acc = 0.9600 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 10:27:30.765828: step 66400, loss = 0.1115, acc = 0.9660 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 10:27:39.864735: step 66410, loss = 0.1388, acc = 0.9540 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 10:27:48.938645: step 66420, loss = 0.1062, acc = 0.9660 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 10:27:58.070009: step 66430, loss = 0.1177, acc = 0.9720 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 10:28:07.162108: step 66440, loss = 0.1146, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 10:28:16.217269: step 66450, loss = 0.1135, acc = 0.9680 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 10:28:25.265926: step 66460, loss = 0.1040, acc = 0.9680 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 10:28:34.358268: step 66470, loss = 0.1170, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 10:28:43.466474: step 66480, loss = 0.1000, acc = 0.9700 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 10:28:52.542652: step 66490, loss = 0.1099, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 10:29:01.668994: step 66500, loss = 0.1371, acc = 0.9480 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 10:29:10.865543: step 66510, loss = 0.1203, acc = 0.9600 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 10:29:19.939546: step 66520, loss = 0.1240, acc = 0.9640 (262.1 examples/sec; 0.244 sec/batch)
2017-05-03 10:29:29.775305: step 66530, loss = 0.1148, acc = 0.9700 (301.6 examples/sec; 0.212 sec/batch)
2017-05-03 10:29:38.718909: step 66540, loss = 0.1106, acc = 0.9680 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 10:29:47.803893: step 66550, loss = 0.1389, acc = 0.9640 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 10:29:56.911957: step 66560, loss = 0.1306, acc = 0.9480 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 10:30:06.150257: step 66570, loss = 0.1445, acc = 0.9460 (267.2 examples/sec; 0.240 sec/batch)
2017-05-03 10:30:15.060007: step 66580, loss = 0.1247, acc = 0.9480 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 10:30:24.030465: step 66590, loss = 0.1107, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:30:33.531681: step 66600, loss = 0.1485, acc = 0.9520 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 10:30:42.561691: step 66610, loss = 0.1063, acc = 0.9600 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 10:30:51.607025: step 66620, loss = 0.1276, acc = 0.9540 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 10:31:00.694154: step 66630, loss = 0.1386, acc = 0.9480 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 10:31:09.680718: step 66640, loss = 0.1052, acc = 0.9660 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 10:31:18.709749: step 66650, loss = 0.1252, acc = 0.9480 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 10:31:27.759811: step 66660, loss = 0.1114, acc = 0.9720 (267.5 examples/sec; 0.239 sec/batch)
2017-05-03 10:31:37.016425: step 66670, loss = 0.1116, acc = 0.9660 (258.7 examples/sec; 0.247 sec/batch)
2017-05-03 10:31:46.128498: step 66680, loss = 0.1332, acc = 0.9640 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 10:31:55.150996: step 66690, loss = 0.1226, acc = 0.9680 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 10:32:04.193953: step 66700, loss = 0.1397, acc = 0.9440 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 10:32:13.146362: step 66710, loss = 0.1276, acc = 0.9560 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 10:32:22.278531: step 66720, loss = 0.1245, acc = 0.9660 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:32:31.329759: step 66730, loss = 0.1145, acc = 0.9740 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 10:32:40.354964: step 66740, loss = 0.1100, acc = 0.9600 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 10:32:49.298796: step 66750, loss = 0.1239, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 10:32:58.249596: step 66760, loss = 0.1496, acc = 0.9520 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 10:33:07.261814: step 66770, loss = 0.0996, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 10:33:16.178898: step 66780, loss = 0.1431, acc = 0.9460 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 10:33:25.520058: step 66790, loss = 0.1135, acc = 0.9780 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 10:33:34.785067: step 66800, loss = 0.1347, acc = 0.9500 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 10:33:43.962567: step 66810, loss = 0.1046, acc = 0.9760 (266.6 examples/sec; 0.240 sec/batch)
2017-05-03 10:33:52.948927: step 66820, loss = 0.1380, acc = 0.9560 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 10:34:02.164404: step 66830, loss = 0.1635, acc = 0.9420 (264.0 examples/sec; 0.242 sec/batch)
2017-05-03 10:34:11.290932: step 66840, loss = 0.1334, acc = 0.9560 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 10:34:20.342537: step 66850, loss = 0.1057, acc = 0.9720 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 10:34:29.573339: step 66860, loss = 0.1158, acc = 0.9720 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 10:34:38.621748: step 66870, loss = 0.1291, acc = 0.9520 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:34:47.579097: step 66880, loss = 0.1109, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 10:34:56.563532: step 66890, loss = 0.1266, acc = 0.9680 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 10:35:05.454563: step 66900, loss = 0.1042, acc = 0.9680 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 10:35:14.463188: step 66910, loss = 0.1226, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 10:35:23.411179: step 66920, loss = 0.1234, acc = 0.9580 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 10:35:32.377649: step 66930, loss = 0.1390, acc = 0.9520 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 10:35:41.794407: step 66940, loss = 0.1518, acc = 0.9460 (199.7 examples/sec; 0.321 sec/batch)
2017-05-03 10:35:50.891159: step 66950, loss = 0.1145, acc = 0.9580 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 10:35:59.950069: step 66960, loss = 0.1069, acc = 0.9680 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 10:36:08.991381: step 66970, loss = 0.1271, acc = 0.9600 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 10:36:18.109973: step 66980, loss = 0.1025, acc = 0.9760 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 10:36:27.359304: step 66990, loss = 0.1094, acc = 0.9600 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 10:36:36.469857: step 67000, loss = 0.1391, acc = 0.9480 (255.4 examples/sec; 0.251 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 10:36:36.937740: step 67000, loss = 0.1081, acc = 0.9590, f1neg = 0.9550, f1pos = 0.9623, f1 = 0.9587
[Eval_batch(1)(2000,4000)] 2017-05-03 10:36:37.429890: step 67000, loss = 0.1113, acc = 0.9640, f1neg = 0.9580, f1pos = 0.9685, f1 = 0.9633
[Eval_batch(2)(2000,6000)] 2017-05-03 10:36:37.925201: step 67000, loss = 0.1215, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9599
[Eval_batch(3)(2000,8000)] 2017-05-03 10:36:38.419962: step 67000, loss = 0.1249, acc = 0.9580, f1neg = 0.9579, f1pos = 0.9581, f1 = 0.9580
[Eval_batch(4)(2000,10000)] 2017-05-03 10:36:38.915462: step 67000, loss = 0.1281, acc = 0.9585, f1neg = 0.9596, f1pos = 0.9573, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 10:36:39.395038: step 67000, loss = 0.1250, acc = 0.9550, f1neg = 0.9518, f1pos = 0.9578, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 10:36:39.861232: step 67000, loss = 0.1163, acc = 0.9650, f1neg = 0.9643, f1pos = 0.9657, f1 = 0.9650
[Eval_batch(7)(2000,16000)] 2017-05-03 10:36:40.318956: step 67000, loss = 0.1157, acc = 0.9610, f1neg = 0.9533, f1pos = 0.9665, f1 = 0.9599
[Eval_batch(8)(2000,18000)] 2017-05-03 10:36:40.784588: step 67000, loss = 0.1089, acc = 0.9625, f1neg = 0.9573, f1pos = 0.9666, f1 = 0.9619
[Eval_batch(9)(2000,20000)] 2017-05-03 10:36:41.226580: step 67000, loss = 0.1138, acc = 0.9660, f1neg = 0.9636, f1pos = 0.9681, f1 = 0.9658
[Eval_batch(10)(2000,22000)] 2017-05-03 10:36:41.694242: step 67000, loss = 0.1204, acc = 0.9615, f1neg = 0.9588, f1pos = 0.9639, f1 = 0.9613
[Eval_batch(11)(2000,24000)] 2017-05-03 10:36:42.166428: step 67000, loss = 0.1223, acc = 0.9605, f1neg = 0.9545, f1pos = 0.9651, f1 = 0.9598
[Eval_batch(12)(2000,26000)] 2017-05-03 10:36:42.645175: step 67000, loss = 0.1195, acc = 0.9535, f1neg = 0.9522, f1pos = 0.9547, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 10:36:43.098580: step 67000, loss = 0.1094, acc = 0.9650, f1neg = 0.9561, f1pos = 0.9709, f1 = 0.9635
[Eval_batch(14)(2000,30000)] 2017-05-03 10:36:43.576717: step 67000, loss = 0.1364, acc = 0.9525, f1neg = 0.9436, f1pos = 0.9590, f1 = 0.9513
[Eval_batch(15)(2000,32000)] 2017-05-03 10:36:44.054866: step 67000, loss = 0.1109, acc = 0.9695, f1neg = 0.9670, f1pos = 0.9716, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 10:36:44.527772: step 67000, loss = 0.1062, acc = 0.9665, f1neg = 0.9651, f1pos = 0.9678, f1 = 0.9664
[Eval_batch(17)(2000,36000)] 2017-05-03 10:36:45.004535: step 67000, loss = 0.1350, acc = 0.9570, f1neg = 0.9572, f1pos = 0.9568, f1 = 0.9570
[Eval_batch(18)(2000,38000)] 2017-05-03 10:36:45.508662: step 67000, loss = 0.1294, acc = 0.9580, f1neg = 0.9597, f1pos = 0.9562, f1 = 0.9579
[Eval_batch(19)(2000,40000)] 2017-05-03 10:36:45.988027: step 67000, loss = 0.1112, acc = 0.9675, f1neg = 0.9624, f1pos = 0.9714, f1 = 0.9669
[Eval_batch(20)(2000,42000)] 2017-05-03 10:36:46.463894: step 67000, loss = 0.1132, acc = 0.9610, f1neg = 0.9654, f1pos = 0.9554, f1 = 0.9604
[Eval_batch(21)(2000,44000)] 2017-05-03 10:36:46.937967: step 67000, loss = 0.1056, acc = 0.9655, f1neg = 0.9625, f1pos = 0.9681, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 10:36:47.386980: step 67000, loss = 0.1146, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 10:36:47.885683: step 67000, loss = 0.1192, acc = 0.9590, f1neg = 0.9536, f1pos = 0.9633, f1 = 0.9584
[Eval_batch(24)(2000,50000)] 2017-05-03 10:36:48.389405: step 67000, loss = 0.1050, acc = 0.9660, f1neg = 0.9610, f1pos = 0.9699, f1 = 0.9654
[Eval_batch(25)(2000,52000)] 2017-05-03 10:36:48.851447: step 67000, loss = 0.0973, acc = 0.9720, f1neg = 0.9693, f1pos = 0.9743, f1 = 0.9718
[Eval_batch(26)(2000,54000)] 2017-05-03 10:36:49.320013: step 67000, loss = 0.1115, acc = 0.9620, f1neg = 0.9640, f1pos = 0.9597, f1 = 0.9619
[Eval_batch(27)(2000,56000)] 2017-05-03 10:36:49.901907: step 67000, loss = 0.0962, acc = 0.9710, f1neg = 0.9696, f1pos = 0.9723, f1 = 0.9709
[Eval] 2017-05-03 10:36:49.902015: step 67000, acc = 0.9620, f1 = 0.9617
[Test_batch(0)(2000,2000)] 2017-05-03 10:36:50.381401: step 67000, loss = 0.1468, acc = 0.9515, f1neg = 0.9538, f1pos = 0.9489, f1 = 0.9514
[Test_batch(1)(2000,4000)] 2017-05-03 10:36:50.856227: step 67000, loss = 0.1254, acc = 0.9615, f1neg = 0.9658, f1pos = 0.9559, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 10:36:51.316986: step 67000, loss = 0.1433, acc = 0.9565, f1neg = 0.9599, f1pos = 0.9524, f1 = 0.9562
[Test_batch(3)(2000,8000)] 2017-05-03 10:36:51.777065: step 67000, loss = 0.1488, acc = 0.9515, f1neg = 0.9545, f1pos = 0.9481, f1 = 0.9513
[Test_batch(4)(2000,10000)] 2017-05-03 10:36:52.249227: step 67000, loss = 0.1462, acc = 0.9505, f1neg = 0.9507, f1pos = 0.9503, f1 = 0.9505
[Test_batch(5)(2000,12000)] 2017-05-03 10:36:52.702215: step 67000, loss = 0.1593, acc = 0.9415, f1neg = 0.9421, f1pos = 0.9409, f1 = 0.9415
[Test_batch(6)(2000,14000)] 2017-05-03 10:36:53.178632: step 67000, loss = 0.1444, acc = 0.9480, f1neg = 0.9461, f1pos = 0.9498, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 10:36:53.644791: step 67000, loss = 0.1352, acc = 0.9535, f1neg = 0.9577, f1pos = 0.9483, f1 = 0.9530
[Test_batch(8)(2000,18000)] 2017-05-03 10:36:54.111529: step 67000, loss = 0.1358, acc = 0.9495, f1neg = 0.9489, f1pos = 0.9501, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 10:36:54.581189: step 67000, loss = 0.1637, acc = 0.9380, f1neg = 0.9346, f1pos = 0.9411, f1 = 0.9378
[Test_batch(10)(2000,22000)] 2017-05-03 10:36:55.041951: step 67000, loss = 0.1420, acc = 0.9530, f1neg = 0.9471, f1pos = 0.9577, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 10:36:55.550447: step 67000, loss = 0.1441, acc = 0.9500, f1neg = 0.9514, f1pos = 0.9486, f1 = 0.9500
[Test_batch(12)(2000,26000)] 2017-05-03 10:36:56.015367: step 67000, loss = 0.1180, acc = 0.9620, f1neg = 0.9635, f1pos = 0.9603, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 10:36:56.494435: step 67000, loss = 0.1429, acc = 0.9540, f1neg = 0.9491, f1pos = 0.9580, f1 = 0.9536
[Test_batch(14)(2000,30000)] 2017-05-03 10:36:57.000476: step 67000, loss = 0.1183, acc = 0.9600, f1neg = 0.9539, f1pos = 0.9647, f1 = 0.9593
[Test_batch(15)(2000,32000)] 2017-05-03 10:36:57.481812: step 67000, loss = 0.1424, acc = 0.9570, f1neg = 0.9566, f1pos = 0.9574, f1 = 0.9570
[Test_batch(16)(2000,34000)] 2017-05-03 10:36:57.978687: step 67000, loss = 0.1246, acc = 0.9565, f1neg = 0.9554, f1pos = 0.9576, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 10:36:58.443417: step 67000, loss = 0.1159, acc = 0.9660, f1neg = 0.9627, f1pos = 0.9688, f1 = 0.9657
[Test_batch(18)(2000,38000)] 2017-05-03 10:36:58.964909: step 67000, loss = 0.1105, acc = 0.9650, f1neg = 0.9643, f1pos = 0.9657, f1 = 0.9650
[Test] 2017-05-03 10:36:58.964993: step 67000, acc = 0.9540, f1 = 0.9538
[Status] 2017-05-03 10:36:58.965022: step 67000, maxindex = 66000, maxdev = 0.9626, maxtst = 0.9543
2017-05-03 10:37:08.020260: step 67010, loss = 0.1146, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 10:37:17.010572: step 67020, loss = 0.1068, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:37:26.165433: step 67030, loss = 0.1119, acc = 0.9700 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 10:37:35.220299: step 67040, loss = 0.1431, acc = 0.9540 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 10:37:44.772069: step 67050, loss = 0.1208, acc = 0.9580 (257.1 examples/sec; 0.249 sec/batch)
2017-05-03 10:37:53.803700: step 67060, loss = 0.1597, acc = 0.9360 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:38:02.810815: step 67070, loss = 0.1419, acc = 0.9560 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:38:11.959377: step 67080, loss = 0.1130, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 10:38:21.099041: step 67090, loss = 0.1184, acc = 0.9600 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 10:38:30.639801: step 67100, loss = 0.1805, acc = 0.9280 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 10:38:39.725131: step 67110, loss = 0.1604, acc = 0.9400 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 10:38:48.829896: step 67120, loss = 0.1009, acc = 0.9700 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 10:38:58.277849: step 67130, loss = 0.1376, acc = 0.9500 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 10:39:07.358614: step 67140, loss = 0.1060, acc = 0.9660 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 10:39:16.578809: step 67150, loss = 0.0966, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 10:39:25.675726: step 67160, loss = 0.1148, acc = 0.9720 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 10:39:34.756153: step 67170, loss = 0.1488, acc = 0.9480 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 10:39:43.776090: step 67180, loss = 0.1164, acc = 0.9680 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 10:39:52.792278: step 67190, loss = 0.1199, acc = 0.9640 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 10:40:02.059123: step 67200, loss = 0.1446, acc = 0.9500 (265.7 examples/sec; 0.241 sec/batch)
2017-05-03 10:40:10.949709: step 67210, loss = 0.0946, acc = 0.9780 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 10:40:20.214177: step 67220, loss = 0.1137, acc = 0.9620 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 10:40:29.303529: step 67230, loss = 0.1292, acc = 0.9560 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 10:40:38.292251: step 67240, loss = 0.1321, acc = 0.9500 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 10:40:47.280331: step 67250, loss = 0.1353, acc = 0.9560 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:40:56.384163: step 67260, loss = 0.1183, acc = 0.9600 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 10:41:05.427405: step 67270, loss = 0.1216, acc = 0.9560 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 10:41:14.690954: step 67280, loss = 0.1194, acc = 0.9580 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 10:41:23.629627: step 67290, loss = 0.1336, acc = 0.9500 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 10:41:32.653630: step 67300, loss = 0.1203, acc = 0.9580 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 10:41:41.598118: step 67310, loss = 0.1217, acc = 0.9580 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 10:41:50.685230: step 67320, loss = 0.1117, acc = 0.9660 (261.1 examples/sec; 0.245 sec/batch)
2017-05-03 10:41:59.734341: step 67330, loss = 0.1314, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 10:42:08.723274: step 67340, loss = 0.1339, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 10:42:17.727158: step 67350, loss = 0.1003, acc = 0.9720 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 10:42:26.892736: step 67360, loss = 0.1262, acc = 0.9620 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 10:42:35.936906: step 67370, loss = 0.1279, acc = 0.9560 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 10:42:45.075610: step 67380, loss = 0.1194, acc = 0.9640 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 10:42:53.968807: step 67390, loss = 0.1338, acc = 0.9580 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 10:43:03.025718: step 67400, loss = 0.1189, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 10:43:11.902637: step 67410, loss = 0.1224, acc = 0.9600 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 10:43:20.905250: step 67420, loss = 0.1367, acc = 0.9540 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 10:43:30.179672: step 67430, loss = 0.1331, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 10:43:39.241130: step 67440, loss = 0.1121, acc = 0.9660 (298.2 examples/sec; 0.215 sec/batch)
2017-05-03 10:43:48.349759: step 67450, loss = 0.1119, acc = 0.9640 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 10:43:57.397359: step 67460, loss = 0.1178, acc = 0.9600 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 10:44:06.530165: step 67470, loss = 0.1299, acc = 0.9520 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:44:15.605818: step 67480, loss = 0.1308, acc = 0.9520 (269.6 examples/sec; 0.237 sec/batch)
2017-05-03 10:44:24.733996: step 67490, loss = 0.1246, acc = 0.9580 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 10:44:33.901733: step 67500, loss = 0.1657, acc = 0.9440 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 10:44:42.856263: step 67510, loss = 0.1364, acc = 0.9500 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 10:44:51.802065: step 67520, loss = 0.1355, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 10:45:00.722485: step 67530, loss = 0.1332, acc = 0.9500 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 10:45:10.573802: step 67540, loss = 0.1003, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 10:45:19.661178: step 67550, loss = 0.1555, acc = 0.9500 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 10:45:28.745006: step 67560, loss = 0.1190, acc = 0.9720 (257.5 examples/sec; 0.249 sec/batch)
2017-05-03 10:45:37.801147: step 67570, loss = 0.1316, acc = 0.9600 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 10:45:46.860321: step 67580, loss = 0.0884, acc = 0.9820 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 10:45:56.157376: step 67590, loss = 0.1086, acc = 0.9740 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 10:46:05.345752: step 67600, loss = 0.1204, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 10:46:14.442481: step 67610, loss = 0.1144, acc = 0.9660 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 10:46:23.424396: step 67620, loss = 0.1299, acc = 0.9660 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 10:46:32.409670: step 67630, loss = 0.1432, acc = 0.9440 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 10:46:41.690819: step 67640, loss = 0.1182, acc = 0.9560 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 10:46:50.864839: step 67650, loss = 0.1654, acc = 0.9440 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:47:00.214609: step 67660, loss = 0.1127, acc = 0.9620 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:47:09.456021: step 67670, loss = 0.1359, acc = 0.9660 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 10:47:18.506235: step 67680, loss = 0.1396, acc = 0.9500 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 10:47:27.514521: step 67690, loss = 0.1024, acc = 0.9660 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 10:47:36.669335: step 67700, loss = 0.1241, acc = 0.9600 (258.8 examples/sec; 0.247 sec/batch)
2017-05-03 10:47:45.560397: step 67710, loss = 0.1364, acc = 0.9620 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 10:47:54.545797: step 67720, loss = 0.0878, acc = 0.9820 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 10:48:03.699913: step 67730, loss = 0.1103, acc = 0.9700 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 10:48:12.888538: step 67740, loss = 0.1414, acc = 0.9480 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 10:48:21.876114: step 67750, loss = 0.1196, acc = 0.9580 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 10:48:30.734593: step 67760, loss = 0.1337, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 10:48:39.906039: step 67770, loss = 0.0976, acc = 0.9660 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 10:48:48.994110: step 67780, loss = 0.1012, acc = 0.9700 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 10:48:58.341686: step 67790, loss = 0.0939, acc = 0.9840 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 10:49:07.415982: step 67800, loss = 0.1255, acc = 0.9620 (255.7 examples/sec; 0.250 sec/batch)
2017-05-03 10:49:16.567093: step 67810, loss = 0.1129, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:49:25.449358: step 67820, loss = 0.0921, acc = 0.9800 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 10:49:34.605176: step 67830, loss = 0.1383, acc = 0.9500 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 10:49:43.663243: step 67840, loss = 0.1079, acc = 0.9680 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 10:49:52.793203: step 67850, loss = 0.1420, acc = 0.9540 (300.7 examples/sec; 0.213 sec/batch)
2017-05-03 10:50:01.869984: step 67860, loss = 0.1213, acc = 0.9620 (264.8 examples/sec; 0.242 sec/batch)
2017-05-03 10:50:10.958888: step 67870, loss = 0.0976, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 10:50:19.983251: step 67880, loss = 0.1204, acc = 0.9620 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 10:50:29.074866: step 67890, loss = 0.1201, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 10:50:38.187666: step 67900, loss = 0.1236, acc = 0.9560 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 10:50:47.205661: step 67910, loss = 0.1231, acc = 0.9540 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 10:50:56.155181: step 67920, loss = 0.1207, acc = 0.9640 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 10:51:05.057933: step 67930, loss = 0.0976, acc = 0.9800 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 10:51:14.130693: step 67940, loss = 0.0944, acc = 0.9780 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 10:51:23.259038: step 67950, loss = 0.1401, acc = 0.9520 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 10:51:32.526384: step 67960, loss = 0.1313, acc = 0.9560 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 10:51:41.493955: step 67970, loss = 0.1288, acc = 0.9620 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:51:50.435750: step 67980, loss = 0.1205, acc = 0.9560 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 10:51:59.508113: step 67990, loss = 0.1159, acc = 0.9640 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 10:52:08.552231: step 68000, loss = 0.1523, acc = 0.9560 (276.8 examples/sec; 0.231 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 10:52:09.020884: step 68000, loss = 0.1055, acc = 0.9610, f1neg = 0.9575, f1pos = 0.9640, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 10:52:09.499120: step 68000, loss = 0.1126, acc = 0.9645, f1neg = 0.9590, f1pos = 0.9687, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 10:52:09.959324: step 68000, loss = 0.1209, acc = 0.9590, f1neg = 0.9568, f1pos = 0.9610, f1 = 0.9589
[Eval_batch(3)(2000,8000)] 2017-05-03 10:52:10.428458: step 68000, loss = 0.1236, acc = 0.9595, f1neg = 0.9598, f1pos = 0.9592, f1 = 0.9595
[Eval_batch(4)(2000,10000)] 2017-05-03 10:52:10.886599: step 68000, loss = 0.1256, acc = 0.9565, f1neg = 0.9581, f1pos = 0.9548, f1 = 0.9564
[Eval_batch(5)(2000,12000)] 2017-05-03 10:52:11.393632: step 68000, loss = 0.1234, acc = 0.9555, f1neg = 0.9527, f1pos = 0.9580, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 10:52:11.906368: step 68000, loss = 0.1147, acc = 0.9685, f1neg = 0.9681, f1pos = 0.9689, f1 = 0.9685
[Eval_batch(7)(2000,16000)] 2017-05-03 10:52:12.369337: step 68000, loss = 0.1166, acc = 0.9600, f1neg = 0.9526, f1pos = 0.9654, f1 = 0.9590
[Eval_batch(8)(2000,18000)] 2017-05-03 10:52:12.812561: step 68000, loss = 0.1097, acc = 0.9645, f1neg = 0.9599, f1pos = 0.9681, f1 = 0.9640
[Eval_batch(9)(2000,20000)] 2017-05-03 10:52:13.284513: step 68000, loss = 0.1144, acc = 0.9645, f1neg = 0.9622, f1pos = 0.9666, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 10:52:13.753467: step 68000, loss = 0.1211, acc = 0.9620, f1neg = 0.9598, f1pos = 0.9640, f1 = 0.9619
[Eval_batch(11)(2000,24000)] 2017-05-03 10:52:14.252724: step 68000, loss = 0.1234, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Eval_batch(12)(2000,26000)] 2017-05-03 10:52:14.764419: step 68000, loss = 0.1185, acc = 0.9555, f1neg = 0.9546, f1pos = 0.9564, f1 = 0.9555
[Eval_batch(13)(2000,28000)] 2017-05-03 10:52:15.230674: step 68000, loss = 0.1098, acc = 0.9665, f1neg = 0.9583, f1pos = 0.9720, f1 = 0.9652
[Eval_batch(14)(2000,30000)] 2017-05-03 10:52:15.696311: step 68000, loss = 0.1365, acc = 0.9545, f1neg = 0.9463, f1pos = 0.9605, f1 = 0.9534
[Eval_batch(15)(2000,32000)] 2017-05-03 10:52:16.168699: step 68000, loss = 0.1118, acc = 0.9680, f1neg = 0.9657, f1pos = 0.9700, f1 = 0.9679
[Eval_batch(16)(2000,34000)] 2017-05-03 10:52:16.643240: step 68000, loss = 0.1073, acc = 0.9660, f1neg = 0.9649, f1pos = 0.9671, f1 = 0.9660
[Eval_batch(17)(2000,36000)] 2017-05-03 10:52:17.106452: step 68000, loss = 0.1362, acc = 0.9585, f1neg = 0.9591, f1pos = 0.9579, f1 = 0.9585
[Eval_batch(18)(2000,38000)] 2017-05-03 10:52:17.612635: step 68000, loss = 0.1270, acc = 0.9615, f1neg = 0.9634, f1pos = 0.9595, f1 = 0.9614
[Eval_batch(19)(2000,40000)] 2017-05-03 10:52:18.116220: step 68000, loss = 0.1114, acc = 0.9685, f1neg = 0.9638, f1pos = 0.9721, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 10:52:18.595661: step 68000, loss = 0.1104, acc = 0.9635, f1neg = 0.9678, f1pos = 0.9579, f1 = 0.9628
[Eval_batch(21)(2000,44000)] 2017-05-03 10:52:19.052159: step 68000, loss = 0.1078, acc = 0.9650, f1neg = 0.9621, f1pos = 0.9675, f1 = 0.9648
[Eval_batch(22)(2000,46000)] 2017-05-03 10:52:19.526656: step 68000, loss = 0.1144, acc = 0.9610, f1neg = 0.9616, f1pos = 0.9604, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 10:52:20.019013: step 68000, loss = 0.1211, acc = 0.9610, f1neg = 0.9564, f1pos = 0.9647, f1 = 0.9606
[Eval_batch(24)(2000,50000)] 2017-05-03 10:52:20.512298: step 68000, loss = 0.1060, acc = 0.9665, f1neg = 0.9619, f1pos = 0.9701, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 10:52:20.972453: step 68000, loss = 0.0961, acc = 0.9725, f1neg = 0.9700, f1pos = 0.9746, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 10:52:21.475542: step 68000, loss = 0.1097, acc = 0.9620, f1neg = 0.9643, f1pos = 0.9594, f1 = 0.9618
[Eval_batch(27)(2000,56000)] 2017-05-03 10:52:21.992735: step 68000, loss = 0.0955, acc = 0.9725, f1neg = 0.9713, f1pos = 0.9736, f1 = 0.9725
[Eval] 2017-05-03 10:52:21.992823: step 68000, acc = 0.9628, f1 = 0.9625
[Test_batch(0)(2000,2000)] 2017-05-03 10:52:22.452539: step 68000, loss = 0.1460, acc = 0.9525, f1neg = 0.9551, f1pos = 0.9495, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 10:52:22.946884: step 68000, loss = 0.1234, acc = 0.9605, f1neg = 0.9651, f1pos = 0.9545, f1 = 0.9598
[Test_batch(2)(2000,6000)] 2017-05-03 10:52:23.415699: step 68000, loss = 0.1413, acc = 0.9575, f1neg = 0.9612, f1pos = 0.9530, f1 = 0.9571
[Test_batch(3)(2000,8000)] 2017-05-03 10:52:23.880441: step 68000, loss = 0.1486, acc = 0.9490, f1neg = 0.9526, f1pos = 0.9447, f1 = 0.9487
[Test_batch(4)(2000,10000)] 2017-05-03 10:52:24.385892: step 68000, loss = 0.1452, acc = 0.9515, f1neg = 0.9521, f1pos = 0.9509, f1 = 0.9515
[Test_batch(5)(2000,12000)] 2017-05-03 10:52:24.855459: step 68000, loss = 0.1581, acc = 0.9425, f1neg = 0.9437, f1pos = 0.9412, f1 = 0.9425
[Test_batch(6)(2000,14000)] 2017-05-03 10:52:25.322319: step 68000, loss = 0.1421, acc = 0.9480, f1neg = 0.9465, f1pos = 0.9494, f1 = 0.9480
[Test_batch(7)(2000,16000)] 2017-05-03 10:52:25.827416: step 68000, loss = 0.1353, acc = 0.9520, f1neg = 0.9568, f1pos = 0.9461, f1 = 0.9514
[Test_batch(8)(2000,18000)] 2017-05-03 10:52:26.303769: step 68000, loss = 0.1342, acc = 0.9495, f1neg = 0.9492, f1pos = 0.9498, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 10:52:26.808403: step 68000, loss = 0.1658, acc = 0.9360, f1neg = 0.9333, f1pos = 0.9385, f1 = 0.9359
[Test_batch(10)(2000,22000)] 2017-05-03 10:52:27.285311: step 68000, loss = 0.1445, acc = 0.9545, f1neg = 0.9492, f1pos = 0.9588, f1 = 0.9540
[Test_batch(11)(2000,24000)] 2017-05-03 10:52:27.747646: step 68000, loss = 0.1437, acc = 0.9520, f1neg = 0.9536, f1pos = 0.9503, f1 = 0.9519
[Test_batch(12)(2000,26000)] 2017-05-03 10:52:28.210790: step 68000, loss = 0.1200, acc = 0.9620, f1neg = 0.9637, f1pos = 0.9602, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 10:52:28.688774: step 68000, loss = 0.1440, acc = 0.9515, f1neg = 0.9469, f1pos = 0.9554, f1 = 0.9511
[Test_batch(14)(2000,30000)] 2017-05-03 10:52:29.189592: step 68000, loss = 0.1201, acc = 0.9600, f1neg = 0.9543, f1pos = 0.9644, f1 = 0.9594
[Test_batch(15)(2000,32000)] 2017-05-03 10:52:29.695852: step 68000, loss = 0.1430, acc = 0.9575, f1neg = 0.9572, f1pos = 0.9578, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 10:52:30.174792: step 68000, loss = 0.1248, acc = 0.9580, f1neg = 0.9573, f1pos = 0.9587, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 10:52:30.677978: step 68000, loss = 0.1156, acc = 0.9650, f1neg = 0.9620, f1pos = 0.9676, f1 = 0.9648
[Test_batch(18)(2000,38000)] 2017-05-03 10:52:31.210235: step 68000, loss = 0.1089, acc = 0.9660, f1neg = 0.9655, f1pos = 0.9665, f1 = 0.9660
[Test] 2017-05-03 10:52:31.210325: step 68000, acc = 0.9540, f1 = 0.9538
[Status] 2017-05-03 10:52:31.210354: step 68000, maxindex = 68000, maxdev = 0.9628, maxtst = 0.9540
2017-05-03 10:52:43.560478: step 68010, loss = 0.1008, acc = 0.9720 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 10:52:52.559046: step 68020, loss = 0.0964, acc = 0.9740 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 10:53:01.722782: step 68030, loss = 0.1087, acc = 0.9640 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 10:53:10.677899: step 68040, loss = 0.1036, acc = 0.9720 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:53:20.016462: step 68050, loss = 0.1012, acc = 0.9740 (220.5 examples/sec; 0.290 sec/batch)
2017-05-03 10:53:29.057901: step 68060, loss = 0.1198, acc = 0.9560 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 10:53:38.046160: step 68070, loss = 0.1247, acc = 0.9580 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 10:53:47.000220: step 68080, loss = 0.1150, acc = 0.9620 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 10:53:56.086933: step 68090, loss = 0.1459, acc = 0.9500 (266.2 examples/sec; 0.240 sec/batch)
2017-05-03 10:54:05.056427: step 68100, loss = 0.1250, acc = 0.9540 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 10:54:14.034808: step 68110, loss = 0.1217, acc = 0.9640 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 10:54:23.112511: step 68120, loss = 0.1567, acc = 0.9400 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 10:54:32.110899: step 68130, loss = 0.1271, acc = 0.9500 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 10:54:40.982087: step 68140, loss = 0.1076, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 10:54:50.068773: step 68150, loss = 0.1604, acc = 0.9440 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 10:54:59.106739: step 68160, loss = 0.1262, acc = 0.9600 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 10:55:08.142924: step 68170, loss = 0.1172, acc = 0.9560 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 10:55:17.195096: step 68180, loss = 0.1218, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 10:55:26.228564: step 68190, loss = 0.1192, acc = 0.9660 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 10:55:35.217488: step 68200, loss = 0.1316, acc = 0.9580 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 10:55:44.334722: step 68210, loss = 0.1178, acc = 0.9660 (266.9 examples/sec; 0.240 sec/batch)
2017-05-03 10:55:53.304533: step 68220, loss = 0.1246, acc = 0.9600 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:56:02.432866: step 68230, loss = 0.1171, acc = 0.9600 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 10:56:11.423059: step 68240, loss = 0.1236, acc = 0.9620 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 10:56:20.510355: step 68250, loss = 0.1364, acc = 0.9500 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 10:56:29.708153: step 68260, loss = 0.1131, acc = 0.9580 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 10:56:38.909809: step 68270, loss = 0.1120, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 10:56:47.971367: step 68280, loss = 0.1320, acc = 0.9640 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 10:56:57.149347: step 68290, loss = 0.1189, acc = 0.9640 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 10:57:06.065200: step 68300, loss = 0.1173, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 10:57:15.097548: step 68310, loss = 0.1317, acc = 0.9700 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 10:57:24.362362: step 68320, loss = 0.1259, acc = 0.9580 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 10:57:33.457299: step 68330, loss = 0.1171, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 10:57:42.430811: step 68340, loss = 0.1175, acc = 0.9680 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 10:57:51.477146: step 68350, loss = 0.1211, acc = 0.9540 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 10:58:00.554363: step 68360, loss = 0.0968, acc = 0.9760 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:58:09.451475: step 68370, loss = 0.1098, acc = 0.9720 (301.3 examples/sec; 0.212 sec/batch)
2017-05-03 10:58:18.766744: step 68380, loss = 0.1156, acc = 0.9680 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 10:58:27.781617: step 68390, loss = 0.1013, acc = 0.9760 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 10:58:36.790693: step 68400, loss = 0.1185, acc = 0.9620 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 10:58:45.780188: step 68410, loss = 0.1132, acc = 0.9660 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 10:58:54.740599: step 68420, loss = 0.1144, acc = 0.9620 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 10:59:03.716878: step 68430, loss = 0.1189, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 10:59:12.770774: step 68440, loss = 0.1001, acc = 0.9680 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 10:59:21.835753: step 68450, loss = 0.1054, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 10:59:30.931713: step 68460, loss = 0.1465, acc = 0.9620 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 10:59:40.113850: step 68470, loss = 0.1263, acc = 0.9620 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 10:59:49.202526: step 68480, loss = 0.1205, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 10:59:58.164150: step 68490, loss = 0.1279, acc = 0.9640 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 11:00:07.099854: step 68500, loss = 0.1148, acc = 0.9580 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 11:00:16.157869: step 68510, loss = 0.1157, acc = 0.9640 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 11:00:25.113495: step 68520, loss = 0.0999, acc = 0.9600 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 11:00:34.124004: step 68530, loss = 0.1361, acc = 0.9520 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 11:00:43.189232: step 68540, loss = 0.1106, acc = 0.9720 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 11:00:52.672769: step 68550, loss = 0.1374, acc = 0.9500 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 11:01:01.689554: step 68560, loss = 0.1194, acc = 0.9580 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 11:01:10.532576: step 68570, loss = 0.1369, acc = 0.9580 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 11:01:19.496194: step 68580, loss = 0.1148, acc = 0.9620 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 11:01:28.594217: step 68590, loss = 0.1390, acc = 0.9500 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 11:01:37.824383: step 68600, loss = 0.0955, acc = 0.9720 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 11:01:46.821529: step 68610, loss = 0.1558, acc = 0.9400 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 11:01:55.879039: step 68620, loss = 0.1421, acc = 0.9520 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 11:02:04.993389: step 68630, loss = 0.0925, acc = 0.9780 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 11:02:14.212700: step 68640, loss = 0.1421, acc = 0.9520 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 11:02:23.151604: step 68650, loss = 0.1534, acc = 0.9520 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 11:02:32.059369: step 68660, loss = 0.1488, acc = 0.9400 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 11:02:41.032122: step 68670, loss = 0.1534, acc = 0.9340 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 11:02:49.893740: step 68680, loss = 0.1003, acc = 0.9760 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 11:02:58.853634: step 68690, loss = 0.0982, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 11:03:07.816212: step 68700, loss = 0.1082, acc = 0.9640 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 11:03:16.837242: step 68710, loss = 0.1374, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:03:25.916537: step 68720, loss = 0.1186, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 11:03:34.897352: step 68730, loss = 0.1157, acc = 0.9600 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 11:03:43.892266: step 68740, loss = 0.1216, acc = 0.9580 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 11:03:52.909301: step 68750, loss = 0.1163, acc = 0.9620 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 11:04:01.975544: step 68760, loss = 0.1176, acc = 0.9660 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 11:04:10.968961: step 68770, loss = 0.0827, acc = 0.9860 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 11:04:19.935654: step 68780, loss = 0.1175, acc = 0.9600 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 11:04:28.900025: step 68790, loss = 0.1260, acc = 0.9760 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 11:04:37.931773: step 68800, loss = 0.1226, acc = 0.9620 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 11:04:46.805958: step 68810, loss = 0.1315, acc = 0.9600 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 11:04:55.816149: step 68820, loss = 0.1067, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 11:05:04.806901: step 68830, loss = 0.1009, acc = 0.9720 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 11:05:13.915379: step 68840, loss = 0.1339, acc = 0.9520 (250.5 examples/sec; 0.255 sec/batch)
2017-05-03 11:05:23.065911: step 68850, loss = 0.1321, acc = 0.9520 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 11:05:32.273210: step 68860, loss = 0.1140, acc = 0.9640 (297.4 examples/sec; 0.215 sec/batch)
2017-05-03 11:05:41.293091: step 68870, loss = 0.0914, acc = 0.9780 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 11:05:50.390746: step 68880, loss = 0.0915, acc = 0.9820 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 11:05:59.438824: step 68890, loss = 0.1305, acc = 0.9560 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 11:06:08.535718: step 68900, loss = 0.1140, acc = 0.9640 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 11:06:17.615650: step 68910, loss = 0.1214, acc = 0.9620 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 11:06:26.772561: step 68920, loss = 0.1421, acc = 0.9420 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 11:06:35.628644: step 68930, loss = 0.1285, acc = 0.9520 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 11:06:44.713346: step 68940, loss = 0.1364, acc = 0.9680 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:06:53.839928: step 68950, loss = 0.1267, acc = 0.9660 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 11:07:02.795961: step 68960, loss = 0.1470, acc = 0.9480 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 11:07:11.878448: step 68970, loss = 0.1034, acc = 0.9720 (273.4 examples/sec; 0.234 sec/batch)
2017-05-03 11:07:21.022018: step 68980, loss = 0.1229, acc = 0.9620 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 11:07:30.026990: step 68990, loss = 0.1184, acc = 0.9640 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 11:07:39.041405: step 69000, loss = 0.1127, acc = 0.9720 (266.8 examples/sec; 0.240 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 11:07:39.565954: step 69000, loss = 0.1063, acc = 0.9605, f1neg = 0.9568, f1pos = 0.9636, f1 = 0.9602
[Eval_batch(1)(2000,4000)] 2017-05-03 11:07:40.038742: step 69000, loss = 0.1111, acc = 0.9665, f1neg = 0.9610, f1pos = 0.9706, f1 = 0.9658
[Eval_batch(2)(2000,6000)] 2017-05-03 11:07:40.515725: step 69000, loss = 0.1208, acc = 0.9605, f1neg = 0.9582, f1pos = 0.9626, f1 = 0.9604
[Eval_batch(3)(2000,8000)] 2017-05-03 11:07:40.998616: step 69000, loss = 0.1253, acc = 0.9580, f1neg = 0.9581, f1pos = 0.9579, f1 = 0.9580
[Eval_batch(4)(2000,10000)] 2017-05-03 11:07:41.474066: step 69000, loss = 0.1269, acc = 0.9575, f1neg = 0.9588, f1pos = 0.9561, f1 = 0.9575
[Eval_batch(5)(2000,12000)] 2017-05-03 11:07:41.940887: step 69000, loss = 0.1255, acc = 0.9520, f1neg = 0.9487, f1pos = 0.9549, f1 = 0.9518
[Eval_batch(6)(2000,14000)] 2017-05-03 11:07:42.450504: step 69000, loss = 0.1153, acc = 0.9670, f1neg = 0.9664, f1pos = 0.9676, f1 = 0.9670
[Eval_batch(7)(2000,16000)] 2017-05-03 11:07:42.915754: step 69000, loss = 0.1158, acc = 0.9590, f1neg = 0.9512, f1pos = 0.9647, f1 = 0.9579
[Eval_batch(8)(2000,18000)] 2017-05-03 11:07:43.420172: step 69000, loss = 0.1080, acc = 0.9640, f1neg = 0.9590, f1pos = 0.9679, f1 = 0.9635
[Eval_batch(9)(2000,20000)] 2017-05-03 11:07:43.884586: step 69000, loss = 0.1146, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9667, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 11:07:44.390470: step 69000, loss = 0.1206, acc = 0.9605, f1neg = 0.9580, f1pos = 0.9628, f1 = 0.9604
[Eval_batch(11)(2000,24000)] 2017-05-03 11:07:44.860653: step 69000, loss = 0.1218, acc = 0.9610, f1neg = 0.9552, f1pos = 0.9655, f1 = 0.9603
[Eval_batch(12)(2000,26000)] 2017-05-03 11:07:45.368775: step 69000, loss = 0.1184, acc = 0.9525, f1neg = 0.9512, f1pos = 0.9538, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-03 11:07:45.841007: step 69000, loss = 0.1091, acc = 0.9655, f1neg = 0.9570, f1pos = 0.9712, f1 = 0.9641
[Eval_batch(14)(2000,30000)] 2017-05-03 11:07:46.341380: step 69000, loss = 0.1366, acc = 0.9550, f1neg = 0.9467, f1pos = 0.9611, f1 = 0.9539
[Eval_batch(15)(2000,32000)] 2017-05-03 11:07:46.846940: step 69000, loss = 0.1113, acc = 0.9695, f1neg = 0.9670, f1pos = 0.9716, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 11:07:47.284757: step 69000, loss = 0.1065, acc = 0.9685, f1neg = 0.9673, f1pos = 0.9696, f1 = 0.9685
[Eval_batch(17)(2000,36000)] 2017-05-03 11:07:47.791540: step 69000, loss = 0.1349, acc = 0.9575, f1neg = 0.9579, f1pos = 0.9571, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 11:07:48.303588: step 69000, loss = 0.1267, acc = 0.9590, f1neg = 0.9607, f1pos = 0.9571, f1 = 0.9589
[Eval_batch(19)(2000,40000)] 2017-05-03 11:07:48.814641: step 69000, loss = 0.1108, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 11:07:49.330905: step 69000, loss = 0.1126, acc = 0.9625, f1neg = 0.9668, f1pos = 0.9569, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 11:07:49.840081: step 69000, loss = 0.1058, acc = 0.9645, f1neg = 0.9615, f1pos = 0.9671, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 11:07:50.345869: step 69000, loss = 0.1145, acc = 0.9590, f1neg = 0.9594, f1pos = 0.9585, f1 = 0.9590
[Eval_batch(23)(2000,48000)] 2017-05-03 11:07:50.851239: step 69000, loss = 0.1197, acc = 0.9615, f1neg = 0.9568, f1pos = 0.9653, f1 = 0.9610
[Eval_batch(24)(2000,50000)] 2017-05-03 11:07:51.359910: step 69000, loss = 0.1045, acc = 0.9660, f1neg = 0.9612, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 11:07:51.862302: step 69000, loss = 0.0959, acc = 0.9735, f1neg = 0.9710, f1pos = 0.9756, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 11:07:52.361765: step 69000, loss = 0.1102, acc = 0.9620, f1neg = 0.9642, f1pos = 0.9596, f1 = 0.9619
[Eval_batch(27)(2000,56000)] 2017-05-03 11:07:52.972929: step 69000, loss = 0.0953, acc = 0.9725, f1neg = 0.9712, f1pos = 0.9737, f1 = 0.9724
[Eval] 2017-05-03 11:07:52.972999: step 69000, acc = 0.9624, f1 = 0.9621
[Test_batch(0)(2000,2000)] 2017-05-03 11:07:53.479691: step 69000, loss = 0.1455, acc = 0.9530, f1neg = 0.9554, f1pos = 0.9503, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 11:07:53.987428: step 69000, loss = 0.1245, acc = 0.9615, f1neg = 0.9659, f1pos = 0.9559, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 11:07:54.506879: step 69000, loss = 0.1422, acc = 0.9580, f1neg = 0.9615, f1pos = 0.9538, f1 = 0.9577
[Test_batch(3)(2000,8000)] 2017-05-03 11:07:55.010228: step 69000, loss = 0.1481, acc = 0.9490, f1neg = 0.9524, f1pos = 0.9451, f1 = 0.9487
[Test_batch(4)(2000,10000)] 2017-05-03 11:07:55.513002: step 69000, loss = 0.1442, acc = 0.9495, f1neg = 0.9498, f1pos = 0.9492, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 11:07:56.014930: step 69000, loss = 0.1577, acc = 0.9455, f1neg = 0.9462, f1pos = 0.9448, f1 = 0.9455
[Test_batch(6)(2000,14000)] 2017-05-03 11:07:56.495675: step 69000, loss = 0.1424, acc = 0.9480, f1neg = 0.9462, f1pos = 0.9497, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 11:07:56.998029: step 69000, loss = 0.1330, acc = 0.9530, f1neg = 0.9574, f1pos = 0.9477, f1 = 0.9525
[Test_batch(8)(2000,18000)] 2017-05-03 11:07:57.461775: step 69000, loss = 0.1353, acc = 0.9485, f1neg = 0.9480, f1pos = 0.9490, f1 = 0.9485
[Test_batch(9)(2000,20000)] 2017-05-03 11:07:57.940147: step 69000, loss = 0.1634, acc = 0.9400, f1neg = 0.9370, f1pos = 0.9427, f1 = 0.9399
[Test_batch(10)(2000,22000)] 2017-05-03 11:07:58.413473: step 69000, loss = 0.1413, acc = 0.9550, f1neg = 0.9494, f1pos = 0.9595, f1 = 0.9544
[Test_batch(11)(2000,24000)] 2017-05-03 11:07:58.909449: step 69000, loss = 0.1440, acc = 0.9515, f1neg = 0.9529, f1pos = 0.9500, f1 = 0.9515
[Test_batch(12)(2000,26000)] 2017-05-03 11:07:59.419424: step 69000, loss = 0.1193, acc = 0.9650, f1neg = 0.9664, f1pos = 0.9634, f1 = 0.9649
[Test_batch(13)(2000,28000)] 2017-05-03 11:07:59.929448: step 69000, loss = 0.1417, acc = 0.9525, f1neg = 0.9477, f1pos = 0.9565, f1 = 0.9521
[Test_batch(14)(2000,30000)] 2017-05-03 11:08:00.427983: step 69000, loss = 0.1186, acc = 0.9575, f1neg = 0.9512, f1pos = 0.9623, f1 = 0.9568
[Test_batch(15)(2000,32000)] 2017-05-03 11:08:00.925969: step 69000, loss = 0.1429, acc = 0.9575, f1neg = 0.9570, f1pos = 0.9579, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 11:08:01.408906: step 69000, loss = 0.1234, acc = 0.9580, f1neg = 0.9570, f1pos = 0.9589, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 11:08:01.888901: step 69000, loss = 0.1144, acc = 0.9665, f1neg = 0.9634, f1pos = 0.9691, f1 = 0.9663
[Test_batch(18)(2000,38000)] 2017-05-03 11:08:02.458051: step 69000, loss = 0.1094, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 11:08:02.458139: step 69000, acc = 0.9544, f1 = 0.9542
[Status] 2017-05-03 11:08:02.458163: step 69000, maxindex = 68000, maxdev = 0.9628, maxtst = 0.9540
2017-05-03 11:08:11.478286: step 69010, loss = 0.1143, acc = 0.9620 (282.6 examples/sec; 0.227 sec/batch)
2017-05-03 11:08:20.423987: step 69020, loss = 0.1117, acc = 0.9680 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 11:08:29.497411: step 69030, loss = 0.1295, acc = 0.9620 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 11:08:38.347566: step 69040, loss = 0.1273, acc = 0.9620 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 11:08:47.338556: step 69050, loss = 0.1052, acc = 0.9720 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 11:08:56.225899: step 69060, loss = 0.1451, acc = 0.9520 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 11:09:05.390939: step 69070, loss = 0.1307, acc = 0.9500 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 11:09:14.392452: step 69080, loss = 0.1137, acc = 0.9620 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 11:09:23.400088: step 69090, loss = 0.1105, acc = 0.9660 (268.8 examples/sec; 0.238 sec/batch)
2017-05-03 11:09:32.239018: step 69100, loss = 0.1144, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 11:09:41.285959: step 69110, loss = 0.1300, acc = 0.9500 (298.3 examples/sec; 0.215 sec/batch)
2017-05-03 11:09:50.207396: step 69120, loss = 0.1247, acc = 0.9620 (298.8 examples/sec; 0.214 sec/batch)
2017-05-03 11:09:59.146122: step 69130, loss = 0.1246, acc = 0.9620 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 11:10:08.346112: step 69140, loss = 0.1138, acc = 0.9660 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 11:10:17.287214: step 69150, loss = 0.0794, acc = 0.9880 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 11:10:26.298303: step 69160, loss = 0.1556, acc = 0.9520 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:10:35.255275: step 69170, loss = 0.1009, acc = 0.9720 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 11:10:44.494865: step 69180, loss = 0.1234, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 11:10:53.790874: step 69190, loss = 0.1319, acc = 0.9560 (259.3 examples/sec; 0.247 sec/batch)
2017-05-03 11:11:02.918136: step 69200, loss = 0.1180, acc = 0.9580 (300.2 examples/sec; 0.213 sec/batch)
2017-05-03 11:11:11.969991: step 69210, loss = 0.1068, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 11:11:20.990157: step 69220, loss = 0.1138, acc = 0.9700 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:11:29.887381: step 69230, loss = 0.1120, acc = 0.9600 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 11:11:38.972033: step 69240, loss = 0.1404, acc = 0.9560 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:11:48.224563: step 69250, loss = 0.1307, acc = 0.9560 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 11:11:57.591016: step 69260, loss = 0.1204, acc = 0.9680 (244.6 examples/sec; 0.262 sec/batch)
2017-05-03 11:12:06.614013: step 69270, loss = 0.1369, acc = 0.9540 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 11:12:15.789780: step 69280, loss = 0.1457, acc = 0.9480 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 11:12:24.876795: step 69290, loss = 0.1275, acc = 0.9620 (302.0 examples/sec; 0.212 sec/batch)
2017-05-03 11:12:34.284811: step 69300, loss = 0.1232, acc = 0.9540 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 11:12:43.323821: step 69310, loss = 0.1081, acc = 0.9700 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 11:12:52.404109: step 69320, loss = 0.1093, acc = 0.9660 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 11:13:01.478175: step 69330, loss = 0.1291, acc = 0.9580 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 11:13:10.615435: step 69340, loss = 0.1176, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 11:13:19.667220: step 69350, loss = 0.1382, acc = 0.9480 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 11:13:28.733613: step 69360, loss = 0.1102, acc = 0.9720 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 11:13:37.747309: step 69370, loss = 0.1220, acc = 0.9620 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 11:13:47.069005: step 69380, loss = 0.1143, acc = 0.9700 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 11:13:55.990594: step 69390, loss = 0.1283, acc = 0.9540 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 11:14:05.083882: step 69400, loss = 0.0929, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 11:14:14.595488: step 69410, loss = 0.1108, acc = 0.9600 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 11:14:23.639342: step 69420, loss = 0.1159, acc = 0.9600 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 11:14:32.677202: step 69430, loss = 0.0936, acc = 0.9720 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 11:14:41.640707: step 69440, loss = 0.1242, acc = 0.9640 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 11:14:50.810216: step 69450, loss = 0.1168, acc = 0.9580 (298.6 examples/sec; 0.214 sec/batch)
2017-05-03 11:15:00.010936: step 69460, loss = 0.1083, acc = 0.9700 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 11:15:09.060855: step 69470, loss = 0.1456, acc = 0.9520 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 11:15:18.103337: step 69480, loss = 0.1521, acc = 0.9580 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 11:15:27.009845: step 69490, loss = 0.1188, acc = 0.9620 (301.9 examples/sec; 0.212 sec/batch)
2017-05-03 11:15:36.220820: step 69500, loss = 0.1243, acc = 0.9680 (262.0 examples/sec; 0.244 sec/batch)
2017-05-03 11:15:45.356960: step 69510, loss = 0.1128, acc = 0.9560 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 11:15:54.430890: step 69520, loss = 0.1125, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 11:16:03.618153: step 69530, loss = 0.1166, acc = 0.9600 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 11:16:12.653976: step 69540, loss = 0.1162, acc = 0.9680 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 11:16:21.626006: step 69550, loss = 0.1146, acc = 0.9700 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 11:16:31.936209: step 69560, loss = 0.1263, acc = 0.9600 (188.6 examples/sec; 0.339 sec/batch)
2017-05-03 11:16:40.990165: step 69570, loss = 0.1213, acc = 0.9560 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 11:16:50.050042: step 69580, loss = 0.0975, acc = 0.9760 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 11:16:59.333030: step 69590, loss = 0.0914, acc = 0.9740 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 11:17:08.734026: step 69600, loss = 0.1364, acc = 0.9400 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 11:17:17.779281: step 69610, loss = 0.1259, acc = 0.9600 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 11:17:26.743216: step 69620, loss = 0.1131, acc = 0.9680 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 11:17:35.917634: step 69630, loss = 0.1338, acc = 0.9560 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 11:17:44.930132: step 69640, loss = 0.1418, acc = 0.9600 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 11:17:53.842722: step 69650, loss = 0.1144, acc = 0.9640 (300.9 examples/sec; 0.213 sec/batch)
2017-05-03 11:18:02.906241: step 69660, loss = 0.1054, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 11:18:11.968331: step 69670, loss = 0.1093, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 11:18:20.944356: step 69680, loss = 0.1340, acc = 0.9580 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 11:18:30.005639: step 69690, loss = 0.1223, acc = 0.9640 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 11:18:39.334403: step 69700, loss = 0.1291, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 11:18:48.354901: step 69710, loss = 0.1115, acc = 0.9620 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 11:18:57.230114: step 69720, loss = 0.1285, acc = 0.9540 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 11:19:06.254940: step 69730, loss = 0.1144, acc = 0.9680 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 11:19:15.432165: step 69740, loss = 0.1234, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 11:19:24.461398: step 69750, loss = 0.1257, acc = 0.9580 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 11:19:33.688475: step 69760, loss = 0.1413, acc = 0.9480 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 11:19:42.612100: step 69770, loss = 0.1264, acc = 0.9520 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 11:19:51.688395: step 69780, loss = 0.1081, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 11:20:00.775013: step 69790, loss = 0.1269, acc = 0.9560 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 11:20:09.893028: step 69800, loss = 0.1231, acc = 0.9620 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 11:20:18.964838: step 69810, loss = 0.0852, acc = 0.9860 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 11:20:28.068896: step 69820, loss = 0.1238, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 11:20:37.212943: step 69830, loss = 0.1009, acc = 0.9680 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 11:20:46.211180: step 69840, loss = 0.1144, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 11:20:55.296846: step 69850, loss = 0.1000, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 11:21:04.244378: step 69860, loss = 0.1553, acc = 0.9560 (297.3 examples/sec; 0.215 sec/batch)
2017-05-03 11:21:13.258255: step 69870, loss = 0.1368, acc = 0.9600 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 11:21:22.621955: step 69880, loss = 0.1303, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 11:21:31.602186: step 69890, loss = 0.0924, acc = 0.9800 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 11:21:40.776965: step 69900, loss = 0.1106, acc = 0.9680 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 11:21:49.784450: step 69910, loss = 0.1103, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 11:21:58.941007: step 69920, loss = 0.1380, acc = 0.9500 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 11:22:07.985937: step 69930, loss = 0.1251, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 11:22:16.934340: step 69940, loss = 0.1181, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 11:22:25.991812: step 69950, loss = 0.1365, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 11:22:35.277460: step 69960, loss = 0.1009, acc = 0.9700 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 11:22:44.331816: step 69970, loss = 0.1054, acc = 0.9620 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 11:22:53.404132: step 69980, loss = 0.1475, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 11:23:02.369302: step 69990, loss = 0.1344, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 11:23:11.365429: step 70000, loss = 0.0935, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 11:23:11.837114: step 70000, loss = 0.1090, acc = 0.9605, f1neg = 0.9566, f1pos = 0.9637, f1 = 0.9602
[Eval_batch(1)(2000,4000)] 2017-05-03 11:23:12.313418: step 70000, loss = 0.1103, acc = 0.9655, f1neg = 0.9597, f1pos = 0.9698, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 11:23:12.785537: step 70000, loss = 0.1240, acc = 0.9575, f1neg = 0.9547, f1pos = 0.9600, f1 = 0.9573
[Eval_batch(3)(2000,8000)] 2017-05-03 11:23:13.288937: step 70000, loss = 0.1265, acc = 0.9585, f1neg = 0.9584, f1pos = 0.9586, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 11:23:13.765836: step 70000, loss = 0.1287, acc = 0.9600, f1neg = 0.9610, f1pos = 0.9589, f1 = 0.9600
[Eval_batch(5)(2000,12000)] 2017-05-03 11:23:14.241772: step 70000, loss = 0.1276, acc = 0.9550, f1neg = 0.9516, f1pos = 0.9580, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 11:23:14.748745: step 70000, loss = 0.1186, acc = 0.9655, f1neg = 0.9648, f1pos = 0.9662, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 11:23:15.228742: step 70000, loss = 0.1161, acc = 0.9615, f1neg = 0.9539, f1pos = 0.9670, f1 = 0.9604
[Eval_batch(8)(2000,18000)] 2017-05-03 11:23:15.695525: step 70000, loss = 0.1104, acc = 0.9630, f1neg = 0.9577, f1pos = 0.9671, f1 = 0.9624
[Eval_batch(9)(2000,20000)] 2017-05-03 11:23:16.172636: step 70000, loss = 0.1157, acc = 0.9655, f1neg = 0.9629, f1pos = 0.9677, f1 = 0.9653
[Eval_batch(10)(2000,22000)] 2017-05-03 11:23:16.633864: step 70000, loss = 0.1224, acc = 0.9595, f1neg = 0.9565, f1pos = 0.9621, f1 = 0.9593
[Eval_batch(11)(2000,24000)] 2017-05-03 11:23:17.125776: step 70000, loss = 0.1227, acc = 0.9600, f1neg = 0.9538, f1pos = 0.9648, f1 = 0.9593
[Eval_batch(12)(2000,26000)] 2017-05-03 11:23:17.602388: step 70000, loss = 0.1195, acc = 0.9550, f1neg = 0.9535, f1pos = 0.9564, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 11:23:18.109425: step 70000, loss = 0.1107, acc = 0.9660, f1neg = 0.9573, f1pos = 0.9718, f1 = 0.9645
[Eval_batch(14)(2000,30000)] 2017-05-03 11:23:18.579307: step 70000, loss = 0.1380, acc = 0.9535, f1neg = 0.9445, f1pos = 0.9600, f1 = 0.9522
[Eval_batch(15)(2000,32000)] 2017-05-03 11:23:19.082837: step 70000, loss = 0.1121, acc = 0.9695, f1neg = 0.9669, f1pos = 0.9717, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 11:23:19.533101: step 70000, loss = 0.1066, acc = 0.9690, f1neg = 0.9677, f1pos = 0.9702, f1 = 0.9690
[Eval_batch(17)(2000,36000)] 2017-05-03 11:23:20.040285: step 70000, loss = 0.1360, acc = 0.9570, f1neg = 0.9573, f1pos = 0.9567, f1 = 0.9570
[Eval_batch(18)(2000,38000)] 2017-05-03 11:23:20.543537: step 70000, loss = 0.1300, acc = 0.9560, f1neg = 0.9577, f1pos = 0.9542, f1 = 0.9559
[Eval_batch(19)(2000,40000)] 2017-05-03 11:23:21.043816: step 70000, loss = 0.1121, acc = 0.9685, f1neg = 0.9636, f1pos = 0.9723, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 11:23:21.521980: step 70000, loss = 0.1162, acc = 0.9615, f1neg = 0.9658, f1pos = 0.9559, f1 = 0.9609
[Eval_batch(21)(2000,44000)] 2017-05-03 11:23:21.999492: step 70000, loss = 0.1048, acc = 0.9655, f1neg = 0.9624, f1pos = 0.9681, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 11:23:22.479905: step 70000, loss = 0.1150, acc = 0.9610, f1neg = 0.9612, f1pos = 0.9608, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 11:23:22.925941: step 70000, loss = 0.1196, acc = 0.9595, f1neg = 0.9542, f1pos = 0.9637, f1 = 0.9589
[Eval_batch(24)(2000,50000)] 2017-05-03 11:23:23.412597: step 70000, loss = 0.1048, acc = 0.9675, f1neg = 0.9627, f1pos = 0.9712, f1 = 0.9669
[Eval_batch(25)(2000,52000)] 2017-05-03 11:23:23.893928: step 70000, loss = 0.0976, acc = 0.9725, f1neg = 0.9699, f1pos = 0.9747, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 11:23:24.404801: step 70000, loss = 0.1134, acc = 0.9610, f1neg = 0.9630, f1pos = 0.9587, f1 = 0.9609
[Eval_batch(27)(2000,56000)] 2017-05-03 11:23:25.021604: step 70000, loss = 0.0995, acc = 0.9680, f1neg = 0.9663, f1pos = 0.9695, f1 = 0.9679
[Eval] 2017-05-03 11:23:25.021686: step 70000, acc = 0.9623, f1 = 0.9619
[Test_batch(0)(2000,2000)] 2017-05-03 11:23:25.507655: step 70000, loss = 0.1479, acc = 0.9530, f1neg = 0.9552, f1pos = 0.9505, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 11:23:25.989622: step 70000, loss = 0.1259, acc = 0.9625, f1neg = 0.9667, f1pos = 0.9572, f1 = 0.9619
[Test_batch(2)(2000,6000)] 2017-05-03 11:23:26.461403: step 70000, loss = 0.1450, acc = 0.9530, f1neg = 0.9566, f1pos = 0.9487, f1 = 0.9527
[Test_batch(3)(2000,8000)] 2017-05-03 11:23:26.945181: step 70000, loss = 0.1499, acc = 0.9500, f1neg = 0.9530, f1pos = 0.9466, f1 = 0.9498
[Test_batch(4)(2000,10000)] 2017-05-03 11:23:27.447463: step 70000, loss = 0.1453, acc = 0.9495, f1neg = 0.9497, f1pos = 0.9493, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 11:23:27.919880: step 70000, loss = 0.1582, acc = 0.9435, f1neg = 0.9440, f1pos = 0.9430, f1 = 0.9435
[Test_batch(6)(2000,14000)] 2017-05-03 11:23:28.402998: step 70000, loss = 0.1441, acc = 0.9500, f1neg = 0.9481, f1pos = 0.9517, f1 = 0.9499
[Test_batch(7)(2000,16000)] 2017-05-03 11:23:28.878791: step 70000, loss = 0.1349, acc = 0.9530, f1neg = 0.9572, f1pos = 0.9480, f1 = 0.9526
[Test_batch(8)(2000,18000)] 2017-05-03 11:23:29.383502: step 70000, loss = 0.1359, acc = 0.9500, f1neg = 0.9493, f1pos = 0.9507, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 11:23:29.859516: step 70000, loss = 0.1637, acc = 0.9360, f1neg = 0.9323, f1pos = 0.9393, f1 = 0.9358
[Test_batch(10)(2000,22000)] 2017-05-03 11:23:30.353874: step 70000, loss = 0.1410, acc = 0.9545, f1neg = 0.9487, f1pos = 0.9591, f1 = 0.9539
[Test_batch(11)(2000,24000)] 2017-05-03 11:23:30.836108: step 70000, loss = 0.1454, acc = 0.9515, f1neg = 0.9528, f1pos = 0.9501, f1 = 0.9515
[Test_batch(12)(2000,26000)] 2017-05-03 11:23:31.312179: step 70000, loss = 0.1187, acc = 0.9635, f1neg = 0.9648, f1pos = 0.9621, f1 = 0.9634
[Test_batch(13)(2000,28000)] 2017-05-03 11:23:31.781229: step 70000, loss = 0.1413, acc = 0.9545, f1neg = 0.9492, f1pos = 0.9588, f1 = 0.9540
[Test_batch(14)(2000,30000)] 2017-05-03 11:23:32.266987: step 70000, loss = 0.1180, acc = 0.9610, f1neg = 0.9550, f1pos = 0.9656, f1 = 0.9603
[Test_batch(15)(2000,32000)] 2017-05-03 11:23:32.745430: step 70000, loss = 0.1432, acc = 0.9565, f1neg = 0.9558, f1pos = 0.9572, f1 = 0.9565
[Test_batch(16)(2000,34000)] 2017-05-03 11:23:33.229026: step 70000, loss = 0.1247, acc = 0.9595, f1neg = 0.9583, f1pos = 0.9606, f1 = 0.9595
[Test_batch(17)(2000,36000)] 2017-05-03 11:23:33.702269: step 70000, loss = 0.1167, acc = 0.9675, f1neg = 0.9644, f1pos = 0.9701, f1 = 0.9672
[Test_batch(18)(2000,38000)] 2017-05-03 11:23:34.242177: step 70000, loss = 0.1113, acc = 0.9650, f1neg = 0.9643, f1pos = 0.9657, f1 = 0.9650
[Test] 2017-05-03 11:23:34.242265: step 70000, acc = 0.9544, f1 = 0.9542
[Status] 2017-05-03 11:23:34.242291: step 70000, maxindex = 68000, maxdev = 0.9628, maxtst = 0.9540
2017-05-03 11:23:43.297905: step 70010, loss = 0.1324, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 11:23:52.456436: step 70020, loss = 0.1103, acc = 0.9720 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 11:24:01.548894: step 70030, loss = 0.1211, acc = 0.9540 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 11:24:10.515837: step 70040, loss = 0.1362, acc = 0.9540 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 11:24:20.023359: step 70050, loss = 0.0944, acc = 0.9760 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 11:24:29.070881: step 70060, loss = 0.1223, acc = 0.9580 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 11:24:38.276956: step 70070, loss = 0.1096, acc = 0.9660 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 11:24:47.357966: step 70080, loss = 0.1034, acc = 0.9740 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 11:24:56.296664: step 70090, loss = 0.1235, acc = 0.9560 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 11:25:06.103423: step 70100, loss = 0.0999, acc = 0.9700 (188.2 examples/sec; 0.340 sec/batch)
2017-05-03 11:25:15.253900: step 70110, loss = 0.1019, acc = 0.9680 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 11:25:24.387764: step 70120, loss = 0.1528, acc = 0.9560 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 11:25:33.527854: step 70130, loss = 0.1519, acc = 0.9420 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 11:25:42.572669: step 70140, loss = 0.1122, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 11:25:51.645217: step 70150, loss = 0.1289, acc = 0.9580 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 11:26:00.631781: step 70160, loss = 0.1349, acc = 0.9540 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 11:26:09.676599: step 70170, loss = 0.1385, acc = 0.9540 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 11:26:18.565928: step 70180, loss = 0.1269, acc = 0.9660 (298.7 examples/sec; 0.214 sec/batch)
2017-05-03 11:26:27.556702: step 70190, loss = 0.1171, acc = 0.9620 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 11:26:36.643040: step 70200, loss = 0.1199, acc = 0.9660 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 11:26:45.797945: step 70210, loss = 0.0999, acc = 0.9660 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 11:26:54.819911: step 70220, loss = 0.1157, acc = 0.9680 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 11:27:03.791286: step 70230, loss = 0.0975, acc = 0.9760 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 11:27:12.871834: step 70240, loss = 0.1313, acc = 0.9620 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 11:27:21.862463: step 70250, loss = 0.1204, acc = 0.9580 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 11:27:30.990282: step 70260, loss = 0.1199, acc = 0.9680 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 11:27:39.956178: step 70270, loss = 0.1014, acc = 0.9760 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 11:27:49.574497: step 70280, loss = 0.1132, acc = 0.9720 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 11:27:58.600977: step 70290, loss = 0.1388, acc = 0.9500 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 11:28:07.531044: step 70300, loss = 0.1227, acc = 0.9600 (259.8 examples/sec; 0.246 sec/batch)
2017-05-03 11:28:16.419382: step 70310, loss = 0.0958, acc = 0.9780 (297.0 examples/sec; 0.215 sec/batch)
2017-05-03 11:28:25.537292: step 70320, loss = 0.1118, acc = 0.9700 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 11:28:34.516262: step 70330, loss = 0.1058, acc = 0.9740 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 11:28:43.552491: step 70340, loss = 0.1294, acc = 0.9520 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 11:28:52.646514: step 70350, loss = 0.1031, acc = 0.9760 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 11:29:01.691444: step 70360, loss = 0.1327, acc = 0.9540 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 11:29:11.161501: step 70370, loss = 0.1357, acc = 0.9560 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 11:29:20.392636: step 70380, loss = 0.1190, acc = 0.9680 (295.7 examples/sec; 0.216 sec/batch)
2017-05-03 11:29:29.395563: step 70390, loss = 0.1404, acc = 0.9540 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 11:29:38.465767: step 70400, loss = 0.1360, acc = 0.9560 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 11:29:47.476880: step 70410, loss = 0.1227, acc = 0.9580 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 11:29:56.521823: step 70420, loss = 0.1206, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 11:30:05.623937: step 70430, loss = 0.0995, acc = 0.9760 (265.8 examples/sec; 0.241 sec/batch)
2017-05-03 11:30:15.011734: step 70440, loss = 0.1060, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 11:30:24.083476: step 70450, loss = 0.1319, acc = 0.9500 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 11:30:33.019729: step 70460, loss = 0.1253, acc = 0.9600 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 11:30:42.021294: step 70470, loss = 0.1147, acc = 0.9640 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 11:30:50.973225: step 70480, loss = 0.1084, acc = 0.9660 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 11:31:00.017193: step 70490, loss = 0.0969, acc = 0.9740 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 11:31:09.103496: step 70500, loss = 0.1236, acc = 0.9620 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 11:31:18.338862: step 70510, loss = 0.0990, acc = 0.9740 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 11:31:27.477067: step 70520, loss = 0.1261, acc = 0.9560 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 11:31:36.347819: step 70530, loss = 0.1303, acc = 0.9540 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 11:31:45.452950: step 70540, loss = 0.1206, acc = 0.9600 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 11:31:54.606463: step 70550, loss = 0.0996, acc = 0.9760 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 11:32:04.264217: step 70560, loss = 0.1182, acc = 0.9580 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 11:32:13.121441: step 70570, loss = 0.1159, acc = 0.9660 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 11:32:22.055932: step 70580, loss = 0.1010, acc = 0.9720 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 11:32:31.055439: step 70590, loss = 0.1371, acc = 0.9600 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 11:32:40.056077: step 70600, loss = 0.1038, acc = 0.9740 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 11:32:49.113534: step 70610, loss = 0.1112, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 11:32:58.167429: step 70620, loss = 0.1077, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 11:33:07.356986: step 70630, loss = 0.1406, acc = 0.9440 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 11:33:16.437219: step 70640, loss = 0.1250, acc = 0.9600 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 11:33:25.469844: step 70650, loss = 0.1279, acc = 0.9560 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 11:33:34.502751: step 70660, loss = 0.1038, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 11:33:43.662598: step 70670, loss = 0.1168, acc = 0.9600 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 11:33:52.730616: step 70680, loss = 0.1143, acc = 0.9760 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 11:34:01.747546: step 70690, loss = 0.1328, acc = 0.9500 (261.5 examples/sec; 0.245 sec/batch)
2017-05-03 11:34:10.773907: step 70700, loss = 0.1261, acc = 0.9640 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 11:34:19.719790: step 70710, loss = 0.1028, acc = 0.9760 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 11:34:28.773435: step 70720, loss = 0.0811, acc = 0.9880 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 11:34:37.863253: step 70730, loss = 0.1341, acc = 0.9540 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 11:34:46.898273: step 70740, loss = 0.1324, acc = 0.9600 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 11:34:56.066626: step 70750, loss = 0.1411, acc = 0.9500 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 11:35:05.104853: step 70760, loss = 0.1096, acc = 0.9660 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 11:35:14.118222: step 70770, loss = 0.1104, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 11:35:23.507321: step 70780, loss = 0.1133, acc = 0.9600 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 11:35:32.582232: step 70790, loss = 0.1439, acc = 0.9480 (273.4 examples/sec; 0.234 sec/batch)
2017-05-03 11:35:41.667075: step 70800, loss = 0.1045, acc = 0.9640 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 11:35:50.888322: step 70810, loss = 0.1204, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 11:35:59.862514: step 70820, loss = 0.1269, acc = 0.9540 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 11:36:08.804465: step 70830, loss = 0.1104, acc = 0.9700 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 11:36:17.777036: step 70840, loss = 0.1237, acc = 0.9560 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 11:36:26.797439: step 70850, loss = 0.1221, acc = 0.9560 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 11:36:35.844014: step 70860, loss = 0.1063, acc = 0.9640 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 11:36:44.927799: step 70870, loss = 0.1234, acc = 0.9460 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 11:36:53.992357: step 70880, loss = 0.0907, acc = 0.9780 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 11:37:02.774328: step 70890, loss = 0.1344, acc = 0.9620 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 11:37:11.797626: step 70900, loss = 0.1117, acc = 0.9680 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 11:37:21.150935: step 70910, loss = 0.1349, acc = 0.9580 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 11:37:30.169375: step 70920, loss = 0.1262, acc = 0.9580 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 11:37:39.262294: step 70930, loss = 0.1057, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 11:37:48.318845: step 70940, loss = 0.1210, acc = 0.9520 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 11:37:57.250590: step 70950, loss = 0.0906, acc = 0.9720 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 11:38:06.137692: step 70960, loss = 0.1126, acc = 0.9720 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 11:38:15.218550: step 70970, loss = 0.1282, acc = 0.9600 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 11:38:24.516239: step 70980, loss = 0.0935, acc = 0.9700 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 11:38:33.532098: step 70990, loss = 0.1172, acc = 0.9660 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 11:38:42.671480: step 71000, loss = 0.0824, acc = 0.9820 (282.0 examples/sec; 0.227 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 11:38:43.168774: step 71000, loss = 0.1092, acc = 0.9590, f1neg = 0.9548, f1pos = 0.9625, f1 = 0.9586
[Eval_batch(1)(2000,4000)] 2017-05-03 11:38:43.605673: step 71000, loss = 0.1104, acc = 0.9655, f1neg = 0.9595, f1pos = 0.9699, f1 = 0.9647
[Eval_batch(2)(2000,6000)] 2017-05-03 11:38:44.081127: step 71000, loss = 0.1224, acc = 0.9600, f1neg = 0.9572, f1pos = 0.9624, f1 = 0.9598
[Eval_batch(3)(2000,8000)] 2017-05-03 11:38:44.562888: step 71000, loss = 0.1302, acc = 0.9585, f1neg = 0.9583, f1pos = 0.9587, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 11:38:45.040815: step 71000, loss = 0.1307, acc = 0.9580, f1neg = 0.9588, f1pos = 0.9571, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 11:38:45.550243: step 71000, loss = 0.1268, acc = 0.9535, f1neg = 0.9497, f1pos = 0.9568, f1 = 0.9532
[Eval_batch(6)(2000,14000)] 2017-05-03 11:38:46.014767: step 71000, loss = 0.1201, acc = 0.9635, f1neg = 0.9627, f1pos = 0.9643, f1 = 0.9635
[Eval_batch(7)(2000,16000)] 2017-05-03 11:38:46.455305: step 71000, loss = 0.1172, acc = 0.9595, f1neg = 0.9514, f1pos = 0.9653, f1 = 0.9583
[Eval_batch(8)(2000,18000)] 2017-05-03 11:38:46.932963: step 71000, loss = 0.1099, acc = 0.9630, f1neg = 0.9576, f1pos = 0.9672, f1 = 0.9624
[Eval_batch(9)(2000,20000)] 2017-05-03 11:38:47.404644: step 71000, loss = 0.1152, acc = 0.9640, f1neg = 0.9612, f1pos = 0.9664, f1 = 0.9638
[Eval_batch(10)(2000,22000)] 2017-05-03 11:38:47.870297: step 71000, loss = 0.1234, acc = 0.9605, f1neg = 0.9575, f1pos = 0.9631, f1 = 0.9603
[Eval_batch(11)(2000,24000)] 2017-05-03 11:38:48.341686: step 71000, loss = 0.1220, acc = 0.9605, f1neg = 0.9542, f1pos = 0.9653, f1 = 0.9597
[Eval_batch(12)(2000,26000)] 2017-05-03 11:38:48.820420: step 71000, loss = 0.1214, acc = 0.9530, f1neg = 0.9513, f1pos = 0.9546, f1 = 0.9529
[Eval_batch(13)(2000,28000)] 2017-05-03 11:38:49.286088: step 71000, loss = 0.1102, acc = 0.9665, f1neg = 0.9578, f1pos = 0.9722, f1 = 0.9650
[Eval_batch(14)(2000,30000)] 2017-05-03 11:38:49.730697: step 71000, loss = 0.1377, acc = 0.9515, f1neg = 0.9420, f1pos = 0.9583, f1 = 0.9502
[Eval_batch(15)(2000,32000)] 2017-05-03 11:38:50.236178: step 71000, loss = 0.1121, acc = 0.9685, f1neg = 0.9658, f1pos = 0.9708, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 11:38:50.724696: step 71000, loss = 0.1069, acc = 0.9670, f1neg = 0.9654, f1pos = 0.9684, f1 = 0.9669
[Eval_batch(17)(2000,36000)] 2017-05-03 11:38:51.238187: step 71000, loss = 0.1356, acc = 0.9555, f1neg = 0.9556, f1pos = 0.9554, f1 = 0.9555
[Eval_batch(18)(2000,38000)] 2017-05-03 11:38:51.750458: step 71000, loss = 0.1317, acc = 0.9560, f1neg = 0.9576, f1pos = 0.9543, f1 = 0.9559
[Eval_batch(19)(2000,40000)] 2017-05-03 11:38:52.214828: step 71000, loss = 0.1128, acc = 0.9680, f1neg = 0.9629, f1pos = 0.9719, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 11:38:52.683642: step 71000, loss = 0.1185, acc = 0.9635, f1neg = 0.9675, f1pos = 0.9584, f1 = 0.9629
[Eval_batch(21)(2000,44000)] 2017-05-03 11:38:53.155203: step 71000, loss = 0.1046, acc = 0.9660, f1neg = 0.9628, f1pos = 0.9687, f1 = 0.9658
[Eval_batch(22)(2000,46000)] 2017-05-03 11:38:53.658125: step 71000, loss = 0.1159, acc = 0.9605, f1neg = 0.9607, f1pos = 0.9603, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 11:38:54.156503: step 71000, loss = 0.1205, acc = 0.9575, f1neg = 0.9517, f1pos = 0.9620, f1 = 0.9569
[Eval_batch(24)(2000,50000)] 2017-05-03 11:38:54.632041: step 71000, loss = 0.1056, acc = 0.9660, f1neg = 0.9609, f1pos = 0.9699, f1 = 0.9654
[Eval_batch(25)(2000,52000)] 2017-05-03 11:38:55.109284: step 71000, loss = 0.0982, acc = 0.9725, f1neg = 0.9697, f1pos = 0.9748, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 11:38:55.620765: step 71000, loss = 0.1141, acc = 0.9605, f1neg = 0.9625, f1pos = 0.9583, f1 = 0.9604
[Eval_batch(27)(2000,56000)] 2017-05-03 11:38:56.229479: step 71000, loss = 0.0997, acc = 0.9670, f1neg = 0.9652, f1pos = 0.9687, f1 = 0.9669
[Eval] 2017-05-03 11:38:56.229605: step 71000, acc = 0.9616, f1 = 0.9612
[Test_batch(0)(2000,2000)] 2017-05-03 11:38:56.704637: step 71000, loss = 0.1480, acc = 0.9520, f1neg = 0.9541, f1pos = 0.9497, f1 = 0.9519
[Test_batch(1)(2000,4000)] 2017-05-03 11:38:57.173136: step 71000, loss = 0.1291, acc = 0.9605, f1neg = 0.9647, f1pos = 0.9551, f1 = 0.9599
[Test_batch(2)(2000,6000)] 2017-05-03 11:38:57.631837: step 71000, loss = 0.1467, acc = 0.9535, f1neg = 0.9569, f1pos = 0.9495, f1 = 0.9532
[Test_batch(3)(2000,8000)] 2017-05-03 11:38:58.110399: step 71000, loss = 0.1513, acc = 0.9495, f1neg = 0.9522, f1pos = 0.9464, f1 = 0.9493
[Test_batch(4)(2000,10000)] 2017-05-03 11:38:58.587464: step 71000, loss = 0.1454, acc = 0.9480, f1neg = 0.9478, f1pos = 0.9482, f1 = 0.9480
[Test_batch(5)(2000,12000)] 2017-05-03 11:38:59.104946: step 71000, loss = 0.1597, acc = 0.9420, f1neg = 0.9422, f1pos = 0.9418, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 11:38:59.612281: step 71000, loss = 0.1473, acc = 0.9460, f1neg = 0.9436, f1pos = 0.9482, f1 = 0.9459
[Test_batch(7)(2000,16000)] 2017-05-03 11:39:00.108939: step 71000, loss = 0.1379, acc = 0.9535, f1neg = 0.9575, f1pos = 0.9486, f1 = 0.9531
[Test_batch(8)(2000,18000)] 2017-05-03 11:39:00.611440: step 71000, loss = 0.1353, acc = 0.9505, f1neg = 0.9497, f1pos = 0.9513, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 11:39:01.082502: step 71000, loss = 0.1634, acc = 0.9385, f1neg = 0.9347, f1pos = 0.9418, f1 = 0.9383
[Test_batch(10)(2000,22000)] 2017-05-03 11:39:01.590646: step 71000, loss = 0.1403, acc = 0.9510, f1neg = 0.9444, f1pos = 0.9562, f1 = 0.9503
[Test_batch(11)(2000,24000)] 2017-05-03 11:39:02.088509: step 71000, loss = 0.1465, acc = 0.9510, f1neg = 0.9521, f1pos = 0.9498, f1 = 0.9510
[Test_batch(12)(2000,26000)] 2017-05-03 11:39:02.585776: step 71000, loss = 0.1182, acc = 0.9630, f1neg = 0.9643, f1pos = 0.9616, f1 = 0.9630
[Test_batch(13)(2000,28000)] 2017-05-03 11:39:03.066590: step 71000, loss = 0.1417, acc = 0.9525, f1neg = 0.9468, f1pos = 0.9571, f1 = 0.9519
[Test_batch(14)(2000,30000)] 2017-05-03 11:39:03.546725: step 71000, loss = 0.1177, acc = 0.9605, f1neg = 0.9543, f1pos = 0.9652, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 11:39:03.986020: step 71000, loss = 0.1437, acc = 0.9530, f1neg = 0.9520, f1pos = 0.9539, f1 = 0.9530
[Test_batch(16)(2000,34000)] 2017-05-03 11:39:04.470507: step 71000, loss = 0.1248, acc = 0.9580, f1neg = 0.9566, f1pos = 0.9593, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 11:39:04.954831: step 71000, loss = 0.1169, acc = 0.9685, f1neg = 0.9654, f1pos = 0.9711, f1 = 0.9682
[Test_batch(18)(2000,38000)] 2017-05-03 11:39:05.468608: step 71000, loss = 0.1130, acc = 0.9620, f1neg = 0.9611, f1pos = 0.9629, f1 = 0.9620
[Test] 2017-05-03 11:39:05.468685: step 71000, acc = 0.9533, f1 = 0.9531
[Status] 2017-05-03 11:39:05.468697: step 71000, maxindex = 68000, maxdev = 0.9628, maxtst = 0.9540
2017-05-03 11:39:14.443436: step 71010, loss = 0.1221, acc = 0.9540 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 11:39:23.394243: step 71020, loss = 0.1606, acc = 0.9400 (295.6 examples/sec; 0.217 sec/batch)
2017-05-03 11:39:32.534812: step 71030, loss = 0.0892, acc = 0.9800 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 11:39:41.571371: step 71040, loss = 0.1168, acc = 0.9540 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 11:39:50.691451: step 71050, loss = 0.1170, acc = 0.9680 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 11:39:59.746346: step 71060, loss = 0.1013, acc = 0.9720 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 11:40:08.953895: step 71070, loss = 0.1202, acc = 0.9640 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 11:40:18.163116: step 71080, loss = 0.1298, acc = 0.9520 (258.3 examples/sec; 0.248 sec/batch)
2017-05-03 11:40:27.127618: step 71090, loss = 0.1044, acc = 0.9700 (299.7 examples/sec; 0.214 sec/batch)
2017-05-03 11:40:35.983961: step 71100, loss = 0.1416, acc = 0.9520 (300.6 examples/sec; 0.213 sec/batch)
2017-05-03 11:40:45.083546: step 71110, loss = 0.1146, acc = 0.9560 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 11:40:54.324646: step 71120, loss = 0.1214, acc = 0.9580 (266.8 examples/sec; 0.240 sec/batch)
2017-05-03 11:41:03.419481: step 71130, loss = 0.1229, acc = 0.9720 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 11:41:12.716241: step 71140, loss = 0.1339, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 11:41:21.873735: step 71150, loss = 0.0971, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 11:41:30.949163: step 71160, loss = 0.1229, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 11:41:40.103495: step 71170, loss = 0.1279, acc = 0.9640 (268.8 examples/sec; 0.238 sec/batch)
2017-05-03 11:41:49.092166: step 71180, loss = 0.1198, acc = 0.9520 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 11:41:58.024951: step 71190, loss = 0.1089, acc = 0.9660 (295.1 examples/sec; 0.217 sec/batch)
2017-05-03 11:42:07.049398: step 71200, loss = 0.1470, acc = 0.9560 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 11:42:16.040741: step 71210, loss = 0.1042, acc = 0.9760 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 11:42:25.290111: step 71220, loss = 0.1110, acc = 0.9660 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 11:42:34.228201: step 71230, loss = 0.1216, acc = 0.9680 (270.5 examples/sec; 0.237 sec/batch)
2017-05-03 11:42:43.171363: step 71240, loss = 0.1170, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 11:42:52.097378: step 71250, loss = 0.1231, acc = 0.9620 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 11:43:01.164361: step 71260, loss = 0.1349, acc = 0.9560 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 11:43:10.319064: step 71270, loss = 0.1269, acc = 0.9600 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 11:43:19.336108: step 71280, loss = 0.0954, acc = 0.9700 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 11:43:28.389847: step 71290, loss = 0.0988, acc = 0.9720 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 11:43:37.528896: step 71300, loss = 0.1373, acc = 0.9600 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 11:43:46.530828: step 71310, loss = 0.1300, acc = 0.9580 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 11:43:55.627239: step 71320, loss = 0.1232, acc = 0.9560 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 11:44:04.659331: step 71330, loss = 0.0888, acc = 0.9780 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 11:44:13.576674: step 71340, loss = 0.1273, acc = 0.9620 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 11:44:22.680508: step 71350, loss = 0.1420, acc = 0.9540 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 11:44:31.772318: step 71360, loss = 0.1328, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 11:44:40.884951: step 71370, loss = 0.1682, acc = 0.9440 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 11:44:50.088009: step 71380, loss = 0.1110, acc = 0.9760 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 11:44:58.952327: step 71390, loss = 0.0981, acc = 0.9760 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 11:45:08.057711: step 71400, loss = 0.1167, acc = 0.9720 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 11:45:17.013417: step 71410, loss = 0.1356, acc = 0.9500 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 11:45:26.196046: step 71420, loss = 0.0899, acc = 0.9740 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 11:45:35.201369: step 71430, loss = 0.1024, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 11:45:44.095055: step 71440, loss = 0.1192, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 11:45:53.068591: step 71450, loss = 0.1088, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 11:46:02.115738: step 71460, loss = 0.1553, acc = 0.9520 (297.9 examples/sec; 0.215 sec/batch)
2017-05-03 11:46:11.156837: step 71470, loss = 0.1247, acc = 0.9580 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 11:46:20.106515: step 71480, loss = 0.1315, acc = 0.9620 (268.9 examples/sec; 0.238 sec/batch)
2017-05-03 11:46:29.153153: step 71490, loss = 0.1168, acc = 0.9700 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 11:46:38.361816: step 71500, loss = 0.1176, acc = 0.9680 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 11:46:47.441851: step 71510, loss = 0.1195, acc = 0.9560 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 11:46:56.643244: step 71520, loss = 0.1001, acc = 0.9720 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 11:47:05.763890: step 71530, loss = 0.1237, acc = 0.9640 (262.1 examples/sec; 0.244 sec/batch)
2017-05-03 11:47:14.706768: step 71540, loss = 0.1043, acc = 0.9740 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 11:47:23.914420: step 71550, loss = 0.1165, acc = 0.9600 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 11:47:33.062579: step 71560, loss = 0.1256, acc = 0.9700 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 11:47:42.998023: step 71570, loss = 0.1067, acc = 0.9600 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 11:47:52.016637: step 71580, loss = 0.1053, acc = 0.9620 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 11:48:01.206931: step 71590, loss = 0.1158, acc = 0.9620 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 11:48:10.307010: step 71600, loss = 0.1036, acc = 0.9720 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 11:48:19.240015: step 71610, loss = 0.1152, acc = 0.9660 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 11:48:28.249013: step 71620, loss = 0.1052, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 11:48:37.657147: step 71630, loss = 0.1033, acc = 0.9700 (215.2 examples/sec; 0.297 sec/batch)
2017-05-03 11:48:46.777925: step 71640, loss = 0.1415, acc = 0.9580 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 11:48:55.803712: step 71650, loss = 0.1215, acc = 0.9600 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 11:49:05.043904: step 71660, loss = 0.1153, acc = 0.9540 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 11:49:14.124445: step 71670, loss = 0.1173, acc = 0.9600 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 11:49:23.319291: step 71680, loss = 0.1361, acc = 0.9660 (241.6 examples/sec; 0.265 sec/batch)
2017-05-03 11:49:32.282632: step 71690, loss = 0.1145, acc = 0.9620 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 11:49:41.482443: step 71700, loss = 0.1464, acc = 0.9460 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 11:49:50.824325: step 71710, loss = 0.1279, acc = 0.9680 (300.2 examples/sec; 0.213 sec/batch)
2017-05-03 11:49:59.888443: step 71720, loss = 0.1260, acc = 0.9560 (269.5 examples/sec; 0.238 sec/batch)
2017-05-03 11:50:08.920831: step 71730, loss = 0.1391, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 11:50:18.056900: step 71740, loss = 0.1131, acc = 0.9700 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 11:50:27.073839: step 71750, loss = 0.1304, acc = 0.9520 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 11:50:36.159752: step 71760, loss = 0.0905, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 11:50:45.297732: step 71770, loss = 0.1056, acc = 0.9680 (266.2 examples/sec; 0.240 sec/batch)
2017-05-03 11:50:54.290289: step 71780, loss = 0.1222, acc = 0.9540 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 11:51:03.455738: step 71790, loss = 0.1197, acc = 0.9720 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 11:51:12.544175: step 71800, loss = 0.1023, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 11:51:21.621326: step 71810, loss = 0.1288, acc = 0.9620 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 11:51:30.641731: step 71820, loss = 0.1249, acc = 0.9600 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 11:51:39.607473: step 71830, loss = 0.1350, acc = 0.9660 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 11:51:48.742544: step 71840, loss = 0.1276, acc = 0.9600 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 11:51:57.693095: step 71850, loss = 0.0956, acc = 0.9700 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 11:52:06.778760: step 71860, loss = 0.1113, acc = 0.9660 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 11:52:15.901861: step 71870, loss = 0.1080, acc = 0.9780 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 11:52:25.126154: step 71880, loss = 0.1098, acc = 0.9640 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 11:52:34.186158: step 71890, loss = 0.1405, acc = 0.9500 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 11:52:43.250859: step 71900, loss = 0.1290, acc = 0.9560 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 11:52:52.162978: step 71910, loss = 0.1117, acc = 0.9700 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 11:53:01.101982: step 71920, loss = 0.1077, acc = 0.9660 (298.0 examples/sec; 0.215 sec/batch)
2017-05-03 11:53:10.356415: step 71930, loss = 0.1255, acc = 0.9620 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 11:53:19.195336: step 71940, loss = 0.1164, acc = 0.9700 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 11:53:28.244155: step 71950, loss = 0.1209, acc = 0.9560 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 11:53:37.341721: step 71960, loss = 0.1048, acc = 0.9680 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 11:53:46.362084: step 71970, loss = 0.1555, acc = 0.9440 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 11:53:55.362144: step 71980, loss = 0.1145, acc = 0.9560 (302.7 examples/sec; 0.211 sec/batch)
2017-05-03 11:54:04.343528: step 71990, loss = 0.1045, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 11:54:13.199525: step 72000, loss = 0.1062, acc = 0.9660 (293.8 examples/sec; 0.218 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 11:54:13.668313: step 72000, loss = 0.1041, acc = 0.9615, f1neg = 0.9580, f1pos = 0.9645, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 11:54:14.138353: step 72000, loss = 0.1120, acc = 0.9650, f1neg = 0.9595, f1pos = 0.9692, f1 = 0.9644
[Eval_batch(2)(2000,6000)] 2017-05-03 11:54:14.585644: step 72000, loss = 0.1212, acc = 0.9595, f1neg = 0.9573, f1pos = 0.9615, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 11:54:15.050009: step 72000, loss = 0.1232, acc = 0.9615, f1neg = 0.9617, f1pos = 0.9612, f1 = 0.9615
[Eval_batch(4)(2000,10000)] 2017-05-03 11:54:15.564347: step 72000, loss = 0.1266, acc = 0.9575, f1neg = 0.9591, f1pos = 0.9558, f1 = 0.9574
[Eval_batch(5)(2000,12000)] 2017-05-03 11:54:16.028169: step 72000, loss = 0.1244, acc = 0.9535, f1neg = 0.9507, f1pos = 0.9560, f1 = 0.9534
[Eval_batch(6)(2000,14000)] 2017-05-03 11:54:16.500484: step 72000, loss = 0.1135, acc = 0.9690, f1neg = 0.9686, f1pos = 0.9694, f1 = 0.9690
[Eval_batch(7)(2000,16000)] 2017-05-03 11:54:17.019291: step 72000, loss = 0.1163, acc = 0.9600, f1neg = 0.9526, f1pos = 0.9654, f1 = 0.9590
[Eval_batch(8)(2000,18000)] 2017-05-03 11:54:17.482832: step 72000, loss = 0.1088, acc = 0.9640, f1neg = 0.9594, f1pos = 0.9677, f1 = 0.9635
[Eval_batch(9)(2000,20000)] 2017-05-03 11:54:18.002499: step 72000, loss = 0.1137, acc = 0.9635, f1neg = 0.9611, f1pos = 0.9656, f1 = 0.9634
[Eval_batch(10)(2000,22000)] 2017-05-03 11:54:18.445215: step 72000, loss = 0.1209, acc = 0.9625, f1neg = 0.9603, f1pos = 0.9644, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 11:54:18.912371: step 72000, loss = 0.1228, acc = 0.9610, f1neg = 0.9552, f1pos = 0.9655, f1 = 0.9603
[Eval_batch(12)(2000,26000)] 2017-05-03 11:54:19.385765: step 72000, loss = 0.1173, acc = 0.9550, f1neg = 0.9539, f1pos = 0.9561, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 11:54:19.859338: step 72000, loss = 0.1090, acc = 0.9675, f1neg = 0.9596, f1pos = 0.9728, f1 = 0.9662
[Eval_batch(14)(2000,30000)] 2017-05-03 11:54:20.333003: step 72000, loss = 0.1360, acc = 0.9565, f1neg = 0.9486, f1pos = 0.9623, f1 = 0.9555
[Eval_batch(15)(2000,32000)] 2017-05-03 11:54:20.808570: step 72000, loss = 0.1106, acc = 0.9680, f1neg = 0.9657, f1pos = 0.9700, f1 = 0.9679
[Eval_batch(16)(2000,34000)] 2017-05-03 11:54:21.318786: step 72000, loss = 0.1057, acc = 0.9665, f1neg = 0.9654, f1pos = 0.9675, f1 = 0.9665
[Eval_batch(17)(2000,36000)] 2017-05-03 11:54:21.760904: step 72000, loss = 0.1345, acc = 0.9590, f1neg = 0.9595, f1pos = 0.9585, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 11:54:22.222357: step 72000, loss = 0.1261, acc = 0.9590, f1neg = 0.9610, f1pos = 0.9568, f1 = 0.9589
[Eval_batch(19)(2000,40000)] 2017-05-03 11:54:22.726728: step 72000, loss = 0.1108, acc = 0.9695, f1neg = 0.9650, f1pos = 0.9730, f1 = 0.9690
[Eval_batch(20)(2000,42000)] 2017-05-03 11:54:23.200390: step 72000, loss = 0.1115, acc = 0.9620, f1neg = 0.9664, f1pos = 0.9562, f1 = 0.9613
[Eval_batch(21)(2000,44000)] 2017-05-03 11:54:23.674960: step 72000, loss = 0.1072, acc = 0.9645, f1neg = 0.9616, f1pos = 0.9670, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 11:54:24.140284: step 72000, loss = 0.1133, acc = 0.9615, f1neg = 0.9621, f1pos = 0.9609, f1 = 0.9615
[Eval_batch(23)(2000,48000)] 2017-05-03 11:54:24.652151: step 72000, loss = 0.1205, acc = 0.9600, f1neg = 0.9553, f1pos = 0.9638, f1 = 0.9595
[Eval_batch(24)(2000,50000)] 2017-05-03 11:54:25.117218: step 72000, loss = 0.1047, acc = 0.9660, f1neg = 0.9613, f1pos = 0.9697, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 11:54:25.591031: step 72000, loss = 0.0950, acc = 0.9735, f1neg = 0.9711, f1pos = 0.9755, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 11:54:26.067407: step 72000, loss = 0.1092, acc = 0.9650, f1neg = 0.9672, f1pos = 0.9625, f1 = 0.9648
[Eval_batch(27)(2000,56000)] 2017-05-03 11:54:26.705197: step 72000, loss = 0.0953, acc = 0.9730, f1neg = 0.9718, f1pos = 0.9741, f1 = 0.9730
[Eval] 2017-05-03 11:54:26.705285: step 72000, acc = 0.9630, f1 = 0.9627
[Test_batch(0)(2000,2000)] 2017-05-03 11:54:27.205178: step 72000, loss = 0.1447, acc = 0.9540, f1neg = 0.9566, f1pos = 0.9510, f1 = 0.9538
[Test_batch(1)(2000,4000)] 2017-05-03 11:54:27.672161: step 72000, loss = 0.1240, acc = 0.9620, f1neg = 0.9664, f1pos = 0.9563, f1 = 0.9613
[Test_batch(2)(2000,6000)] 2017-05-03 11:54:28.136170: step 72000, loss = 0.1406, acc = 0.9580, f1neg = 0.9616, f1pos = 0.9536, f1 = 0.9576
[Test_batch(3)(2000,8000)] 2017-05-03 11:54:28.608994: step 72000, loss = 0.1477, acc = 0.9500, f1neg = 0.9535, f1pos = 0.9459, f1 = 0.9497
[Test_batch(4)(2000,10000)] 2017-05-03 11:54:29.073317: step 72000, loss = 0.1438, acc = 0.9525, f1neg = 0.9532, f1pos = 0.9518, f1 = 0.9525
[Test_batch(5)(2000,12000)] 2017-05-03 11:54:29.524493: step 72000, loss = 0.1576, acc = 0.9430, f1neg = 0.9442, f1pos = 0.9417, f1 = 0.9430
[Test_batch(6)(2000,14000)] 2017-05-03 11:54:30.003686: step 72000, loss = 0.1415, acc = 0.9485, f1neg = 0.9472, f1pos = 0.9498, f1 = 0.9485
[Test_batch(7)(2000,16000)] 2017-05-03 11:54:30.507813: step 72000, loss = 0.1330, acc = 0.9520, f1neg = 0.9567, f1pos = 0.9461, f1 = 0.9514
[Test_batch(8)(2000,18000)] 2017-05-03 11:54:30.977399: step 72000, loss = 0.1352, acc = 0.9500, f1neg = 0.9497, f1pos = 0.9503, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 11:54:31.461655: step 72000, loss = 0.1652, acc = 0.9390, f1neg = 0.9365, f1pos = 0.9413, f1 = 0.9389
[Test_batch(10)(2000,22000)] 2017-05-03 11:54:31.965399: step 72000, loss = 0.1441, acc = 0.9540, f1neg = 0.9486, f1pos = 0.9584, f1 = 0.9535
[Test_batch(11)(2000,24000)] 2017-05-03 11:54:32.435362: step 72000, loss = 0.1428, acc = 0.9510, f1neg = 0.9527, f1pos = 0.9491, f1 = 0.9509
[Test_batch(12)(2000,26000)] 2017-05-03 11:54:32.899322: step 72000, loss = 0.1183, acc = 0.9635, f1neg = 0.9651, f1pos = 0.9618, f1 = 0.9634
[Test_batch(13)(2000,28000)] 2017-05-03 11:54:33.369899: step 72000, loss = 0.1420, acc = 0.9525, f1neg = 0.9477, f1pos = 0.9565, f1 = 0.9521
[Test_batch(14)(2000,30000)] 2017-05-03 11:54:33.815031: step 72000, loss = 0.1185, acc = 0.9595, f1neg = 0.9537, f1pos = 0.9640, f1 = 0.9589
[Test_batch(15)(2000,32000)] 2017-05-03 11:54:34.279245: step 72000, loss = 0.1429, acc = 0.9575, f1neg = 0.9572, f1pos = 0.9578, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 11:54:34.763072: step 72000, loss = 0.1245, acc = 0.9575, f1neg = 0.9567, f1pos = 0.9582, f1 = 0.9575
[Test_batch(17)(2000,36000)] 2017-05-03 11:54:35.224242: step 72000, loss = 0.1143, acc = 0.9660, f1neg = 0.9630, f1pos = 0.9686, f1 = 0.9658
[Test_batch(18)(2000,38000)] 2017-05-03 11:54:35.740678: step 72000, loss = 0.1084, acc = 0.9655, f1neg = 0.9649, f1pos = 0.9661, f1 = 0.9655
[Test] 2017-05-03 11:54:35.740778: step 72000, acc = 0.9545, f1 = 0.9543
[Status] 2017-05-03 11:54:35.740805: step 72000, maxindex = 72000, maxdev = 0.9630, maxtst = 0.9545
2017-05-03 11:54:47.798569: step 72010, loss = 0.1286, acc = 0.9520 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 11:54:56.709919: step 72020, loss = 0.1150, acc = 0.9620 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 11:55:05.741170: step 72030, loss = 0.1187, acc = 0.9580 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 11:55:14.772210: step 72040, loss = 0.1108, acc = 0.9560 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 11:55:23.853551: step 72050, loss = 0.1176, acc = 0.9560 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 11:55:32.987872: step 72060, loss = 0.1144, acc = 0.9700 (298.5 examples/sec; 0.214 sec/batch)
2017-05-03 11:55:41.953490: step 72070, loss = 0.1168, acc = 0.9580 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 11:55:51.113972: step 72080, loss = 0.1292, acc = 0.9540 (265.7 examples/sec; 0.241 sec/batch)
2017-05-03 11:56:00.178841: step 72090, loss = 0.1288, acc = 0.9640 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 11:56:09.343585: step 72100, loss = 0.1253, acc = 0.9620 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 11:56:18.339567: step 72110, loss = 0.1381, acc = 0.9640 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 11:56:27.464128: step 72120, loss = 0.1049, acc = 0.9640 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 11:56:36.506490: step 72130, loss = 0.1281, acc = 0.9580 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 11:56:45.607971: step 72140, loss = 0.1430, acc = 0.9460 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 11:56:54.699150: step 72150, loss = 0.1139, acc = 0.9720 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 11:57:03.660211: step 72160, loss = 0.1000, acc = 0.9720 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 11:57:13.089792: step 72170, loss = 0.1215, acc = 0.9580 (244.3 examples/sec; 0.262 sec/batch)
2017-05-03 11:57:22.601814: step 72180, loss = 0.1233, acc = 0.9540 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 11:57:31.791609: step 72190, loss = 0.1337, acc = 0.9580 (266.6 examples/sec; 0.240 sec/batch)
2017-05-03 11:57:41.020528: step 72200, loss = 0.1224, acc = 0.9600 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 11:57:50.136515: step 72210, loss = 0.1513, acc = 0.9560 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 11:57:59.433737: step 72220, loss = 0.1245, acc = 0.9560 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 11:58:08.589356: step 72230, loss = 0.1375, acc = 0.9480 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 11:58:17.836451: step 72240, loss = 0.0988, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 11:58:26.915397: step 72250, loss = 0.1413, acc = 0.9480 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:58:36.089785: step 72260, loss = 0.1411, acc = 0.9560 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 11:58:45.137714: step 72270, loss = 0.0986, acc = 0.9740 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 11:58:54.205423: step 72280, loss = 0.1173, acc = 0.9720 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 11:59:03.191483: step 72290, loss = 0.1179, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 11:59:12.249179: step 72300, loss = 0.1149, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 11:59:21.277257: step 72310, loss = 0.1361, acc = 0.9580 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 11:59:30.299946: step 72320, loss = 0.1305, acc = 0.9640 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 11:59:39.286237: step 72330, loss = 0.1447, acc = 0.9460 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 11:59:48.583950: step 72340, loss = 0.1284, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 11:59:57.733996: step 72350, loss = 0.1072, acc = 0.9680 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 12:00:06.930711: step 72360, loss = 0.1103, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 12:00:16.061739: step 72370, loss = 0.1260, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 12:00:25.018851: step 72380, loss = 0.1300, acc = 0.9520 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 12:00:33.935898: step 72390, loss = 0.1264, acc = 0.9580 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 12:00:42.976320: step 72400, loss = 0.1384, acc = 0.9420 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 12:00:52.153664: step 72410, loss = 0.0950, acc = 0.9720 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 12:01:01.138333: step 72420, loss = 0.1090, acc = 0.9660 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 12:01:10.101652: step 72430, loss = 0.1154, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 12:01:19.258004: step 72440, loss = 0.1339, acc = 0.9600 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 12:01:28.393391: step 72450, loss = 0.1429, acc = 0.9500 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 12:01:37.686272: step 72460, loss = 0.1386, acc = 0.9620 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 12:01:46.882507: step 72470, loss = 0.1339, acc = 0.9560 (265.0 examples/sec; 0.241 sec/batch)
2017-05-03 12:01:56.052493: step 72480, loss = 0.1135, acc = 0.9620 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 12:02:05.183459: step 72490, loss = 0.1037, acc = 0.9640 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 12:02:14.494993: step 72500, loss = 0.1363, acc = 0.9560 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 12:02:23.517684: step 72510, loss = 0.0918, acc = 0.9820 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 12:02:32.491859: step 72520, loss = 0.1032, acc = 0.9640 (300.4 examples/sec; 0.213 sec/batch)
2017-05-03 12:02:41.598218: step 72530, loss = 0.1253, acc = 0.9660 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 12:02:50.527164: step 72540, loss = 0.1367, acc = 0.9520 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 12:02:59.432051: step 72550, loss = 0.0956, acc = 0.9740 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 12:03:08.383448: step 72560, loss = 0.1459, acc = 0.9520 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 12:03:17.465120: step 72570, loss = 0.1329, acc = 0.9560 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 12:03:27.087222: step 72580, loss = 0.1131, acc = 0.9600 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 12:03:35.844851: step 72590, loss = 0.0908, acc = 0.9800 (292.9 examples/sec; 0.219 sec/batch)
2017-05-03 12:03:44.853764: step 72600, loss = 0.1215, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 12:03:53.669102: step 72610, loss = 0.0786, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 12:04:02.670462: step 72620, loss = 0.1219, acc = 0.9540 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 12:04:11.614288: step 72630, loss = 0.1173, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 12:04:20.705879: step 72640, loss = 0.1444, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 12:04:29.838675: step 72650, loss = 0.1174, acc = 0.9700 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 12:04:39.263726: step 72660, loss = 0.1143, acc = 0.9600 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 12:04:48.592524: step 72670, loss = 0.1200, acc = 0.9640 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 12:04:57.806809: step 72680, loss = 0.1047, acc = 0.9700 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 12:05:06.716681: step 72690, loss = 0.1384, acc = 0.9520 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 12:05:15.782845: step 72700, loss = 0.1487, acc = 0.9600 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 12:05:24.992538: step 72710, loss = 0.0999, acc = 0.9740 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 12:05:33.976644: step 72720, loss = 0.1257, acc = 0.9600 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 12:05:43.001787: step 72730, loss = 0.1336, acc = 0.9600 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 12:05:51.971628: step 72740, loss = 0.0915, acc = 0.9780 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 12:06:01.004026: step 72750, loss = 0.1527, acc = 0.9500 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 12:06:10.057089: step 72760, loss = 0.1540, acc = 0.9500 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 12:06:19.056047: step 72770, loss = 0.1330, acc = 0.9540 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 12:06:28.083165: step 72780, loss = 0.1262, acc = 0.9580 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 12:06:37.215739: step 72790, loss = 0.1133, acc = 0.9660 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 12:06:46.229701: step 72800, loss = 0.1454, acc = 0.9520 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 12:06:55.147853: step 72810, loss = 0.1266, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 12:07:04.067666: step 72820, loss = 0.1069, acc = 0.9700 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 12:07:13.030514: step 72830, loss = 0.1488, acc = 0.9520 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 12:07:22.092366: step 72840, loss = 0.1305, acc = 0.9440 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 12:07:31.078445: step 72850, loss = 0.1425, acc = 0.9600 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 12:07:39.895953: step 72860, loss = 0.1067, acc = 0.9740 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 12:07:48.821870: step 72870, loss = 0.0996, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 12:07:57.803588: step 72880, loss = 0.1039, acc = 0.9640 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 12:08:06.804934: step 72890, loss = 0.1198, acc = 0.9680 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 12:08:15.834820: step 72900, loss = 0.1195, acc = 0.9520 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 12:08:24.854408: step 72910, loss = 0.1339, acc = 0.9520 (297.3 examples/sec; 0.215 sec/batch)
2017-05-03 12:08:34.021604: step 72920, loss = 0.1283, acc = 0.9620 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 12:08:42.990949: step 72930, loss = 0.1165, acc = 0.9660 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 12:08:51.905105: step 72940, loss = 0.1194, acc = 0.9680 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 12:09:00.986024: step 72950, loss = 0.1002, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 12:09:09.990681: step 72960, loss = 0.1078, acc = 0.9680 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 12:09:19.060200: step 72970, loss = 0.1121, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 12:09:28.143015: step 72980, loss = 0.1193, acc = 0.9560 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 12:09:36.976099: step 72990, loss = 0.1298, acc = 0.9560 (299.2 examples/sec; 0.214 sec/batch)
2017-05-03 12:09:45.983051: step 73000, loss = 0.1216, acc = 0.9560 (288.9 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 12:09:46.457886: step 73000, loss = 0.1056, acc = 0.9610, f1neg = 0.9573, f1pos = 0.9641, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 12:09:46.926578: step 73000, loss = 0.1105, acc = 0.9670, f1neg = 0.9617, f1pos = 0.9710, f1 = 0.9663
[Eval_batch(2)(2000,6000)] 2017-05-03 12:09:47.403157: step 73000, loss = 0.1199, acc = 0.9580, f1neg = 0.9556, f1pos = 0.9602, f1 = 0.9579
[Eval_batch(3)(2000,8000)] 2017-05-03 12:09:47.885935: step 73000, loss = 0.1249, acc = 0.9590, f1neg = 0.9591, f1pos = 0.9589, f1 = 0.9590
[Eval_batch(4)(2000,10000)] 2017-05-03 12:09:48.392497: step 73000, loss = 0.1270, acc = 0.9585, f1neg = 0.9598, f1pos = 0.9571, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 12:09:48.897287: step 73000, loss = 0.1245, acc = 0.9550, f1neg = 0.9518, f1pos = 0.9578, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 12:09:49.375243: step 73000, loss = 0.1155, acc = 0.9685, f1neg = 0.9680, f1pos = 0.9690, f1 = 0.9685
[Eval_batch(7)(2000,16000)] 2017-05-03 12:09:49.842216: step 73000, loss = 0.1164, acc = 0.9605, f1neg = 0.9530, f1pos = 0.9659, f1 = 0.9595
[Eval_batch(8)(2000,18000)] 2017-05-03 12:09:50.345927: step 73000, loss = 0.1081, acc = 0.9625, f1neg = 0.9574, f1pos = 0.9665, f1 = 0.9620
[Eval_batch(9)(2000,20000)] 2017-05-03 12:09:50.847951: step 73000, loss = 0.1133, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9667, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 12:09:51.344493: step 73000, loss = 0.1210, acc = 0.9595, f1neg = 0.9569, f1pos = 0.9618, f1 = 0.9594
[Eval_batch(11)(2000,24000)] 2017-05-03 12:09:51.842059: step 73000, loss = 0.1211, acc = 0.9620, f1neg = 0.9563, f1pos = 0.9664, f1 = 0.9613
[Eval_batch(12)(2000,26000)] 2017-05-03 12:09:52.348224: step 73000, loss = 0.1184, acc = 0.9550, f1neg = 0.9538, f1pos = 0.9561, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 12:09:52.852605: step 73000, loss = 0.1088, acc = 0.9665, f1neg = 0.9582, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 12:09:53.364377: step 73000, loss = 0.1361, acc = 0.9560, f1neg = 0.9479, f1pos = 0.9619, f1 = 0.9549
[Eval_batch(15)(2000,32000)] 2017-05-03 12:09:53.850944: step 73000, loss = 0.1111, acc = 0.9690, f1neg = 0.9665, f1pos = 0.9711, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 12:09:54.355229: step 73000, loss = 0.1049, acc = 0.9705, f1neg = 0.9694, f1pos = 0.9715, f1 = 0.9705
[Eval_batch(17)(2000,36000)] 2017-05-03 12:09:54.849487: step 73000, loss = 0.1341, acc = 0.9590, f1neg = 0.9593, f1pos = 0.9587, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 12:09:55.347245: step 73000, loss = 0.1264, acc = 0.9600, f1neg = 0.9617, f1pos = 0.9581, f1 = 0.9599
[Eval_batch(19)(2000,40000)] 2017-05-03 12:09:55.845164: step 73000, loss = 0.1105, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 12:09:56.332181: step 73000, loss = 0.1130, acc = 0.9625, f1neg = 0.9668, f1pos = 0.9570, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 12:09:56.809388: step 73000, loss = 0.1061, acc = 0.9645, f1neg = 0.9615, f1pos = 0.9671, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 12:09:57.315724: step 73000, loss = 0.1134, acc = 0.9600, f1neg = 0.9605, f1pos = 0.9595, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 12:09:57.824069: step 73000, loss = 0.1194, acc = 0.9620, f1neg = 0.9573, f1pos = 0.9658, f1 = 0.9615
[Eval_batch(24)(2000,50000)] 2017-05-03 12:09:58.337876: step 73000, loss = 0.1037, acc = 0.9660, f1neg = 0.9611, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 12:09:58.842481: step 73000, loss = 0.0955, acc = 0.9725, f1neg = 0.9699, f1pos = 0.9747, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 12:09:59.317749: step 73000, loss = 0.1102, acc = 0.9615, f1neg = 0.9637, f1pos = 0.9590, f1 = 0.9614
[Eval_batch(27)(2000,56000)] 2017-05-03 12:09:59.899494: step 73000, loss = 0.0964, acc = 0.9725, f1neg = 0.9712, f1pos = 0.9737, f1 = 0.9724
[Eval] 2017-05-03 12:09:59.899583: step 73000, acc = 0.9629, f1 = 0.9626
[Test_batch(0)(2000,2000)] 2017-05-03 12:10:00.379158: step 73000, loss = 0.1453, acc = 0.9530, f1neg = 0.9554, f1pos = 0.9503, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 12:10:00.877798: step 73000, loss = 0.1249, acc = 0.9615, f1neg = 0.9659, f1pos = 0.9559, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 12:10:01.352633: step 73000, loss = 0.1408, acc = 0.9590, f1neg = 0.9624, f1pos = 0.9549, f1 = 0.9587
[Test_batch(3)(2000,8000)] 2017-05-03 12:10:01.857172: step 73000, loss = 0.1485, acc = 0.9500, f1neg = 0.9532, f1pos = 0.9463, f1 = 0.9498
[Test_batch(4)(2000,10000)] 2017-05-03 12:10:02.337173: step 73000, loss = 0.1427, acc = 0.9520, f1neg = 0.9523, f1pos = 0.9517, f1 = 0.9520
[Test_batch(5)(2000,12000)] 2017-05-03 12:10:02.843240: step 73000, loss = 0.1570, acc = 0.9440, f1neg = 0.9448, f1pos = 0.9432, f1 = 0.9440
[Test_batch(6)(2000,14000)] 2017-05-03 12:10:03.346093: step 73000, loss = 0.1415, acc = 0.9485, f1neg = 0.9468, f1pos = 0.9501, f1 = 0.9484
[Test_batch(7)(2000,16000)] 2017-05-03 12:10:03.816641: step 73000, loss = 0.1334, acc = 0.9515, f1neg = 0.9560, f1pos = 0.9459, f1 = 0.9510
[Test_batch(8)(2000,18000)] 2017-05-03 12:10:04.296230: step 73000, loss = 0.1342, acc = 0.9505, f1neg = 0.9499, f1pos = 0.9511, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 12:10:04.803828: step 73000, loss = 0.1632, acc = 0.9385, f1neg = 0.9356, f1pos = 0.9411, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 12:10:05.307949: step 73000, loss = 0.1411, acc = 0.9530, f1neg = 0.9472, f1pos = 0.9577, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 12:10:05.793675: step 73000, loss = 0.1448, acc = 0.9515, f1neg = 0.9530, f1pos = 0.9499, f1 = 0.9515
[Test_batch(12)(2000,26000)] 2017-05-03 12:10:06.276131: step 73000, loss = 0.1177, acc = 0.9655, f1neg = 0.9669, f1pos = 0.9640, f1 = 0.9654
[Test_batch(13)(2000,28000)] 2017-05-03 12:10:06.746696: step 73000, loss = 0.1404, acc = 0.9555, f1neg = 0.9509, f1pos = 0.9593, f1 = 0.9551
[Test_batch(14)(2000,30000)] 2017-05-03 12:10:07.260155: step 73000, loss = 0.1175, acc = 0.9605, f1neg = 0.9546, f1pos = 0.9650, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 12:10:07.770362: step 73000, loss = 0.1427, acc = 0.9575, f1neg = 0.9571, f1pos = 0.9579, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 12:10:08.283038: step 73000, loss = 0.1227, acc = 0.9585, f1neg = 0.9575, f1pos = 0.9594, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 12:10:08.781353: step 73000, loss = 0.1140, acc = 0.9665, f1neg = 0.9634, f1pos = 0.9691, f1 = 0.9663
[Test_batch(18)(2000,38000)] 2017-05-03 12:10:09.352566: step 73000, loss = 0.1093, acc = 0.9655, f1neg = 0.9648, f1pos = 0.9661, f1 = 0.9655
[Test] 2017-05-03 12:10:09.352653: step 73000, acc = 0.9549, f1 = 0.9547
[Status] 2017-05-03 12:10:09.352677: step 73000, maxindex = 72000, maxdev = 0.9630, maxtst = 0.9545
2017-05-03 12:10:18.420591: step 73010, loss = 0.1220, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 12:10:27.366736: step 73020, loss = 0.1380, acc = 0.9560 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 12:10:36.506456: step 73030, loss = 0.1051, acc = 0.9680 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 12:10:45.535917: step 73040, loss = 0.1205, acc = 0.9620 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 12:10:54.642611: step 73050, loss = 0.1285, acc = 0.9500 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 12:11:03.739959: step 73060, loss = 0.1218, acc = 0.9660 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 12:11:12.741101: step 73070, loss = 0.1307, acc = 0.9560 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 12:11:21.602062: step 73080, loss = 0.0928, acc = 0.9780 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 12:11:30.584382: step 73090, loss = 0.1298, acc = 0.9580 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 12:11:39.683422: step 73100, loss = 0.1435, acc = 0.9500 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 12:11:48.674500: step 73110, loss = 0.1282, acc = 0.9660 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 12:11:57.633717: step 73120, loss = 0.1202, acc = 0.9600 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 12:12:06.647766: step 73130, loss = 0.1017, acc = 0.9720 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 12:12:15.708431: step 73140, loss = 0.1074, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 12:12:24.753017: step 73150, loss = 0.1006, acc = 0.9700 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 12:12:33.701413: step 73160, loss = 0.1147, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 12:12:42.809686: step 73170, loss = 0.1053, acc = 0.9740 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 12:12:51.934696: step 73180, loss = 0.1254, acc = 0.9700 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 12:13:01.021830: step 73190, loss = 0.1237, acc = 0.9600 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 12:13:09.958186: step 73200, loss = 0.1020, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 12:13:19.075682: step 73210, loss = 0.1343, acc = 0.9460 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 12:13:28.116946: step 73220, loss = 0.1301, acc = 0.9540 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 12:13:37.211190: step 73230, loss = 0.1366, acc = 0.9460 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 12:13:46.396448: step 73240, loss = 0.1583, acc = 0.9500 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 12:13:55.592935: step 73250, loss = 0.1293, acc = 0.9600 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 12:14:04.855545: step 73260, loss = 0.1225, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 12:14:13.939650: step 73270, loss = 0.1098, acc = 0.9740 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 12:14:23.036000: step 73280, loss = 0.1119, acc = 0.9720 (240.3 examples/sec; 0.266 sec/batch)
2017-05-03 12:14:32.048776: step 73290, loss = 0.1370, acc = 0.9660 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 12:14:41.184497: step 73300, loss = 0.1039, acc = 0.9760 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 12:14:50.180410: step 73310, loss = 0.1075, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 12:14:59.095861: step 73320, loss = 0.1568, acc = 0.9520 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 12:15:08.155587: step 73330, loss = 0.1656, acc = 0.9480 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 12:15:17.180162: step 73340, loss = 0.1242, acc = 0.9640 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 12:15:26.150785: step 73350, loss = 0.1337, acc = 0.9540 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 12:15:35.064825: step 73360, loss = 0.1057, acc = 0.9700 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 12:15:44.228456: step 73370, loss = 0.1169, acc = 0.9620 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 12:15:53.045273: step 73380, loss = 0.1003, acc = 0.9760 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 12:16:02.164448: step 73390, loss = 0.1190, acc = 0.9660 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 12:16:11.302202: step 73400, loss = 0.1017, acc = 0.9660 (266.0 examples/sec; 0.241 sec/batch)
2017-05-03 12:16:20.299096: step 73410, loss = 0.1244, acc = 0.9640 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 12:16:29.384756: step 73420, loss = 0.1330, acc = 0.9580 (270.1 examples/sec; 0.237 sec/batch)
2017-05-03 12:16:38.516316: step 73430, loss = 0.1240, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 12:16:47.530494: step 73440, loss = 0.1059, acc = 0.9660 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 12:16:56.626084: step 73450, loss = 0.1192, acc = 0.9600 (244.9 examples/sec; 0.261 sec/batch)
2017-05-03 12:17:06.284984: step 73460, loss = 0.1077, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 12:17:15.253359: step 73470, loss = 0.1396, acc = 0.9580 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 12:17:24.420617: step 73480, loss = 0.1160, acc = 0.9700 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 12:17:34.591653: step 73490, loss = 0.1330, acc = 0.9500 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 12:17:43.679940: step 73500, loss = 0.1224, acc = 0.9640 (261.2 examples/sec; 0.245 sec/batch)
2017-05-03 12:17:52.627139: step 73510, loss = 0.0960, acc = 0.9800 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 12:18:01.708135: step 73520, loss = 0.1206, acc = 0.9560 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 12:18:11.015888: step 73530, loss = 0.1211, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 12:18:20.078428: step 73540, loss = 0.1380, acc = 0.9520 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 12:18:29.036032: step 73550, loss = 0.1085, acc = 0.9720 (304.4 examples/sec; 0.210 sec/batch)
2017-05-03 12:18:38.189835: step 73560, loss = 0.1092, acc = 0.9680 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 12:18:47.276740: step 73570, loss = 0.1244, acc = 0.9600 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 12:18:56.269286: step 73580, loss = 0.1262, acc = 0.9580 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 12:19:06.231044: step 73590, loss = 0.0992, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 12:19:15.313090: step 73600, loss = 0.1238, acc = 0.9640 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 12:19:24.400583: step 73610, loss = 0.1292, acc = 0.9600 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 12:19:33.609047: step 73620, loss = 0.1291, acc = 0.9640 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 12:19:42.583835: step 73630, loss = 0.1046, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 12:19:51.446722: step 73640, loss = 0.1226, acc = 0.9660 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 12:20:00.528488: step 73650, loss = 0.1101, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 12:20:09.499289: step 73660, loss = 0.1294, acc = 0.9620 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 12:20:18.487364: step 73670, loss = 0.1144, acc = 0.9580 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 12:20:27.543534: step 73680, loss = 0.1439, acc = 0.9580 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 12:20:36.932610: step 73690, loss = 0.1357, acc = 0.9560 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 12:20:45.794788: step 73700, loss = 0.1503, acc = 0.9480 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 12:20:54.843883: step 73710, loss = 0.1044, acc = 0.9720 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 12:21:03.772926: step 73720, loss = 0.0996, acc = 0.9660 (268.7 examples/sec; 0.238 sec/batch)
2017-05-03 12:21:12.738171: step 73730, loss = 0.1256, acc = 0.9580 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 12:21:21.964752: step 73740, loss = 0.0987, acc = 0.9680 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 12:21:31.187488: step 73750, loss = 0.1262, acc = 0.9680 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 12:21:40.621931: step 73760, loss = 0.1290, acc = 0.9660 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 12:21:49.896113: step 73770, loss = 0.1188, acc = 0.9640 (240.1 examples/sec; 0.267 sec/batch)
2017-05-03 12:21:58.973020: step 73780, loss = 0.1521, acc = 0.9460 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 12:22:07.959585: step 73790, loss = 0.1018, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 12:22:16.923109: step 73800, loss = 0.1269, acc = 0.9640 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 12:22:25.869279: step 73810, loss = 0.1585, acc = 0.9420 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 12:22:34.838588: step 73820, loss = 0.1042, acc = 0.9720 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 12:22:43.869878: step 73830, loss = 0.1150, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 12:22:52.930468: step 73840, loss = 0.1112, acc = 0.9600 (264.5 examples/sec; 0.242 sec/batch)
2017-05-03 12:23:02.036147: step 73850, loss = 0.1275, acc = 0.9660 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 12:23:11.039874: step 73860, loss = 0.1202, acc = 0.9580 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 12:23:19.991849: step 73870, loss = 0.0983, acc = 0.9700 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 12:23:29.066763: step 73880, loss = 0.1076, acc = 0.9660 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 12:23:38.264432: step 73890, loss = 0.1294, acc = 0.9500 (266.4 examples/sec; 0.240 sec/batch)
2017-05-03 12:23:47.168313: step 73900, loss = 0.0896, acc = 0.9820 (303.3 examples/sec; 0.211 sec/batch)
2017-05-03 12:23:56.182436: step 73910, loss = 0.1018, acc = 0.9720 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 12:24:05.255880: step 73920, loss = 0.1098, acc = 0.9640 (261.1 examples/sec; 0.245 sec/batch)
2017-05-03 12:24:14.213130: step 73930, loss = 0.1106, acc = 0.9700 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 12:24:23.278243: step 73940, loss = 0.1082, acc = 0.9740 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 12:24:32.402805: step 73950, loss = 0.1096, acc = 0.9660 (296.7 examples/sec; 0.216 sec/batch)
2017-05-03 12:24:41.313028: step 73960, loss = 0.1141, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 12:24:50.310914: step 73970, loss = 0.1281, acc = 0.9640 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 12:24:59.422878: step 73980, loss = 0.1241, acc = 0.9600 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 12:25:08.516755: step 73990, loss = 0.0975, acc = 0.9640 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 12:25:17.553325: step 74000, loss = 0.1133, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 12:25:18.077142: step 74000, loss = 0.1083, acc = 0.9585, f1neg = 0.9543, f1pos = 0.9620, f1 = 0.9581
[Eval_batch(1)(2000,4000)] 2017-05-03 12:25:18.517924: step 74000, loss = 0.1099, acc = 0.9655, f1neg = 0.9595, f1pos = 0.9699, f1 = 0.9647
[Eval_batch(2)(2000,6000)] 2017-05-03 12:25:18.991278: step 74000, loss = 0.1225, acc = 0.9595, f1neg = 0.9567, f1pos = 0.9620, f1 = 0.9593
[Eval_batch(3)(2000,8000)] 2017-05-03 12:25:19.472701: step 74000, loss = 0.1275, acc = 0.9575, f1neg = 0.9572, f1pos = 0.9578, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 12:25:19.954754: step 74000, loss = 0.1300, acc = 0.9605, f1neg = 0.9614, f1pos = 0.9595, f1 = 0.9605
[Eval_batch(5)(2000,12000)] 2017-05-03 12:25:20.449163: step 74000, loss = 0.1269, acc = 0.9555, f1neg = 0.9520, f1pos = 0.9585, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 12:25:20.925087: step 74000, loss = 0.1185, acc = 0.9650, f1neg = 0.9643, f1pos = 0.9657, f1 = 0.9650
[Eval_batch(7)(2000,16000)] 2017-05-03 12:25:21.404604: step 74000, loss = 0.1158, acc = 0.9615, f1neg = 0.9537, f1pos = 0.9671, f1 = 0.9604
[Eval_batch(8)(2000,18000)] 2017-05-03 12:25:21.873180: step 74000, loss = 0.1091, acc = 0.9615, f1neg = 0.9559, f1pos = 0.9659, f1 = 0.9609
[Eval_batch(9)(2000,20000)] 2017-05-03 12:25:22.320396: step 74000, loss = 0.1144, acc = 0.9650, f1neg = 0.9623, f1pos = 0.9673, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 12:25:22.788070: step 74000, loss = 0.1217, acc = 0.9605, f1neg = 0.9575, f1pos = 0.9631, f1 = 0.9603
[Eval_batch(11)(2000,24000)] 2017-05-03 12:25:23.256474: step 74000, loss = 0.1214, acc = 0.9610, f1neg = 0.9549, f1pos = 0.9656, f1 = 0.9603
[Eval_batch(12)(2000,26000)] 2017-05-03 12:25:23.765194: step 74000, loss = 0.1201, acc = 0.9550, f1neg = 0.9534, f1pos = 0.9565, f1 = 0.9549
[Eval_batch(13)(2000,28000)] 2017-05-03 12:25:24.277818: step 74000, loss = 0.1098, acc = 0.9655, f1neg = 0.9567, f1pos = 0.9713, f1 = 0.9640
[Eval_batch(14)(2000,30000)] 2017-05-03 12:25:24.778212: step 74000, loss = 0.1368, acc = 0.9560, f1neg = 0.9472, f1pos = 0.9623, f1 = 0.9548
[Eval_batch(15)(2000,32000)] 2017-05-03 12:25:25.284973: step 74000, loss = 0.1108, acc = 0.9700, f1neg = 0.9675, f1pos = 0.9721, f1 = 0.9698
[Eval_batch(16)(2000,34000)] 2017-05-03 12:25:25.789263: step 74000, loss = 0.1061, acc = 0.9685, f1neg = 0.9670, f1pos = 0.9698, f1 = 0.9684
[Eval_batch(17)(2000,36000)] 2017-05-03 12:25:26.293442: step 74000, loss = 0.1353, acc = 0.9570, f1neg = 0.9572, f1pos = 0.9568, f1 = 0.9570
[Eval_batch(18)(2000,38000)] 2017-05-03 12:25:26.801433: step 74000, loss = 0.1303, acc = 0.9560, f1neg = 0.9576, f1pos = 0.9543, f1 = 0.9559
[Eval_batch(19)(2000,40000)] 2017-05-03 12:25:27.284196: step 74000, loss = 0.1118, acc = 0.9680, f1neg = 0.9629, f1pos = 0.9719, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 12:25:27.789444: step 74000, loss = 0.1168, acc = 0.9620, f1neg = 0.9662, f1pos = 0.9566, f1 = 0.9614
[Eval_batch(21)(2000,44000)] 2017-05-03 12:25:28.289662: step 74000, loss = 0.1033, acc = 0.9655, f1neg = 0.9624, f1pos = 0.9682, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 12:25:28.753814: step 74000, loss = 0.1148, acc = 0.9600, f1neg = 0.9602, f1pos = 0.9598, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 12:25:29.219117: step 74000, loss = 0.1196, acc = 0.9585, f1neg = 0.9529, f1pos = 0.9629, f1 = 0.9579
[Eval_batch(24)(2000,50000)] 2017-05-03 12:25:29.701067: step 74000, loss = 0.1031, acc = 0.9680, f1neg = 0.9632, f1pos = 0.9717, f1 = 0.9674
[Eval_batch(25)(2000,52000)] 2017-05-03 12:25:30.205270: step 74000, loss = 0.0971, acc = 0.9740, f1neg = 0.9714, f1pos = 0.9762, f1 = 0.9738
[Eval_batch(26)(2000,54000)] 2017-05-03 12:25:30.671203: step 74000, loss = 0.1123, acc = 0.9610, f1neg = 0.9630, f1pos = 0.9587, f1 = 0.9609
[Eval_batch(27)(2000,56000)] 2017-05-03 12:25:31.221524: step 74000, loss = 0.0991, acc = 0.9680, f1neg = 0.9662, f1pos = 0.9696, f1 = 0.9679
[Eval] 2017-05-03 12:25:31.221602: step 74000, acc = 0.9623, f1 = 0.9619
[Test_batch(0)(2000,2000)] 2017-05-03 12:25:31.700939: step 74000, loss = 0.1475, acc = 0.9525, f1neg = 0.9547, f1pos = 0.9501, f1 = 0.9524
[Test_batch(1)(2000,4000)] 2017-05-03 12:25:32.205188: step 74000, loss = 0.1268, acc = 0.9595, f1neg = 0.9639, f1pos = 0.9540, f1 = 0.9589
[Test_batch(2)(2000,6000)] 2017-05-03 12:25:32.668340: step 74000, loss = 0.1453, acc = 0.9540, f1neg = 0.9574, f1pos = 0.9500, f1 = 0.9537
[Test_batch(3)(2000,8000)] 2017-05-03 12:25:33.148534: step 74000, loss = 0.1499, acc = 0.9495, f1neg = 0.9523, f1pos = 0.9464, f1 = 0.9493
[Test_batch(4)(2000,10000)] 2017-05-03 12:25:33.629098: step 74000, loss = 0.1448, acc = 0.9500, f1neg = 0.9499, f1pos = 0.9501, f1 = 0.9500
[Test_batch(5)(2000,12000)] 2017-05-03 12:25:34.109265: step 74000, loss = 0.1606, acc = 0.9410, f1neg = 0.9413, f1pos = 0.9407, f1 = 0.9410
[Test_batch(6)(2000,14000)] 2017-05-03 12:25:34.574832: step 74000, loss = 0.1458, acc = 0.9475, f1neg = 0.9453, f1pos = 0.9495, f1 = 0.9474
[Test_batch(7)(2000,16000)] 2017-05-03 12:25:35.080319: step 74000, loss = 0.1361, acc = 0.9545, f1neg = 0.9585, f1pos = 0.9497, f1 = 0.9541
[Test_batch(8)(2000,18000)] 2017-05-03 12:25:35.571791: step 74000, loss = 0.1359, acc = 0.9495, f1neg = 0.9487, f1pos = 0.9503, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 12:25:36.076450: step 74000, loss = 0.1626, acc = 0.9370, f1neg = 0.9333, f1pos = 0.9403, f1 = 0.9368
[Test_batch(10)(2000,22000)] 2017-05-03 12:25:36.541734: step 74000, loss = 0.1404, acc = 0.9510, f1neg = 0.9446, f1pos = 0.9561, f1 = 0.9503
[Test_batch(11)(2000,24000)] 2017-05-03 12:25:37.023557: step 74000, loss = 0.1453, acc = 0.9505, f1neg = 0.9517, f1pos = 0.9492, f1 = 0.9505
[Test_batch(12)(2000,26000)] 2017-05-03 12:25:37.505771: step 74000, loss = 0.1180, acc = 0.9645, f1neg = 0.9658, f1pos = 0.9632, f1 = 0.9645
[Test_batch(13)(2000,28000)] 2017-05-03 12:25:37.978549: step 74000, loss = 0.1409, acc = 0.9510, f1neg = 0.9451, f1pos = 0.9558, f1 = 0.9504
[Test_batch(14)(2000,30000)] 2017-05-03 12:25:38.468928: step 74000, loss = 0.1175, acc = 0.9615, f1neg = 0.9555, f1pos = 0.9661, f1 = 0.9608
[Test_batch(15)(2000,32000)] 2017-05-03 12:25:38.947396: step 74000, loss = 0.1431, acc = 0.9555, f1neg = 0.9547, f1pos = 0.9563, f1 = 0.9555
[Test_batch(16)(2000,34000)] 2017-05-03 12:25:39.430058: step 74000, loss = 0.1241, acc = 0.9585, f1neg = 0.9572, f1pos = 0.9597, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 12:25:39.912353: step 74000, loss = 0.1162, acc = 0.9685, f1neg = 0.9654, f1pos = 0.9711, f1 = 0.9682
[Test_batch(18)(2000,38000)] 2017-05-03 12:25:40.521282: step 74000, loss = 0.1116, acc = 0.9650, f1neg = 0.9642, f1pos = 0.9657, f1 = 0.9650
[Test] 2017-05-03 12:25:40.521354: step 74000, acc = 0.9537, f1 = 0.9535
[Status] 2017-05-03 12:25:40.521369: step 74000, maxindex = 72000, maxdev = 0.9630, maxtst = 0.9545
2017-05-03 12:25:49.378789: step 74010, loss = 0.1259, acc = 0.9620 (296.9 examples/sec; 0.216 sec/batch)
2017-05-03 12:25:58.533130: step 74020, loss = 0.1083, acc = 0.9660 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 12:26:07.494932: step 74030, loss = 0.1185, acc = 0.9620 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 12:26:16.599492: step 74040, loss = 0.0995, acc = 0.9740 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 12:26:25.803895: step 74050, loss = 0.1399, acc = 0.9540 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 12:26:34.722310: step 74060, loss = 0.1184, acc = 0.9560 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 12:26:43.694417: step 74070, loss = 0.1220, acc = 0.9560 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 12:26:52.897284: step 74080, loss = 0.1265, acc = 0.9520 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 12:27:02.020208: step 74090, loss = 0.1286, acc = 0.9560 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 12:27:11.132164: step 74100, loss = 0.1401, acc = 0.9640 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 12:27:20.425526: step 74110, loss = 0.1240, acc = 0.9660 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 12:27:29.706227: step 74120, loss = 0.0934, acc = 0.9800 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 12:27:38.741678: step 74130, loss = 0.1264, acc = 0.9660 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 12:27:47.839501: step 74140, loss = 0.1069, acc = 0.9640 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 12:27:56.876232: step 74150, loss = 0.1198, acc = 0.9660 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 12:28:05.902786: step 74160, loss = 0.1135, acc = 0.9600 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 12:28:14.962837: step 74170, loss = 0.1143, acc = 0.9620 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 12:28:24.103717: step 74180, loss = 0.1007, acc = 0.9680 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 12:28:33.207344: step 74190, loss = 0.1132, acc = 0.9600 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 12:28:42.274856: step 74200, loss = 0.1283, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 12:28:51.262892: step 74210, loss = 0.1150, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 12:29:00.293423: step 74220, loss = 0.1162, acc = 0.9600 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 12:29:09.654844: step 74230, loss = 0.1044, acc = 0.9680 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 12:29:18.791423: step 74240, loss = 0.1094, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 12:29:28.071616: step 74250, loss = 0.1285, acc = 0.9560 (233.4 examples/sec; 0.274 sec/batch)
2017-05-03 12:29:37.141128: step 74260, loss = 0.1198, acc = 0.9620 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 12:29:46.130586: step 74270, loss = 0.1036, acc = 0.9660 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 12:29:55.086011: step 74280, loss = 0.1128, acc = 0.9640 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 12:30:04.139316: step 74290, loss = 0.0919, acc = 0.9840 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 12:30:13.153974: step 74300, loss = 0.1495, acc = 0.9460 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 12:30:22.133478: step 74310, loss = 0.1761, acc = 0.9360 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 12:30:31.053242: step 74320, loss = 0.1065, acc = 0.9640 (299.7 examples/sec; 0.214 sec/batch)
2017-05-03 12:30:40.121406: step 74330, loss = 0.1243, acc = 0.9560 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 12:30:49.147460: step 74340, loss = 0.1160, acc = 0.9520 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 12:30:58.299764: step 74350, loss = 0.1479, acc = 0.9580 (272.3 examples/sec; 0.235 sec/batch)
2017-05-03 12:31:07.298647: step 74360, loss = 0.1075, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 12:31:16.521155: step 74370, loss = 0.1381, acc = 0.9620 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 12:31:25.561656: step 74380, loss = 0.1161, acc = 0.9620 (263.7 examples/sec; 0.243 sec/batch)
2017-05-03 12:31:34.576546: step 74390, loss = 0.1267, acc = 0.9580 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 12:31:43.836880: step 74400, loss = 0.1010, acc = 0.9760 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 12:31:52.820505: step 74410, loss = 0.0974, acc = 0.9700 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 12:32:01.986326: step 74420, loss = 0.1090, acc = 0.9740 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 12:32:11.144923: step 74430, loss = 0.1370, acc = 0.9620 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 12:32:20.306174: step 74440, loss = 0.1541, acc = 0.9460 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 12:32:29.411629: step 74450, loss = 0.1141, acc = 0.9520 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 12:32:38.952897: step 74460, loss = 0.1160, acc = 0.9640 (252.3 examples/sec; 0.254 sec/batch)
2017-05-03 12:32:48.038275: step 74470, loss = 0.1054, acc = 0.9680 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 12:32:57.086978: step 74480, loss = 0.1247, acc = 0.9580 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 12:33:06.112586: step 74490, loss = 0.1439, acc = 0.9440 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 12:33:15.265508: step 74500, loss = 0.1231, acc = 0.9560 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 12:33:24.369897: step 74510, loss = 0.1058, acc = 0.9700 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 12:33:33.485094: step 74520, loss = 0.1460, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 12:33:42.517965: step 74530, loss = 0.1425, acc = 0.9480 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 12:33:51.446792: step 74540, loss = 0.1164, acc = 0.9600 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 12:34:00.710625: step 74550, loss = 0.1033, acc = 0.9700 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 12:34:09.604015: step 74560, loss = 0.1368, acc = 0.9600 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 12:34:18.636520: step 74570, loss = 0.0975, acc = 0.9680 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 12:34:27.692921: step 74580, loss = 0.1221, acc = 0.9660 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 12:34:36.773552: step 74590, loss = 0.1019, acc = 0.9680 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 12:34:46.389145: step 74600, loss = 0.1235, acc = 0.9660 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 12:34:55.632452: step 74610, loss = 0.1263, acc = 0.9560 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 12:35:04.643080: step 74620, loss = 0.1225, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 12:35:13.636958: step 74630, loss = 0.0968, acc = 0.9700 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 12:35:22.765768: step 74640, loss = 0.1185, acc = 0.9720 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 12:35:31.757423: step 74650, loss = 0.1399, acc = 0.9540 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 12:35:41.062912: step 74660, loss = 0.1130, acc = 0.9680 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 12:35:50.045715: step 74670, loss = 0.1329, acc = 0.9560 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 12:35:59.072466: step 74680, loss = 0.1097, acc = 0.9640 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 12:36:08.257448: step 74690, loss = 0.1172, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 12:36:17.385681: step 74700, loss = 0.1204, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 12:36:26.856933: step 74710, loss = 0.0936, acc = 0.9760 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 12:36:35.909202: step 74720, loss = 0.0937, acc = 0.9780 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 12:36:44.955364: step 74730, loss = 0.1232, acc = 0.9600 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 12:36:54.029092: step 74740, loss = 0.0982, acc = 0.9840 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 12:37:03.165695: step 74750, loss = 0.1152, acc = 0.9700 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 12:37:12.203857: step 74760, loss = 0.1400, acc = 0.9640 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 12:37:21.151404: step 74770, loss = 0.0960, acc = 0.9840 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 12:37:30.291587: step 74780, loss = 0.1152, acc = 0.9680 (245.8 examples/sec; 0.260 sec/batch)
2017-05-03 12:37:39.260918: step 74790, loss = 0.1213, acc = 0.9480 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 12:37:48.357042: step 74800, loss = 0.1349, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 12:37:57.587730: step 74810, loss = 0.1216, acc = 0.9700 (251.9 examples/sec; 0.254 sec/batch)
2017-05-03 12:38:06.778240: step 74820, loss = 0.1383, acc = 0.9560 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 12:38:15.905063: step 74830, loss = 0.1244, acc = 0.9580 (265.9 examples/sec; 0.241 sec/batch)
2017-05-03 12:38:25.091228: step 74840, loss = 0.1120, acc = 0.9720 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 12:38:34.050344: step 74850, loss = 0.1316, acc = 0.9560 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 12:38:43.090797: step 74860, loss = 0.1193, acc = 0.9620 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 12:38:52.148096: step 74870, loss = 0.1282, acc = 0.9520 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 12:39:01.200132: step 74880, loss = 0.1451, acc = 0.9520 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 12:39:10.255285: step 74890, loss = 0.1484, acc = 0.9520 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 12:39:19.292338: step 74900, loss = 0.1353, acc = 0.9500 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 12:39:28.440672: step 74910, loss = 0.1049, acc = 0.9640 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 12:39:37.402534: step 74920, loss = 0.1107, acc = 0.9640 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 12:39:46.440921: step 74930, loss = 0.1269, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 12:39:55.404881: step 74940, loss = 0.0917, acc = 0.9740 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 12:40:04.307025: step 74950, loss = 0.1232, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 12:40:13.270365: step 74960, loss = 0.1281, acc = 0.9560 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 12:40:22.460298: step 74970, loss = 0.1055, acc = 0.9640 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 12:40:31.509465: step 74980, loss = 0.1161, acc = 0.9660 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 12:40:40.548687: step 74990, loss = 0.1074, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 12:40:49.683819: step 75000, loss = 0.1149, acc = 0.9540 (288.1 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 12:40:50.191363: step 75000, loss = 0.1088, acc = 0.9595, f1neg = 0.9554, f1pos = 0.9629, f1 = 0.9592
[Eval_batch(1)(2000,4000)] 2017-05-03 12:40:50.662474: step 75000, loss = 0.1096, acc = 0.9665, f1neg = 0.9607, f1pos = 0.9708, f1 = 0.9658
[Eval_batch(2)(2000,6000)] 2017-05-03 12:40:51.132080: step 75000, loss = 0.1231, acc = 0.9600, f1neg = 0.9573, f1pos = 0.9624, f1 = 0.9598
[Eval_batch(3)(2000,8000)] 2017-05-03 12:40:51.573652: step 75000, loss = 0.1288, acc = 0.9585, f1neg = 0.9582, f1pos = 0.9588, f1 = 0.9585
[Eval_batch(4)(2000,10000)] 2017-05-03 12:40:52.072009: step 75000, loss = 0.1298, acc = 0.9595, f1neg = 0.9603, f1pos = 0.9587, f1 = 0.9595
[Eval_batch(5)(2000,12000)] 2017-05-03 12:40:52.577399: step 75000, loss = 0.1263, acc = 0.9540, f1neg = 0.9503, f1pos = 0.9572, f1 = 0.9537
[Eval_batch(6)(2000,14000)] 2017-05-03 12:40:53.079554: step 75000, loss = 0.1190, acc = 0.9650, f1neg = 0.9643, f1pos = 0.9657, f1 = 0.9650
[Eval_batch(7)(2000,16000)] 2017-05-03 12:40:53.557890: step 75000, loss = 0.1167, acc = 0.9615, f1neg = 0.9537, f1pos = 0.9671, f1 = 0.9604
[Eval_batch(8)(2000,18000)] 2017-05-03 12:40:54.030630: step 75000, loss = 0.1093, acc = 0.9620, f1neg = 0.9564, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 12:40:54.504754: step 75000, loss = 0.1148, acc = 0.9625, f1neg = 0.9595, f1pos = 0.9651, f1 = 0.9623
[Eval_batch(10)(2000,22000)] 2017-05-03 12:40:54.948437: step 75000, loss = 0.1222, acc = 0.9580, f1neg = 0.9547, f1pos = 0.9609, f1 = 0.9578
[Eval_batch(11)(2000,24000)] 2017-05-03 12:40:55.424698: step 75000, loss = 0.1213, acc = 0.9595, f1neg = 0.9530, f1pos = 0.9644, f1 = 0.9587
[Eval_batch(12)(2000,26000)] 2017-05-03 12:40:55.904126: step 75000, loss = 0.1198, acc = 0.9535, f1neg = 0.9517, f1pos = 0.9552, f1 = 0.9534
[Eval_batch(13)(2000,28000)] 2017-05-03 12:40:56.377223: step 75000, loss = 0.1099, acc = 0.9660, f1neg = 0.9572, f1pos = 0.9718, f1 = 0.9645
[Eval_batch(14)(2000,30000)] 2017-05-03 12:40:56.841353: step 75000, loss = 0.1372, acc = 0.9530, f1neg = 0.9438, f1pos = 0.9596, f1 = 0.9517
[Eval_batch(15)(2000,32000)] 2017-05-03 12:40:57.302567: step 75000, loss = 0.1112, acc = 0.9700, f1neg = 0.9675, f1pos = 0.9722, f1 = 0.9698
[Eval_batch(16)(2000,34000)] 2017-05-03 12:40:57.768247: step 75000, loss = 0.1061, acc = 0.9675, f1neg = 0.9660, f1pos = 0.9689, f1 = 0.9674
[Eval_batch(17)(2000,36000)] 2017-05-03 12:40:58.232916: step 75000, loss = 0.1355, acc = 0.9555, f1neg = 0.9556, f1pos = 0.9554, f1 = 0.9555
[Eval_batch(18)(2000,38000)] 2017-05-03 12:40:58.699743: step 75000, loss = 0.1301, acc = 0.9550, f1neg = 0.9566, f1pos = 0.9533, f1 = 0.9549
[Eval_batch(19)(2000,40000)] 2017-05-03 12:40:59.167589: step 75000, loss = 0.1118, acc = 0.9680, f1neg = 0.9629, f1pos = 0.9719, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 12:40:59.644333: step 75000, loss = 0.1166, acc = 0.9630, f1neg = 0.9671, f1pos = 0.9578, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 12:41:00.114068: step 75000, loss = 0.1033, acc = 0.9650, f1neg = 0.9617, f1pos = 0.9677, f1 = 0.9647
[Eval_batch(22)(2000,46000)] 2017-05-03 12:41:00.589705: step 75000, loss = 0.1155, acc = 0.9600, f1neg = 0.9602, f1pos = 0.9598, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 12:41:01.057805: step 75000, loss = 0.1188, acc = 0.9570, f1neg = 0.9512, f1pos = 0.9616, f1 = 0.9564
[Eval_batch(24)(2000,50000)] 2017-05-03 12:41:01.510528: step 75000, loss = 0.1038, acc = 0.9675, f1neg = 0.9626, f1pos = 0.9713, f1 = 0.9669
[Eval_batch(25)(2000,52000)] 2017-05-03 12:41:01.989973: step 75000, loss = 0.0971, acc = 0.9725, f1neg = 0.9697, f1pos = 0.9748, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 12:41:02.451842: step 75000, loss = 0.1134, acc = 0.9605, f1neg = 0.9625, f1pos = 0.9582, f1 = 0.9604
[Eval_batch(27)(2000,56000)] 2017-05-03 12:41:03.051783: step 75000, loss = 0.0991, acc = 0.9685, f1neg = 0.9668, f1pos = 0.9701, f1 = 0.9684
[Eval] 2017-05-03 12:41:03.051864: step 75000, acc = 0.9618, f1 = 0.9614
[Test_batch(0)(2000,2000)] 2017-05-03 12:41:03.519349: step 75000, loss = 0.1484, acc = 0.9520, f1neg = 0.9542, f1pos = 0.9496, f1 = 0.9519
[Test_batch(1)(2000,4000)] 2017-05-03 12:41:03.993993: step 75000, loss = 0.1277, acc = 0.9595, f1neg = 0.9639, f1pos = 0.9540, f1 = 0.9589
[Test_batch(2)(2000,6000)] 2017-05-03 12:41:04.458578: step 75000, loss = 0.1466, acc = 0.9550, f1neg = 0.9584, f1pos = 0.9510, f1 = 0.9547
[Test_batch(3)(2000,8000)] 2017-05-03 12:41:04.929206: step 75000, loss = 0.1512, acc = 0.9510, f1neg = 0.9537, f1pos = 0.9480, f1 = 0.9508
[Test_batch(4)(2000,10000)] 2017-05-03 12:41:05.404399: step 75000, loss = 0.1441, acc = 0.9495, f1neg = 0.9494, f1pos = 0.9496, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 12:41:05.881304: step 75000, loss = 0.1592, acc = 0.9420, f1neg = 0.9422, f1pos = 0.9418, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 12:41:06.349905: step 75000, loss = 0.1465, acc = 0.9465, f1neg = 0.9442, f1pos = 0.9486, f1 = 0.9464
[Test_batch(7)(2000,16000)] 2017-05-03 12:41:06.831579: step 75000, loss = 0.1367, acc = 0.9515, f1neg = 0.9557, f1pos = 0.9464, f1 = 0.9511
[Test_batch(8)(2000,18000)] 2017-05-03 12:41:07.284924: step 75000, loss = 0.1351, acc = 0.9510, f1neg = 0.9502, f1pos = 0.9518, f1 = 0.9510
[Test_batch(9)(2000,20000)] 2017-05-03 12:41:07.745765: step 75000, loss = 0.1631, acc = 0.9350, f1neg = 0.9312, f1pos = 0.9384, f1 = 0.9348
[Test_batch(10)(2000,22000)] 2017-05-03 12:41:08.214814: step 75000, loss = 0.1395, acc = 0.9505, f1neg = 0.9438, f1pos = 0.9557, f1 = 0.9498
[Test_batch(11)(2000,24000)] 2017-05-03 12:41:08.682112: step 75000, loss = 0.1458, acc = 0.9495, f1neg = 0.9507, f1pos = 0.9482, f1 = 0.9495
[Test_batch(12)(2000,26000)] 2017-05-03 12:41:09.189256: step 75000, loss = 0.1186, acc = 0.9630, f1neg = 0.9643, f1pos = 0.9617, f1 = 0.9630
[Test_batch(13)(2000,28000)] 2017-05-03 12:41:09.659032: step 75000, loss = 0.1408, acc = 0.9535, f1neg = 0.9479, f1pos = 0.9580, f1 = 0.9530
[Test_batch(14)(2000,30000)] 2017-05-03 12:41:10.140465: step 75000, loss = 0.1167, acc = 0.9615, f1neg = 0.9555, f1pos = 0.9661, f1 = 0.9608
[Test_batch(15)(2000,32000)] 2017-05-03 12:41:10.609882: step 75000, loss = 0.1433, acc = 0.9545, f1neg = 0.9536, f1pos = 0.9553, f1 = 0.9545
[Test_batch(16)(2000,34000)] 2017-05-03 12:41:11.080558: step 75000, loss = 0.1239, acc = 0.9605, f1neg = 0.9592, f1pos = 0.9617, f1 = 0.9605
[Test_batch(17)(2000,36000)] 2017-05-03 12:41:11.553601: step 75000, loss = 0.1166, acc = 0.9685, f1neg = 0.9654, f1pos = 0.9711, f1 = 0.9682
[Test_batch(18)(2000,38000)] 2017-05-03 12:41:12.127838: step 75000, loss = 0.1128, acc = 0.9630, f1neg = 0.9622, f1pos = 0.9638, f1 = 0.9630
[Test] 2017-05-03 12:41:12.127933: step 75000, acc = 0.9536, f1 = 0.9533
[Status] 2017-05-03 12:41:12.127959: step 75000, maxindex = 72000, maxdev = 0.9630, maxtst = 0.9545
2017-05-03 12:41:21.173016: step 75010, loss = 0.1395, acc = 0.9500 (267.1 examples/sec; 0.240 sec/batch)
2017-05-03 12:41:30.290041: step 75020, loss = 0.1066, acc = 0.9700 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 12:41:39.368187: step 75030, loss = 0.1122, acc = 0.9660 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 12:41:48.456840: step 75040, loss = 0.1136, acc = 0.9680 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 12:41:57.281607: step 75050, loss = 0.1163, acc = 0.9640 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 12:42:06.351067: step 75060, loss = 0.1348, acc = 0.9580 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 12:42:15.479119: step 75070, loss = 0.1392, acc = 0.9500 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 12:42:24.577983: step 75080, loss = 0.1053, acc = 0.9680 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 12:42:33.659277: step 75090, loss = 0.1228, acc = 0.9620 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 12:42:42.732716: step 75100, loss = 0.1076, acc = 0.9740 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 12:42:51.771331: step 75110, loss = 0.0975, acc = 0.9660 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 12:43:00.857640: step 75120, loss = 0.1200, acc = 0.9700 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 12:43:10.040698: step 75130, loss = 0.1343, acc = 0.9500 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 12:43:19.078451: step 75140, loss = 0.1236, acc = 0.9540 (270.6 examples/sec; 0.236 sec/batch)
2017-05-03 12:43:28.018805: step 75150, loss = 0.1279, acc = 0.9560 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 12:43:37.046582: step 75160, loss = 0.1151, acc = 0.9720 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 12:43:46.113934: step 75170, loss = 0.1144, acc = 0.9620 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 12:43:55.100050: step 75180, loss = 0.1400, acc = 0.9580 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 12:44:04.151944: step 75190, loss = 0.1360, acc = 0.9500 (257.8 examples/sec; 0.248 sec/batch)
2017-05-03 12:44:13.140393: step 75200, loss = 0.1028, acc = 0.9760 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 12:44:22.205947: step 75210, loss = 0.1069, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 12:44:31.185116: step 75220, loss = 0.1082, acc = 0.9660 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 12:44:40.489238: step 75230, loss = 0.1244, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 12:44:49.423918: step 75240, loss = 0.1007, acc = 0.9760 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 12:44:58.576661: step 75250, loss = 0.1188, acc = 0.9700 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 12:45:07.598406: step 75260, loss = 0.1132, acc = 0.9660 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 12:45:16.548876: step 75270, loss = 0.1161, acc = 0.9660 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 12:45:25.555195: step 75280, loss = 0.1011, acc = 0.9740 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 12:45:34.484099: step 75290, loss = 0.1110, acc = 0.9680 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 12:45:43.603654: step 75300, loss = 0.1225, acc = 0.9540 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 12:45:52.721936: step 75310, loss = 0.1311, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 12:46:01.777705: step 75320, loss = 0.0975, acc = 0.9740 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 12:46:11.165273: step 75330, loss = 0.1005, acc = 0.9620 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 12:46:20.200603: step 75340, loss = 0.1108, acc = 0.9800 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 12:46:29.376332: step 75350, loss = 0.1181, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 12:46:38.460772: step 75360, loss = 0.1261, acc = 0.9620 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 12:46:47.581343: step 75370, loss = 0.1331, acc = 0.9460 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 12:46:56.645675: step 75380, loss = 0.1167, acc = 0.9560 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 12:47:05.736973: step 75390, loss = 0.1498, acc = 0.9480 (266.0 examples/sec; 0.241 sec/batch)
2017-05-03 12:47:14.780871: step 75400, loss = 0.0891, acc = 0.9700 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 12:47:24.285049: step 75410, loss = 0.1389, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 12:47:33.653675: step 75420, loss = 0.1416, acc = 0.9460 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 12:47:42.661857: step 75430, loss = 0.1007, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 12:47:51.708300: step 75440, loss = 0.1200, acc = 0.9720 (301.6 examples/sec; 0.212 sec/batch)
2017-05-03 12:48:00.810408: step 75450, loss = 0.1153, acc = 0.9700 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 12:48:09.856668: step 75460, loss = 0.1347, acc = 0.9580 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 12:48:19.039021: step 75470, loss = 0.1122, acc = 0.9580 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 12:48:28.262291: step 75480, loss = 0.1249, acc = 0.9540 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 12:48:37.333290: step 75490, loss = 0.1320, acc = 0.9600 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 12:48:46.535723: step 75500, loss = 0.1213, acc = 0.9640 (257.4 examples/sec; 0.249 sec/batch)
2017-05-03 12:48:55.807146: step 75510, loss = 0.1450, acc = 0.9540 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 12:49:05.047781: step 75520, loss = 0.1282, acc = 0.9540 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 12:49:13.908880: step 75530, loss = 0.0909, acc = 0.9840 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 12:49:22.985681: step 75540, loss = 0.1304, acc = 0.9520 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 12:49:31.994905: step 75550, loss = 0.1176, acc = 0.9560 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 12:49:41.050529: step 75560, loss = 0.1408, acc = 0.9500 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 12:49:50.392261: step 75570, loss = 0.1105, acc = 0.9680 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 12:49:59.461400: step 75580, loss = 0.1041, acc = 0.9700 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 12:50:08.553326: step 75590, loss = 0.1159, acc = 0.9660 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 12:50:18.500435: step 75600, loss = 0.1057, acc = 0.9700 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 12:50:27.548908: step 75610, loss = 0.1118, acc = 0.9580 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 12:50:36.590647: step 75620, loss = 0.1300, acc = 0.9480 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 12:50:45.553616: step 75630, loss = 0.0906, acc = 0.9680 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 12:50:54.494328: step 75640, loss = 0.1380, acc = 0.9580 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 12:51:03.581643: step 75650, loss = 0.1219, acc = 0.9540 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 12:51:12.658174: step 75660, loss = 0.1040, acc = 0.9660 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 12:51:21.648562: step 75670, loss = 0.1207, acc = 0.9580 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 12:51:30.758584: step 75680, loss = 0.0934, acc = 0.9780 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 12:51:39.861129: step 75690, loss = 0.1201, acc = 0.9660 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 12:51:48.883080: step 75700, loss = 0.1136, acc = 0.9680 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 12:51:57.987716: step 75710, loss = 0.0998, acc = 0.9640 (266.9 examples/sec; 0.240 sec/batch)
2017-05-03 12:52:07.207237: step 75720, loss = 0.1093, acc = 0.9600 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 12:52:16.391892: step 75730, loss = 0.1233, acc = 0.9660 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 12:52:25.619725: step 75740, loss = 0.1521, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 12:52:35.106952: step 75750, loss = 0.1057, acc = 0.9700 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 12:52:44.269297: step 75760, loss = 0.1132, acc = 0.9680 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 12:52:53.516175: step 75770, loss = 0.1196, acc = 0.9620 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 12:53:02.705170: step 75780, loss = 0.1288, acc = 0.9600 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 12:53:11.799717: step 75790, loss = 0.1116, acc = 0.9680 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 12:53:20.743700: step 75800, loss = 0.1375, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 12:53:29.717136: step 75810, loss = 0.1077, acc = 0.9680 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 12:53:38.851972: step 75820, loss = 0.1119, acc = 0.9680 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 12:53:47.920562: step 75830, loss = 0.1437, acc = 0.9520 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 12:53:57.120634: step 75840, loss = 0.1122, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 12:54:06.146730: step 75850, loss = 0.1309, acc = 0.9560 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 12:54:15.189133: step 75860, loss = 0.1096, acc = 0.9760 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 12:54:24.187023: step 75870, loss = 0.1053, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 12:54:33.397895: step 75880, loss = 0.1184, acc = 0.9680 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 12:54:42.616843: step 75890, loss = 0.1259, acc = 0.9560 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 12:54:51.643716: step 75900, loss = 0.1165, acc = 0.9620 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 12:55:00.936623: step 75910, loss = 0.1159, acc = 0.9600 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 12:55:10.093031: step 75920, loss = 0.1200, acc = 0.9640 (263.7 examples/sec; 0.243 sec/batch)
2017-05-03 12:55:19.467957: step 75930, loss = 0.1064, acc = 0.9740 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 12:55:28.634264: step 75940, loss = 0.1049, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 12:55:37.685195: step 75950, loss = 0.1008, acc = 0.9680 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 12:55:46.907307: step 75960, loss = 0.1311, acc = 0.9500 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 12:55:55.901510: step 75970, loss = 0.1325, acc = 0.9620 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 12:56:04.989943: step 75980, loss = 0.1387, acc = 0.9620 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 12:56:14.018328: step 75990, loss = 0.1117, acc = 0.9680 (265.6 examples/sec; 0.241 sec/batch)
2017-05-03 12:56:23.209553: step 76000, loss = 0.1472, acc = 0.9460 (265.6 examples/sec; 0.241 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 12:56:23.681136: step 76000, loss = 0.1044, acc = 0.9610, f1neg = 0.9574, f1pos = 0.9640, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 12:56:24.152245: step 76000, loss = 0.1111, acc = 0.9655, f1neg = 0.9600, f1pos = 0.9697, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 12:56:24.628304: step 76000, loss = 0.1210, acc = 0.9595, f1neg = 0.9571, f1pos = 0.9616, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 12:56:25.136402: step 76000, loss = 0.1225, acc = 0.9600, f1neg = 0.9602, f1pos = 0.9598, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 12:56:25.614365: step 76000, loss = 0.1255, acc = 0.9585, f1neg = 0.9599, f1pos = 0.9570, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 12:56:26.117482: step 76000, loss = 0.1243, acc = 0.9535, f1neg = 0.9505, f1pos = 0.9562, f1 = 0.9533
[Eval_batch(6)(2000,14000)] 2017-05-03 12:56:26.624906: step 76000, loss = 0.1133, acc = 0.9690, f1neg = 0.9685, f1pos = 0.9695, f1 = 0.9690
[Eval_batch(7)(2000,16000)] 2017-05-03 12:56:27.132634: step 76000, loss = 0.1155, acc = 0.9595, f1neg = 0.9519, f1pos = 0.9650, f1 = 0.9585
[Eval_batch(8)(2000,18000)] 2017-05-03 12:56:27.586975: step 76000, loss = 0.1072, acc = 0.9635, f1neg = 0.9587, f1pos = 0.9673, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 12:56:28.083317: step 76000, loss = 0.1130, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9666, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 12:56:28.584948: step 76000, loss = 0.1190, acc = 0.9625, f1neg = 0.9602, f1pos = 0.9646, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 12:56:29.094043: step 76000, loss = 0.1217, acc = 0.9625, f1neg = 0.9569, f1pos = 0.9668, f1 = 0.9619
[Eval_batch(12)(2000,26000)] 2017-05-03 12:56:29.575369: step 76000, loss = 0.1174, acc = 0.9545, f1neg = 0.9534, f1pos = 0.9556, f1 = 0.9545
[Eval_batch(13)(2000,28000)] 2017-05-03 12:56:30.052592: step 76000, loss = 0.1085, acc = 0.9660, f1neg = 0.9576, f1pos = 0.9716, f1 = 0.9646
[Eval_batch(14)(2000,30000)] 2017-05-03 12:56:30.535915: step 76000, loss = 0.1354, acc = 0.9560, f1neg = 0.9479, f1pos = 0.9619, f1 = 0.9549
[Eval_batch(15)(2000,32000)] 2017-05-03 12:56:31.042389: step 76000, loss = 0.1101, acc = 0.9680, f1neg = 0.9656, f1pos = 0.9701, f1 = 0.9678
[Eval_batch(16)(2000,34000)] 2017-05-03 12:56:31.547663: step 76000, loss = 0.1049, acc = 0.9685, f1neg = 0.9674, f1pos = 0.9696, f1 = 0.9685
[Eval_batch(17)(2000,36000)] 2017-05-03 12:56:32.048441: step 76000, loss = 0.1343, acc = 0.9580, f1neg = 0.9585, f1pos = 0.9575, f1 = 0.9580
[Eval_batch(18)(2000,38000)] 2017-05-03 12:56:32.544697: step 76000, loss = 0.1253, acc = 0.9600, f1neg = 0.9618, f1pos = 0.9581, f1 = 0.9599
[Eval_batch(19)(2000,40000)] 2017-05-03 12:56:33.027913: step 76000, loss = 0.1101, acc = 0.9685, f1neg = 0.9638, f1pos = 0.9721, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 12:56:33.530218: step 76000, loss = 0.1111, acc = 0.9625, f1neg = 0.9668, f1pos = 0.9569, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 12:56:34.027296: step 76000, loss = 0.1058, acc = 0.9650, f1neg = 0.9620, f1pos = 0.9675, f1 = 0.9648
[Eval_batch(22)(2000,46000)] 2017-05-03 12:56:34.491614: step 76000, loss = 0.1141, acc = 0.9605, f1neg = 0.9610, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 12:56:34.953084: step 76000, loss = 0.1193, acc = 0.9620, f1neg = 0.9574, f1pos = 0.9657, f1 = 0.9616
[Eval_batch(24)(2000,50000)] 2017-05-03 12:56:35.429234: step 76000, loss = 0.1031, acc = 0.9665, f1neg = 0.9617, f1pos = 0.9702, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 12:56:35.912691: step 76000, loss = 0.0947, acc = 0.9730, f1neg = 0.9705, f1pos = 0.9751, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 12:56:36.388318: step 76000, loss = 0.1087, acc = 0.9630, f1neg = 0.9652, f1pos = 0.9605, f1 = 0.9629
[Eval_batch(27)(2000,56000)] 2017-05-03 12:56:36.983727: step 76000, loss = 0.0946, acc = 0.9710, f1neg = 0.9697, f1pos = 0.9722, f1 = 0.9709
[Eval] 2017-05-03 12:56:36.983817: step 76000, acc = 0.9629, f1 = 0.9626
[Test_batch(0)(2000,2000)] 2017-05-03 12:56:37.464274: step 76000, loss = 0.1444, acc = 0.9515, f1neg = 0.9542, f1pos = 0.9485, f1 = 0.9513
[Test_batch(1)(2000,4000)] 2017-05-03 12:56:37.926070: step 76000, loss = 0.1227, acc = 0.9625, f1neg = 0.9668, f1pos = 0.9570, f1 = 0.9619
[Test_batch(2)(2000,6000)] 2017-05-03 12:56:38.390767: step 76000, loss = 0.1403, acc = 0.9595, f1neg = 0.9629, f1pos = 0.9554, f1 = 0.9592
[Test_batch(3)(2000,8000)] 2017-05-03 12:56:38.870821: step 76000, loss = 0.1462, acc = 0.9530, f1neg = 0.9561, f1pos = 0.9494, f1 = 0.9528
[Test_batch(4)(2000,10000)] 2017-05-03 12:56:39.344179: step 76000, loss = 0.1432, acc = 0.9525, f1neg = 0.9530, f1pos = 0.9519, f1 = 0.9525
[Test_batch(5)(2000,12000)] 2017-05-03 12:56:39.823695: step 76000, loss = 0.1574, acc = 0.9430, f1neg = 0.9438, f1pos = 0.9421, f1 = 0.9430
[Test_batch(6)(2000,14000)] 2017-05-03 12:56:40.295444: step 76000, loss = 0.1411, acc = 0.9480, f1neg = 0.9464, f1pos = 0.9495, f1 = 0.9480
[Test_batch(7)(2000,16000)] 2017-05-03 12:56:40.747440: step 76000, loss = 0.1323, acc = 0.9520, f1neg = 0.9566, f1pos = 0.9464, f1 = 0.9515
[Test_batch(8)(2000,18000)] 2017-05-03 12:56:41.255094: step 76000, loss = 0.1344, acc = 0.9495, f1neg = 0.9491, f1pos = 0.9499, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 12:56:41.720983: step 76000, loss = 0.1625, acc = 0.9390, f1neg = 0.9362, f1pos = 0.9416, f1 = 0.9389
[Test_batch(10)(2000,22000)] 2017-05-03 12:56:42.170287: step 76000, loss = 0.1425, acc = 0.9530, f1neg = 0.9473, f1pos = 0.9576, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 12:56:42.671601: step 76000, loss = 0.1431, acc = 0.9505, f1neg = 0.9521, f1pos = 0.9488, f1 = 0.9504
[Test_batch(12)(2000,26000)] 2017-05-03 12:56:43.138950: step 76000, loss = 0.1168, acc = 0.9635, f1neg = 0.9650, f1pos = 0.9618, f1 = 0.9634
[Test_batch(13)(2000,28000)] 2017-05-03 12:56:43.644374: step 76000, loss = 0.1405, acc = 0.9540, f1neg = 0.9494, f1pos = 0.9578, f1 = 0.9536
[Test_batch(14)(2000,30000)] 2017-05-03 12:56:44.097481: step 76000, loss = 0.1175, acc = 0.9605, f1neg = 0.9547, f1pos = 0.9650, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 12:56:44.583102: step 76000, loss = 0.1416, acc = 0.9605, f1neg = 0.9602, f1pos = 0.9608, f1 = 0.9605
[Test_batch(16)(2000,34000)] 2017-05-03 12:56:45.054212: step 76000, loss = 0.1230, acc = 0.9570, f1neg = 0.9561, f1pos = 0.9579, f1 = 0.9570
[Test_batch(17)(2000,36000)] 2017-05-03 12:56:45.519080: step 76000, loss = 0.1138, acc = 0.9655, f1neg = 0.9624, f1pos = 0.9682, f1 = 0.9653
[Test_batch(18)(2000,38000)] 2017-05-03 12:56:46.055786: step 76000, loss = 0.1074, acc = 0.9660, f1neg = 0.9654, f1pos = 0.9666, f1 = 0.9660
[Test] 2017-05-03 12:56:46.055876: step 76000, acc = 0.9548, f1 = 0.9546
[Status] 2017-05-03 12:56:46.055901: step 76000, maxindex = 72000, maxdev = 0.9630, maxtst = 0.9545
2017-05-03 12:56:55.082671: step 76010, loss = 0.1446, acc = 0.9440 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 12:57:04.187055: step 76020, loss = 0.1181, acc = 0.9620 (270.6 examples/sec; 0.236 sec/batch)
2017-05-03 12:57:13.124829: step 76030, loss = 0.1252, acc = 0.9560 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 12:57:22.318220: step 76040, loss = 0.1185, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 12:57:31.746586: step 76050, loss = 0.1290, acc = 0.9520 (200.7 examples/sec; 0.319 sec/batch)
2017-05-03 12:57:40.905061: step 76060, loss = 0.1377, acc = 0.9640 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 12:57:50.005807: step 76070, loss = 0.1273, acc = 0.9500 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 12:57:58.989953: step 76080, loss = 0.1089, acc = 0.9720 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 12:58:07.982763: step 76090, loss = 0.1162, acc = 0.9660 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 12:58:16.967572: step 76100, loss = 0.1060, acc = 0.9640 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 12:58:26.139223: step 76110, loss = 0.1160, acc = 0.9660 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 12:58:35.489122: step 76120, loss = 0.1301, acc = 0.9580 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 12:58:44.565698: step 76130, loss = 0.1312, acc = 0.9620 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 12:58:53.769912: step 76140, loss = 0.1381, acc = 0.9540 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 12:59:02.962940: step 76150, loss = 0.1083, acc = 0.9700 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 12:59:12.000516: step 76160, loss = 0.1143, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 12:59:21.060690: step 76170, loss = 0.1158, acc = 0.9600 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 12:59:30.258524: step 76180, loss = 0.1168, acc = 0.9680 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 12:59:39.126275: step 76190, loss = 0.1484, acc = 0.9460 (300.0 examples/sec; 0.213 sec/batch)
2017-05-03 12:59:48.102517: step 76200, loss = 0.1285, acc = 0.9660 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 12:59:57.113190: step 76210, loss = 0.1160, acc = 0.9680 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 13:00:06.359837: step 76220, loss = 0.1291, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 13:00:15.531829: step 76230, loss = 0.1175, acc = 0.9700 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 13:00:24.462668: step 76240, loss = 0.1273, acc = 0.9680 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 13:00:33.487508: step 76250, loss = 0.1092, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 13:00:42.582640: step 76260, loss = 0.1362, acc = 0.9620 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 13:00:51.784880: step 76270, loss = 0.1005, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 13:01:00.915931: step 76280, loss = 0.1059, acc = 0.9740 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 13:01:09.910740: step 76290, loss = 0.1239, acc = 0.9560 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 13:01:18.845010: step 76300, loss = 0.1260, acc = 0.9600 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 13:01:28.154996: step 76310, loss = 0.1238, acc = 0.9660 (265.2 examples/sec; 0.241 sec/batch)
2017-05-03 13:01:37.113453: step 76320, loss = 0.1286, acc = 0.9560 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 13:01:46.133153: step 76330, loss = 0.1152, acc = 0.9600 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 13:01:55.200193: step 76340, loss = 0.1428, acc = 0.9400 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 13:02:04.159592: step 76350, loss = 0.1178, acc = 0.9660 (295.6 examples/sec; 0.216 sec/batch)
2017-05-03 13:02:13.323704: step 76360, loss = 0.1359, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 13:02:22.246361: step 76370, loss = 0.1360, acc = 0.9500 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 13:02:31.402613: step 76380, loss = 0.1092, acc = 0.9740 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 13:02:40.818926: step 76390, loss = 0.1191, acc = 0.9620 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 13:02:50.008132: step 76400, loss = 0.1011, acc = 0.9700 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:02:59.021292: step 76410, loss = 0.1356, acc = 0.9480 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:03:08.148669: step 76420, loss = 0.1031, acc = 0.9720 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 13:03:17.157099: step 76430, loss = 0.1243, acc = 0.9640 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 13:03:26.217926: step 76440, loss = 0.1288, acc = 0.9540 (265.9 examples/sec; 0.241 sec/batch)
2017-05-03 13:03:35.180399: step 76450, loss = 0.1297, acc = 0.9580 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 13:03:44.066609: step 76460, loss = 0.1009, acc = 0.9680 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 13:03:53.118757: step 76470, loss = 0.1253, acc = 0.9660 (299.5 examples/sec; 0.214 sec/batch)
2017-05-03 13:04:02.132763: step 76480, loss = 0.1111, acc = 0.9680 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 13:04:11.191310: step 76490, loss = 0.1055, acc = 0.9700 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 13:04:20.258603: step 76500, loss = 0.1129, acc = 0.9720 (267.3 examples/sec; 0.239 sec/batch)
2017-05-03 13:04:29.416946: step 76510, loss = 0.1215, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 13:04:38.629523: step 76520, loss = 0.0871, acc = 0.9740 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 13:04:47.747952: step 76530, loss = 0.0986, acc = 0.9780 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 13:04:56.613218: step 76540, loss = 0.1309, acc = 0.9580 (305.5 examples/sec; 0.209 sec/batch)
2017-05-03 13:05:05.598228: step 76550, loss = 0.1278, acc = 0.9600 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 13:05:14.527275: step 76560, loss = 0.1077, acc = 0.9660 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 13:05:23.819232: step 76570, loss = 0.1041, acc = 0.9640 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 13:05:32.570061: step 76580, loss = 0.1169, acc = 0.9580 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 13:05:41.555039: step 76590, loss = 0.1067, acc = 0.9700 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 13:05:50.956971: step 76600, loss = 0.1060, acc = 0.9680 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 13:06:00.607348: step 76610, loss = 0.1355, acc = 0.9700 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 13:06:09.649832: step 76620, loss = 0.1253, acc = 0.9520 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 13:06:18.692459: step 76630, loss = 0.1276, acc = 0.9620 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 13:06:27.578701: step 76640, loss = 0.1238, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 13:06:36.589346: step 76650, loss = 0.1100, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 13:06:45.546400: step 76660, loss = 0.1081, acc = 0.9680 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 13:06:54.950607: step 76670, loss = 0.1132, acc = 0.9620 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 13:07:03.980704: step 76680, loss = 0.1093, acc = 0.9600 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 13:07:12.958470: step 76690, loss = 0.1368, acc = 0.9520 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 13:07:22.016735: step 76700, loss = 0.1109, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 13:07:31.063491: step 76710, loss = 0.1169, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 13:07:40.224614: step 76720, loss = 0.1184, acc = 0.9720 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 13:07:49.399356: step 76730, loss = 0.1107, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 13:07:58.464838: step 76740, loss = 0.1102, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 13:08:07.429588: step 76750, loss = 0.1264, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:08:16.531532: step 76760, loss = 0.0942, acc = 0.9840 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 13:08:25.618326: step 76770, loss = 0.0858, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 13:08:34.746694: step 76780, loss = 0.1218, acc = 0.9540 (264.2 examples/sec; 0.242 sec/batch)
2017-05-03 13:08:43.972981: step 76790, loss = 0.1175, acc = 0.9640 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 13:08:52.967852: step 76800, loss = 0.1206, acc = 0.9660 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 13:09:01.870444: step 76810, loss = 0.1165, acc = 0.9620 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 13:09:10.986249: step 76820, loss = 0.1115, acc = 0.9720 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 13:09:20.052968: step 76830, loss = 0.1206, acc = 0.9680 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 13:09:29.026379: step 76840, loss = 0.1255, acc = 0.9640 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 13:09:38.108554: step 76850, loss = 0.1090, acc = 0.9760 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 13:09:47.292473: step 76860, loss = 0.1119, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 13:09:56.357477: step 76870, loss = 0.1042, acc = 0.9720 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 13:10:05.478239: step 76880, loss = 0.1124, acc = 0.9700 (272.9 examples/sec; 0.234 sec/batch)
2017-05-03 13:10:14.730890: step 76890, loss = 0.1100, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 13:10:23.754515: step 76900, loss = 0.1073, acc = 0.9640 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 13:10:32.961902: step 76910, loss = 0.1485, acc = 0.9540 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 13:10:41.859760: step 76920, loss = 0.1128, acc = 0.9680 (300.1 examples/sec; 0.213 sec/batch)
2017-05-03 13:10:50.986011: step 76930, loss = 0.1393, acc = 0.9520 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 13:10:59.889544: step 76940, loss = 0.1309, acc = 0.9520 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 13:11:08.893987: step 76950, loss = 0.0908, acc = 0.9800 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 13:11:18.027447: step 76960, loss = 0.1184, acc = 0.9600 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 13:11:27.112088: step 76970, loss = 0.1141, acc = 0.9600 (267.8 examples/sec; 0.239 sec/batch)
2017-05-03 13:11:36.278635: step 76980, loss = 0.1219, acc = 0.9620 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 13:11:45.435891: step 76990, loss = 0.1090, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 13:11:54.398403: step 77000, loss = 0.1062, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 13:11:54.881975: step 77000, loss = 0.1071, acc = 0.9605, f1neg = 0.9566, f1pos = 0.9637, f1 = 0.9602
[Eval_batch(1)(2000,4000)] 2017-05-03 13:11:55.353393: step 77000, loss = 0.1090, acc = 0.9660, f1neg = 0.9603, f1pos = 0.9703, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 13:11:55.827942: step 77000, loss = 0.1214, acc = 0.9605, f1neg = 0.9580, f1pos = 0.9627, f1 = 0.9604
[Eval_batch(3)(2000,8000)] 2017-05-03 13:11:56.263231: step 77000, loss = 0.1252, acc = 0.9575, f1neg = 0.9574, f1pos = 0.9576, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 13:11:56.770455: step 77000, loss = 0.1265, acc = 0.9615, f1neg = 0.9625, f1pos = 0.9605, f1 = 0.9615
[Eval_batch(5)(2000,12000)] 2017-05-03 13:11:57.276816: step 77000, loss = 0.1253, acc = 0.9545, f1neg = 0.9511, f1pos = 0.9575, f1 = 0.9543
[Eval_batch(6)(2000,14000)] 2017-05-03 13:11:57.753204: step 77000, loss = 0.1175, acc = 0.9655, f1neg = 0.9648, f1pos = 0.9661, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 13:11:58.256850: step 77000, loss = 0.1153, acc = 0.9625, f1neg = 0.9551, f1pos = 0.9678, f1 = 0.9615
[Eval_batch(8)(2000,18000)] 2017-05-03 13:11:58.770352: step 77000, loss = 0.1077, acc = 0.9630, f1neg = 0.9578, f1pos = 0.9671, f1 = 0.9624
[Eval_batch(9)(2000,20000)] 2017-05-03 13:11:59.266477: step 77000, loss = 0.1142, acc = 0.9650, f1neg = 0.9624, f1pos = 0.9672, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 13:11:59.706315: step 77000, loss = 0.1200, acc = 0.9615, f1neg = 0.9588, f1pos = 0.9639, f1 = 0.9613
[Eval_batch(11)(2000,24000)] 2017-05-03 13:12:00.217289: step 77000, loss = 0.1207, acc = 0.9615, f1neg = 0.9556, f1pos = 0.9660, f1 = 0.9608
[Eval_batch(12)(2000,26000)] 2017-05-03 13:12:00.730081: step 77000, loss = 0.1186, acc = 0.9555, f1neg = 0.9541, f1pos = 0.9568, f1 = 0.9555
[Eval_batch(13)(2000,28000)] 2017-05-03 13:12:01.201797: step 77000, loss = 0.1099, acc = 0.9660, f1neg = 0.9573, f1pos = 0.9717, f1 = 0.9645
[Eval_batch(14)(2000,30000)] 2017-05-03 13:12:01.669051: step 77000, loss = 0.1367, acc = 0.9535, f1neg = 0.9445, f1pos = 0.9600, f1 = 0.9523
[Eval_batch(15)(2000,32000)] 2017-05-03 13:12:02.137006: step 77000, loss = 0.1099, acc = 0.9690, f1neg = 0.9665, f1pos = 0.9712, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 13:12:02.614012: step 77000, loss = 0.1045, acc = 0.9680, f1neg = 0.9667, f1pos = 0.9692, f1 = 0.9679
[Eval_batch(17)(2000,36000)] 2017-05-03 13:12:03.077430: step 77000, loss = 0.1349, acc = 0.9575, f1neg = 0.9578, f1pos = 0.9572, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 13:12:03.554247: step 77000, loss = 0.1291, acc = 0.9580, f1neg = 0.9596, f1pos = 0.9562, f1 = 0.9579
[Eval_batch(19)(2000,40000)] 2017-05-03 13:12:04.035086: step 77000, loss = 0.1107, acc = 0.9685, f1neg = 0.9636, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 13:12:04.541685: step 77000, loss = 0.1144, acc = 0.9625, f1neg = 0.9667, f1pos = 0.9571, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 13:12:05.020996: step 77000, loss = 0.1041, acc = 0.9655, f1neg = 0.9625, f1pos = 0.9681, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 13:12:05.470897: step 77000, loss = 0.1143, acc = 0.9610, f1neg = 0.9613, f1pos = 0.9607, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 13:12:05.934952: step 77000, loss = 0.1189, acc = 0.9585, f1neg = 0.9531, f1pos = 0.9628, f1 = 0.9579
[Eval_batch(24)(2000,50000)] 2017-05-03 13:12:06.439734: step 77000, loss = 0.1030, acc = 0.9665, f1neg = 0.9616, f1pos = 0.9703, f1 = 0.9659
[Eval_batch(25)(2000,52000)] 2017-05-03 13:12:06.915090: step 77000, loss = 0.0958, acc = 0.9730, f1neg = 0.9704, f1pos = 0.9752, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 13:12:07.383893: step 77000, loss = 0.1119, acc = 0.9625, f1neg = 0.9645, f1pos = 0.9603, f1 = 0.9624
[Eval_batch(27)(2000,56000)] 2017-05-03 13:12:07.925145: step 77000, loss = 0.0963, acc = 0.9720, f1neg = 0.9706, f1pos = 0.9733, f1 = 0.9719
[Eval] 2017-05-03 13:12:07.925243: step 77000, acc = 0.9628, f1 = 0.9624
[Test_batch(0)(2000,2000)] 2017-05-03 13:12:08.412161: step 77000, loss = 0.1464, acc = 0.9530, f1neg = 0.9552, f1pos = 0.9506, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 13:12:08.893796: step 77000, loss = 0.1251, acc = 0.9635, f1neg = 0.9676, f1pos = 0.9582, f1 = 0.9629
[Test_batch(2)(2000,6000)] 2017-05-03 13:12:09.369278: step 77000, loss = 0.1437, acc = 0.9570, f1neg = 0.9604, f1pos = 0.9529, f1 = 0.9567
[Test_batch(3)(2000,8000)] 2017-05-03 13:12:09.850412: step 77000, loss = 0.1479, acc = 0.9525, f1neg = 0.9553, f1pos = 0.9493, f1 = 0.9523
[Test_batch(4)(2000,10000)] 2017-05-03 13:12:10.354032: step 77000, loss = 0.1434, acc = 0.9495, f1neg = 0.9495, f1pos = 0.9495, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 13:12:10.838846: step 77000, loss = 0.1582, acc = 0.9420, f1neg = 0.9425, f1pos = 0.9415, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 13:12:11.313483: step 77000, loss = 0.1430, acc = 0.9505, f1neg = 0.9487, f1pos = 0.9522, f1 = 0.9504
[Test_batch(7)(2000,16000)] 2017-05-03 13:12:11.818440: step 77000, loss = 0.1347, acc = 0.9535, f1neg = 0.9577, f1pos = 0.9484, f1 = 0.9530
[Test_batch(8)(2000,18000)] 2017-05-03 13:12:12.296359: step 77000, loss = 0.1346, acc = 0.9505, f1neg = 0.9498, f1pos = 0.9512, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 13:12:12.796882: step 77000, loss = 0.1626, acc = 0.9365, f1neg = 0.9330, f1pos = 0.9397, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-03 13:12:13.300205: step 77000, loss = 0.1411, acc = 0.9530, f1neg = 0.9470, f1pos = 0.9578, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 13:12:13.811772: step 77000, loss = 0.1447, acc = 0.9495, f1neg = 0.9509, f1pos = 0.9481, f1 = 0.9495
[Test_batch(12)(2000,26000)] 2017-05-03 13:12:14.322777: step 77000, loss = 0.1170, acc = 0.9640, f1neg = 0.9653, f1pos = 0.9626, f1 = 0.9639
[Test_batch(13)(2000,28000)] 2017-05-03 13:12:14.832840: step 77000, loss = 0.1404, acc = 0.9535, f1neg = 0.9482, f1pos = 0.9578, f1 = 0.9530
[Test_batch(14)(2000,30000)] 2017-05-03 13:12:15.342323: step 77000, loss = 0.1171, acc = 0.9600, f1neg = 0.9538, f1pos = 0.9647, f1 = 0.9593
[Test_batch(15)(2000,32000)] 2017-05-03 13:12:15.847445: step 77000, loss = 0.1417, acc = 0.9550, f1neg = 0.9543, f1pos = 0.9557, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 13:12:16.356692: step 77000, loss = 0.1229, acc = 0.9605, f1neg = 0.9593, f1pos = 0.9616, f1 = 0.9605
[Test_batch(17)(2000,36000)] 2017-05-03 13:12:16.796729: step 77000, loss = 0.1152, acc = 0.9685, f1neg = 0.9654, f1pos = 0.9711, f1 = 0.9683
[Test_batch(18)(2000,38000)] 2017-05-03 13:12:17.381454: step 77000, loss = 0.1114, acc = 0.9640, f1neg = 0.9633, f1pos = 0.9647, f1 = 0.9640
[Test] 2017-05-03 13:12:17.381578: step 77000, acc = 0.9546, f1 = 0.9543
[Status] 2017-05-03 13:12:17.381609: step 77000, maxindex = 72000, maxdev = 0.9630, maxtst = 0.9545
2017-05-03 13:12:26.707133: step 77010, loss = 0.1139, acc = 0.9720 (225.5 examples/sec; 0.284 sec/batch)
2017-05-03 13:12:35.830123: step 77020, loss = 0.1209, acc = 0.9580 (265.0 examples/sec; 0.242 sec/batch)
2017-05-03 13:12:44.890044: step 77030, loss = 0.1064, acc = 0.9660 (256.2 examples/sec; 0.250 sec/batch)
2017-05-03 13:12:54.007236: step 77040, loss = 0.1101, acc = 0.9640 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 13:13:03.071375: step 77050, loss = 0.1182, acc = 0.9680 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 13:13:12.093544: step 77060, loss = 0.1307, acc = 0.9600 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 13:13:21.251007: step 77070, loss = 0.1189, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 13:13:30.405490: step 77080, loss = 0.0947, acc = 0.9700 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 13:13:39.466454: step 77090, loss = 0.0952, acc = 0.9760 (266.2 examples/sec; 0.240 sec/batch)
2017-05-03 13:13:48.600744: step 77100, loss = 0.1228, acc = 0.9560 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 13:13:57.710937: step 77110, loss = 0.1535, acc = 0.9600 (295.6 examples/sec; 0.217 sec/batch)
2017-05-03 13:14:06.831723: step 77120, loss = 0.0997, acc = 0.9780 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 13:14:15.918749: step 77130, loss = 0.1204, acc = 0.9580 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:14:25.124319: step 77140, loss = 0.1144, acc = 0.9700 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 13:14:34.299343: step 77150, loss = 0.1039, acc = 0.9700 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 13:14:43.420304: step 77160, loss = 0.1298, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 13:14:52.475668: step 77170, loss = 0.1454, acc = 0.9460 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 13:15:01.557588: step 77180, loss = 0.1688, acc = 0.9520 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 13:15:10.552445: step 77190, loss = 0.1269, acc = 0.9560 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 13:15:19.546697: step 77200, loss = 0.1232, acc = 0.9620 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 13:15:28.560479: step 77210, loss = 0.0982, acc = 0.9740 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 13:15:37.650898: step 77220, loss = 0.1164, acc = 0.9600 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 13:15:46.891803: step 77230, loss = 0.1135, acc = 0.9700 (251.7 examples/sec; 0.254 sec/batch)
2017-05-03 13:15:56.093992: step 77240, loss = 0.1592, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 13:16:05.323733: step 77250, loss = 0.1184, acc = 0.9560 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 13:16:14.177165: step 77260, loss = 0.1202, acc = 0.9680 (299.8 examples/sec; 0.213 sec/batch)
2017-05-03 13:16:23.268790: step 77270, loss = 0.1052, acc = 0.9680 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 13:16:32.389006: step 77280, loss = 0.1448, acc = 0.9500 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 13:16:41.397707: step 77290, loss = 0.1471, acc = 0.9520 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 13:16:50.420303: step 77300, loss = 0.1110, acc = 0.9600 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 13:16:59.469544: step 77310, loss = 0.1652, acc = 0.9520 (300.6 examples/sec; 0.213 sec/batch)
2017-05-03 13:17:08.440568: step 77320, loss = 0.1092, acc = 0.9580 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 13:17:17.497039: step 77330, loss = 0.1120, acc = 0.9560 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 13:17:26.615351: step 77340, loss = 0.1138, acc = 0.9700 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 13:17:35.455633: step 77350, loss = 0.1191, acc = 0.9640 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 13:17:44.537913: step 77360, loss = 0.1135, acc = 0.9680 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 13:17:53.526817: step 77370, loss = 0.1060, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 13:18:02.563960: step 77380, loss = 0.1463, acc = 0.9480 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 13:18:11.761614: step 77390, loss = 0.1034, acc = 0.9660 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 13:18:20.893467: step 77400, loss = 0.1184, acc = 0.9600 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 13:18:29.865674: step 77410, loss = 0.1221, acc = 0.9660 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 13:18:38.841145: step 77420, loss = 0.1212, acc = 0.9620 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 13:18:47.884591: step 77430, loss = 0.1443, acc = 0.9420 (268.3 examples/sec; 0.238 sec/batch)
2017-05-03 13:18:56.959977: step 77440, loss = 0.0962, acc = 0.9740 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 13:19:06.034438: step 77450, loss = 0.1110, acc = 0.9660 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 13:19:14.972304: step 77460, loss = 0.1370, acc = 0.9600 (310.8 examples/sec; 0.206 sec/batch)
2017-05-03 13:19:24.106056: step 77470, loss = 0.0923, acc = 0.9740 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 13:19:33.132042: step 77480, loss = 0.1225, acc = 0.9600 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 13:19:42.052620: step 77490, loss = 0.1348, acc = 0.9680 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 13:19:51.164333: step 77500, loss = 0.1236, acc = 0.9700 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 13:20:00.082344: step 77510, loss = 0.1196, acc = 0.9640 (297.6 examples/sec; 0.215 sec/batch)
2017-05-03 13:20:09.111030: step 77520, loss = 0.1111, acc = 0.9600 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 13:20:18.191124: step 77530, loss = 0.1136, acc = 0.9600 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 13:20:27.268191: step 77540, loss = 0.0891, acc = 0.9780 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 13:20:36.568807: step 77550, loss = 0.1268, acc = 0.9540 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 13:20:45.848444: step 77560, loss = 0.1032, acc = 0.9760 (227.0 examples/sec; 0.282 sec/batch)
2017-05-03 13:20:54.892472: step 77570, loss = 0.1181, acc = 0.9700 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 13:21:04.119256: step 77580, loss = 0.1076, acc = 0.9680 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 13:21:13.286840: step 77590, loss = 0.1478, acc = 0.9460 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 13:21:22.563585: step 77600, loss = 0.1315, acc = 0.9540 (264.6 examples/sec; 0.242 sec/batch)
2017-05-03 13:21:31.987712: step 77610, loss = 0.1444, acc = 0.9560 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 13:21:42.021323: step 77620, loss = 0.1245, acc = 0.9600 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 13:21:50.979718: step 77630, loss = 0.1160, acc = 0.9740 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 13:22:00.199104: step 77640, loss = 0.1360, acc = 0.9580 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 13:22:09.195382: step 77650, loss = 0.1004, acc = 0.9760 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 13:22:18.260775: step 77660, loss = 0.1101, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 13:22:27.248163: step 77670, loss = 0.1092, acc = 0.9620 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 13:22:36.639851: step 77680, loss = 0.1050, acc = 0.9700 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 13:22:45.786313: step 77690, loss = 0.0878, acc = 0.9820 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 13:22:54.732740: step 77700, loss = 0.1092, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 13:23:03.782322: step 77710, loss = 0.1326, acc = 0.9480 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 13:23:12.739622: step 77720, loss = 0.1169, acc = 0.9700 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 13:23:21.689938: step 77730, loss = 0.1222, acc = 0.9620 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 13:23:30.736681: step 77740, loss = 0.0949, acc = 0.9720 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 13:23:39.945309: step 77750, loss = 0.1206, acc = 0.9720 (250.8 examples/sec; 0.255 sec/batch)
2017-05-03 13:23:48.906980: step 77760, loss = 0.1074, acc = 0.9660 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 13:23:57.715606: step 77770, loss = 0.1293, acc = 0.9520 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 13:24:06.898250: step 77780, loss = 0.1308, acc = 0.9640 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 13:24:15.920326: step 77790, loss = 0.1112, acc = 0.9700 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 13:24:24.898947: step 77800, loss = 0.1120, acc = 0.9700 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 13:24:34.075500: step 77810, loss = 0.1372, acc = 0.9600 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:24:43.187475: step 77820, loss = 0.1369, acc = 0.9560 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 13:24:52.351144: step 77830, loss = 0.1222, acc = 0.9580 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 13:25:01.391093: step 77840, loss = 0.1231, acc = 0.9660 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 13:25:10.412960: step 77850, loss = 0.1244, acc = 0.9560 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 13:25:19.322627: step 77860, loss = 0.1074, acc = 0.9720 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 13:25:28.212344: step 77870, loss = 0.1351, acc = 0.9580 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 13:25:37.246266: step 77880, loss = 0.1122, acc = 0.9720 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 13:25:46.248008: step 77890, loss = 0.1335, acc = 0.9580 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 13:25:55.134856: step 77900, loss = 0.1349, acc = 0.9640 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 13:26:04.162620: step 77910, loss = 0.1113, acc = 0.9660 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 13:26:13.322671: step 77920, loss = 0.1128, acc = 0.9680 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 13:26:22.425002: step 77930, loss = 0.1084, acc = 0.9700 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 13:26:31.278156: step 77940, loss = 0.1314, acc = 0.9620 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 13:26:40.265306: step 77950, loss = 0.1244, acc = 0.9540 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 13:26:49.195501: step 77960, loss = 0.1077, acc = 0.9680 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 13:26:58.734623: step 77970, loss = 0.1379, acc = 0.9580 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 13:27:07.803113: step 77980, loss = 0.1233, acc = 0.9600 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 13:27:16.682622: step 77990, loss = 0.1089, acc = 0.9700 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:27:25.674345: step 78000, loss = 0.1209, acc = 0.9620 (287.8 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 13:27:26.173176: step 78000, loss = 0.1046, acc = 0.9615, f1neg = 0.9579, f1pos = 0.9645, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 13:27:26.686585: step 78000, loss = 0.1114, acc = 0.9675, f1neg = 0.9624, f1pos = 0.9714, f1 = 0.9669
[Eval_batch(2)(2000,6000)] 2017-05-03 13:27:27.196664: step 78000, loss = 0.1197, acc = 0.9595, f1neg = 0.9572, f1pos = 0.9616, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 13:27:27.700797: step 78000, loss = 0.1226, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 13:27:28.194833: step 78000, loss = 0.1244, acc = 0.9580, f1neg = 0.9595, f1pos = 0.9563, f1 = 0.9579
[Eval_batch(5)(2000,12000)] 2017-05-03 13:27:28.677094: step 78000, loss = 0.1233, acc = 0.9525, f1neg = 0.9495, f1pos = 0.9552, f1 = 0.9523
[Eval_batch(6)(2000,14000)] 2017-05-03 13:27:29.152859: step 78000, loss = 0.1134, acc = 0.9685, f1neg = 0.9681, f1pos = 0.9689, f1 = 0.9685
[Eval_batch(7)(2000,16000)] 2017-05-03 13:27:29.630120: step 78000, loss = 0.1154, acc = 0.9595, f1neg = 0.9520, f1pos = 0.9650, f1 = 0.9585
[Eval_batch(8)(2000,18000)] 2017-05-03 13:27:30.140255: step 78000, loss = 0.1080, acc = 0.9650, f1neg = 0.9605, f1pos = 0.9686, f1 = 0.9645
[Eval_batch(9)(2000,20000)] 2017-05-03 13:27:30.580240: step 78000, loss = 0.1132, acc = 0.9630, f1neg = 0.9606, f1pos = 0.9651, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-03 13:27:31.085611: step 78000, loss = 0.1190, acc = 0.9610, f1neg = 0.9588, f1pos = 0.9630, f1 = 0.9609
[Eval_batch(11)(2000,24000)] 2017-05-03 13:27:31.588430: step 78000, loss = 0.1214, acc = 0.9630, f1neg = 0.9576, f1pos = 0.9672, f1 = 0.9624
[Eval_batch(12)(2000,26000)] 2017-05-03 13:27:32.098441: step 78000, loss = 0.1164, acc = 0.9555, f1neg = 0.9545, f1pos = 0.9564, f1 = 0.9555
[Eval_batch(13)(2000,28000)] 2017-05-03 13:27:32.608549: step 78000, loss = 0.1093, acc = 0.9675, f1neg = 0.9595, f1pos = 0.9729, f1 = 0.9662
[Eval_batch(14)(2000,30000)] 2017-05-03 13:27:33.116835: step 78000, loss = 0.1349, acc = 0.9565, f1neg = 0.9487, f1pos = 0.9623, f1 = 0.9555
[Eval_batch(15)(2000,32000)] 2017-05-03 13:27:33.628239: step 78000, loss = 0.1104, acc = 0.9670, f1neg = 0.9646, f1pos = 0.9691, f1 = 0.9668
[Eval_batch(16)(2000,34000)] 2017-05-03 13:27:34.126661: step 78000, loss = 0.1050, acc = 0.9685, f1neg = 0.9674, f1pos = 0.9695, f1 = 0.9685
[Eval_batch(17)(2000,36000)] 2017-05-03 13:27:34.627550: step 78000, loss = 0.1340, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9589, f1 = 0.9595
[Eval_batch(18)(2000,38000)] 2017-05-03 13:27:35.137259: step 78000, loss = 0.1244, acc = 0.9615, f1neg = 0.9633, f1pos = 0.9595, f1 = 0.9614
[Eval_batch(19)(2000,40000)] 2017-05-03 13:27:35.649492: step 78000, loss = 0.1097, acc = 0.9685, f1neg = 0.9638, f1pos = 0.9721, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 13:27:36.155427: step 78000, loss = 0.1093, acc = 0.9635, f1neg = 0.9678, f1pos = 0.9579, f1 = 0.9628
[Eval_batch(21)(2000,44000)] 2017-05-03 13:27:36.657992: step 78000, loss = 0.1062, acc = 0.9650, f1neg = 0.9622, f1pos = 0.9674, f1 = 0.9648
[Eval_batch(22)(2000,46000)] 2017-05-03 13:27:37.161593: step 78000, loss = 0.1133, acc = 0.9615, f1neg = 0.9621, f1pos = 0.9609, f1 = 0.9615
[Eval_batch(23)(2000,48000)] 2017-05-03 13:27:37.670356: step 78000, loss = 0.1197, acc = 0.9615, f1neg = 0.9570, f1pos = 0.9652, f1 = 0.9611
[Eval_batch(24)(2000,50000)] 2017-05-03 13:27:38.173672: step 78000, loss = 0.1037, acc = 0.9660, f1neg = 0.9614, f1pos = 0.9696, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 13:27:38.684336: step 78000, loss = 0.0951, acc = 0.9725, f1neg = 0.9700, f1pos = 0.9746, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 13:27:39.184782: step 78000, loss = 0.1085, acc = 0.9635, f1neg = 0.9658, f1pos = 0.9609, f1 = 0.9633
[Eval_batch(27)(2000,56000)] 2017-05-03 13:27:39.816126: step 78000, loss = 0.0937, acc = 0.9740, f1neg = 0.9729, f1pos = 0.9750, f1 = 0.9740
[Eval] 2017-05-03 13:27:39.816216: step 78000, acc = 0.9633, f1 = 0.9629
[Test_batch(0)(2000,2000)] 2017-05-03 13:27:40.321500: step 78000, loss = 0.1442, acc = 0.9515, f1neg = 0.9543, f1pos = 0.9484, f1 = 0.9513
[Test_batch(1)(2000,4000)] 2017-05-03 13:27:40.822952: step 78000, loss = 0.1226, acc = 0.9635, f1neg = 0.9677, f1pos = 0.9580, f1 = 0.9629
[Test_batch(2)(2000,6000)] 2017-05-03 13:27:41.328528: step 78000, loss = 0.1394, acc = 0.9575, f1neg = 0.9612, f1pos = 0.9530, f1 = 0.9571
[Test_batch(3)(2000,8000)] 2017-05-03 13:27:41.834492: step 78000, loss = 0.1463, acc = 0.9505, f1neg = 0.9540, f1pos = 0.9464, f1 = 0.9502
[Test_batch(4)(2000,10000)] 2017-05-03 13:27:42.351215: step 78000, loss = 0.1424, acc = 0.9520, f1neg = 0.9527, f1pos = 0.9513, f1 = 0.9520
[Test_batch(5)(2000,12000)] 2017-05-03 13:27:42.858022: step 78000, loss = 0.1568, acc = 0.9455, f1neg = 0.9466, f1pos = 0.9443, f1 = 0.9455
[Test_batch(6)(2000,14000)] 2017-05-03 13:27:43.370135: step 78000, loss = 0.1398, acc = 0.9495, f1neg = 0.9481, f1pos = 0.9508, f1 = 0.9495
[Test_batch(7)(2000,16000)] 2017-05-03 13:27:43.887639: step 78000, loss = 0.1326, acc = 0.9540, f1neg = 0.9586, f1pos = 0.9483, f1 = 0.9534
[Test_batch(8)(2000,18000)] 2017-05-03 13:27:44.400905: step 78000, loss = 0.1335, acc = 0.9510, f1neg = 0.9508, f1pos = 0.9512, f1 = 0.9510
[Test_batch(9)(2000,20000)] 2017-05-03 13:27:44.883235: step 78000, loss = 0.1640, acc = 0.9365, f1neg = 0.9338, f1pos = 0.9390, f1 = 0.9364
[Test_batch(10)(2000,22000)] 2017-05-03 13:27:45.392162: step 78000, loss = 0.1429, acc = 0.9520, f1neg = 0.9464, f1pos = 0.9565, f1 = 0.9515
[Test_batch(11)(2000,24000)] 2017-05-03 13:27:45.893311: step 78000, loss = 0.1424, acc = 0.9540, f1neg = 0.9556, f1pos = 0.9522, f1 = 0.9539
[Test_batch(12)(2000,26000)] 2017-05-03 13:27:46.399649: step 78000, loss = 0.1184, acc = 0.9605, f1neg = 0.9623, f1pos = 0.9585, f1 = 0.9604
[Test_batch(13)(2000,28000)] 2017-05-03 13:27:46.915121: step 78000, loss = 0.1422, acc = 0.9535, f1neg = 0.9489, f1pos = 0.9573, f1 = 0.9531
[Test_batch(14)(2000,30000)] 2017-05-03 13:27:47.433779: step 78000, loss = 0.1190, acc = 0.9610, f1neg = 0.9555, f1pos = 0.9653, f1 = 0.9604
[Test_batch(15)(2000,32000)] 2017-05-03 13:27:47.939641: step 78000, loss = 0.1416, acc = 0.9585, f1neg = 0.9582, f1pos = 0.9588, f1 = 0.9585
[Test_batch(16)(2000,34000)] 2017-05-03 13:27:48.446533: step 78000, loss = 0.1226, acc = 0.9565, f1neg = 0.9557, f1pos = 0.9572, f1 = 0.9565
[Test_batch(17)(2000,36000)] 2017-05-03 13:27:48.961790: step 78000, loss = 0.1129, acc = 0.9650, f1neg = 0.9619, f1pos = 0.9676, f1 = 0.9648
[Test_batch(18)(2000,38000)] 2017-05-03 13:27:49.593314: step 78000, loss = 0.1075, acc = 0.9655, f1neg = 0.9650, f1pos = 0.9660, f1 = 0.9655
[Test] 2017-05-03 13:27:49.593408: step 78000, acc = 0.9546, f1 = 0.9544
[Status] 2017-05-03 13:27:49.593436: step 78000, maxindex = 78000, maxdev = 0.9633, maxtst = 0.9546
2017-05-03 13:28:02.069293: step 78010, loss = 0.1117, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 13:28:10.925530: step 78020, loss = 0.1123, acc = 0.9660 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 13:28:19.878776: step 78030, loss = 0.1123, acc = 0.9680 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 13:28:28.791553: step 78040, loss = 0.1131, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 13:28:37.769479: step 78050, loss = 0.0869, acc = 0.9800 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 13:28:46.755258: step 78060, loss = 0.1384, acc = 0.9540 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 13:28:56.425000: step 78070, loss = 0.1229, acc = 0.9640 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 13:29:05.599394: step 78080, loss = 0.1105, acc = 0.9640 (250.4 examples/sec; 0.256 sec/batch)
2017-05-03 13:29:14.567982: step 78090, loss = 0.1339, acc = 0.9560 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 13:29:23.671552: step 78100, loss = 0.1222, acc = 0.9600 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 13:29:32.806299: step 78110, loss = 0.1412, acc = 0.9560 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 13:29:41.704387: step 78120, loss = 0.1087, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 13:29:50.750787: step 78130, loss = 0.1334, acc = 0.9580 (267.2 examples/sec; 0.239 sec/batch)
2017-05-03 13:29:59.709753: step 78140, loss = 0.1037, acc = 0.9680 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 13:30:08.739466: step 78150, loss = 0.1209, acc = 0.9560 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 13:30:18.123044: step 78160, loss = 0.1437, acc = 0.9500 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 13:30:27.057089: step 78170, loss = 0.1107, acc = 0.9700 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 13:30:36.127080: step 78180, loss = 0.1264, acc = 0.9640 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 13:30:45.137124: step 78190, loss = 0.1128, acc = 0.9700 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 13:30:54.246561: step 78200, loss = 0.1222, acc = 0.9620 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 13:31:03.333550: step 78210, loss = 0.1361, acc = 0.9640 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 13:31:12.393952: step 78220, loss = 0.1266, acc = 0.9600 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 13:31:21.631535: step 78230, loss = 0.1355, acc = 0.9560 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 13:31:30.508695: step 78240, loss = 0.0980, acc = 0.9680 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 13:31:39.500950: step 78250, loss = 0.0991, acc = 0.9780 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 13:31:48.664331: step 78260, loss = 0.1411, acc = 0.9500 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 13:31:57.824995: step 78270, loss = 0.1335, acc = 0.9620 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 13:32:06.734307: step 78280, loss = 0.1250, acc = 0.9600 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 13:32:15.893535: step 78290, loss = 0.1164, acc = 0.9620 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 13:32:24.946965: step 78300, loss = 0.1265, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 13:32:34.002335: step 78310, loss = 0.1127, acc = 0.9640 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 13:32:43.058042: step 78320, loss = 0.1289, acc = 0.9520 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 13:32:51.976598: step 78330, loss = 0.0929, acc = 0.9720 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 13:33:00.931281: step 78340, loss = 0.1169, acc = 0.9660 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 13:33:09.954417: step 78350, loss = 0.0760, acc = 0.9920 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 13:33:18.962285: step 78360, loss = 0.1329, acc = 0.9540 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 13:33:27.984653: step 78370, loss = 0.1024, acc = 0.9640 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 13:33:36.960582: step 78380, loss = 0.1213, acc = 0.9700 (271.7 examples/sec; 0.236 sec/batch)
2017-05-03 13:33:46.045694: step 78390, loss = 0.1076, acc = 0.9600 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:33:55.076202: step 78400, loss = 0.0942, acc = 0.9760 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 13:34:04.195892: step 78410, loss = 0.1023, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 13:34:13.245100: step 78420, loss = 0.1031, acc = 0.9700 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 13:34:22.408343: step 78430, loss = 0.1129, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 13:34:31.772300: step 78440, loss = 0.1092, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 13:34:40.744345: step 78450, loss = 0.1347, acc = 0.9460 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 13:34:49.881110: step 78460, loss = 0.1307, acc = 0.9660 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 13:34:58.981426: step 78470, loss = 0.1153, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 13:35:08.085458: step 78480, loss = 0.1192, acc = 0.9540 (264.7 examples/sec; 0.242 sec/batch)
2017-05-03 13:35:17.105984: step 78490, loss = 0.1116, acc = 0.9560 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 13:35:26.040991: step 78500, loss = 0.1155, acc = 0.9620 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 13:35:35.050131: step 78510, loss = 0.1240, acc = 0.9680 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 13:35:44.115443: step 78520, loss = 0.1331, acc = 0.9560 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 13:35:53.040751: step 78530, loss = 0.1039, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 13:36:01.953440: step 78540, loss = 0.1137, acc = 0.9600 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 13:36:11.032371: step 78550, loss = 0.1150, acc = 0.9780 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 13:36:20.150968: step 78560, loss = 0.1044, acc = 0.9580 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 13:36:29.289582: step 78570, loss = 0.1302, acc = 0.9600 (269.6 examples/sec; 0.237 sec/batch)
2017-05-03 13:36:38.528824: step 78580, loss = 0.1003, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 13:36:47.704084: step 78590, loss = 0.1064, acc = 0.9720 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 13:36:56.761387: step 78600, loss = 0.1353, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 13:37:05.706827: step 78610, loss = 0.1297, acc = 0.9580 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 13:37:14.684228: step 78620, loss = 0.0990, acc = 0.9720 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 13:37:24.600424: step 78630, loss = 0.1430, acc = 0.9500 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 13:37:33.693160: step 78640, loss = 0.1347, acc = 0.9540 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 13:37:42.744383: step 78650, loss = 0.1007, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 13:37:51.701465: step 78660, loss = 0.1390, acc = 0.9580 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 13:38:00.735825: step 78670, loss = 0.1211, acc = 0.9700 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 13:38:09.749117: step 78680, loss = 0.1204, acc = 0.9580 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 13:38:18.616430: step 78690, loss = 0.1134, acc = 0.9620 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 13:38:27.623045: step 78700, loss = 0.0980, acc = 0.9680 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 13:38:36.713096: step 78710, loss = 0.0994, acc = 0.9760 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 13:38:45.670453: step 78720, loss = 0.1089, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 13:38:55.187163: step 78730, loss = 0.1441, acc = 0.9580 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 13:39:04.160599: step 78740, loss = 0.0930, acc = 0.9760 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 13:39:13.175364: step 78750, loss = 0.1566, acc = 0.9580 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 13:39:22.108275: step 78760, loss = 0.1565, acc = 0.9500 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 13:39:31.170249: step 78770, loss = 0.1050, acc = 0.9680 (303.8 examples/sec; 0.211 sec/batch)
2017-05-03 13:39:40.103537: step 78780, loss = 0.1303, acc = 0.9700 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 13:39:49.136205: step 78790, loss = 0.1162, acc = 0.9560 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 13:39:58.223769: step 78800, loss = 0.1152, acc = 0.9680 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 13:40:07.333728: step 78810, loss = 0.1201, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 13:40:16.404280: step 78820, loss = 0.1387, acc = 0.9500 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:40:25.434921: step 78830, loss = 0.1232, acc = 0.9640 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 13:40:34.452821: step 78840, loss = 0.1192, acc = 0.9660 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 13:40:43.537809: step 78850, loss = 0.1501, acc = 0.9540 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 13:40:52.750547: step 78860, loss = 0.1126, acc = 0.9680 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 13:41:01.799545: step 78870, loss = 0.0981, acc = 0.9740 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 13:41:10.709463: step 78880, loss = 0.1125, acc = 0.9660 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 13:41:19.896161: step 78890, loss = 0.1330, acc = 0.9620 (265.2 examples/sec; 0.241 sec/batch)
2017-05-03 13:41:28.879688: step 78900, loss = 0.1328, acc = 0.9580 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 13:41:38.054887: step 78910, loss = 0.1184, acc = 0.9680 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 13:41:47.091401: step 78920, loss = 0.1223, acc = 0.9580 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 13:41:56.033175: step 78930, loss = 0.1183, acc = 0.9640 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 13:42:05.099367: step 78940, loss = 0.1360, acc = 0.9640 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 13:42:14.074859: step 78950, loss = 0.1217, acc = 0.9680 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 13:42:23.519329: step 78960, loss = 0.0953, acc = 0.9700 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 13:42:32.633376: step 78970, loss = 0.1247, acc = 0.9660 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 13:42:41.704033: step 78980, loss = 0.1003, acc = 0.9720 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 13:42:50.848009: step 78990, loss = 0.1061, acc = 0.9620 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 13:42:59.807506: step 79000, loss = 0.1256, acc = 0.9720 (286.0 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 13:43:00.288768: step 79000, loss = 0.1049, acc = 0.9615, f1neg = 0.9579, f1pos = 0.9646, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 13:43:00.757784: step 79000, loss = 0.1098, acc = 0.9660, f1neg = 0.9605, f1pos = 0.9702, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 13:43:01.259539: step 79000, loss = 0.1202, acc = 0.9595, f1neg = 0.9570, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 13:43:01.755008: step 79000, loss = 0.1236, acc = 0.9605, f1neg = 0.9605, f1pos = 0.9605, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 13:43:02.220229: step 79000, loss = 0.1262, acc = 0.9590, f1neg = 0.9603, f1pos = 0.9576, f1 = 0.9590
[Eval_batch(5)(2000,12000)] 2017-05-03 13:43:02.729366: step 79000, loss = 0.1243, acc = 0.9560, f1neg = 0.9529, f1pos = 0.9587, f1 = 0.9558
[Eval_batch(6)(2000,14000)] 2017-05-03 13:43:03.203233: step 79000, loss = 0.1142, acc = 0.9675, f1neg = 0.9670, f1pos = 0.9680, f1 = 0.9675
[Eval_batch(7)(2000,16000)] 2017-05-03 13:43:03.676154: step 79000, loss = 0.1152, acc = 0.9600, f1neg = 0.9523, f1pos = 0.9655, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 13:43:04.168843: step 79000, loss = 0.1071, acc = 0.9620, f1neg = 0.9568, f1pos = 0.9661, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 13:43:04.640753: step 79000, loss = 0.1120, acc = 0.9650, f1neg = 0.9626, f1pos = 0.9671, f1 = 0.9649
[Eval_batch(10)(2000,22000)] 2017-05-03 13:43:05.134898: step 79000, loss = 0.1201, acc = 0.9615, f1neg = 0.9589, f1pos = 0.9638, f1 = 0.9613
[Eval_batch(11)(2000,24000)] 2017-05-03 13:43:05.600774: step 79000, loss = 0.1202, acc = 0.9615, f1neg = 0.9557, f1pos = 0.9659, f1 = 0.9608
[Eval_batch(12)(2000,26000)] 2017-05-03 13:43:06.072564: step 79000, loss = 0.1181, acc = 0.9530, f1neg = 0.9518, f1pos = 0.9541, f1 = 0.9530
[Eval_batch(13)(2000,28000)] 2017-05-03 13:43:06.579364: step 79000, loss = 0.1075, acc = 0.9665, f1neg = 0.9582, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 13:43:07.083929: step 79000, loss = 0.1356, acc = 0.9550, f1neg = 0.9467, f1pos = 0.9610, f1 = 0.9539
[Eval_batch(15)(2000,32000)] 2017-05-03 13:43:07.540048: step 79000, loss = 0.1099, acc = 0.9705, f1neg = 0.9682, f1pos = 0.9725, f1 = 0.9703
[Eval_batch(16)(2000,34000)] 2017-05-03 13:43:08.043660: step 79000, loss = 0.1042, acc = 0.9685, f1neg = 0.9673, f1pos = 0.9696, f1 = 0.9685
[Eval_batch(17)(2000,36000)] 2017-05-03 13:43:08.521901: step 79000, loss = 0.1336, acc = 0.9575, f1neg = 0.9578, f1pos = 0.9572, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 13:43:08.993188: step 79000, loss = 0.1259, acc = 0.9610, f1neg = 0.9626, f1pos = 0.9592, f1 = 0.9609
[Eval_batch(19)(2000,40000)] 2017-05-03 13:43:09.501407: step 79000, loss = 0.1093, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9717, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 13:43:09.965736: step 79000, loss = 0.1123, acc = 0.9630, f1neg = 0.9672, f1pos = 0.9575, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 13:43:10.425240: step 79000, loss = 0.1047, acc = 0.9645, f1neg = 0.9615, f1pos = 0.9671, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 13:43:10.906738: step 79000, loss = 0.1125, acc = 0.9610, f1neg = 0.9615, f1pos = 0.9605, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 13:43:11.370570: step 79000, loss = 0.1189, acc = 0.9610, f1neg = 0.9561, f1pos = 0.9649, f1 = 0.9605
[Eval_batch(24)(2000,50000)] 2017-05-03 13:43:11.844569: step 79000, loss = 0.1027, acc = 0.9660, f1neg = 0.9611, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 13:43:12.321884: step 79000, loss = 0.0955, acc = 0.9735, f1neg = 0.9710, f1pos = 0.9756, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 13:43:12.802236: step 79000, loss = 0.1092, acc = 0.9625, f1neg = 0.9646, f1pos = 0.9601, f1 = 0.9624
[Eval_batch(27)(2000,56000)] 2017-05-03 13:43:13.402754: step 79000, loss = 0.0951, acc = 0.9725, f1neg = 0.9712, f1pos = 0.9737, f1 = 0.9724
[Eval] 2017-05-03 13:43:13.402838: step 79000, acc = 0.9630, f1 = 0.9627
[Test_batch(0)(2000,2000)] 2017-05-03 13:43:13.878425: step 79000, loss = 0.1450, acc = 0.9530, f1neg = 0.9555, f1pos = 0.9503, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 13:43:14.347228: step 79000, loss = 0.1234, acc = 0.9615, f1neg = 0.9659, f1pos = 0.9559, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 13:43:14.847417: step 79000, loss = 0.1410, acc = 0.9570, f1neg = 0.9604, f1pos = 0.9529, f1 = 0.9567
[Test_batch(3)(2000,8000)] 2017-05-03 13:43:15.325478: step 79000, loss = 0.1476, acc = 0.9515, f1neg = 0.9546, f1pos = 0.9480, f1 = 0.9513
[Test_batch(4)(2000,10000)] 2017-05-03 13:43:15.836418: step 79000, loss = 0.1419, acc = 0.9520, f1neg = 0.9522, f1pos = 0.9518, f1 = 0.9520
[Test_batch(5)(2000,12000)] 2017-05-03 13:43:16.300061: step 79000, loss = 0.1569, acc = 0.9450, f1neg = 0.9457, f1pos = 0.9443, f1 = 0.9450
[Test_batch(6)(2000,14000)] 2017-05-03 13:43:16.773268: step 79000, loss = 0.1417, acc = 0.9505, f1neg = 0.9488, f1pos = 0.9521, f1 = 0.9504
[Test_batch(7)(2000,16000)] 2017-05-03 13:43:17.248212: step 79000, loss = 0.1326, acc = 0.9545, f1neg = 0.9587, f1pos = 0.9493, f1 = 0.9540
[Test_batch(8)(2000,18000)] 2017-05-03 13:43:17.713705: step 79000, loss = 0.1339, acc = 0.9500, f1neg = 0.9495, f1pos = 0.9505, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 13:43:18.222583: step 79000, loss = 0.1616, acc = 0.9390, f1neg = 0.9361, f1pos = 0.9417, f1 = 0.9389
[Test_batch(10)(2000,22000)] 2017-05-03 13:43:18.700521: step 79000, loss = 0.1411, acc = 0.9525, f1neg = 0.9466, f1pos = 0.9572, f1 = 0.9519
[Test_batch(11)(2000,24000)] 2017-05-03 13:43:19.189294: step 79000, loss = 0.1435, acc = 0.9515, f1neg = 0.9529, f1pos = 0.9500, f1 = 0.9515
[Test_batch(12)(2000,26000)] 2017-05-03 13:43:19.664030: step 79000, loss = 0.1169, acc = 0.9620, f1neg = 0.9635, f1pos = 0.9604, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 13:43:20.143134: step 79000, loss = 0.1391, acc = 0.9570, f1neg = 0.9524, f1pos = 0.9608, f1 = 0.9566
[Test_batch(14)(2000,30000)] 2017-05-03 13:43:20.626172: step 79000, loss = 0.1166, acc = 0.9605, f1neg = 0.9546, f1pos = 0.9651, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 13:43:21.116738: step 79000, loss = 0.1413, acc = 0.9565, f1neg = 0.9560, f1pos = 0.9570, f1 = 0.9565
[Test_batch(16)(2000,34000)] 2017-05-03 13:43:21.617411: step 79000, loss = 0.1222, acc = 0.9595, f1neg = 0.9586, f1pos = 0.9604, f1 = 0.9595
[Test_batch(17)(2000,36000)] 2017-05-03 13:43:22.099532: step 79000, loss = 0.1131, acc = 0.9665, f1neg = 0.9634, f1pos = 0.9691, f1 = 0.9663
[Test_batch(18)(2000,38000)] 2017-05-03 13:43:22.658687: step 79000, loss = 0.1084, acc = 0.9660, f1neg = 0.9654, f1pos = 0.9666, f1 = 0.9660
[Test] 2017-05-03 13:43:22.658777: step 79000, acc = 0.9551, f1 = 0.9548
[Status] 2017-05-03 13:43:22.658802: step 79000, maxindex = 78000, maxdev = 0.9633, maxtst = 0.9546
2017-05-03 13:43:31.714511: step 79010, loss = 0.1245, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 13:43:40.848152: step 79020, loss = 0.1283, acc = 0.9600 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:43:50.112581: step 79030, loss = 0.1297, acc = 0.9560 (227.0 examples/sec; 0.282 sec/batch)
2017-05-03 13:43:59.033281: step 79040, loss = 0.1534, acc = 0.9480 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 13:44:07.967421: step 79050, loss = 0.1299, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 13:44:16.942623: step 79060, loss = 0.1048, acc = 0.9680 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 13:44:26.039497: step 79070, loss = 0.1158, acc = 0.9560 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 13:44:35.014945: step 79080, loss = 0.1453, acc = 0.9560 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 13:44:44.040505: step 79090, loss = 0.1376, acc = 0.9640 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:44:53.317419: step 79100, loss = 0.1243, acc = 0.9580 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 13:45:02.199314: step 79110, loss = 0.1652, acc = 0.9500 (300.8 examples/sec; 0.213 sec/batch)
2017-05-03 13:45:11.216898: step 79120, loss = 0.1003, acc = 0.9720 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 13:45:20.211453: step 79130, loss = 0.1204, acc = 0.9580 (274.2 examples/sec; 0.233 sec/batch)
2017-05-03 13:45:29.201863: step 79140, loss = 0.1153, acc = 0.9760 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 13:45:38.318747: step 79150, loss = 0.1227, acc = 0.9660 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 13:45:47.445883: step 79160, loss = 0.1131, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 13:45:56.466405: step 79170, loss = 0.1126, acc = 0.9620 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 13:46:05.474454: step 79180, loss = 0.1159, acc = 0.9720 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 13:46:14.634371: step 79190, loss = 0.1080, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 13:46:23.600594: step 79200, loss = 0.1126, acc = 0.9700 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 13:46:32.701040: step 79210, loss = 0.1063, acc = 0.9720 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 13:46:41.861112: step 79220, loss = 0.1138, acc = 0.9660 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 13:46:50.830032: step 79230, loss = 0.1026, acc = 0.9640 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 13:46:59.722861: step 79240, loss = 0.1185, acc = 0.9680 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 13:47:08.760964: step 79250, loss = 0.1164, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 13:47:17.802136: step 79260, loss = 0.1222, acc = 0.9560 (275.3 examples/sec; 0.233 sec/batch)
2017-05-03 13:47:26.868392: step 79270, loss = 0.1020, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 13:47:35.947225: step 79280, loss = 0.1421, acc = 0.9500 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 13:47:44.962531: step 79290, loss = 0.1199, acc = 0.9640 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 13:47:53.939171: step 79300, loss = 0.1146, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 13:48:02.860388: step 79310, loss = 0.0988, acc = 0.9700 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 13:48:11.773701: step 79320, loss = 0.1287, acc = 0.9580 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 13:48:20.920249: step 79330, loss = 0.1049, acc = 0.9700 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 13:48:29.854445: step 79340, loss = 0.1457, acc = 0.9560 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 13:48:38.807859: step 79350, loss = 0.1185, acc = 0.9640 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 13:48:47.771826: step 79360, loss = 0.1254, acc = 0.9640 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 13:48:56.634816: step 79370, loss = 0.1385, acc = 0.9520 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 13:49:06.011984: step 79380, loss = 0.1121, acc = 0.9700 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 13:49:15.317496: step 79390, loss = 0.1028, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 13:49:24.248743: step 79400, loss = 0.1242, acc = 0.9620 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 13:49:33.193809: step 79410, loss = 0.1046, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 13:49:42.338783: step 79420, loss = 0.1593, acc = 0.9420 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 13:49:51.346348: step 79430, loss = 0.1476, acc = 0.9580 (263.7 examples/sec; 0.243 sec/batch)
2017-05-03 13:50:00.392358: step 79440, loss = 0.1178, acc = 0.9660 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 13:50:09.401492: step 79450, loss = 0.1280, acc = 0.9600 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 13:50:18.466459: step 79460, loss = 0.1222, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 13:50:27.550550: step 79470, loss = 0.1323, acc = 0.9540 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 13:50:36.896854: step 79480, loss = 0.1499, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:50:45.874326: step 79490, loss = 0.1130, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 13:50:54.939439: step 79500, loss = 0.1390, acc = 0.9500 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 13:51:04.044080: step 79510, loss = 0.1264, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 13:51:12.964405: step 79520, loss = 0.1358, acc = 0.9480 (300.1 examples/sec; 0.213 sec/batch)
2017-05-03 13:51:22.023619: step 79530, loss = 0.1178, acc = 0.9640 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 13:51:30.943506: step 79540, loss = 0.1159, acc = 0.9660 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 13:51:40.075078: step 79550, loss = 0.1072, acc = 0.9680 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 13:51:49.123628: step 79560, loss = 0.1173, acc = 0.9640 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 13:51:58.098934: step 79570, loss = 0.1218, acc = 0.9620 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 13:52:07.221316: step 79580, loss = 0.1080, acc = 0.9700 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 13:52:16.268499: step 79590, loss = 0.1086, acc = 0.9660 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 13:52:25.350260: step 79600, loss = 0.1289, acc = 0.9580 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 13:52:34.257026: step 79610, loss = 0.1296, acc = 0.9640 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 13:52:43.281375: step 79620, loss = 0.0921, acc = 0.9760 (306.8 examples/sec; 0.209 sec/batch)
2017-05-03 13:52:52.208075: step 79630, loss = 0.0997, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 13:53:02.159590: step 79640, loss = 0.1340, acc = 0.9500 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 13:53:11.178449: step 79650, loss = 0.1487, acc = 0.9500 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 13:53:20.350707: step 79660, loss = 0.1158, acc = 0.9720 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 13:53:29.450319: step 79670, loss = 0.1211, acc = 0.9660 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:53:38.539500: step 79680, loss = 0.1116, acc = 0.9640 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 13:53:47.771116: step 79690, loss = 0.1082, acc = 0.9640 (252.0 examples/sec; 0.254 sec/batch)
2017-05-03 13:53:56.924431: step 79700, loss = 0.1184, acc = 0.9640 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 13:54:05.823901: step 79710, loss = 0.1356, acc = 0.9560 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:54:14.776003: step 79720, loss = 0.1316, acc = 0.9520 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 13:54:23.892194: step 79730, loss = 0.1266, acc = 0.9680 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 13:54:32.895810: step 79740, loss = 0.0969, acc = 0.9760 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 13:54:41.817460: step 79750, loss = 0.1325, acc = 0.9620 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 13:54:50.746516: step 79760, loss = 0.1431, acc = 0.9500 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 13:54:59.805685: step 79770, loss = 0.1309, acc = 0.9500 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 13:55:08.688497: step 79780, loss = 0.1258, acc = 0.9560 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 13:55:17.578350: step 79790, loss = 0.1009, acc = 0.9660 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 13:55:26.599784: step 79800, loss = 0.1129, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 13:55:35.554578: step 79810, loss = 0.1530, acc = 0.9500 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 13:55:44.485466: step 79820, loss = 0.1397, acc = 0.9460 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 13:55:53.838280: step 79830, loss = 0.1134, acc = 0.9600 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 13:56:02.864760: step 79840, loss = 0.1459, acc = 0.9580 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 13:56:12.004490: step 79850, loss = 0.1102, acc = 0.9640 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 13:56:20.946726: step 79860, loss = 0.1102, acc = 0.9640 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 13:56:29.924030: step 79870, loss = 0.1154, acc = 0.9640 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 13:56:38.938741: step 79880, loss = 0.1108, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 13:56:47.983266: step 79890, loss = 0.1157, acc = 0.9640 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 13:56:56.980408: step 79900, loss = 0.1434, acc = 0.9460 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 13:57:05.993146: step 79910, loss = 0.1062, acc = 0.9780 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 13:57:15.136209: step 79920, loss = 0.1078, acc = 0.9700 (249.5 examples/sec; 0.257 sec/batch)
2017-05-03 13:57:24.177118: step 79930, loss = 0.1197, acc = 0.9620 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 13:57:33.151919: step 79940, loss = 0.1242, acc = 0.9720 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 13:57:42.133575: step 79950, loss = 0.1040, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 13:57:51.005773: step 79960, loss = 0.1240, acc = 0.9580 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 13:58:00.153939: step 79970, loss = 0.1177, acc = 0.9600 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 13:58:09.224386: step 79980, loss = 0.1166, acc = 0.9720 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 13:58:18.277343: step 79990, loss = 0.1129, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 13:58:27.449606: step 80000, loss = 0.1014, acc = 0.9700 (275.7 examples/sec; 0.232 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 13:58:27.963398: step 80000, loss = 0.1068, acc = 0.9615, f1neg = 0.9578, f1pos = 0.9646, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 13:58:28.426978: step 80000, loss = 0.1081, acc = 0.9655, f1neg = 0.9598, f1pos = 0.9698, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 13:58:28.909771: step 80000, loss = 0.1201, acc = 0.9595, f1neg = 0.9570, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 13:58:29.389803: step 80000, loss = 0.1238, acc = 0.9595, f1neg = 0.9595, f1pos = 0.9595, f1 = 0.9595
[Eval_batch(4)(2000,10000)] 2017-05-03 13:58:29.845995: step 80000, loss = 0.1259, acc = 0.9600, f1neg = 0.9611, f1pos = 0.9588, f1 = 0.9600
[Eval_batch(5)(2000,12000)] 2017-05-03 13:58:30.346515: step 80000, loss = 0.1242, acc = 0.9540, f1neg = 0.9506, f1pos = 0.9570, f1 = 0.9538
[Eval_batch(6)(2000,14000)] 2017-05-03 13:58:30.820979: step 80000, loss = 0.1158, acc = 0.9655, f1neg = 0.9649, f1pos = 0.9661, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 13:58:31.333751: step 80000, loss = 0.1148, acc = 0.9635, f1neg = 0.9564, f1pos = 0.9686, f1 = 0.9625
[Eval_batch(8)(2000,18000)] 2017-05-03 13:58:31.843152: step 80000, loss = 0.1075, acc = 0.9625, f1neg = 0.9574, f1pos = 0.9665, f1 = 0.9619
[Eval_batch(9)(2000,20000)] 2017-05-03 13:58:32.323652: step 80000, loss = 0.1132, acc = 0.9655, f1neg = 0.9630, f1pos = 0.9677, f1 = 0.9653
[Eval_batch(10)(2000,22000)] 2017-05-03 13:58:32.802845: step 80000, loss = 0.1195, acc = 0.9600, f1neg = 0.9572, f1pos = 0.9625, f1 = 0.9598
[Eval_batch(11)(2000,24000)] 2017-05-03 13:58:33.314520: step 80000, loss = 0.1199, acc = 0.9620, f1neg = 0.9562, f1pos = 0.9665, f1 = 0.9613
[Eval_batch(12)(2000,26000)] 2017-05-03 13:58:33.817549: step 80000, loss = 0.1171, acc = 0.9570, f1neg = 0.9557, f1pos = 0.9583, f1 = 0.9570
[Eval_batch(13)(2000,28000)] 2017-05-03 13:58:34.319067: step 80000, loss = 0.1091, acc = 0.9670, f1neg = 0.9586, f1pos = 0.9725, f1 = 0.9656
[Eval_batch(14)(2000,30000)] 2017-05-03 13:58:34.784461: step 80000, loss = 0.1352, acc = 0.9540, f1neg = 0.9453, f1pos = 0.9603, f1 = 0.9528
[Eval_batch(15)(2000,32000)] 2017-05-03 13:58:35.254695: step 80000, loss = 0.1097, acc = 0.9700, f1neg = 0.9676, f1pos = 0.9721, f1 = 0.9698
[Eval_batch(16)(2000,34000)] 2017-05-03 13:58:35.738705: step 80000, loss = 0.1040, acc = 0.9680, f1neg = 0.9666, f1pos = 0.9693, f1 = 0.9679
[Eval_batch(17)(2000,36000)] 2017-05-03 13:58:36.248810: step 80000, loss = 0.1335, acc = 0.9565, f1neg = 0.9568, f1pos = 0.9562, f1 = 0.9565
[Eval_batch(18)(2000,38000)] 2017-05-03 13:58:36.763023: step 80000, loss = 0.1262, acc = 0.9600, f1neg = 0.9616, f1pos = 0.9582, f1 = 0.9599
[Eval_batch(19)(2000,40000)] 2017-05-03 13:58:37.269487: step 80000, loss = 0.1103, acc = 0.9685, f1neg = 0.9636, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 13:58:37.780829: step 80000, loss = 0.1119, acc = 0.9630, f1neg = 0.9672, f1pos = 0.9576, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 13:58:38.291324: step 80000, loss = 0.1032, acc = 0.9645, f1neg = 0.9614, f1pos = 0.9672, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 13:58:38.762470: step 80000, loss = 0.1129, acc = 0.9625, f1neg = 0.9629, f1pos = 0.9621, f1 = 0.9625
[Eval_batch(23)(2000,48000)] 2017-05-03 13:58:39.268496: step 80000, loss = 0.1172, acc = 0.9620, f1neg = 0.9572, f1pos = 0.9658, f1 = 0.9615
[Eval_batch(24)(2000,50000)] 2017-05-03 13:58:39.773797: step 80000, loss = 0.1015, acc = 0.9670, f1neg = 0.9622, f1pos = 0.9707, f1 = 0.9665
[Eval_batch(25)(2000,52000)] 2017-05-03 13:58:40.249275: step 80000, loss = 0.0949, acc = 0.9740, f1neg = 0.9716, f1pos = 0.9761, f1 = 0.9738
[Eval_batch(26)(2000,54000)] 2017-05-03 13:58:40.724692: step 80000, loss = 0.1104, acc = 0.9620, f1neg = 0.9640, f1pos = 0.9597, f1 = 0.9619
[Eval_batch(27)(2000,56000)] 2017-05-03 13:58:41.294610: step 80000, loss = 0.0961, acc = 0.9710, f1neg = 0.9696, f1pos = 0.9723, f1 = 0.9709
[Eval] 2017-05-03 13:58:41.294696: step 80000, acc = 0.9631, f1 = 0.9627
[Test_batch(0)(2000,2000)] 2017-05-03 13:58:41.801077: step 80000, loss = 0.1460, acc = 0.9530, f1neg = 0.9553, f1pos = 0.9505, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 13:58:42.274107: step 80000, loss = 0.1236, acc = 0.9630, f1neg = 0.9671, f1pos = 0.9577, f1 = 0.9624
[Test_batch(2)(2000,6000)] 2017-05-03 13:58:42.753634: step 80000, loss = 0.1421, acc = 0.9590, f1neg = 0.9623, f1pos = 0.9551, f1 = 0.9587
[Test_batch(3)(2000,8000)] 2017-05-03 13:58:43.256565: step 80000, loss = 0.1474, acc = 0.9515, f1neg = 0.9545, f1pos = 0.9480, f1 = 0.9513
[Test_batch(4)(2000,10000)] 2017-05-03 13:58:43.767604: step 80000, loss = 0.1415, acc = 0.9495, f1neg = 0.9498, f1pos = 0.9492, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 13:58:44.244377: step 80000, loss = 0.1560, acc = 0.9440, f1neg = 0.9446, f1pos = 0.9434, f1 = 0.9440
[Test_batch(6)(2000,14000)] 2017-05-03 13:58:44.736761: step 80000, loss = 0.1415, acc = 0.9485, f1neg = 0.9467, f1pos = 0.9502, f1 = 0.9484
[Test_batch(7)(2000,16000)] 2017-05-03 13:58:45.215564: step 80000, loss = 0.1339, acc = 0.9525, f1neg = 0.9568, f1pos = 0.9473, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 13:58:45.696990: step 80000, loss = 0.1320, acc = 0.9510, f1neg = 0.9504, f1pos = 0.9516, f1 = 0.9510
[Test_batch(9)(2000,20000)] 2017-05-03 13:58:46.168674: step 80000, loss = 0.1621, acc = 0.9355, f1neg = 0.9320, f1pos = 0.9387, f1 = 0.9353
[Test_batch(10)(2000,22000)] 2017-05-03 13:58:46.632164: step 80000, loss = 0.1396, acc = 0.9535, f1neg = 0.9477, f1pos = 0.9582, f1 = 0.9529
[Test_batch(11)(2000,24000)] 2017-05-03 13:58:47.143137: step 80000, loss = 0.1425, acc = 0.9510, f1neg = 0.9523, f1pos = 0.9496, f1 = 0.9510
[Test_batch(12)(2000,26000)] 2017-05-03 13:58:47.608697: step 80000, loss = 0.1169, acc = 0.9650, f1neg = 0.9663, f1pos = 0.9636, f1 = 0.9649
[Test_batch(13)(2000,28000)] 2017-05-03 13:58:48.089159: step 80000, loss = 0.1400, acc = 0.9520, f1neg = 0.9465, f1pos = 0.9564, f1 = 0.9515
[Test_batch(14)(2000,30000)] 2017-05-03 13:58:48.558178: step 80000, loss = 0.1168, acc = 0.9595, f1neg = 0.9534, f1pos = 0.9642, f1 = 0.9588
[Test_batch(15)(2000,32000)] 2017-05-03 13:58:49.025806: step 80000, loss = 0.1408, acc = 0.9535, f1neg = 0.9528, f1pos = 0.9542, f1 = 0.9535
[Test_batch(16)(2000,34000)] 2017-05-03 13:58:49.493171: step 80000, loss = 0.1218, acc = 0.9605, f1neg = 0.9594, f1pos = 0.9615, f1 = 0.9605
[Test_batch(17)(2000,36000)] 2017-05-03 13:58:49.960419: step 80000, loss = 0.1136, acc = 0.9675, f1neg = 0.9644, f1pos = 0.9701, f1 = 0.9672
[Test_batch(18)(2000,38000)] 2017-05-03 13:58:50.535993: step 80000, loss = 0.1100, acc = 0.9640, f1neg = 0.9632, f1pos = 0.9647, f1 = 0.9640
[Test] 2017-05-03 13:58:50.536084: step 80000, acc = 0.9544, f1 = 0.9542
[Status] 2017-05-03 13:58:50.536109: step 80000, maxindex = 78000, maxdev = 0.9633, maxtst = 0.9546
2017-05-03 13:58:59.557606: step 80010, loss = 0.1067, acc = 0.9660 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 13:59:08.601822: step 80020, loss = 0.1082, acc = 0.9580 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 13:59:17.719395: step 80030, loss = 0.1335, acc = 0.9560 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 13:59:26.693986: step 80040, loss = 0.1051, acc = 0.9720 (266.5 examples/sec; 0.240 sec/batch)
2017-05-03 13:59:35.806642: step 80050, loss = 0.0969, acc = 0.9700 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 13:59:44.735112: step 80060, loss = 0.1148, acc = 0.9660 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 13:59:53.758427: step 80070, loss = 0.1662, acc = 0.9400 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 14:00:02.772355: step 80080, loss = 0.1266, acc = 0.9540 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 14:00:11.763183: step 80090, loss = 0.1087, acc = 0.9660 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:00:20.715050: step 80100, loss = 0.1097, acc = 0.9680 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 14:00:29.723751: step 80110, loss = 0.1160, acc = 0.9700 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 14:00:38.850713: step 80120, loss = 0.0875, acc = 0.9760 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 14:00:47.819051: step 80130, loss = 0.1301, acc = 0.9560 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:00:57.060426: step 80140, loss = 0.1553, acc = 0.9520 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 14:01:06.330271: step 80150, loss = 0.1097, acc = 0.9660 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 14:01:15.552308: step 80160, loss = 0.0866, acc = 0.9800 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 14:01:24.645374: step 80170, loss = 0.1218, acc = 0.9540 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 14:01:33.530684: step 80180, loss = 0.1172, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 14:01:42.535928: step 80190, loss = 0.1230, acc = 0.9660 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:01:51.638157: step 80200, loss = 0.1089, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 14:02:00.640363: step 80210, loss = 0.1086, acc = 0.9700 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:02:09.897953: step 80220, loss = 0.1562, acc = 0.9400 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 14:02:18.881652: step 80230, loss = 0.1027, acc = 0.9760 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 14:02:27.937468: step 80240, loss = 0.1372, acc = 0.9560 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 14:02:36.922980: step 80250, loss = 0.1059, acc = 0.9620 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 14:02:45.972805: step 80260, loss = 0.1112, acc = 0.9720 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:02:54.986229: step 80270, loss = 0.1099, acc = 0.9700 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 14:03:04.064622: step 80280, loss = 0.0970, acc = 0.9780 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 14:03:13.365587: step 80290, loss = 0.1382, acc = 0.9620 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 14:03:22.269818: step 80300, loss = 0.1084, acc = 0.9620 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 14:03:31.441351: step 80310, loss = 0.0910, acc = 0.9780 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 14:03:40.520421: step 80320, loss = 0.1077, acc = 0.9660 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:03:49.630555: step 80330, loss = 0.1199, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:03:58.571693: step 80340, loss = 0.1223, acc = 0.9560 (296.8 examples/sec; 0.216 sec/batch)
2017-05-03 14:04:07.521166: step 80350, loss = 0.1445, acc = 0.9540 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 14:04:16.609140: step 80360, loss = 0.1330, acc = 0.9620 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 14:04:25.473104: step 80370, loss = 0.1616, acc = 0.9540 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 14:04:34.519736: step 80380, loss = 0.1261, acc = 0.9580 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 14:04:43.709453: step 80390, loss = 0.1322, acc = 0.9540 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 14:04:52.776312: step 80400, loss = 0.1303, acc = 0.9520 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 14:05:01.686854: step 80410, loss = 0.0889, acc = 0.9740 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 14:05:10.759841: step 80420, loss = 0.1158, acc = 0.9580 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 14:05:19.838731: step 80430, loss = 0.1280, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 14:05:28.708346: step 80440, loss = 0.1146, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 14:05:37.730886: step 80450, loss = 0.1294, acc = 0.9520 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 14:05:46.712793: step 80460, loss = 0.1309, acc = 0.9600 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 14:05:55.638745: step 80470, loss = 0.1099, acc = 0.9680 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:06:04.656894: step 80480, loss = 0.1390, acc = 0.9620 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 14:06:13.631635: step 80490, loss = 0.1239, acc = 0.9600 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 14:06:22.664162: step 80500, loss = 0.1106, acc = 0.9680 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 14:06:31.691952: step 80510, loss = 0.1014, acc = 0.9840 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 14:06:40.784154: step 80520, loss = 0.1039, acc = 0.9740 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 14:06:50.159981: step 80530, loss = 0.0929, acc = 0.9740 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 14:06:59.317551: step 80540, loss = 0.0921, acc = 0.9700 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 14:07:08.374760: step 80550, loss = 0.1284, acc = 0.9560 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 14:07:17.440318: step 80560, loss = 0.1001, acc = 0.9700 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 14:07:26.527182: step 80570, loss = 0.1358, acc = 0.9560 (258.7 examples/sec; 0.247 sec/batch)
2017-05-03 14:07:35.481608: step 80580, loss = 0.1440, acc = 0.9560 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 14:07:44.520914: step 80590, loss = 0.0928, acc = 0.9760 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 14:07:53.636602: step 80600, loss = 0.0993, acc = 0.9720 (269.7 examples/sec; 0.237 sec/batch)
2017-05-03 14:08:02.710849: step 80610, loss = 0.0989, acc = 0.9720 (263.7 examples/sec; 0.243 sec/batch)
2017-05-03 14:08:11.734585: step 80620, loss = 0.1033, acc = 0.9720 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 14:08:20.819558: step 80630, loss = 0.1150, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 14:08:30.444380: step 80640, loss = 0.1189, acc = 0.9540 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:08:39.434336: step 80650, loss = 0.0957, acc = 0.9740 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 14:08:48.546844: step 80660, loss = 0.0937, acc = 0.9740 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 14:08:57.671563: step 80670, loss = 0.1002, acc = 0.9760 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 14:09:06.654957: step 80680, loss = 0.1019, acc = 0.9760 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 14:09:15.626615: step 80690, loss = 0.1080, acc = 0.9740 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 14:09:24.757273: step 80700, loss = 0.1498, acc = 0.9580 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 14:09:33.634378: step 80710, loss = 0.1442, acc = 0.9480 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 14:09:42.656397: step 80720, loss = 0.1272, acc = 0.9560 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 14:09:51.760924: step 80730, loss = 0.1210, acc = 0.9640 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 14:10:00.808698: step 80740, loss = 0.0999, acc = 0.9760 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 14:10:09.850442: step 80750, loss = 0.0954, acc = 0.9640 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 14:10:18.808370: step 80760, loss = 0.0791, acc = 0.9800 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:10:27.808636: step 80770, loss = 0.0991, acc = 0.9660 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 14:10:36.947189: step 80780, loss = 0.1431, acc = 0.9580 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 14:10:46.005239: step 80790, loss = 0.1077, acc = 0.9700 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 14:10:55.052319: step 80800, loss = 0.1015, acc = 0.9680 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 14:11:04.068705: step 80810, loss = 0.1270, acc = 0.9580 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 14:11:12.950805: step 80820, loss = 0.1006, acc = 0.9760 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 14:11:22.062625: step 80830, loss = 0.0952, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 14:11:31.004536: step 80840, loss = 0.1321, acc = 0.9620 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 14:11:39.970329: step 80850, loss = 0.1004, acc = 0.9680 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 14:11:49.014088: step 80860, loss = 0.1365, acc = 0.9460 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 14:11:57.971239: step 80870, loss = 0.1117, acc = 0.9640 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 14:12:06.811701: step 80880, loss = 0.1092, acc = 0.9700 (300.3 examples/sec; 0.213 sec/batch)
2017-05-03 14:12:15.744361: step 80890, loss = 0.1083, acc = 0.9740 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 14:12:24.636962: step 80900, loss = 0.1453, acc = 0.9520 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 14:12:33.730560: step 80910, loss = 0.1150, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 14:12:42.742428: step 80920, loss = 0.1265, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 14:12:51.692741: step 80930, loss = 0.1215, acc = 0.9620 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 14:13:00.677699: step 80940, loss = 0.0908, acc = 0.9800 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 14:13:09.663776: step 80950, loss = 0.1345, acc = 0.9480 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 14:13:18.671938: step 80960, loss = 0.1253, acc = 0.9640 (266.5 examples/sec; 0.240 sec/batch)
2017-05-03 14:13:27.894547: step 80970, loss = 0.1084, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 14:13:36.916744: step 80980, loss = 0.1110, acc = 0.9640 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 14:13:45.900585: step 80990, loss = 0.1081, acc = 0.9620 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 14:13:55.189462: step 81000, loss = 0.1323, acc = 0.9500 (275.7 examples/sec; 0.232 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 14:13:55.710891: step 81000, loss = 0.1031, acc = 0.9625, f1neg = 0.9593, f1pos = 0.9652, f1 = 0.9623
[Eval_batch(1)(2000,4000)] 2017-05-03 14:13:56.180444: step 81000, loss = 0.1143, acc = 0.9660, f1neg = 0.9608, f1pos = 0.9700, f1 = 0.9654
[Eval_batch(2)(2000,6000)] 2017-05-03 14:13:56.688293: step 81000, loss = 0.1210, acc = 0.9585, f1neg = 0.9564, f1pos = 0.9604, f1 = 0.9584
[Eval_batch(3)(2000,8000)] 2017-05-03 14:13:57.158671: step 81000, loss = 0.1228, acc = 0.9615, f1neg = 0.9619, f1pos = 0.9611, f1 = 0.9615
[Eval_batch(4)(2000,10000)] 2017-05-03 14:13:57.635999: step 81000, loss = 0.1246, acc = 0.9565, f1neg = 0.9582, f1pos = 0.9547, f1 = 0.9564
[Eval_batch(5)(2000,12000)] 2017-05-03 14:13:58.138220: step 81000, loss = 0.1250, acc = 0.9510, f1neg = 0.9481, f1pos = 0.9536, f1 = 0.9508
[Eval_batch(6)(2000,14000)] 2017-05-03 14:13:58.580192: step 81000, loss = 0.1128, acc = 0.9685, f1neg = 0.9682, f1pos = 0.9688, f1 = 0.9685
[Eval_batch(7)(2000,16000)] 2017-05-03 14:13:59.052426: step 81000, loss = 0.1176, acc = 0.9615, f1neg = 0.9547, f1pos = 0.9665, f1 = 0.9606
[Eval_batch(8)(2000,18000)] 2017-05-03 14:13:59.520658: step 81000, loss = 0.1083, acc = 0.9665, f1neg = 0.9624, f1pos = 0.9698, f1 = 0.9661
[Eval_batch(9)(2000,20000)] 2017-05-03 14:14:00.017453: step 81000, loss = 0.1139, acc = 0.9630, f1neg = 0.9607, f1pos = 0.9650, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-03 14:14:00.486610: step 81000, loss = 0.1203, acc = 0.9630, f1neg = 0.9611, f1pos = 0.9647, f1 = 0.9629
[Eval_batch(11)(2000,24000)] 2017-05-03 14:14:00.985031: step 81000, loss = 0.1237, acc = 0.9595, f1neg = 0.9537, f1pos = 0.9640, f1 = 0.9589
[Eval_batch(12)(2000,26000)] 2017-05-03 14:14:01.489132: step 81000, loss = 0.1191, acc = 0.9525, f1neg = 0.9519, f1pos = 0.9531, f1 = 0.9525
[Eval_batch(13)(2000,28000)] 2017-05-03 14:14:02.002105: step 81000, loss = 0.1093, acc = 0.9685, f1neg = 0.9610, f1pos = 0.9736, f1 = 0.9673
[Eval_batch(14)(2000,30000)] 2017-05-03 14:14:02.478061: step 81000, loss = 0.1369, acc = 0.9515, f1neg = 0.9432, f1pos = 0.9577, f1 = 0.9504
[Eval_batch(15)(2000,32000)] 2017-05-03 14:14:02.956250: step 81000, loss = 0.1113, acc = 0.9680, f1neg = 0.9658, f1pos = 0.9700, f1 = 0.9679
[Eval_batch(16)(2000,34000)] 2017-05-03 14:14:03.464953: step 81000, loss = 0.1060, acc = 0.9655, f1neg = 0.9645, f1pos = 0.9665, f1 = 0.9655
[Eval_batch(17)(2000,36000)] 2017-05-03 14:14:03.975042: step 81000, loss = 0.1360, acc = 0.9605, f1neg = 0.9611, f1pos = 0.9599, f1 = 0.9605
[Eval_batch(18)(2000,38000)] 2017-05-03 14:14:04.484455: step 81000, loss = 0.1240, acc = 0.9605, f1neg = 0.9626, f1pos = 0.9581, f1 = 0.9604
[Eval_batch(19)(2000,40000)] 2017-05-03 14:14:04.998557: step 81000, loss = 0.1115, acc = 0.9675, f1neg = 0.9628, f1pos = 0.9711, f1 = 0.9670
[Eval_batch(20)(2000,42000)] 2017-05-03 14:14:05.462889: step 81000, loss = 0.1097, acc = 0.9650, f1neg = 0.9692, f1pos = 0.9595, f1 = 0.9644
[Eval_batch(21)(2000,44000)] 2017-05-03 14:14:05.935259: step 81000, loss = 0.1099, acc = 0.9645, f1neg = 0.9617, f1pos = 0.9669, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 14:14:06.411409: step 81000, loss = 0.1145, acc = 0.9610, f1neg = 0.9617, f1pos = 0.9603, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 14:14:06.886757: step 81000, loss = 0.1226, acc = 0.9635, f1neg = 0.9594, f1pos = 0.9668, f1 = 0.9631
[Eval_batch(24)(2000,50000)] 2017-05-03 14:14:07.321718: step 81000, loss = 0.1055, acc = 0.9675, f1neg = 0.9632, f1pos = 0.9709, f1 = 0.9671
[Eval_batch(25)(2000,52000)] 2017-05-03 14:14:07.820896: step 81000, loss = 0.0956, acc = 0.9725, f1neg = 0.9701, f1pos = 0.9745, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 14:14:08.268566: step 81000, loss = 0.1075, acc = 0.9655, f1neg = 0.9678, f1pos = 0.9629, f1 = 0.9653
[Eval_batch(27)(2000,56000)] 2017-05-03 14:14:08.858487: step 81000, loss = 0.0940, acc = 0.9735, f1neg = 0.9724, f1pos = 0.9745, f1 = 0.9735
[Eval] 2017-05-03 14:14:08.858548: step 81000, acc = 0.9631, f1 = 0.9628
[Test_batch(0)(2000,2000)] 2017-05-03 14:14:09.358856: step 81000, loss = 0.1439, acc = 0.9550, f1neg = 0.9578, f1pos = 0.9518, f1 = 0.9548
[Test_batch(1)(2000,4000)] 2017-05-03 14:14:09.856873: step 81000, loss = 0.1227, acc = 0.9595, f1neg = 0.9644, f1pos = 0.9531, f1 = 0.9587
[Test_batch(2)(2000,6000)] 2017-05-03 14:14:10.364464: step 81000, loss = 0.1399, acc = 0.9560, f1neg = 0.9600, f1pos = 0.9511, f1 = 0.9556
[Test_batch(3)(2000,8000)] 2017-05-03 14:14:10.867721: step 81000, loss = 0.1474, acc = 0.9495, f1neg = 0.9532, f1pos = 0.9452, f1 = 0.9492
[Test_batch(4)(2000,10000)] 2017-05-03 14:14:11.368016: step 81000, loss = 0.1446, acc = 0.9530, f1neg = 0.9538, f1pos = 0.9522, f1 = 0.9530
[Test_batch(5)(2000,12000)] 2017-05-03 14:14:11.834788: step 81000, loss = 0.1588, acc = 0.9445, f1neg = 0.9459, f1pos = 0.9430, f1 = 0.9445
[Test_batch(6)(2000,14000)] 2017-05-03 14:14:12.298859: step 81000, loss = 0.1401, acc = 0.9515, f1neg = 0.9504, f1pos = 0.9526, f1 = 0.9515
[Test_batch(7)(2000,16000)] 2017-05-03 14:14:12.774006: step 81000, loss = 0.1326, acc = 0.9545, f1neg = 0.9591, f1pos = 0.9487, f1 = 0.9539
[Test_batch(8)(2000,18000)] 2017-05-03 14:14:13.292248: step 81000, loss = 0.1351, acc = 0.9515, f1neg = 0.9513, f1pos = 0.9517, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 14:14:13.798278: step 81000, loss = 0.1656, acc = 0.9360, f1neg = 0.9339, f1pos = 0.9380, f1 = 0.9359
[Test_batch(10)(2000,22000)] 2017-05-03 14:14:14.277154: step 81000, loss = 0.1474, acc = 0.9505, f1neg = 0.9450, f1pos = 0.9550, f1 = 0.9500
[Test_batch(11)(2000,24000)] 2017-05-03 14:14:14.717116: step 81000, loss = 0.1439, acc = 0.9525, f1neg = 0.9543, f1pos = 0.9505, f1 = 0.9524
[Test_batch(12)(2000,26000)] 2017-05-03 14:14:15.220304: step 81000, loss = 0.1195, acc = 0.9620, f1neg = 0.9637, f1pos = 0.9601, f1 = 0.9619
[Test_batch(13)(2000,28000)] 2017-05-03 14:14:15.688555: step 81000, loss = 0.1434, acc = 0.9545, f1neg = 0.9503, f1pos = 0.9580, f1 = 0.9542
[Test_batch(14)(2000,30000)] 2017-05-03 14:14:16.193625: step 81000, loss = 0.1207, acc = 0.9620, f1neg = 0.9569, f1pos = 0.9660, f1 = 0.9615
[Test_batch(15)(2000,32000)] 2017-05-03 14:14:16.669606: step 81000, loss = 0.1441, acc = 0.9575, f1neg = 0.9574, f1pos = 0.9576, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 14:14:17.171826: step 81000, loss = 0.1246, acc = 0.9585, f1neg = 0.9580, f1pos = 0.9590, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 14:14:17.617819: step 81000, loss = 0.1134, acc = 0.9650, f1neg = 0.9621, f1pos = 0.9675, f1 = 0.9648
[Test_batch(18)(2000,38000)] 2017-05-03 14:14:18.160365: step 81000, loss = 0.1067, acc = 0.9640, f1neg = 0.9636, f1pos = 0.9644, f1 = 0.9640
[Test] 2017-05-03 14:14:18.160448: step 81000, acc = 0.9546, f1 = 0.9544
[Status] 2017-05-03 14:14:18.160472: step 81000, maxindex = 78000, maxdev = 0.9633, maxtst = 0.9546
2017-05-03 14:14:27.196236: step 81010, loss = 0.1082, acc = 0.9620 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:14:36.253718: step 81020, loss = 0.1178, acc = 0.9700 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 14:14:45.251404: step 81030, loss = 0.1081, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 14:14:54.166985: step 81040, loss = 0.1105, acc = 0.9660 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 14:15:03.189661: step 81050, loss = 0.1217, acc = 0.9700 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 14:15:12.211711: step 81060, loss = 0.1198, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 14:15:21.267590: step 81070, loss = 0.1127, acc = 0.9680 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 14:15:30.332667: step 81080, loss = 0.1172, acc = 0.9680 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 14:15:39.700390: step 81090, loss = 0.1224, acc = 0.9700 (204.1 examples/sec; 0.314 sec/batch)
2017-05-03 14:15:48.667086: step 81100, loss = 0.1178, acc = 0.9580 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 14:15:57.665213: step 81110, loss = 0.0926, acc = 0.9760 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 14:16:06.783468: step 81120, loss = 0.1267, acc = 0.9500 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:16:16.018475: step 81130, loss = 0.1123, acc = 0.9760 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 14:16:25.162500: step 81140, loss = 0.1203, acc = 0.9580 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 14:16:34.063902: step 81150, loss = 0.1163, acc = 0.9620 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 14:16:43.293060: step 81160, loss = 0.1149, acc = 0.9620 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 14:16:52.419350: step 81170, loss = 0.1228, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 14:17:01.455277: step 81180, loss = 0.1190, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:17:10.587358: step 81190, loss = 0.1184, acc = 0.9720 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 14:17:19.542305: step 81200, loss = 0.1157, acc = 0.9600 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 14:17:28.518591: step 81210, loss = 0.1134, acc = 0.9660 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 14:17:37.479753: step 81220, loss = 0.1224, acc = 0.9720 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 14:17:46.593145: step 81230, loss = 0.1207, acc = 0.9580 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 14:17:55.698789: step 81240, loss = 0.1281, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 14:18:04.799252: step 81250, loss = 0.1525, acc = 0.9480 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 14:18:13.695208: step 81260, loss = 0.1172, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 14:18:22.689171: step 81270, loss = 0.1284, acc = 0.9540 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 14:18:31.716413: step 81280, loss = 0.1040, acc = 0.9740 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 14:18:40.735218: step 81290, loss = 0.1406, acc = 0.9500 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 14:18:49.688110: step 81300, loss = 0.0881, acc = 0.9740 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 14:18:58.745467: step 81310, loss = 0.1102, acc = 0.9680 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 14:19:07.835779: step 81320, loss = 0.1144, acc = 0.9600 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 14:19:16.889534: step 81330, loss = 0.1021, acc = 0.9680 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 14:19:25.854719: step 81340, loss = 0.1275, acc = 0.9620 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:19:34.628150: step 81350, loss = 0.1416, acc = 0.9640 (302.1 examples/sec; 0.212 sec/batch)
2017-05-03 14:19:43.791441: step 81360, loss = 0.1014, acc = 0.9760 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:19:52.744871: step 81370, loss = 0.1408, acc = 0.9540 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 14:20:01.849720: step 81380, loss = 0.1110, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 14:20:10.760691: step 81390, loss = 0.1147, acc = 0.9620 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 14:20:19.886104: step 81400, loss = 0.1355, acc = 0.9540 (232.7 examples/sec; 0.275 sec/batch)
2017-05-03 14:20:29.167626: step 81410, loss = 0.1088, acc = 0.9700 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 14:20:38.178435: step 81420, loss = 0.1272, acc = 0.9640 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 14:20:46.950793: step 81430, loss = 0.1136, acc = 0.9680 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 14:20:56.260792: step 81440, loss = 0.1205, acc = 0.9660 (201.6 examples/sec; 0.317 sec/batch)
2017-05-03 14:21:05.488395: step 81450, loss = 0.1028, acc = 0.9700 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 14:21:14.459982: step 81460, loss = 0.0840, acc = 0.9780 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 14:21:23.428792: step 81470, loss = 0.1068, acc = 0.9640 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 14:21:32.566936: step 81480, loss = 0.1132, acc = 0.9620 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 14:21:41.417788: step 81490, loss = 0.1249, acc = 0.9660 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 14:21:50.389594: step 81500, loss = 0.1227, acc = 0.9640 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 14:21:59.427453: step 81510, loss = 0.1040, acc = 0.9680 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 14:22:08.563927: step 81520, loss = 0.1301, acc = 0.9540 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 14:22:17.603724: step 81530, loss = 0.1589, acc = 0.9540 (269.9 examples/sec; 0.237 sec/batch)
2017-05-03 14:22:26.509709: step 81540, loss = 0.1300, acc = 0.9540 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 14:22:35.500897: step 81550, loss = 0.1276, acc = 0.9560 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 14:22:44.482176: step 81560, loss = 0.1471, acc = 0.9420 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 14:22:53.560524: step 81570, loss = 0.1350, acc = 0.9540 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 14:23:02.682381: step 81580, loss = 0.1221, acc = 0.9600 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 14:23:11.739688: step 81590, loss = 0.1452, acc = 0.9460 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 14:23:20.869893: step 81600, loss = 0.1264, acc = 0.9520 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 14:23:29.795163: step 81610, loss = 0.1329, acc = 0.9560 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:23:38.788239: step 81620, loss = 0.1392, acc = 0.9540 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 14:23:48.126772: step 81630, loss = 0.1186, acc = 0.9720 (258.5 examples/sec; 0.248 sec/batch)
2017-05-03 14:23:57.163983: step 81640, loss = 0.1043, acc = 0.9600 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 14:24:07.277508: step 81650, loss = 0.0960, acc = 0.9840 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 14:24:16.301587: step 81660, loss = 0.1434, acc = 0.9500 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 14:24:25.324113: step 81670, loss = 0.1370, acc = 0.9620 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 14:24:34.523803: step 81680, loss = 0.1223, acc = 0.9700 (263.4 examples/sec; 0.243 sec/batch)
2017-05-03 14:24:43.532149: step 81690, loss = 0.0977, acc = 0.9760 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 14:24:52.485079: step 81700, loss = 0.0988, acc = 0.9680 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:25:01.644884: step 81710, loss = 0.1095, acc = 0.9700 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 14:25:10.907885: step 81720, loss = 0.1229, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 14:25:19.931360: step 81730, loss = 0.1078, acc = 0.9720 (302.0 examples/sec; 0.212 sec/batch)
2017-05-03 14:25:28.838603: step 81740, loss = 0.1143, acc = 0.9620 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 14:25:37.989367: step 81750, loss = 0.1114, acc = 0.9720 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:25:46.901781: step 81760, loss = 0.0934, acc = 0.9720 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 14:25:55.939004: step 81770, loss = 0.1060, acc = 0.9620 (275.3 examples/sec; 0.233 sec/batch)
2017-05-03 14:26:04.977036: step 81780, loss = 0.1051, acc = 0.9640 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 14:26:14.009778: step 81790, loss = 0.1224, acc = 0.9560 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 14:26:22.967133: step 81800, loss = 0.1234, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 14:26:31.888934: step 81810, loss = 0.1022, acc = 0.9700 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 14:26:40.890663: step 81820, loss = 0.1214, acc = 0.9660 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 14:26:49.834850: step 81830, loss = 0.1019, acc = 0.9760 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 14:26:58.881769: step 81840, loss = 0.1276, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 14:27:07.934660: step 81850, loss = 0.1121, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 14:27:17.146945: step 81860, loss = 0.1100, acc = 0.9640 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 14:27:26.076202: step 81870, loss = 0.1212, acc = 0.9660 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 14:27:35.113301: step 81880, loss = 0.0962, acc = 0.9860 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 14:27:44.346586: step 81890, loss = 0.1258, acc = 0.9640 (272.9 examples/sec; 0.235 sec/batch)
2017-05-03 14:27:53.434903: step 81900, loss = 0.1404, acc = 0.9520 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 14:28:02.528954: step 81910, loss = 0.1408, acc = 0.9700 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 14:28:11.502919: step 81920, loss = 0.1150, acc = 0.9620 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 14:28:20.392985: step 81930, loss = 0.1161, acc = 0.9740 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 14:28:29.311528: step 81940, loss = 0.1068, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 14:28:38.318566: step 81950, loss = 0.1222, acc = 0.9560 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 14:28:47.324814: step 81960, loss = 0.1244, acc = 0.9520 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:28:56.325544: step 81970, loss = 0.1431, acc = 0.9500 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 14:29:05.250918: step 81980, loss = 0.1148, acc = 0.9640 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 14:29:14.336731: step 81990, loss = 0.0943, acc = 0.9700 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 14:29:23.157260: step 82000, loss = 0.1137, acc = 0.9640 (288.9 examples/sec; 0.222 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 14:29:23.657507: step 82000, loss = 0.1040, acc = 0.9600, f1neg = 0.9561, f1pos = 0.9632, f1 = 0.9597
[Eval_batch(1)(2000,4000)] 2017-05-03 14:29:24.129763: step 82000, loss = 0.1100, acc = 0.9660, f1neg = 0.9605, f1pos = 0.9701, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 14:29:24.604218: step 82000, loss = 0.1196, acc = 0.9615, f1neg = 0.9592, f1pos = 0.9636, f1 = 0.9614
[Eval_batch(3)(2000,8000)] 2017-05-03 14:29:25.107918: step 82000, loss = 0.1226, acc = 0.9580, f1neg = 0.9581, f1pos = 0.9579, f1 = 0.9580
[Eval_batch(4)(2000,10000)] 2017-05-03 14:29:25.618477: step 82000, loss = 0.1253, acc = 0.9590, f1neg = 0.9603, f1pos = 0.9576, f1 = 0.9590
[Eval_batch(5)(2000,12000)] 2017-05-03 14:29:26.094703: step 82000, loss = 0.1221, acc = 0.9570, f1neg = 0.9540, f1pos = 0.9597, f1 = 0.9568
[Eval_batch(6)(2000,14000)] 2017-05-03 14:29:26.572535: step 82000, loss = 0.1137, acc = 0.9680, f1neg = 0.9675, f1pos = 0.9685, f1 = 0.9680
[Eval_batch(7)(2000,16000)] 2017-05-03 14:29:27.075754: step 82000, loss = 0.1151, acc = 0.9615, f1neg = 0.9543, f1pos = 0.9667, f1 = 0.9605
[Eval_batch(8)(2000,18000)] 2017-05-03 14:29:27.578523: step 82000, loss = 0.1061, acc = 0.9620, f1neg = 0.9568, f1pos = 0.9661, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 14:29:28.054781: step 82000, loss = 0.1127, acc = 0.9650, f1neg = 0.9625, f1pos = 0.9672, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 14:29:28.566571: step 82000, loss = 0.1184, acc = 0.9630, f1neg = 0.9607, f1pos = 0.9651, f1 = 0.9629
[Eval_batch(11)(2000,24000)] 2017-05-03 14:29:29.079002: step 82000, loss = 0.1205, acc = 0.9625, f1neg = 0.9569, f1pos = 0.9668, f1 = 0.9619
[Eval_batch(12)(2000,26000)] 2017-05-03 14:29:29.542447: step 82000, loss = 0.1166, acc = 0.9545, f1neg = 0.9534, f1pos = 0.9556, f1 = 0.9545
[Eval_batch(13)(2000,28000)] 2017-05-03 14:29:30.046768: step 82000, loss = 0.1077, acc = 0.9660, f1neg = 0.9575, f1pos = 0.9717, f1 = 0.9646
[Eval_batch(14)(2000,30000)] 2017-05-03 14:29:30.546717: step 82000, loss = 0.1350, acc = 0.9560, f1neg = 0.9479, f1pos = 0.9619, f1 = 0.9549
[Eval_batch(15)(2000,32000)] 2017-05-03 14:29:31.023468: step 82000, loss = 0.1089, acc = 0.9685, f1neg = 0.9660, f1pos = 0.9706, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 14:29:31.535112: step 82000, loss = 0.1043, acc = 0.9690, f1neg = 0.9678, f1pos = 0.9701, f1 = 0.9690
[Eval_batch(17)(2000,36000)] 2017-05-03 14:29:32.010234: step 82000, loss = 0.1334, acc = 0.9590, f1neg = 0.9594, f1pos = 0.9586, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 14:29:32.489950: step 82000, loss = 0.1255, acc = 0.9575, f1neg = 0.9593, f1pos = 0.9555, f1 = 0.9574
[Eval_batch(19)(2000,40000)] 2017-05-03 14:29:32.961523: step 82000, loss = 0.1098, acc = 0.9690, f1neg = 0.9643, f1pos = 0.9726, f1 = 0.9685
[Eval_batch(20)(2000,42000)] 2017-05-03 14:29:33.425037: step 82000, loss = 0.1114, acc = 0.9640, f1neg = 0.9681, f1pos = 0.9586, f1 = 0.9634
[Eval_batch(21)(2000,44000)] 2017-05-03 14:29:33.899673: step 82000, loss = 0.1045, acc = 0.9655, f1neg = 0.9626, f1pos = 0.9680, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 14:29:34.372129: step 82000, loss = 0.1140, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9590, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 14:29:34.875374: step 82000, loss = 0.1189, acc = 0.9625, f1neg = 0.9578, f1pos = 0.9663, f1 = 0.9620
[Eval_batch(24)(2000,50000)] 2017-05-03 14:29:35.388053: step 82000, loss = 0.1023, acc = 0.9675, f1neg = 0.9629, f1pos = 0.9711, f1 = 0.9670
[Eval_batch(25)(2000,52000)] 2017-05-03 14:29:35.889472: step 82000, loss = 0.0948, acc = 0.9745, f1neg = 0.9721, f1pos = 0.9765, f1 = 0.9743
[Eval_batch(26)(2000,54000)] 2017-05-03 14:29:36.403618: step 82000, loss = 0.1090, acc = 0.9635, f1neg = 0.9656, f1pos = 0.9611, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 14:29:36.997843: step 82000, loss = 0.0949, acc = 0.9720, f1neg = 0.9707, f1pos = 0.9732, f1 = 0.9719
[Eval] 2017-05-03 14:29:36.997928: step 82000, acc = 0.9633, f1 = 0.9630
[Test_batch(0)(2000,2000)] 2017-05-03 14:29:37.478609: step 82000, loss = 0.1429, acc = 0.9535, f1neg = 0.9559, f1pos = 0.9508, f1 = 0.9534
[Test_batch(1)(2000,4000)] 2017-05-03 14:29:37.954669: step 82000, loss = 0.1227, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9565, f1 = 0.9614
[Test_batch(2)(2000,6000)] 2017-05-03 14:29:38.457406: step 82000, loss = 0.1404, acc = 0.9565, f1neg = 0.9601, f1pos = 0.9522, f1 = 0.9561
[Test_batch(3)(2000,8000)] 2017-05-03 14:29:38.959698: step 82000, loss = 0.1462, acc = 0.9530, f1neg = 0.9560, f1pos = 0.9495, f1 = 0.9528
[Test_batch(4)(2000,10000)] 2017-05-03 14:29:39.446898: step 82000, loss = 0.1411, acc = 0.9530, f1neg = 0.9534, f1pos = 0.9526, f1 = 0.9530
[Test_batch(5)(2000,12000)] 2017-05-03 14:29:39.955434: step 82000, loss = 0.1552, acc = 0.9460, f1neg = 0.9468, f1pos = 0.9452, f1 = 0.9460
[Test_batch(6)(2000,14000)] 2017-05-03 14:29:40.432202: step 82000, loss = 0.1406, acc = 0.9480, f1neg = 0.9463, f1pos = 0.9496, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 14:29:40.906247: step 82000, loss = 0.1326, acc = 0.9525, f1neg = 0.9569, f1pos = 0.9471, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 14:29:41.408738: step 82000, loss = 0.1327, acc = 0.9500, f1neg = 0.9495, f1pos = 0.9504, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 14:29:41.873532: step 82000, loss = 0.1616, acc = 0.9370, f1neg = 0.9338, f1pos = 0.9399, f1 = 0.9368
[Test_batch(10)(2000,22000)] 2017-05-03 14:29:42.359291: step 82000, loss = 0.1406, acc = 0.9545, f1neg = 0.9489, f1pos = 0.9590, f1 = 0.9539
[Test_batch(11)(2000,24000)] 2017-05-03 14:29:42.858822: step 82000, loss = 0.1427, acc = 0.9530, f1neg = 0.9545, f1pos = 0.9514, f1 = 0.9530
[Test_batch(12)(2000,26000)] 2017-05-03 14:29:43.322951: step 82000, loss = 0.1160, acc = 0.9635, f1neg = 0.9650, f1pos = 0.9619, f1 = 0.9634
[Test_batch(13)(2000,28000)] 2017-05-03 14:29:43.788226: step 82000, loss = 0.1398, acc = 0.9535, f1neg = 0.9486, f1pos = 0.9575, f1 = 0.9531
[Test_batch(14)(2000,30000)] 2017-05-03 14:29:44.290672: step 82000, loss = 0.1168, acc = 0.9605, f1neg = 0.9546, f1pos = 0.9650, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 14:29:44.768590: step 82000, loss = 0.1406, acc = 0.9570, f1neg = 0.9563, f1pos = 0.9576, f1 = 0.9570
[Test_batch(16)(2000,34000)] 2017-05-03 14:29:45.250207: step 82000, loss = 0.1213, acc = 0.9585, f1neg = 0.9576, f1pos = 0.9594, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 14:29:45.731738: step 82000, loss = 0.1127, acc = 0.9655, f1neg = 0.9624, f1pos = 0.9682, f1 = 0.9653
[Test_batch(18)(2000,38000)] 2017-05-03 14:29:46.270874: step 82000, loss = 0.1081, acc = 0.9655, f1neg = 0.9648, f1pos = 0.9662, f1 = 0.9655
[Test] 2017-05-03 14:29:46.270961: step 82000, acc = 0.9549, f1 = 0.9547
[Status] 2017-05-03 14:29:46.270986: step 82000, maxindex = 82000, maxdev = 0.9633, maxtst = 0.9549
2017-05-03 14:29:58.442031: step 82010, loss = 0.1340, acc = 0.9580 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 14:30:07.277052: step 82020, loss = 0.0903, acc = 0.9760 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 14:30:16.173498: step 82030, loss = 0.1227, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 14:30:25.135567: step 82040, loss = 0.1303, acc = 0.9600 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 14:30:33.974682: step 82050, loss = 0.1184, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 14:30:42.900099: step 82060, loss = 0.1304, acc = 0.9620 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 14:30:51.888854: step 82070, loss = 0.1181, acc = 0.9640 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 14:31:00.882433: step 82080, loss = 0.1423, acc = 0.9540 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 14:31:09.909713: step 82090, loss = 0.1020, acc = 0.9700 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 14:31:18.966282: step 82100, loss = 0.1325, acc = 0.9560 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 14:31:27.976810: step 82110, loss = 0.1405, acc = 0.9560 (261.3 examples/sec; 0.245 sec/batch)
2017-05-03 14:31:36.777641: step 82120, loss = 0.0994, acc = 0.9820 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 14:31:45.762338: step 82130, loss = 0.1270, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 14:31:54.722512: step 82140, loss = 0.0990, acc = 0.9760 (300.5 examples/sec; 0.213 sec/batch)
2017-05-03 14:32:03.745097: step 82150, loss = 0.1241, acc = 0.9620 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 14:32:12.784028: step 82160, loss = 0.1043, acc = 0.9680 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 14:32:21.681409: step 82170, loss = 0.0942, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 14:32:30.575395: step 82180, loss = 0.1350, acc = 0.9540 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 14:32:39.454412: step 82190, loss = 0.1008, acc = 0.9680 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 14:32:48.584916: step 82200, loss = 0.0943, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 14:32:57.588861: step 82210, loss = 0.1008, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 14:33:06.547396: step 82220, loss = 0.1038, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 14:33:15.432539: step 82230, loss = 0.1310, acc = 0.9600 (296.2 examples/sec; 0.216 sec/batch)
2017-05-03 14:33:24.547742: step 82240, loss = 0.1293, acc = 0.9580 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 14:33:33.432562: step 82250, loss = 0.0939, acc = 0.9720 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:33:42.808733: step 82260, loss = 0.1164, acc = 0.9560 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 14:33:52.056637: step 82270, loss = 0.1779, acc = 0.9360 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 14:34:01.089628: step 82280, loss = 0.0833, acc = 0.9820 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:34:10.131794: step 82290, loss = 0.1390, acc = 0.9540 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 14:34:19.102847: step 82300, loss = 0.1119, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 14:34:28.182658: step 82310, loss = 0.1382, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 14:34:37.187854: step 82320, loss = 0.1175, acc = 0.9680 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 14:34:46.224942: step 82330, loss = 0.1246, acc = 0.9560 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 14:34:55.304622: step 82340, loss = 0.1292, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 14:35:04.248467: step 82350, loss = 0.1081, acc = 0.9680 (295.6 examples/sec; 0.216 sec/batch)
2017-05-03 14:35:13.119261: step 82360, loss = 0.1306, acc = 0.9620 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 14:35:22.410943: step 82370, loss = 0.1063, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 14:35:31.432262: step 82380, loss = 0.1146, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 14:35:40.444699: step 82390, loss = 0.1579, acc = 0.9480 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 14:35:49.478410: step 82400, loss = 0.1052, acc = 0.9720 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 14:35:58.460635: step 82410, loss = 0.1230, acc = 0.9600 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 14:36:07.441736: step 82420, loss = 0.0895, acc = 0.9820 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 14:36:16.487880: step 82430, loss = 0.1109, acc = 0.9640 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 14:36:25.484268: step 82440, loss = 0.1242, acc = 0.9640 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 14:36:34.381278: step 82450, loss = 0.1363, acc = 0.9500 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 14:36:43.354601: step 82460, loss = 0.1147, acc = 0.9640 (273.1 examples/sec; 0.234 sec/batch)
2017-05-03 14:36:52.482483: step 82470, loss = 0.1211, acc = 0.9660 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:37:01.470934: step 82480, loss = 0.1242, acc = 0.9500 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 14:37:10.758648: step 82490, loss = 0.1041, acc = 0.9620 (239.7 examples/sec; 0.267 sec/batch)
2017-05-03 14:37:19.856944: step 82500, loss = 0.1074, acc = 0.9700 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 14:37:28.955226: step 82510, loss = 0.1218, acc = 0.9540 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 14:37:37.955442: step 82520, loss = 0.1012, acc = 0.9700 (267.5 examples/sec; 0.239 sec/batch)
2017-05-03 14:37:46.916964: step 82530, loss = 0.1173, acc = 0.9560 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:37:55.985862: step 82540, loss = 0.0941, acc = 0.9820 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 14:38:05.119608: step 82550, loss = 0.1132, acc = 0.9680 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 14:38:14.181841: step 82560, loss = 0.1148, acc = 0.9740 (270.5 examples/sec; 0.237 sec/batch)
2017-05-03 14:38:23.098197: step 82570, loss = 0.1088, acc = 0.9640 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 14:38:31.971000: step 82580, loss = 0.1123, acc = 0.9700 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 14:38:41.057584: step 82590, loss = 0.1209, acc = 0.9680 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 14:38:50.181947: step 82600, loss = 0.1008, acc = 0.9720 (266.0 examples/sec; 0.241 sec/batch)
2017-05-03 14:38:59.197462: step 82610, loss = 0.1051, acc = 0.9760 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 14:39:08.363007: step 82620, loss = 0.1332, acc = 0.9560 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 14:39:17.396964: step 82630, loss = 0.0970, acc = 0.9820 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 14:39:26.452583: step 82640, loss = 0.1051, acc = 0.9720 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 14:39:35.502917: step 82650, loss = 0.1216, acc = 0.9660 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 14:39:45.279962: step 82660, loss = 0.0928, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 14:39:54.326244: step 82670, loss = 0.1323, acc = 0.9560 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 14:40:03.247213: step 82680, loss = 0.1051, acc = 0.9600 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 14:40:12.268425: step 82690, loss = 0.1221, acc = 0.9700 (271.2 examples/sec; 0.236 sec/batch)
2017-05-03 14:40:21.346632: step 82700, loss = 0.1025, acc = 0.9740 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 14:40:30.485324: step 82710, loss = 0.1490, acc = 0.9500 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 14:40:39.422670: step 82720, loss = 0.1056, acc = 0.9760 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 14:40:48.422843: step 82730, loss = 0.1105, acc = 0.9700 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 14:40:57.547030: step 82740, loss = 0.0920, acc = 0.9720 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 14:41:06.679536: step 82750, loss = 0.0904, acc = 0.9780 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 14:41:15.797349: step 82760, loss = 0.1039, acc = 0.9680 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 14:41:25.053421: step 82770, loss = 0.1028, acc = 0.9760 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 14:41:34.072749: step 82780, loss = 0.1067, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 14:41:43.208128: step 82790, loss = 0.1221, acc = 0.9600 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 14:41:52.250995: step 82800, loss = 0.1162, acc = 0.9560 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 14:42:01.281212: step 82810, loss = 0.1350, acc = 0.9640 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 14:42:10.328407: step 82820, loss = 0.0892, acc = 0.9760 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 14:42:19.476572: step 82830, loss = 0.1267, acc = 0.9560 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 14:42:28.371181: step 82840, loss = 0.1213, acc = 0.9680 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 14:42:37.487100: step 82850, loss = 0.1157, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:42:46.464117: step 82860, loss = 0.1015, acc = 0.9700 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 14:42:55.459590: step 82870, loss = 0.1071, acc = 0.9660 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 14:43:04.372425: step 82880, loss = 0.1084, acc = 0.9680 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 14:43:13.356657: step 82890, loss = 0.1114, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 14:43:22.235980: step 82900, loss = 0.1139, acc = 0.9600 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 14:43:31.343816: step 82910, loss = 0.1086, acc = 0.9720 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:43:40.156584: step 82920, loss = 0.1241, acc = 0.9660 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 14:43:49.209441: step 82930, loss = 0.1417, acc = 0.9520 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 14:43:58.150290: step 82940, loss = 0.0997, acc = 0.9660 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 14:44:07.130675: step 82950, loss = 0.1234, acc = 0.9580 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 14:44:16.490313: step 82960, loss = 0.1065, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 14:44:25.490922: step 82970, loss = 0.1152, acc = 0.9660 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 14:44:34.606777: step 82980, loss = 0.0910, acc = 0.9800 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 14:44:43.592444: step 82990, loss = 0.1260, acc = 0.9500 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 14:44:52.554968: step 83000, loss = 0.1153, acc = 0.9640 (275.2 examples/sec; 0.233 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 14:44:53.066270: step 83000, loss = 0.1056, acc = 0.9610, f1neg = 0.9572, f1pos = 0.9642, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 14:44:53.572706: step 83000, loss = 0.1086, acc = 0.9650, f1neg = 0.9591, f1pos = 0.9694, f1 = 0.9643
[Eval_batch(2)(2000,6000)] 2017-05-03 14:44:54.015419: step 83000, loss = 0.1201, acc = 0.9615, f1neg = 0.9591, f1pos = 0.9636, f1 = 0.9614
[Eval_batch(3)(2000,8000)] 2017-05-03 14:44:54.488651: step 83000, loss = 0.1235, acc = 0.9595, f1neg = 0.9594, f1pos = 0.9596, f1 = 0.9595
[Eval_batch(4)(2000,10000)] 2017-05-03 14:44:54.959747: step 83000, loss = 0.1263, acc = 0.9565, f1neg = 0.9577, f1pos = 0.9552, f1 = 0.9565
[Eval_batch(5)(2000,12000)] 2017-05-03 14:44:55.398609: step 83000, loss = 0.1239, acc = 0.9550, f1neg = 0.9517, f1pos = 0.9579, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 14:44:55.903316: step 83000, loss = 0.1149, acc = 0.9665, f1neg = 0.9659, f1pos = 0.9671, f1 = 0.9665
[Eval_batch(7)(2000,16000)] 2017-05-03 14:44:56.397927: step 83000, loss = 0.1147, acc = 0.9610, f1neg = 0.9533, f1pos = 0.9665, f1 = 0.9599
[Eval_batch(8)(2000,18000)] 2017-05-03 14:44:56.900355: step 83000, loss = 0.1067, acc = 0.9620, f1neg = 0.9567, f1pos = 0.9662, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 14:44:57.338177: step 83000, loss = 0.1131, acc = 0.9645, f1neg = 0.9618, f1pos = 0.9668, f1 = 0.9643
[Eval_batch(10)(2000,22000)] 2017-05-03 14:44:57.812031: step 83000, loss = 0.1196, acc = 0.9615, f1neg = 0.9589, f1pos = 0.9638, f1 = 0.9613
[Eval_batch(11)(2000,24000)] 2017-05-03 14:44:58.287624: step 83000, loss = 0.1192, acc = 0.9625, f1neg = 0.9568, f1pos = 0.9669, f1 = 0.9618
[Eval_batch(12)(2000,26000)] 2017-05-03 14:44:58.770027: step 83000, loss = 0.1178, acc = 0.9535, f1neg = 0.9520, f1pos = 0.9549, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 14:44:59.233710: step 83000, loss = 0.1077, acc = 0.9675, f1neg = 0.9592, f1pos = 0.9730, f1 = 0.9661
[Eval_batch(14)(2000,30000)] 2017-05-03 14:44:59.733177: step 83000, loss = 0.1349, acc = 0.9530, f1neg = 0.9441, f1pos = 0.9594, f1 = 0.9518
[Eval_batch(15)(2000,32000)] 2017-05-03 14:45:00.198815: step 83000, loss = 0.1093, acc = 0.9695, f1neg = 0.9670, f1pos = 0.9717, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 14:45:00.702109: step 83000, loss = 0.1040, acc = 0.9685, f1neg = 0.9671, f1pos = 0.9698, f1 = 0.9684
[Eval_batch(17)(2000,36000)] 2017-05-03 14:45:01.161860: step 83000, loss = 0.1336, acc = 0.9590, f1neg = 0.9593, f1pos = 0.9587, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 14:45:01.667114: step 83000, loss = 0.1273, acc = 0.9575, f1neg = 0.9592, f1pos = 0.9557, f1 = 0.9574
[Eval_batch(19)(2000,40000)] 2017-05-03 14:45:02.132592: step 83000, loss = 0.1102, acc = 0.9675, f1neg = 0.9625, f1pos = 0.9713, f1 = 0.9669
[Eval_batch(20)(2000,42000)] 2017-05-03 14:45:02.637961: step 83000, loss = 0.1125, acc = 0.9615, f1neg = 0.9658, f1pos = 0.9559, f1 = 0.9609
[Eval_batch(21)(2000,44000)] 2017-05-03 14:45:03.113482: step 83000, loss = 0.1035, acc = 0.9640, f1neg = 0.9608, f1pos = 0.9667, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 14:45:03.620095: step 83000, loss = 0.1138, acc = 0.9615, f1neg = 0.9618, f1pos = 0.9612, f1 = 0.9615
[Eval_batch(23)(2000,48000)] 2017-05-03 14:45:04.103780: step 83000, loss = 0.1183, acc = 0.9590, f1neg = 0.9536, f1pos = 0.9633, f1 = 0.9584
[Eval_batch(24)(2000,50000)] 2017-05-03 14:45:04.613040: step 83000, loss = 0.1015, acc = 0.9665, f1neg = 0.9616, f1pos = 0.9703, f1 = 0.9659
[Eval_batch(25)(2000,52000)] 2017-05-03 14:45:05.116606: step 83000, loss = 0.0951, acc = 0.9735, f1neg = 0.9710, f1pos = 0.9756, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 14:45:05.617242: step 83000, loss = 0.1102, acc = 0.9610, f1neg = 0.9631, f1pos = 0.9586, f1 = 0.9609
[Eval_batch(27)(2000,56000)] 2017-05-03 14:45:06.175621: step 83000, loss = 0.0954, acc = 0.9710, f1neg = 0.9696, f1pos = 0.9723, f1 = 0.9709
[Eval] 2017-05-03 14:45:06.175717: step 83000, acc = 0.9625, f1 = 0.9622
[Test_batch(0)(2000,2000)] 2017-05-03 14:45:06.631427: step 83000, loss = 0.1455, acc = 0.9540, f1neg = 0.9562, f1pos = 0.9515, f1 = 0.9539
[Test_batch(1)(2000,4000)] 2017-05-03 14:45:07.074654: step 83000, loss = 0.1240, acc = 0.9620, f1neg = 0.9662, f1pos = 0.9566, f1 = 0.9614
[Test_batch(2)(2000,6000)] 2017-05-03 14:45:07.548583: step 83000, loss = 0.1421, acc = 0.9565, f1neg = 0.9600, f1pos = 0.9524, f1 = 0.9562
[Test_batch(3)(2000,8000)] 2017-05-03 14:45:07.991943: step 83000, loss = 0.1470, acc = 0.9535, f1neg = 0.9563, f1pos = 0.9503, f1 = 0.9533
[Test_batch(4)(2000,10000)] 2017-05-03 14:45:08.471404: step 83000, loss = 0.1415, acc = 0.9510, f1neg = 0.9511, f1pos = 0.9509, f1 = 0.9510
[Test_batch(5)(2000,12000)] 2017-05-03 14:45:08.913929: step 83000, loss = 0.1560, acc = 0.9450, f1neg = 0.9456, f1pos = 0.9444, f1 = 0.9450
[Test_batch(6)(2000,14000)] 2017-05-03 14:45:09.385687: step 83000, loss = 0.1423, acc = 0.9495, f1neg = 0.9476, f1pos = 0.9513, f1 = 0.9494
[Test_batch(7)(2000,16000)] 2017-05-03 14:45:09.854653: step 83000, loss = 0.1333, acc = 0.9550, f1neg = 0.9591, f1pos = 0.9501, f1 = 0.9546
[Test_batch(8)(2000,18000)] 2017-05-03 14:45:10.333468: step 83000, loss = 0.1328, acc = 0.9500, f1neg = 0.9494, f1pos = 0.9506, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 14:45:10.791356: step 83000, loss = 0.1611, acc = 0.9355, f1neg = 0.9319, f1pos = 0.9387, f1 = 0.9353
[Test_batch(10)(2000,22000)] 2017-05-03 14:45:11.239828: step 83000, loss = 0.1389, acc = 0.9530, f1neg = 0.9470, f1pos = 0.9578, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 14:45:11.699045: step 83000, loss = 0.1427, acc = 0.9525, f1neg = 0.9538, f1pos = 0.9511, f1 = 0.9525
[Test_batch(12)(2000,26000)] 2017-05-03 14:45:12.179527: step 83000, loss = 0.1165, acc = 0.9645, f1neg = 0.9658, f1pos = 0.9631, f1 = 0.9644
[Test_batch(13)(2000,28000)] 2017-05-03 14:45:12.656890: step 83000, loss = 0.1393, acc = 0.9535, f1neg = 0.9484, f1pos = 0.9577, f1 = 0.9530
[Test_batch(14)(2000,30000)] 2017-05-03 14:45:13.124949: step 83000, loss = 0.1171, acc = 0.9590, f1neg = 0.9528, f1pos = 0.9637, f1 = 0.9583
[Test_batch(15)(2000,32000)] 2017-05-03 14:45:13.592960: step 83000, loss = 0.1407, acc = 0.9570, f1neg = 0.9563, f1pos = 0.9576, f1 = 0.9570
[Test_batch(16)(2000,34000)] 2017-05-03 14:45:14.066513: step 83000, loss = 0.1217, acc = 0.9590, f1neg = 0.9579, f1pos = 0.9600, f1 = 0.9590
[Test_batch(17)(2000,36000)] 2017-05-03 14:45:14.510920: step 83000, loss = 0.1129, acc = 0.9685, f1neg = 0.9655, f1pos = 0.9710, f1 = 0.9683
[Test_batch(18)(2000,38000)] 2017-05-03 14:45:15.076811: step 83000, loss = 0.1099, acc = 0.9645, f1neg = 0.9638, f1pos = 0.9652, f1 = 0.9645
[Test] 2017-05-03 14:45:15.076892: step 83000, acc = 0.9549, f1 = 0.9547
[Status] 2017-05-03 14:45:15.076917: step 83000, maxindex = 82000, maxdev = 0.9633, maxtst = 0.9549
2017-05-03 14:45:24.215770: step 83010, loss = 0.1299, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 14:45:33.148244: step 83020, loss = 0.1192, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 14:45:42.346448: step 83030, loss = 0.1372, acc = 0.9640 (264.3 examples/sec; 0.242 sec/batch)
2017-05-03 14:45:51.479019: step 83040, loss = 0.1166, acc = 0.9640 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 14:46:00.595349: step 83050, loss = 0.1009, acc = 0.9800 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 14:46:09.563214: step 83060, loss = 0.1090, acc = 0.9640 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 14:46:18.508100: step 83070, loss = 0.1035, acc = 0.9620 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 14:46:27.504875: step 83080, loss = 0.1078, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 14:46:36.642748: step 83090, loss = 0.1211, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 14:46:45.624181: step 83100, loss = 0.1153, acc = 0.9680 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 14:46:54.888136: step 83110, loss = 0.1347, acc = 0.9560 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 14:47:03.949524: step 83120, loss = 0.1315, acc = 0.9560 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 14:47:12.916463: step 83130, loss = 0.1682, acc = 0.9380 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 14:47:22.033057: step 83140, loss = 0.1232, acc = 0.9620 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 14:47:31.442769: step 83150, loss = 0.1046, acc = 0.9640 (222.2 examples/sec; 0.288 sec/batch)
2017-05-03 14:47:40.413385: step 83160, loss = 0.1304, acc = 0.9560 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 14:47:49.452306: step 83170, loss = 0.1604, acc = 0.9500 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 14:47:58.499507: step 83180, loss = 0.1270, acc = 0.9660 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 14:48:07.808016: step 83190, loss = 0.1040, acc = 0.9800 (229.3 examples/sec; 0.279 sec/batch)
2017-05-03 14:48:16.916950: step 83200, loss = 0.1166, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:48:25.860851: step 83210, loss = 0.1134, acc = 0.9560 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 14:48:34.844704: step 83220, loss = 0.1117, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 14:48:43.866233: step 83230, loss = 0.1218, acc = 0.9620 (265.0 examples/sec; 0.242 sec/batch)
2017-05-03 14:48:52.886891: step 83240, loss = 0.0958, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:49:02.024290: step 83250, loss = 0.1186, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 14:49:11.202108: step 83260, loss = 0.1065, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 14:49:20.328987: step 83270, loss = 0.1426, acc = 0.9520 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:49:29.602383: step 83280, loss = 0.1320, acc = 0.9560 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 14:49:38.710860: step 83290, loss = 0.1075, acc = 0.9600 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 14:49:47.992155: step 83300, loss = 0.0921, acc = 0.9800 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 14:49:57.072547: step 83310, loss = 0.1210, acc = 0.9700 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 14:50:05.965642: step 83320, loss = 0.1248, acc = 0.9580 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 14:50:15.061090: step 83330, loss = 0.1184, acc = 0.9640 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 14:50:24.178895: step 83340, loss = 0.1277, acc = 0.9600 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 14:50:33.202058: step 83350, loss = 0.1256, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 14:50:42.463305: step 83360, loss = 0.1141, acc = 0.9540 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 14:50:51.451380: step 83370, loss = 0.1245, acc = 0.9560 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 14:51:00.362170: step 83380, loss = 0.1152, acc = 0.9680 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 14:51:09.408427: step 83390, loss = 0.1223, acc = 0.9680 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 14:51:18.357652: step 83400, loss = 0.1090, acc = 0.9800 (301.4 examples/sec; 0.212 sec/batch)
2017-05-03 14:51:27.462590: step 83410, loss = 0.1120, acc = 0.9680 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 14:51:36.494932: step 83420, loss = 0.1199, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 14:51:45.445743: step 83430, loss = 0.1404, acc = 0.9620 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 14:51:54.498509: step 83440, loss = 0.0957, acc = 0.9780 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 14:52:03.559742: step 83450, loss = 0.1200, acc = 0.9560 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 14:52:12.727075: step 83460, loss = 0.1209, acc = 0.9680 (267.7 examples/sec; 0.239 sec/batch)
2017-05-03 14:52:21.952037: step 83470, loss = 0.1003, acc = 0.9700 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 14:52:30.979660: step 83480, loss = 0.1065, acc = 0.9740 (288.9 examples/sec; 0.221 sec/batch)
2017-05-03 14:52:39.836081: step 83490, loss = 0.1469, acc = 0.9540 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 14:52:48.912652: step 83500, loss = 0.1234, acc = 0.9620 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 14:52:58.118107: step 83510, loss = 0.1054, acc = 0.9720 (258.2 examples/sec; 0.248 sec/batch)
2017-05-03 14:53:07.187425: step 83520, loss = 0.0998, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 14:53:16.628981: step 83530, loss = 0.1188, acc = 0.9560 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 14:53:25.764693: step 83540, loss = 0.1206, acc = 0.9680 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 14:53:34.759962: step 83550, loss = 0.1192, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 14:53:43.604298: step 83560, loss = 0.1340, acc = 0.9480 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 14:53:52.727114: step 83570, loss = 0.1187, acc = 0.9580 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 14:54:01.757493: step 83580, loss = 0.1286, acc = 0.9500 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 14:54:10.930086: step 83590, loss = 0.1278, acc = 0.9580 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 14:54:19.844287: step 83600, loss = 0.1156, acc = 0.9700 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 14:54:28.815788: step 83610, loss = 0.0922, acc = 0.9780 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:54:37.931919: step 83620, loss = 0.1044, acc = 0.9680 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 14:54:47.275016: step 83630, loss = 0.1140, acc = 0.9600 (254.3 examples/sec; 0.252 sec/batch)
2017-05-03 14:54:56.502772: step 83640, loss = 0.1064, acc = 0.9740 (263.9 examples/sec; 0.243 sec/batch)
2017-05-03 14:55:05.631305: step 83650, loss = 0.0970, acc = 0.9740 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 14:55:14.811428: step 83660, loss = 0.1156, acc = 0.9660 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 14:55:24.873349: step 83670, loss = 0.1249, acc = 0.9600 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 14:55:33.854204: step 83680, loss = 0.1381, acc = 0.9580 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 14:55:43.024258: step 83690, loss = 0.1252, acc = 0.9600 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 14:55:52.028942: step 83700, loss = 0.1056, acc = 0.9700 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 14:56:01.082666: step 83710, loss = 0.1346, acc = 0.9520 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 14:56:10.391412: step 83720, loss = 0.1130, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 14:56:19.442144: step 83730, loss = 0.1004, acc = 0.9680 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 14:56:28.303374: step 83740, loss = 0.1084, acc = 0.9680 (291.6 examples/sec; 0.220 sec/batch)
2017-05-03 14:56:37.194589: step 83750, loss = 0.1372, acc = 0.9540 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 14:56:46.091372: step 83760, loss = 0.1042, acc = 0.9720 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 14:56:55.241053: step 83770, loss = 0.1269, acc = 0.9660 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 14:57:04.188519: step 83780, loss = 0.1262, acc = 0.9640 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 14:57:13.112754: step 83790, loss = 0.1077, acc = 0.9640 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 14:57:22.172546: step 83800, loss = 0.1314, acc = 0.9460 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 14:57:31.153837: step 83810, loss = 0.0926, acc = 0.9780 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 14:57:40.105538: step 83820, loss = 0.1029, acc = 0.9640 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 14:57:49.144353: step 83830, loss = 0.1311, acc = 0.9540 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 14:57:58.122722: step 83840, loss = 0.1099, acc = 0.9680 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 14:58:07.217868: step 83850, loss = 0.1057, acc = 0.9660 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 14:58:16.333073: step 83860, loss = 0.1009, acc = 0.9780 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 14:58:25.291666: step 83870, loss = 0.0868, acc = 0.9860 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 14:58:34.291352: step 83880, loss = 0.1018, acc = 0.9680 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 14:58:43.230436: step 83890, loss = 0.0984, acc = 0.9700 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 14:58:52.286525: step 83900, loss = 0.1029, acc = 0.9700 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 14:59:01.668858: step 83910, loss = 0.1053, acc = 0.9700 (227.8 examples/sec; 0.281 sec/batch)
2017-05-03 14:59:10.777214: step 83920, loss = 0.1133, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 14:59:19.822130: step 83930, loss = 0.1146, acc = 0.9700 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 14:59:28.921631: step 83940, loss = 0.1370, acc = 0.9560 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 14:59:37.961865: step 83950, loss = 0.1193, acc = 0.9600 (296.7 examples/sec; 0.216 sec/batch)
2017-05-03 14:59:47.009610: step 83960, loss = 0.1149, acc = 0.9620 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 14:59:56.076953: step 83970, loss = 0.1299, acc = 0.9500 (269.5 examples/sec; 0.237 sec/batch)
2017-05-03 15:00:04.988289: step 83980, loss = 0.0848, acc = 0.9800 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 15:00:13.937471: step 83990, loss = 0.1207, acc = 0.9660 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:00:23.110765: step 84000, loss = 0.1368, acc = 0.9540 (275.8 examples/sec; 0.232 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 15:00:23.621987: step 84000, loss = 0.1033, acc = 0.9615, f1neg = 0.9582, f1pos = 0.9643, f1 = 0.9613
[Eval_batch(1)(2000,4000)] 2017-05-03 15:00:24.093148: step 84000, loss = 0.1139, acc = 0.9645, f1neg = 0.9591, f1pos = 0.9687, f1 = 0.9639
[Eval_batch(2)(2000,6000)] 2017-05-03 15:00:24.563763: step 84000, loss = 0.1206, acc = 0.9595, f1neg = 0.9574, f1pos = 0.9614, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 15:00:25.029137: step 84000, loss = 0.1232, acc = 0.9600, f1neg = 0.9604, f1pos = 0.9596, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 15:00:25.534217: step 84000, loss = 0.1254, acc = 0.9570, f1neg = 0.9587, f1pos = 0.9551, f1 = 0.9569
[Eval_batch(5)(2000,12000)] 2017-05-03 15:00:26.009307: step 84000, loss = 0.1256, acc = 0.9530, f1neg = 0.9503, f1pos = 0.9555, f1 = 0.9529
[Eval_batch(6)(2000,14000)] 2017-05-03 15:00:26.448328: step 84000, loss = 0.1137, acc = 0.9690, f1neg = 0.9687, f1pos = 0.9693, f1 = 0.9690
[Eval_batch(7)(2000,16000)] 2017-05-03 15:00:26.955668: step 84000, loss = 0.1171, acc = 0.9610, f1neg = 0.9540, f1pos = 0.9661, f1 = 0.9601
[Eval_batch(8)(2000,18000)] 2017-05-03 15:00:27.428860: step 84000, loss = 0.1079, acc = 0.9645, f1neg = 0.9600, f1pos = 0.9681, f1 = 0.9641
[Eval_batch(9)(2000,20000)] 2017-05-03 15:00:27.937906: step 84000, loss = 0.1138, acc = 0.9630, f1neg = 0.9607, f1pos = 0.9651, f1 = 0.9629
[Eval_batch(10)(2000,22000)] 2017-05-03 15:00:28.411609: step 84000, loss = 0.1199, acc = 0.9640, f1neg = 0.9621, f1pos = 0.9657, f1 = 0.9639
[Eval_batch(11)(2000,24000)] 2017-05-03 15:00:28.874968: step 84000, loss = 0.1228, acc = 0.9605, f1neg = 0.9549, f1pos = 0.9649, f1 = 0.9599
[Eval_batch(12)(2000,26000)] 2017-05-03 15:00:29.315896: step 84000, loss = 0.1175, acc = 0.9575, f1neg = 0.9568, f1pos = 0.9582, f1 = 0.9575
[Eval_batch(13)(2000,28000)] 2017-05-03 15:00:29.834481: step 84000, loss = 0.1092, acc = 0.9675, f1neg = 0.9598, f1pos = 0.9727, f1 = 0.9662
[Eval_batch(14)(2000,30000)] 2017-05-03 15:00:30.337250: step 84000, loss = 0.1368, acc = 0.9545, f1neg = 0.9464, f1pos = 0.9605, f1 = 0.9534
[Eval_batch(15)(2000,32000)] 2017-05-03 15:00:30.815354: step 84000, loss = 0.1120, acc = 0.9660, f1neg = 0.9636, f1pos = 0.9681, f1 = 0.9659
[Eval_batch(16)(2000,34000)] 2017-05-03 15:00:31.329080: step 84000, loss = 0.1060, acc = 0.9670, f1neg = 0.9660, f1pos = 0.9679, f1 = 0.9670
[Eval_batch(17)(2000,36000)] 2017-05-03 15:00:31.840769: step 84000, loss = 0.1353, acc = 0.9595, f1neg = 0.9601, f1pos = 0.9589, f1 = 0.9595
[Eval_batch(18)(2000,38000)] 2017-05-03 15:00:32.302061: step 84000, loss = 0.1247, acc = 0.9605, f1neg = 0.9626, f1pos = 0.9582, f1 = 0.9604
[Eval_batch(19)(2000,40000)] 2017-05-03 15:00:32.750343: step 84000, loss = 0.1112, acc = 0.9685, f1neg = 0.9639, f1pos = 0.9721, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 15:00:33.196767: step 84000, loss = 0.1106, acc = 0.9655, f1neg = 0.9696, f1pos = 0.9601, f1 = 0.9649
[Eval_batch(21)(2000,44000)] 2017-05-03 15:00:33.674852: step 84000, loss = 0.1090, acc = 0.9645, f1neg = 0.9617, f1pos = 0.9669, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 15:00:34.150880: step 84000, loss = 0.1153, acc = 0.9610, f1neg = 0.9617, f1pos = 0.9603, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 15:00:34.628130: step 84000, loss = 0.1219, acc = 0.9620, f1neg = 0.9576, f1pos = 0.9655, f1 = 0.9616
[Eval_batch(24)(2000,50000)] 2017-05-03 15:00:35.088680: step 84000, loss = 0.1052, acc = 0.9675, f1neg = 0.9632, f1pos = 0.9709, f1 = 0.9670
[Eval_batch(25)(2000,52000)] 2017-05-03 15:00:35.564227: step 84000, loss = 0.0954, acc = 0.9725, f1neg = 0.9701, f1pos = 0.9745, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 15:00:36.039953: step 84000, loss = 0.1091, acc = 0.9650, f1neg = 0.9673, f1pos = 0.9624, f1 = 0.9648
[Eval_batch(27)(2000,56000)] 2017-05-03 15:00:36.668011: step 84000, loss = 0.0939, acc = 0.9735, f1neg = 0.9725, f1pos = 0.9745, f1 = 0.9735
[Eval] 2017-05-03 15:00:36.668102: step 84000, acc = 0.9632, f1 = 0.9629
[Test_batch(0)(2000,2000)] 2017-05-03 15:00:37.168120: step 84000, loss = 0.1438, acc = 0.9535, f1neg = 0.9563, f1pos = 0.9503, f1 = 0.9533
[Test_batch(1)(2000,4000)] 2017-05-03 15:00:37.669328: step 84000, loss = 0.1231, acc = 0.9605, f1neg = 0.9652, f1pos = 0.9544, f1 = 0.9598
[Test_batch(2)(2000,6000)] 2017-05-03 15:00:38.177515: step 84000, loss = 0.1395, acc = 0.9560, f1neg = 0.9600, f1pos = 0.9512, f1 = 0.9556
[Test_batch(3)(2000,8000)] 2017-05-03 15:00:38.676430: step 84000, loss = 0.1471, acc = 0.9500, f1neg = 0.9536, f1pos = 0.9458, f1 = 0.9497
[Test_batch(4)(2000,10000)] 2017-05-03 15:00:39.178171: step 84000, loss = 0.1432, acc = 0.9525, f1neg = 0.9533, f1pos = 0.9517, f1 = 0.9525
[Test_batch(5)(2000,12000)] 2017-05-03 15:00:39.681650: step 84000, loss = 0.1590, acc = 0.9420, f1neg = 0.9434, f1pos = 0.9406, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 15:00:40.163612: step 84000, loss = 0.1401, acc = 0.9500, f1neg = 0.9488, f1pos = 0.9511, f1 = 0.9500
[Test_batch(7)(2000,16000)] 2017-05-03 15:00:40.675796: step 84000, loss = 0.1320, acc = 0.9525, f1neg = 0.9573, f1pos = 0.9465, f1 = 0.9519
[Test_batch(8)(2000,18000)] 2017-05-03 15:00:41.182038: step 84000, loss = 0.1349, acc = 0.9505, f1neg = 0.9504, f1pos = 0.9506, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 15:00:41.663197: step 84000, loss = 0.1658, acc = 0.9380, f1neg = 0.9357, f1pos = 0.9402, f1 = 0.9379
[Test_batch(10)(2000,22000)] 2017-05-03 15:00:42.146200: step 84000, loss = 0.1455, acc = 0.9510, f1neg = 0.9455, f1pos = 0.9555, f1 = 0.9505
[Test_batch(11)(2000,24000)] 2017-05-03 15:00:42.656760: step 84000, loss = 0.1444, acc = 0.9520, f1neg = 0.9538, f1pos = 0.9501, f1 = 0.9519
[Test_batch(12)(2000,26000)] 2017-05-03 15:00:43.163152: step 84000, loss = 0.1188, acc = 0.9595, f1neg = 0.9614, f1pos = 0.9574, f1 = 0.9594
[Test_batch(13)(2000,28000)] 2017-05-03 15:00:43.636903: step 84000, loss = 0.1417, acc = 0.9545, f1neg = 0.9502, f1pos = 0.9581, f1 = 0.9542
[Test_batch(14)(2000,30000)] 2017-05-03 15:00:44.113136: step 84000, loss = 0.1204, acc = 0.9605, f1neg = 0.9551, f1pos = 0.9647, f1 = 0.9599
[Test_batch(15)(2000,32000)] 2017-05-03 15:00:44.591006: step 84000, loss = 0.1447, acc = 0.9570, f1neg = 0.9569, f1pos = 0.9571, f1 = 0.9570
[Test_batch(16)(2000,34000)] 2017-05-03 15:00:45.093624: step 84000, loss = 0.1239, acc = 0.9580, f1neg = 0.9574, f1pos = 0.9586, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 15:00:45.577622: step 84000, loss = 0.1141, acc = 0.9630, f1neg = 0.9599, f1pos = 0.9657, f1 = 0.9628
[Test_batch(18)(2000,38000)] 2017-05-03 15:00:46.128159: step 84000, loss = 0.1067, acc = 0.9655, f1neg = 0.9650, f1pos = 0.9660, f1 = 0.9655
[Test] 2017-05-03 15:00:46.128246: step 84000, acc = 0.9540, f1 = 0.9538
[Status] 2017-05-03 15:00:46.128271: step 84000, maxindex = 82000, maxdev = 0.9633, maxtst = 0.9549
2017-05-03 15:00:55.151597: step 84010, loss = 0.1068, acc = 0.9660 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 15:01:04.116144: step 84020, loss = 0.1314, acc = 0.9640 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 15:01:12.995583: step 84030, loss = 0.0927, acc = 0.9840 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 15:01:22.161644: step 84040, loss = 0.1399, acc = 0.9560 (254.0 examples/sec; 0.252 sec/batch)
2017-05-03 15:01:31.095822: step 84050, loss = 0.1156, acc = 0.9580 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 15:01:39.982604: step 84060, loss = 0.0909, acc = 0.9740 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 15:01:49.119848: step 84070, loss = 0.1235, acc = 0.9560 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 15:01:58.201939: step 84080, loss = 0.0889, acc = 0.9780 (258.3 examples/sec; 0.248 sec/batch)
2017-05-03 15:02:07.430778: step 84090, loss = 0.1332, acc = 0.9580 (270.6 examples/sec; 0.236 sec/batch)
2017-05-03 15:02:16.598157: step 84100, loss = 0.1091, acc = 0.9680 (257.8 examples/sec; 0.248 sec/batch)
2017-05-03 15:02:25.658407: step 84110, loss = 0.0933, acc = 0.9720 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 15:02:34.684117: step 84120, loss = 0.1318, acc = 0.9620 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 15:02:43.725889: step 84130, loss = 0.1334, acc = 0.9540 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 15:02:52.761144: step 84140, loss = 0.0957, acc = 0.9700 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 15:03:01.976546: step 84150, loss = 0.0997, acc = 0.9740 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 15:03:10.894102: step 84160, loss = 0.1171, acc = 0.9620 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 15:03:19.828190: step 84170, loss = 0.1163, acc = 0.9620 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 15:03:28.971292: step 84180, loss = 0.1130, acc = 0.9660 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 15:03:38.244609: step 84190, loss = 0.1194, acc = 0.9740 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 15:03:47.241170: step 84200, loss = 0.1359, acc = 0.9480 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 15:03:56.526098: step 84210, loss = 0.1035, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 15:04:05.791018: step 84220, loss = 0.1128, acc = 0.9720 (264.3 examples/sec; 0.242 sec/batch)
2017-05-03 15:04:15.017062: step 84230, loss = 0.1530, acc = 0.9520 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 15:04:24.032148: step 84240, loss = 0.1280, acc = 0.9440 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:04:32.961286: step 84250, loss = 0.1112, acc = 0.9740 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 15:04:42.213037: step 84260, loss = 0.0966, acc = 0.9760 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 15:04:51.387535: step 84270, loss = 0.1174, acc = 0.9620 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 15:05:00.493664: step 84280, loss = 0.1101, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 15:05:09.592150: step 84290, loss = 0.1049, acc = 0.9660 (295.6 examples/sec; 0.216 sec/batch)
2017-05-03 15:05:18.549257: step 84300, loss = 0.0839, acc = 0.9880 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 15:05:27.694822: step 84310, loss = 0.0919, acc = 0.9700 (265.2 examples/sec; 0.241 sec/batch)
2017-05-03 15:05:36.742972: step 84320, loss = 0.1247, acc = 0.9520 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 15:05:45.915923: step 84330, loss = 0.1491, acc = 0.9420 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 15:05:54.925180: step 84340, loss = 0.1260, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 15:06:03.920006: step 84350, loss = 0.1206, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 15:06:12.895054: step 84360, loss = 0.0994, acc = 0.9680 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 15:06:22.042884: step 84370, loss = 0.1320, acc = 0.9580 (260.1 examples/sec; 0.246 sec/batch)
2017-05-03 15:06:31.133256: step 84380, loss = 0.1104, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 15:06:40.201278: step 84390, loss = 0.1029, acc = 0.9660 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 15:06:49.398541: step 84400, loss = 0.0914, acc = 0.9760 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 15:06:58.540248: step 84410, loss = 0.1034, acc = 0.9700 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 15:07:07.638256: step 84420, loss = 0.1233, acc = 0.9580 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 15:07:16.906345: step 84430, loss = 0.1470, acc = 0.9500 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 15:07:25.845656: step 84440, loss = 0.1208, acc = 0.9620 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 15:07:34.888306: step 84450, loss = 0.0944, acc = 0.9780 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 15:07:43.949902: step 84460, loss = 0.1201, acc = 0.9560 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 15:07:52.945808: step 84470, loss = 0.1079, acc = 0.9780 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 15:08:01.974821: step 84480, loss = 0.1107, acc = 0.9640 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 15:08:10.962411: step 84490, loss = 0.0934, acc = 0.9800 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 15:08:19.860182: step 84500, loss = 0.1156, acc = 0.9680 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 15:08:28.933180: step 84510, loss = 0.1035, acc = 0.9760 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 15:08:37.957792: step 84520, loss = 0.1210, acc = 0.9540 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:08:46.911761: step 84530, loss = 0.1133, acc = 0.9680 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 15:08:55.765314: step 84540, loss = 0.1192, acc = 0.9700 (294.9 examples/sec; 0.217 sec/batch)
2017-05-03 15:09:04.956440: step 84550, loss = 0.0997, acc = 0.9760 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 15:09:14.020034: step 84560, loss = 0.1230, acc = 0.9580 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 15:09:23.084731: step 84570, loss = 0.1275, acc = 0.9540 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 15:09:32.103563: step 84580, loss = 0.1395, acc = 0.9600 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 15:09:41.253140: step 84590, loss = 0.0870, acc = 0.9740 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 15:09:50.261145: step 84600, loss = 0.1160, acc = 0.9660 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 15:09:59.314171: step 84610, loss = 0.1020, acc = 0.9720 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:10:08.390511: step 84620, loss = 0.1118, acc = 0.9780 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 15:10:17.387903: step 84630, loss = 0.1491, acc = 0.9340 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 15:10:26.336845: step 84640, loss = 0.1319, acc = 0.9580 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 15:10:35.496173: step 84650, loss = 0.1151, acc = 0.9700 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 15:10:44.594690: step 84660, loss = 0.1106, acc = 0.9640 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 15:10:53.567889: step 84670, loss = 0.1270, acc = 0.9580 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 15:11:03.068723: step 84680, loss = 0.1066, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 15:11:12.021553: step 84690, loss = 0.1261, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 15:11:21.017061: step 84700, loss = 0.1467, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 15:11:29.976171: step 84710, loss = 0.1346, acc = 0.9440 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 15:11:38.960789: step 84720, loss = 0.1245, acc = 0.9600 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 15:11:48.239744: step 84730, loss = 0.1163, acc = 0.9700 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 15:11:57.297143: step 84740, loss = 0.1127, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 15:12:06.423273: step 84750, loss = 0.1147, acc = 0.9700 (261.7 examples/sec; 0.245 sec/batch)
2017-05-03 15:12:15.405803: step 84760, loss = 0.1203, acc = 0.9660 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 15:12:24.360758: step 84770, loss = 0.1133, acc = 0.9640 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 15:12:33.407402: step 84780, loss = 0.1278, acc = 0.9660 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 15:12:42.409527: step 84790, loss = 0.1045, acc = 0.9720 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 15:12:51.445437: step 84800, loss = 0.1154, acc = 0.9640 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 15:13:00.406636: step 84810, loss = 0.1377, acc = 0.9560 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 15:13:09.552300: step 84820, loss = 0.1200, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 15:13:18.619138: step 84830, loss = 0.0797, acc = 0.9880 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 15:13:27.740228: step 84840, loss = 0.1032, acc = 0.9680 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 15:13:36.789836: step 84850, loss = 0.1094, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 15:13:45.977767: step 84860, loss = 0.1030, acc = 0.9720 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 15:13:55.022741: step 84870, loss = 0.1214, acc = 0.9560 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 15:14:04.182381: step 84880, loss = 0.1231, acc = 0.9640 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 15:14:13.225426: step 84890, loss = 0.1148, acc = 0.9660 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 15:14:22.440614: step 84900, loss = 0.1207, acc = 0.9640 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 15:14:31.364513: step 84910, loss = 0.1034, acc = 0.9660 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 15:14:40.401452: step 84920, loss = 0.1192, acc = 0.9680 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 15:14:49.440707: step 84930, loss = 0.1270, acc = 0.9500 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 15:14:58.773873: step 84940, loss = 0.1154, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 15:15:07.938877: step 84950, loss = 0.1051, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 15:15:17.015777: step 84960, loss = 0.1294, acc = 0.9660 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 15:15:26.052907: step 84970, loss = 0.1080, acc = 0.9700 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 15:15:34.954254: step 84980, loss = 0.1174, acc = 0.9600 (299.0 examples/sec; 0.214 sec/batch)
2017-05-03 15:15:43.903742: step 84990, loss = 0.1191, acc = 0.9580 (269.6 examples/sec; 0.237 sec/batch)
2017-05-03 15:15:52.969622: step 85000, loss = 0.1096, acc = 0.9660 (285.4 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 15:15:53.483637: step 85000, loss = 0.1034, acc = 0.9625, f1neg = 0.9593, f1pos = 0.9652, f1 = 0.9623
[Eval_batch(1)(2000,4000)] 2017-05-03 15:15:53.944522: step 85000, loss = 0.1130, acc = 0.9650, f1neg = 0.9598, f1pos = 0.9690, f1 = 0.9644
[Eval_batch(2)(2000,6000)] 2017-05-03 15:15:54.417505: step 85000, loss = 0.1201, acc = 0.9590, f1neg = 0.9569, f1pos = 0.9609, f1 = 0.9589
[Eval_batch(3)(2000,8000)] 2017-05-03 15:15:54.931379: step 85000, loss = 0.1230, acc = 0.9590, f1neg = 0.9594, f1pos = 0.9586, f1 = 0.9590
[Eval_batch(4)(2000,10000)] 2017-05-03 15:15:55.433171: step 85000, loss = 0.1244, acc = 0.9555, f1neg = 0.9572, f1pos = 0.9536, f1 = 0.9554
[Eval_batch(5)(2000,12000)] 2017-05-03 15:15:55.910607: step 85000, loss = 0.1246, acc = 0.9535, f1neg = 0.9509, f1pos = 0.9558, f1 = 0.9534
[Eval_batch(6)(2000,14000)] 2017-05-03 15:15:56.410146: step 85000, loss = 0.1132, acc = 0.9665, f1neg = 0.9662, f1pos = 0.9668, f1 = 0.9665
[Eval_batch(7)(2000,16000)] 2017-05-03 15:15:56.857960: step 85000, loss = 0.1170, acc = 0.9610, f1neg = 0.9540, f1pos = 0.9662, f1 = 0.9601
[Eval_batch(8)(2000,18000)] 2017-05-03 15:15:57.330189: step 85000, loss = 0.1085, acc = 0.9645, f1neg = 0.9601, f1pos = 0.9680, f1 = 0.9641
[Eval_batch(9)(2000,20000)] 2017-05-03 15:15:57.808343: step 85000, loss = 0.1138, acc = 0.9635, f1neg = 0.9613, f1pos = 0.9655, f1 = 0.9634
[Eval_batch(10)(2000,22000)] 2017-05-03 15:15:58.286425: step 85000, loss = 0.1192, acc = 0.9630, f1neg = 0.9611, f1pos = 0.9647, f1 = 0.9629
[Eval_batch(11)(2000,24000)] 2017-05-03 15:15:58.758976: step 85000, loss = 0.1222, acc = 0.9630, f1neg = 0.9577, f1pos = 0.9671, f1 = 0.9624
[Eval_batch(12)(2000,26000)] 2017-05-03 15:15:59.222471: step 85000, loss = 0.1164, acc = 0.9550, f1neg = 0.9542, f1pos = 0.9558, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 15:15:59.699218: step 85000, loss = 0.1087, acc = 0.9680, f1neg = 0.9604, f1pos = 0.9732, f1 = 0.9668
[Eval_batch(14)(2000,30000)] 2017-05-03 15:16:00.145933: step 85000, loss = 0.1361, acc = 0.9515, f1neg = 0.9430, f1pos = 0.9578, f1 = 0.9504
[Eval_batch(15)(2000,32000)] 2017-05-03 15:16:00.599474: step 85000, loss = 0.1105, acc = 0.9675, f1neg = 0.9652, f1pos = 0.9695, f1 = 0.9674
[Eval_batch(16)(2000,34000)] 2017-05-03 15:16:01.069092: step 85000, loss = 0.1053, acc = 0.9685, f1neg = 0.9675, f1pos = 0.9694, f1 = 0.9685
[Eval_batch(17)(2000,36000)] 2017-05-03 15:16:01.580905: step 85000, loss = 0.1356, acc = 0.9595, f1neg = 0.9601, f1pos = 0.9589, f1 = 0.9595
[Eval_batch(18)(2000,38000)] 2017-05-03 15:16:02.050062: step 85000, loss = 0.1241, acc = 0.9615, f1neg = 0.9634, f1pos = 0.9594, f1 = 0.9614
[Eval_batch(19)(2000,40000)] 2017-05-03 15:16:02.525298: step 85000, loss = 0.1107, acc = 0.9690, f1neg = 0.9645, f1pos = 0.9725, f1 = 0.9685
[Eval_batch(20)(2000,42000)] 2017-05-03 15:16:03.004710: step 85000, loss = 0.1089, acc = 0.9645, f1neg = 0.9687, f1pos = 0.9590, f1 = 0.9638
[Eval_batch(21)(2000,44000)] 2017-05-03 15:16:03.472405: step 85000, loss = 0.1090, acc = 0.9645, f1neg = 0.9617, f1pos = 0.9669, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 15:16:03.944189: step 85000, loss = 0.1146, acc = 0.9610, f1neg = 0.9617, f1pos = 0.9602, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 15:16:04.410110: step 85000, loss = 0.1210, acc = 0.9635, f1neg = 0.9595, f1pos = 0.9668, f1 = 0.9631
[Eval_batch(24)(2000,50000)] 2017-05-03 15:16:04.881530: step 85000, loss = 0.1047, acc = 0.9665, f1neg = 0.9621, f1pos = 0.9700, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 15:16:05.356465: step 85000, loss = 0.0941, acc = 0.9735, f1neg = 0.9712, f1pos = 0.9755, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 15:16:05.794626: step 85000, loss = 0.1079, acc = 0.9650, f1neg = 0.9673, f1pos = 0.9624, f1 = 0.9648
[Eval_batch(27)(2000,56000)] 2017-05-03 15:16:06.350484: step 85000, loss = 0.0940, acc = 0.9730, f1neg = 0.9719, f1pos = 0.9740, f1 = 0.9730
[Eval] 2017-05-03 15:16:06.350574: step 85000, acc = 0.9631, f1 = 0.9628
[Test_batch(0)(2000,2000)] 2017-05-03 15:16:06.822991: step 85000, loss = 0.1443, acc = 0.9545, f1neg = 0.9573, f1pos = 0.9514, f1 = 0.9543
[Test_batch(1)(2000,4000)] 2017-05-03 15:16:07.278309: step 85000, loss = 0.1221, acc = 0.9615, f1neg = 0.9661, f1pos = 0.9554, f1 = 0.9608
[Test_batch(2)(2000,6000)] 2017-05-03 15:16:07.739399: step 85000, loss = 0.1393, acc = 0.9560, f1neg = 0.9600, f1pos = 0.9512, f1 = 0.9556
[Test_batch(3)(2000,8000)] 2017-05-03 15:16:08.210522: step 85000, loss = 0.1469, acc = 0.9510, f1neg = 0.9546, f1pos = 0.9467, f1 = 0.9507
[Test_batch(4)(2000,10000)] 2017-05-03 15:16:08.686359: step 85000, loss = 0.1431, acc = 0.9515, f1neg = 0.9523, f1pos = 0.9506, f1 = 0.9515
[Test_batch(5)(2000,12000)] 2017-05-03 15:16:09.133941: step 85000, loss = 0.1571, acc = 0.9435, f1neg = 0.9449, f1pos = 0.9420, f1 = 0.9435
[Test_batch(6)(2000,14000)] 2017-05-03 15:16:09.592308: step 85000, loss = 0.1401, acc = 0.9520, f1neg = 0.9510, f1pos = 0.9530, f1 = 0.9520
[Test_batch(7)(2000,16000)] 2017-05-03 15:16:10.065412: step 85000, loss = 0.1325, acc = 0.9530, f1neg = 0.9578, f1pos = 0.9470, f1 = 0.9524
[Test_batch(8)(2000,18000)] 2017-05-03 15:16:10.539631: step 85000, loss = 0.1340, acc = 0.9515, f1neg = 0.9514, f1pos = 0.9516, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 15:16:11.043035: step 85000, loss = 0.1655, acc = 0.9385, f1neg = 0.9362, f1pos = 0.9406, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 15:16:11.547465: step 85000, loss = 0.1451, acc = 0.9545, f1neg = 0.9494, f1pos = 0.9587, f1 = 0.9540
[Test_batch(11)(2000,24000)] 2017-05-03 15:16:12.048129: step 85000, loss = 0.1437, acc = 0.9535, f1neg = 0.9553, f1pos = 0.9516, f1 = 0.9534
[Test_batch(12)(2000,26000)] 2017-05-03 15:16:12.542181: step 85000, loss = 0.1194, acc = 0.9610, f1neg = 0.9628, f1pos = 0.9590, f1 = 0.9609
[Test_batch(13)(2000,28000)] 2017-05-03 15:16:13.043185: step 85000, loss = 0.1411, acc = 0.9545, f1neg = 0.9503, f1pos = 0.9580, f1 = 0.9542
[Test_batch(14)(2000,30000)] 2017-05-03 15:16:13.516220: step 85000, loss = 0.1206, acc = 0.9610, f1neg = 0.9557, f1pos = 0.9651, f1 = 0.9604
[Test_batch(15)(2000,32000)] 2017-05-03 15:16:14.008982: step 85000, loss = 0.1430, acc = 0.9585, f1neg = 0.9583, f1pos = 0.9587, f1 = 0.9585
[Test_batch(16)(2000,34000)] 2017-05-03 15:16:14.450795: step 85000, loss = 0.1234, acc = 0.9575, f1neg = 0.9569, f1pos = 0.9581, f1 = 0.9575
[Test_batch(17)(2000,36000)] 2017-05-03 15:16:14.954662: step 85000, loss = 0.1131, acc = 0.9630, f1neg = 0.9600, f1pos = 0.9656, f1 = 0.9628
[Test_batch(18)(2000,38000)] 2017-05-03 15:16:15.500135: step 85000, loss = 0.1071, acc = 0.9670, f1neg = 0.9666, f1pos = 0.9674, f1 = 0.9670
[Test] 2017-05-03 15:16:15.500212: step 85000, acc = 0.9549, f1 = 0.9547
[Status] 2017-05-03 15:16:15.500231: step 85000, maxindex = 82000, maxdev = 0.9633, maxtst = 0.9549
2017-05-03 15:16:24.395907: step 85010, loss = 0.1218, acc = 0.9580 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 15:16:33.369166: step 85020, loss = 0.1178, acc = 0.9660 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 15:16:42.493979: step 85030, loss = 0.1257, acc = 0.9640 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 15:16:51.551506: step 85040, loss = 0.1074, acc = 0.9720 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 15:17:00.451871: step 85050, loss = 0.1026, acc = 0.9600 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 15:17:09.521543: step 85060, loss = 0.0925, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 15:17:18.656703: step 85070, loss = 0.1313, acc = 0.9560 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 15:17:27.716830: step 85080, loss = 0.1155, acc = 0.9600 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 15:17:36.770515: step 85090, loss = 0.1063, acc = 0.9640 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 15:17:45.791474: step 85100, loss = 0.1135, acc = 0.9700 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 15:17:54.988700: step 85110, loss = 0.1176, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 15:18:04.022798: step 85120, loss = 0.1150, acc = 0.9580 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 15:18:13.195953: step 85130, loss = 0.1001, acc = 0.9760 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 15:18:22.217418: step 85140, loss = 0.0923, acc = 0.9780 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 15:18:31.385238: step 85150, loss = 0.1027, acc = 0.9620 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 15:18:40.658204: step 85160, loss = 0.1299, acc = 0.9660 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 15:18:49.669845: step 85170, loss = 0.0830, acc = 0.9780 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 15:18:58.642467: step 85180, loss = 0.1014, acc = 0.9740 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 15:19:07.724637: step 85190, loss = 0.1260, acc = 0.9540 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 15:19:16.767672: step 85200, loss = 0.1129, acc = 0.9700 (297.4 examples/sec; 0.215 sec/batch)
2017-05-03 15:19:25.802644: step 85210, loss = 0.1045, acc = 0.9660 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 15:19:34.903818: step 85220, loss = 0.1087, acc = 0.9600 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 15:19:43.763236: step 85230, loss = 0.1252, acc = 0.9720 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 15:19:53.050414: step 85240, loss = 0.1117, acc = 0.9600 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 15:20:02.124660: step 85250, loss = 0.0947, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 15:20:11.245525: step 85260, loss = 0.1114, acc = 0.9620 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 15:20:20.401676: step 85270, loss = 0.1048, acc = 0.9680 (277.8 examples/sec; 0.230 sec/batch)
2017-05-03 15:20:29.653857: step 85280, loss = 0.1057, acc = 0.9680 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 15:20:38.970465: step 85290, loss = 0.1116, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 15:20:48.007374: step 85300, loss = 0.1079, acc = 0.9680 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 15:20:57.074900: step 85310, loss = 0.1419, acc = 0.9580 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:21:05.973220: step 85320, loss = 0.1207, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 15:21:14.978961: step 85330, loss = 0.1196, acc = 0.9620 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 15:21:24.069767: step 85340, loss = 0.1317, acc = 0.9540 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:21:33.250270: step 85350, loss = 0.1228, acc = 0.9540 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 15:21:42.418151: step 85360, loss = 0.1088, acc = 0.9720 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 15:21:51.552664: step 85370, loss = 0.0929, acc = 0.9760 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 15:22:00.513023: step 85380, loss = 0.1132, acc = 0.9640 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 15:22:09.697990: step 85390, loss = 0.1180, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 15:22:18.705375: step 85400, loss = 0.1091, acc = 0.9680 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 15:22:27.712755: step 85410, loss = 0.1358, acc = 0.9580 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 15:22:36.604062: step 85420, loss = 0.1149, acc = 0.9620 (306.4 examples/sec; 0.209 sec/batch)
2017-05-03 15:22:45.599302: step 85430, loss = 0.1222, acc = 0.9700 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 15:22:54.602836: step 85440, loss = 0.1174, acc = 0.9600 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 15:23:03.520874: step 85450, loss = 0.1050, acc = 0.9720 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:23:12.602941: step 85460, loss = 0.1220, acc = 0.9500 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 15:23:21.837141: step 85470, loss = 0.0974, acc = 0.9720 (268.2 examples/sec; 0.239 sec/batch)
2017-05-03 15:23:30.831758: step 85480, loss = 0.1075, acc = 0.9700 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 15:23:39.875363: step 85490, loss = 0.1401, acc = 0.9600 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 15:23:48.959763: step 85500, loss = 0.1215, acc = 0.9680 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 15:23:57.903005: step 85510, loss = 0.0925, acc = 0.9800 (302.5 examples/sec; 0.212 sec/batch)
2017-05-03 15:24:06.931865: step 85520, loss = 0.0986, acc = 0.9760 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 15:24:15.828024: step 85530, loss = 0.1035, acc = 0.9660 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 15:24:24.964362: step 85540, loss = 0.1351, acc = 0.9600 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 15:24:34.270698: step 85550, loss = 0.1262, acc = 0.9580 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 15:24:43.186352: step 85560, loss = 0.1274, acc = 0.9540 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 15:24:52.216173: step 85570, loss = 0.1296, acc = 0.9500 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 15:25:01.279070: step 85580, loss = 0.1218, acc = 0.9660 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 15:25:10.214305: step 85590, loss = 0.1375, acc = 0.9540 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 15:25:19.295845: step 85600, loss = 0.1069, acc = 0.9700 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 15:25:28.227769: step 85610, loss = 0.1028, acc = 0.9660 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 15:25:37.454590: step 85620, loss = 0.1237, acc = 0.9680 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 15:25:46.408034: step 85630, loss = 0.1192, acc = 0.9680 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 15:25:55.365864: step 85640, loss = 0.1271, acc = 0.9480 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 15:26:04.346942: step 85650, loss = 0.1284, acc = 0.9600 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 15:26:13.541277: step 85660, loss = 0.1231, acc = 0.9700 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 15:26:22.721221: step 85670, loss = 0.1059, acc = 0.9720 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 15:26:32.798826: step 85680, loss = 0.1342, acc = 0.9600 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 15:26:41.985939: step 85690, loss = 0.1237, acc = 0.9600 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 15:26:51.015162: step 85700, loss = 0.1421, acc = 0.9600 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 15:27:00.035959: step 85710, loss = 0.1126, acc = 0.9700 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 15:27:09.001684: step 85720, loss = 0.1173, acc = 0.9560 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 15:27:18.046091: step 85730, loss = 0.1140, acc = 0.9720 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 15:27:27.009510: step 85740, loss = 0.1115, acc = 0.9580 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 15:27:36.081868: step 85750, loss = 0.0849, acc = 0.9860 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 15:27:45.221705: step 85760, loss = 0.1623, acc = 0.9480 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 15:27:54.250147: step 85770, loss = 0.1079, acc = 0.9660 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 15:28:03.200239: step 85780, loss = 0.1201, acc = 0.9620 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 15:28:12.207195: step 85790, loss = 0.1171, acc = 0.9720 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 15:28:21.180861: step 85800, loss = 0.1307, acc = 0.9660 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 15:28:30.363189: step 85810, loss = 0.1186, acc = 0.9740 (295.6 examples/sec; 0.216 sec/batch)
2017-05-03 15:28:39.410472: step 85820, loss = 0.0814, acc = 0.9780 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 15:28:48.565240: step 85830, loss = 0.1185, acc = 0.9640 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 15:28:57.750259: step 85840, loss = 0.1340, acc = 0.9560 (262.5 examples/sec; 0.244 sec/batch)
2017-05-03 15:29:06.858075: step 85850, loss = 0.1432, acc = 0.9540 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 15:29:15.860144: step 85860, loss = 0.1121, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 15:29:24.950888: step 85870, loss = 0.1307, acc = 0.9600 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 15:29:34.094884: step 85880, loss = 0.1265, acc = 0.9660 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 15:29:43.116314: step 85890, loss = 0.1164, acc = 0.9700 (288.9 examples/sec; 0.221 sec/batch)
2017-05-03 15:29:52.147993: step 85900, loss = 0.1410, acc = 0.9480 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 15:30:00.983989: step 85910, loss = 0.1011, acc = 0.9720 (298.4 examples/sec; 0.214 sec/batch)
2017-05-03 15:30:10.140882: step 85920, loss = 0.1355, acc = 0.9460 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 15:30:19.109668: step 85930, loss = 0.1011, acc = 0.9700 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 15:30:28.063990: step 85940, loss = 0.1015, acc = 0.9800 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 15:30:37.045023: step 85950, loss = 0.1246, acc = 0.9620 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 15:30:46.035938: step 85960, loss = 0.0899, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 15:30:55.074391: step 85970, loss = 0.1516, acc = 0.9500 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 15:31:04.054921: step 85980, loss = 0.1066, acc = 0.9780 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 15:31:13.029783: step 85990, loss = 0.1273, acc = 0.9640 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 15:31:21.954735: step 86000, loss = 0.1172, acc = 0.9580 (299.6 examples/sec; 0.214 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 15:31:22.413958: step 86000, loss = 0.1027, acc = 0.9600, f1neg = 0.9564, f1pos = 0.9631, f1 = 0.9597
[Eval_batch(1)(2000,4000)] 2017-05-03 15:31:22.913211: step 86000, loss = 0.1094, acc = 0.9645, f1neg = 0.9588, f1pos = 0.9688, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 15:31:23.389493: step 86000, loss = 0.1175, acc = 0.9610, f1neg = 0.9587, f1pos = 0.9630, f1 = 0.9609
[Eval_batch(3)(2000,8000)] 2017-05-03 15:31:23.885433: step 86000, loss = 0.1205, acc = 0.9605, f1neg = 0.9607, f1pos = 0.9603, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 15:31:24.387607: step 86000, loss = 0.1237, acc = 0.9575, f1neg = 0.9591, f1pos = 0.9558, f1 = 0.9574
[Eval_batch(5)(2000,12000)] 2017-05-03 15:31:24.864115: step 86000, loss = 0.1220, acc = 0.9545, f1neg = 0.9518, f1pos = 0.9569, f1 = 0.9544
[Eval_batch(6)(2000,14000)] 2017-05-03 15:31:25.331381: step 86000, loss = 0.1127, acc = 0.9690, f1neg = 0.9686, f1pos = 0.9694, f1 = 0.9690
[Eval_batch(7)(2000,16000)] 2017-05-03 15:31:25.834083: step 86000, loss = 0.1143, acc = 0.9600, f1neg = 0.9526, f1pos = 0.9654, f1 = 0.9590
[Eval_batch(8)(2000,18000)] 2017-05-03 15:31:26.330047: step 86000, loss = 0.1062, acc = 0.9640, f1neg = 0.9594, f1pos = 0.9677, f1 = 0.9635
[Eval_batch(9)(2000,20000)] 2017-05-03 15:31:26.837216: step 86000, loss = 0.1119, acc = 0.9650, f1neg = 0.9626, f1pos = 0.9671, f1 = 0.9649
[Eval_batch(10)(2000,22000)] 2017-05-03 15:31:27.345287: step 86000, loss = 0.1184, acc = 0.9630, f1neg = 0.9608, f1pos = 0.9649, f1 = 0.9629
[Eval_batch(11)(2000,24000)] 2017-05-03 15:31:27.852751: step 86000, loss = 0.1198, acc = 0.9615, f1neg = 0.9558, f1pos = 0.9659, f1 = 0.9609
[Eval_batch(12)(2000,26000)] 2017-05-03 15:31:28.350338: step 86000, loss = 0.1155, acc = 0.9565, f1neg = 0.9556, f1pos = 0.9574, f1 = 0.9565
[Eval_batch(13)(2000,28000)] 2017-05-03 15:31:28.847409: step 86000, loss = 0.1070, acc = 0.9675, f1neg = 0.9596, f1pos = 0.9728, f1 = 0.9662
[Eval_batch(14)(2000,30000)] 2017-05-03 15:31:29.349737: step 86000, loss = 0.1338, acc = 0.9550, f1neg = 0.9469, f1pos = 0.9609, f1 = 0.9539
[Eval_batch(15)(2000,32000)] 2017-05-03 15:31:29.854530: step 86000, loss = 0.1096, acc = 0.9675, f1neg = 0.9651, f1pos = 0.9696, f1 = 0.9673
[Eval_batch(16)(2000,34000)] 2017-05-03 15:31:30.358123: step 86000, loss = 0.1035, acc = 0.9695, f1neg = 0.9684, f1pos = 0.9705, f1 = 0.9695
[Eval_batch(17)(2000,36000)] 2017-05-03 15:31:30.837822: step 86000, loss = 0.1337, acc = 0.9595, f1neg = 0.9600, f1pos = 0.9590, f1 = 0.9595
[Eval_batch(18)(2000,38000)] 2017-05-03 15:31:31.300947: step 86000, loss = 0.1241, acc = 0.9605, f1neg = 0.9623, f1pos = 0.9586, f1 = 0.9604
[Eval_batch(19)(2000,40000)] 2017-05-03 15:31:31.774553: step 86000, loss = 0.1090, acc = 0.9690, f1neg = 0.9644, f1pos = 0.9725, f1 = 0.9685
[Eval_batch(20)(2000,42000)] 2017-05-03 15:31:32.280575: step 86000, loss = 0.1094, acc = 0.9640, f1neg = 0.9682, f1pos = 0.9586, f1 = 0.9634
[Eval_batch(21)(2000,44000)] 2017-05-03 15:31:32.782716: step 86000, loss = 0.1051, acc = 0.9635, f1neg = 0.9605, f1pos = 0.9661, f1 = 0.9633
[Eval_batch(22)(2000,46000)] 2017-05-03 15:31:33.288025: step 86000, loss = 0.1127, acc = 0.9605, f1neg = 0.9610, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 15:31:33.747719: step 86000, loss = 0.1186, acc = 0.9630, f1neg = 0.9586, f1pos = 0.9665, f1 = 0.9626
[Eval_batch(24)(2000,50000)] 2017-05-03 15:31:34.190853: step 86000, loss = 0.1021, acc = 0.9685, f1neg = 0.9641, f1pos = 0.9719, f1 = 0.9680
[Eval_batch(25)(2000,52000)] 2017-05-03 15:31:34.677161: step 86000, loss = 0.0931, acc = 0.9730, f1neg = 0.9706, f1pos = 0.9751, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 15:31:35.181158: step 86000, loss = 0.1073, acc = 0.9650, f1neg = 0.9672, f1pos = 0.9625, f1 = 0.9648
[Eval_batch(27)(2000,56000)] 2017-05-03 15:31:35.766999: step 86000, loss = 0.0935, acc = 0.9730, f1neg = 0.9718, f1pos = 0.9741, f1 = 0.9730
[Eval] 2017-05-03 15:31:35.767085: step 86000, acc = 0.9634, f1 = 0.9631
[Test_batch(0)(2000,2000)] 2017-05-03 15:31:36.279201: step 86000, loss = 0.1427, acc = 0.9520, f1neg = 0.9547, f1pos = 0.9489, f1 = 0.9518
[Test_batch(1)(2000,4000)] 2017-05-03 15:31:36.759536: step 86000, loss = 0.1217, acc = 0.9620, f1neg = 0.9664, f1pos = 0.9563, f1 = 0.9613
[Test_batch(2)(2000,6000)] 2017-05-03 15:31:37.210583: step 86000, loss = 0.1382, acc = 0.9580, f1neg = 0.9616, f1pos = 0.9536, f1 = 0.9576
[Test_batch(3)(2000,8000)] 2017-05-03 15:31:37.691385: step 86000, loss = 0.1444, acc = 0.9510, f1neg = 0.9544, f1pos = 0.9470, f1 = 0.9507
[Test_batch(4)(2000,10000)] 2017-05-03 15:31:38.177459: step 86000, loss = 0.1403, acc = 0.9520, f1neg = 0.9526, f1pos = 0.9514, f1 = 0.9520
[Test_batch(5)(2000,12000)] 2017-05-03 15:31:38.643556: step 86000, loss = 0.1565, acc = 0.9440, f1neg = 0.9452, f1pos = 0.9427, f1 = 0.9440
[Test_batch(6)(2000,14000)] 2017-05-03 15:31:39.150253: step 86000, loss = 0.1394, acc = 0.9505, f1neg = 0.9492, f1pos = 0.9518, f1 = 0.9505
[Test_batch(7)(2000,16000)] 2017-05-03 15:31:39.633956: step 86000, loss = 0.1312, acc = 0.9520, f1neg = 0.9567, f1pos = 0.9462, f1 = 0.9514
[Test_batch(8)(2000,18000)] 2017-05-03 15:31:40.103413: step 86000, loss = 0.1322, acc = 0.9515, f1neg = 0.9512, f1pos = 0.9518, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 15:31:40.588729: step 86000, loss = 0.1616, acc = 0.9375, f1neg = 0.9348, f1pos = 0.9400, f1 = 0.9374
[Test_batch(10)(2000,22000)] 2017-05-03 15:31:41.039211: step 86000, loss = 0.1415, acc = 0.9530, f1neg = 0.9475, f1pos = 0.9575, f1 = 0.9525
[Test_batch(11)(2000,24000)] 2017-05-03 15:31:41.510541: step 86000, loss = 0.1420, acc = 0.9530, f1neg = 0.9547, f1pos = 0.9512, f1 = 0.9529
[Test_batch(12)(2000,26000)] 2017-05-03 15:31:41.984647: step 86000, loss = 0.1164, acc = 0.9650, f1neg = 0.9664, f1pos = 0.9634, f1 = 0.9649
[Test_batch(13)(2000,28000)] 2017-05-03 15:31:42.455550: step 86000, loss = 0.1398, acc = 0.9560, f1neg = 0.9518, f1pos = 0.9595, f1 = 0.9557
[Test_batch(14)(2000,30000)] 2017-05-03 15:31:42.925742: step 86000, loss = 0.1173, acc = 0.9595, f1neg = 0.9538, f1pos = 0.9640, f1 = 0.9589
[Test_batch(15)(2000,32000)] 2017-05-03 15:31:43.379523: step 86000, loss = 0.1400, acc = 0.9570, f1neg = 0.9567, f1pos = 0.9573, f1 = 0.9570
[Test_batch(16)(2000,34000)] 2017-05-03 15:31:43.851197: step 86000, loss = 0.1216, acc = 0.9590, f1neg = 0.9582, f1pos = 0.9598, f1 = 0.9590
[Test_batch(17)(2000,36000)] 2017-05-03 15:31:44.342520: step 86000, loss = 0.1116, acc = 0.9670, f1neg = 0.9641, f1pos = 0.9695, f1 = 0.9668
[Test_batch(18)(2000,38000)] 2017-05-03 15:31:44.865417: step 86000, loss = 0.1069, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 15:31:44.865505: step 86000, acc = 0.9550, f1 = 0.9548
[Status] 2017-05-03 15:31:44.865532: step 86000, maxindex = 86000, maxdev = 0.9634, maxtst = 0.9550
2017-05-03 15:31:57.068474: step 86010, loss = 0.1113, acc = 0.9640 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 15:32:06.149869: step 86020, loss = 0.1138, acc = 0.9700 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 15:32:15.363783: step 86030, loss = 0.0928, acc = 0.9720 (275.7 examples/sec; 0.232 sec/batch)
2017-05-03 15:32:24.393505: step 86040, loss = 0.1103, acc = 0.9680 (301.5 examples/sec; 0.212 sec/batch)
2017-05-03 15:32:33.338157: step 86050, loss = 0.0973, acc = 0.9800 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 15:32:42.341738: step 86060, loss = 0.1599, acc = 0.9540 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 15:32:51.404073: step 86070, loss = 0.1028, acc = 0.9760 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 15:33:00.539022: step 86080, loss = 0.1324, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 15:33:09.687831: step 86090, loss = 0.1122, acc = 0.9760 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 15:33:18.735682: step 86100, loss = 0.1222, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:33:27.878319: step 86110, loss = 0.1296, acc = 0.9580 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 15:33:36.926545: step 86120, loss = 0.1235, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 15:33:45.893652: step 86130, loss = 0.1406, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 15:33:54.933059: step 86140, loss = 0.1264, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 15:34:03.964612: step 86150, loss = 0.1113, acc = 0.9620 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 15:34:13.353214: step 86160, loss = 0.1104, acc = 0.9600 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 15:34:22.506582: step 86170, loss = 0.1139, acc = 0.9600 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 15:34:31.698931: step 86180, loss = 0.1236, acc = 0.9600 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 15:34:40.723918: step 86190, loss = 0.0910, acc = 0.9800 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 15:34:49.888225: step 86200, loss = 0.0996, acc = 0.9600 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 15:34:59.271965: step 86210, loss = 0.0983, acc = 0.9700 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 15:35:08.586857: step 86220, loss = 0.1245, acc = 0.9600 (260.0 examples/sec; 0.246 sec/batch)
2017-05-03 15:35:17.695313: step 86230, loss = 0.1003, acc = 0.9660 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:35:26.886315: step 86240, loss = 0.1384, acc = 0.9640 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 15:35:35.728031: step 86250, loss = 0.1100, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 15:35:44.754089: step 86260, loss = 0.0794, acc = 0.9760 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 15:35:53.692609: step 86270, loss = 0.1086, acc = 0.9720 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 15:36:02.765347: step 86280, loss = 0.0955, acc = 0.9740 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 15:36:11.886886: step 86290, loss = 0.1617, acc = 0.9360 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 15:36:20.909068: step 86300, loss = 0.1402, acc = 0.9600 (271.8 examples/sec; 0.236 sec/batch)
2017-05-03 15:36:30.057480: step 86310, loss = 0.1224, acc = 0.9600 (217.4 examples/sec; 0.294 sec/batch)
2017-05-03 15:36:39.076927: step 86320, loss = 0.1103, acc = 0.9700 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 15:36:48.214597: step 86330, loss = 0.1293, acc = 0.9600 (253.9 examples/sec; 0.252 sec/batch)
2017-05-03 15:36:57.611647: step 86340, loss = 0.1239, acc = 0.9600 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 15:37:06.590107: step 86350, loss = 0.1089, acc = 0.9720 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 15:37:15.671458: step 86360, loss = 0.0984, acc = 0.9780 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 15:37:24.820604: step 86370, loss = 0.1055, acc = 0.9720 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 15:37:34.629397: step 86380, loss = 0.0875, acc = 0.9760 (225.6 examples/sec; 0.284 sec/batch)
2017-05-03 15:37:43.679355: step 86390, loss = 0.1464, acc = 0.9340 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 15:37:52.639501: step 86400, loss = 0.1063, acc = 0.9600 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 15:38:01.620264: step 86410, loss = 0.1386, acc = 0.9520 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:38:10.705284: step 86420, loss = 0.1203, acc = 0.9600 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 15:38:19.703233: step 86430, loss = 0.1015, acc = 0.9780 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 15:38:28.860650: step 86440, loss = 0.1167, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 15:38:37.996490: step 86450, loss = 0.1072, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 15:38:47.054362: step 86460, loss = 0.1293, acc = 0.9540 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 15:38:56.251759: step 86470, loss = 0.0899, acc = 0.9740 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 15:39:05.315089: step 86480, loss = 0.1049, acc = 0.9640 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 15:39:14.469315: step 86490, loss = 0.1161, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 15:39:23.456549: step 86500, loss = 0.0925, acc = 0.9640 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 15:39:32.550112: step 86510, loss = 0.1053, acc = 0.9620 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 15:39:41.612766: step 86520, loss = 0.1336, acc = 0.9520 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:39:50.762338: step 86530, loss = 0.1220, acc = 0.9620 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 15:39:59.767132: step 86540, loss = 0.1053, acc = 0.9700 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 15:40:08.921204: step 86550, loss = 0.1175, acc = 0.9720 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 15:40:18.010371: step 86560, loss = 0.1140, acc = 0.9580 (290.5 examples/sec; 0.220 sec/batch)
2017-05-03 15:40:27.125957: step 86570, loss = 0.1150, acc = 0.9620 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:40:36.203035: step 86580, loss = 0.1109, acc = 0.9660 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 15:40:45.224234: step 86590, loss = 0.1026, acc = 0.9680 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 15:40:54.501344: step 86600, loss = 0.0882, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 15:41:03.506788: step 86610, loss = 0.1128, acc = 0.9580 (296.8 examples/sec; 0.216 sec/batch)
2017-05-03 15:41:12.652407: step 86620, loss = 0.1065, acc = 0.9780 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 15:41:21.927013: step 86630, loss = 0.1066, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 15:41:31.026177: step 86640, loss = 0.0961, acc = 0.9740 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 15:41:40.091899: step 86650, loss = 0.1180, acc = 0.9740 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 15:41:49.110862: step 86660, loss = 0.1106, acc = 0.9700 (291.6 examples/sec; 0.220 sec/batch)
2017-05-03 15:41:58.230744: step 86670, loss = 0.1082, acc = 0.9620 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 15:42:07.390631: step 86680, loss = 0.1150, acc = 0.9600 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 15:42:17.189060: step 86690, loss = 0.1234, acc = 0.9640 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 15:42:26.121718: step 86700, loss = 0.0833, acc = 0.9780 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 15:42:35.135898: step 86710, loss = 0.0892, acc = 0.9740 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:42:44.264800: step 86720, loss = 0.1145, acc = 0.9560 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 15:42:53.404953: step 86730, loss = 0.1132, acc = 0.9660 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 15:43:02.572466: step 86740, loss = 0.1023, acc = 0.9640 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 15:43:11.783821: step 86750, loss = 0.1050, acc = 0.9680 (260.6 examples/sec; 0.246 sec/batch)
2017-05-03 15:43:20.841737: step 86760, loss = 0.1175, acc = 0.9600 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 15:43:29.962411: step 86770, loss = 0.1370, acc = 0.9560 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 15:43:38.960039: step 86780, loss = 0.0978, acc = 0.9660 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 15:43:47.947769: step 86790, loss = 0.1021, acc = 0.9740 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 15:43:56.919277: step 86800, loss = 0.1185, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 15:44:05.897382: step 86810, loss = 0.1401, acc = 0.9540 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 15:44:15.176261: step 86820, loss = 0.1894, acc = 0.9420 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 15:44:24.573215: step 86830, loss = 0.1046, acc = 0.9660 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 15:44:33.837055: step 86840, loss = 0.1116, acc = 0.9620 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 15:44:43.004457: step 86850, loss = 0.1018, acc = 0.9640 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 15:44:52.032603: step 86860, loss = 0.1119, acc = 0.9640 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 15:45:01.182728: step 86870, loss = 0.1156, acc = 0.9680 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:45:10.206643: step 86880, loss = 0.1059, acc = 0.9700 (295.7 examples/sec; 0.216 sec/batch)
2017-05-03 15:45:19.350684: step 86890, loss = 0.1358, acc = 0.9580 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 15:45:28.417014: step 86900, loss = 0.1004, acc = 0.9580 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 15:45:37.472678: step 86910, loss = 0.0949, acc = 0.9740 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 15:45:46.476771: step 86920, loss = 0.0876, acc = 0.9780 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 15:45:55.557669: step 86930, loss = 0.1271, acc = 0.9600 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 15:46:04.533689: step 86940, loss = 0.1397, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 15:46:13.536617: step 86950, loss = 0.0921, acc = 0.9800 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 15:46:22.640910: step 86960, loss = 0.1021, acc = 0.9680 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 15:46:31.801085: step 86970, loss = 0.1107, acc = 0.9620 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 15:46:41.015632: step 86980, loss = 0.0982, acc = 0.9720 (255.6 examples/sec; 0.250 sec/batch)
2017-05-03 15:46:50.168205: step 86990, loss = 0.1094, acc = 0.9700 (265.3 examples/sec; 0.241 sec/batch)
2017-05-03 15:46:59.164121: step 87000, loss = 0.1054, acc = 0.9680 (284.5 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 15:46:59.642055: step 87000, loss = 0.1032, acc = 0.9620, f1neg = 0.9588, f1pos = 0.9647, f1 = 0.9618
[Eval_batch(1)(2000,4000)] 2017-05-03 15:47:00.113254: step 87000, loss = 0.1132, acc = 0.9650, f1neg = 0.9597, f1pos = 0.9691, f1 = 0.9644
[Eval_batch(2)(2000,6000)] 2017-05-03 15:47:00.594297: step 87000, loss = 0.1195, acc = 0.9610, f1neg = 0.9590, f1pos = 0.9628, f1 = 0.9609
[Eval_batch(3)(2000,8000)] 2017-05-03 15:47:01.102453: step 87000, loss = 0.1211, acc = 0.9620, f1neg = 0.9624, f1pos = 0.9616, f1 = 0.9620
[Eval_batch(4)(2000,10000)] 2017-05-03 15:47:01.571751: step 87000, loss = 0.1244, acc = 0.9555, f1neg = 0.9573, f1pos = 0.9535, f1 = 0.9554
[Eval_batch(5)(2000,12000)] 2017-05-03 15:47:02.045786: step 87000, loss = 0.1239, acc = 0.9530, f1neg = 0.9503, f1pos = 0.9554, f1 = 0.9529
[Eval_batch(6)(2000,14000)] 2017-05-03 15:47:02.552526: step 87000, loss = 0.1128, acc = 0.9665, f1neg = 0.9662, f1pos = 0.9668, f1 = 0.9665
[Eval_batch(7)(2000,16000)] 2017-05-03 15:47:03.053175: step 87000, loss = 0.1175, acc = 0.9595, f1neg = 0.9522, f1pos = 0.9649, f1 = 0.9585
[Eval_batch(8)(2000,18000)] 2017-05-03 15:47:03.563616: step 87000, loss = 0.1084, acc = 0.9640, f1neg = 0.9597, f1pos = 0.9675, f1 = 0.9636
[Eval_batch(9)(2000,20000)] 2017-05-03 15:47:04.058405: step 87000, loss = 0.1130, acc = 0.9640, f1neg = 0.9618, f1pos = 0.9659, f1 = 0.9639
[Eval_batch(10)(2000,22000)] 2017-05-03 15:47:04.538740: step 87000, loss = 0.1182, acc = 0.9625, f1neg = 0.9605, f1pos = 0.9643, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 15:47:05.048322: step 87000, loss = 0.1217, acc = 0.9595, f1neg = 0.9538, f1pos = 0.9640, f1 = 0.9589
[Eval_batch(12)(2000,26000)] 2017-05-03 15:47:05.498604: step 87000, loss = 0.1168, acc = 0.9535, f1neg = 0.9529, f1pos = 0.9541, f1 = 0.9535
[Eval_batch(13)(2000,28000)] 2017-05-03 15:47:05.977293: step 87000, loss = 0.1088, acc = 0.9705, f1neg = 0.9635, f1pos = 0.9753, f1 = 0.9694
[Eval_batch(14)(2000,30000)] 2017-05-03 15:47:06.483291: step 87000, loss = 0.1361, acc = 0.9520, f1neg = 0.9438, f1pos = 0.9581, f1 = 0.9510
[Eval_batch(15)(2000,32000)] 2017-05-03 15:47:06.956331: step 87000, loss = 0.1112, acc = 0.9670, f1neg = 0.9647, f1pos = 0.9690, f1 = 0.9669
[Eval_batch(16)(2000,34000)] 2017-05-03 15:47:07.450145: step 87000, loss = 0.1047, acc = 0.9660, f1neg = 0.9650, f1pos = 0.9669, f1 = 0.9660
[Eval_batch(17)(2000,36000)] 2017-05-03 15:47:07.919810: step 87000, loss = 0.1346, acc = 0.9605, f1neg = 0.9611, f1pos = 0.9599, f1 = 0.9605
[Eval_batch(18)(2000,38000)] 2017-05-03 15:47:08.367689: step 87000, loss = 0.1242, acc = 0.9625, f1neg = 0.9644, f1pos = 0.9603, f1 = 0.9624
[Eval_batch(19)(2000,40000)] 2017-05-03 15:47:08.838797: step 87000, loss = 0.1098, acc = 0.9685, f1neg = 0.9640, f1pos = 0.9720, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 15:47:09.344622: step 87000, loss = 0.1088, acc = 0.9640, f1neg = 0.9682, f1pos = 0.9585, f1 = 0.9634
[Eval_batch(21)(2000,44000)] 2017-05-03 15:47:09.822121: step 87000, loss = 0.1090, acc = 0.9640, f1neg = 0.9612, f1pos = 0.9664, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 15:47:10.279566: step 87000, loss = 0.1151, acc = 0.9625, f1neg = 0.9632, f1pos = 0.9618, f1 = 0.9625
[Eval_batch(23)(2000,48000)] 2017-05-03 15:47:10.744294: step 87000, loss = 0.1209, acc = 0.9610, f1neg = 0.9565, f1pos = 0.9646, f1 = 0.9606
[Eval_batch(24)(2000,50000)] 2017-05-03 15:47:11.209213: step 87000, loss = 0.1042, acc = 0.9660, f1neg = 0.9615, f1pos = 0.9696, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 15:47:11.688185: step 87000, loss = 0.0942, acc = 0.9740, f1neg = 0.9717, f1pos = 0.9759, f1 = 0.9738
[Eval_batch(26)(2000,54000)] 2017-05-03 15:47:12.156293: step 87000, loss = 0.1068, acc = 0.9655, f1neg = 0.9678, f1pos = 0.9629, f1 = 0.9653
[Eval_batch(27)(2000,56000)] 2017-05-03 15:47:12.698688: step 87000, loss = 0.0933, acc = 0.9725, f1neg = 0.9714, f1pos = 0.9735, f1 = 0.9725
[Eval] 2017-05-03 15:47:12.698778: step 87000, acc = 0.9630, f1 = 0.9627
[Test_batch(0)(2000,2000)] 2017-05-03 15:47:13.167327: step 87000, loss = 0.1431, acc = 0.9525, f1neg = 0.9553, f1pos = 0.9493, f1 = 0.9523
[Test_batch(1)(2000,4000)] 2017-05-03 15:47:13.671047: step 87000, loss = 0.1214, acc = 0.9625, f1neg = 0.9670, f1pos = 0.9566, f1 = 0.9618
[Test_batch(2)(2000,6000)] 2017-05-03 15:47:14.136902: step 87000, loss = 0.1381, acc = 0.9570, f1neg = 0.9609, f1pos = 0.9523, f1 = 0.9566
[Test_batch(3)(2000,8000)] 2017-05-03 15:47:14.641583: step 87000, loss = 0.1467, acc = 0.9515, f1neg = 0.9550, f1pos = 0.9474, f1 = 0.9512
[Test_batch(4)(2000,10000)] 2017-05-03 15:47:15.108745: step 87000, loss = 0.1429, acc = 0.9520, f1neg = 0.9528, f1pos = 0.9512, f1 = 0.9520
[Test_batch(5)(2000,12000)] 2017-05-03 15:47:15.589402: step 87000, loss = 0.1578, acc = 0.9420, f1neg = 0.9434, f1pos = 0.9405, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 15:47:16.062069: step 87000, loss = 0.1397, acc = 0.9500, f1neg = 0.9488, f1pos = 0.9512, f1 = 0.9500
[Test_batch(7)(2000,16000)] 2017-05-03 15:47:16.566242: step 87000, loss = 0.1324, acc = 0.9515, f1neg = 0.9564, f1pos = 0.9454, f1 = 0.9509
[Test_batch(8)(2000,18000)] 2017-05-03 15:47:17.042649: step 87000, loss = 0.1329, acc = 0.9525, f1neg = 0.9524, f1pos = 0.9526, f1 = 0.9525
[Test_batch(9)(2000,20000)] 2017-05-03 15:47:17.555584: step 87000, loss = 0.1649, acc = 0.9385, f1neg = 0.9362, f1pos = 0.9406, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 15:47:18.065978: step 87000, loss = 0.1450, acc = 0.9520, f1neg = 0.9468, f1pos = 0.9563, f1 = 0.9515
[Test_batch(11)(2000,24000)] 2017-05-03 15:47:18.574790: step 87000, loss = 0.1446, acc = 0.9535, f1neg = 0.9553, f1pos = 0.9515, f1 = 0.9534
[Test_batch(12)(2000,26000)] 2017-05-03 15:47:19.055716: step 87000, loss = 0.1187, acc = 0.9590, f1neg = 0.9610, f1pos = 0.9568, f1 = 0.9589
[Test_batch(13)(2000,28000)] 2017-05-03 15:47:19.532139: step 87000, loss = 0.1413, acc = 0.9535, f1neg = 0.9492, f1pos = 0.9571, f1 = 0.9532
[Test_batch(14)(2000,30000)] 2017-05-03 15:47:20.012191: step 87000, loss = 0.1200, acc = 0.9590, f1neg = 0.9536, f1pos = 0.9633, f1 = 0.9584
[Test_batch(15)(2000,32000)] 2017-05-03 15:47:20.509909: step 87000, loss = 0.1420, acc = 0.9580, f1neg = 0.9579, f1pos = 0.9581, f1 = 0.9580
[Test_batch(16)(2000,34000)] 2017-05-03 15:47:20.985309: step 87000, loss = 0.1229, acc = 0.9555, f1neg = 0.9550, f1pos = 0.9560, f1 = 0.9555
[Test_batch(17)(2000,36000)] 2017-05-03 15:47:21.459863: step 87000, loss = 0.1128, acc = 0.9655, f1neg = 0.9625, f1pos = 0.9680, f1 = 0.9653
[Test_batch(18)(2000,38000)] 2017-05-03 15:47:22.023710: step 87000, loss = 0.1063, acc = 0.9650, f1neg = 0.9645, f1pos = 0.9654, f1 = 0.9650
[Test] 2017-05-03 15:47:22.023801: step 87000, acc = 0.9543, f1 = 0.9540
[Status] 2017-05-03 15:47:22.023826: step 87000, maxindex = 86000, maxdev = 0.9634, maxtst = 0.9550
2017-05-03 15:47:30.905602: step 87010, loss = 0.0912, acc = 0.9800 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 15:47:40.705641: step 87020, loss = 0.1258, acc = 0.9560 (293.1 examples/sec; 0.218 sec/batch)
2017-05-03 15:47:50.068064: step 87030, loss = 0.1048, acc = 0.9680 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 15:47:59.081326: step 87040, loss = 0.1466, acc = 0.9560 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:48:08.413794: step 87050, loss = 0.1152, acc = 0.9640 (265.5 examples/sec; 0.241 sec/batch)
2017-05-03 15:48:17.920040: step 87060, loss = 0.1352, acc = 0.9620 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 15:48:26.983637: step 87070, loss = 0.1278, acc = 0.9480 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 15:48:36.886152: step 87080, loss = 0.1238, acc = 0.9600 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 15:48:45.817186: step 87090, loss = 0.0967, acc = 0.9680 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:48:54.942459: step 87100, loss = 0.1126, acc = 0.9720 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 15:49:04.158165: step 87110, loss = 0.1119, acc = 0.9540 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:49:13.288091: step 87120, loss = 0.0824, acc = 0.9780 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 15:49:22.484481: step 87130, loss = 0.0984, acc = 0.9740 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 15:49:31.577192: step 87140, loss = 0.1103, acc = 0.9660 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 15:49:40.619812: step 87150, loss = 0.1163, acc = 0.9640 (266.3 examples/sec; 0.240 sec/batch)
2017-05-03 15:49:49.593397: step 87160, loss = 0.1091, acc = 0.9640 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 15:49:58.768057: step 87170, loss = 0.1197, acc = 0.9560 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 15:50:07.737163: step 87180, loss = 0.0930, acc = 0.9800 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 15:50:16.591836: step 87190, loss = 0.1001, acc = 0.9680 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 15:50:25.766403: step 87200, loss = 0.1113, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 15:50:34.827843: step 87210, loss = 0.1421, acc = 0.9600 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 15:50:43.719899: step 87220, loss = 0.1050, acc = 0.9680 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 15:50:52.719372: step 87230, loss = 0.1010, acc = 0.9720 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 15:51:01.886587: step 87240, loss = 0.1047, acc = 0.9760 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 15:51:10.882944: step 87250, loss = 0.1481, acc = 0.9600 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 15:51:19.959283: step 87260, loss = 0.1125, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 15:51:28.975132: step 87270, loss = 0.1094, acc = 0.9660 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 15:51:38.048147: step 87280, loss = 0.0983, acc = 0.9760 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 15:51:47.210571: step 87290, loss = 0.1440, acc = 0.9500 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 15:51:56.237750: step 87300, loss = 0.1061, acc = 0.9740 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 15:52:05.355665: step 87310, loss = 0.1276, acc = 0.9600 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 15:52:14.649320: step 87320, loss = 0.0968, acc = 0.9720 (231.6 examples/sec; 0.276 sec/batch)
2017-05-03 15:52:23.862468: step 87330, loss = 0.0986, acc = 0.9740 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 15:52:33.077348: step 87340, loss = 0.1194, acc = 0.9620 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:52:42.093304: step 87350, loss = 0.1304, acc = 0.9460 (258.4 examples/sec; 0.248 sec/batch)
2017-05-03 15:52:51.167615: step 87360, loss = 0.1061, acc = 0.9660 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 15:53:00.330941: step 87370, loss = 0.1301, acc = 0.9480 (257.8 examples/sec; 0.248 sec/batch)
2017-05-03 15:53:09.701472: step 87380, loss = 0.1405, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 15:53:18.711257: step 87390, loss = 0.1205, acc = 0.9640 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 15:53:27.741225: step 87400, loss = 0.0962, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:53:36.881805: step 87410, loss = 0.1178, acc = 0.9620 (301.0 examples/sec; 0.213 sec/batch)
2017-05-03 15:53:45.950517: step 87420, loss = 0.1714, acc = 0.9560 (267.4 examples/sec; 0.239 sec/batch)
2017-05-03 15:53:55.108984: step 87430, loss = 0.1183, acc = 0.9620 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 15:54:04.156659: step 87440, loss = 0.1202, acc = 0.9560 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 15:54:13.178153: step 87450, loss = 0.1182, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 15:54:22.527462: step 87460, loss = 0.1347, acc = 0.9520 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 15:54:31.759533: step 87470, loss = 0.1315, acc = 0.9580 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 15:54:40.834664: step 87480, loss = 0.1101, acc = 0.9700 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 15:54:49.917721: step 87490, loss = 0.0975, acc = 0.9760 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 15:54:58.924823: step 87500, loss = 0.1283, acc = 0.9560 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 15:55:08.008679: step 87510, loss = 0.1060, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 15:55:17.118698: step 87520, loss = 0.0956, acc = 0.9760 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 15:55:26.210491: step 87530, loss = 0.1232, acc = 0.9660 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 15:55:35.415050: step 87540, loss = 0.1382, acc = 0.9460 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 15:55:44.518380: step 87550, loss = 0.1195, acc = 0.9620 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 15:55:53.687125: step 87560, loss = 0.1187, acc = 0.9660 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 15:56:02.672898: step 87570, loss = 0.1321, acc = 0.9540 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 15:56:11.608974: step 87580, loss = 0.1162, acc = 0.9620 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 15:56:20.594014: step 87590, loss = 0.1061, acc = 0.9760 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 15:56:29.679230: step 87600, loss = 0.1220, acc = 0.9600 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 15:56:38.713872: step 87610, loss = 0.1285, acc = 0.9660 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 15:56:47.834681: step 87620, loss = 0.1237, acc = 0.9560 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 15:56:57.122331: step 87630, loss = 0.1023, acc = 0.9720 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 15:57:06.000858: step 87640, loss = 0.1145, acc = 0.9600 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 15:57:15.041377: step 87650, loss = 0.1391, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 15:57:24.179406: step 87660, loss = 0.1023, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 15:57:33.290576: step 87670, loss = 0.1138, acc = 0.9680 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 15:57:42.223870: step 87680, loss = 0.1299, acc = 0.9680 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 15:57:51.150652: step 87690, loss = 0.1503, acc = 0.9620 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 15:58:01.152200: step 87700, loss = 0.1361, acc = 0.9480 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 15:58:10.314767: step 87710, loss = 0.1211, acc = 0.9580 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 15:58:19.382824: step 87720, loss = 0.1153, acc = 0.9600 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 15:58:28.322684: step 87730, loss = 0.1144, acc = 0.9680 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 15:58:37.468399: step 87740, loss = 0.0891, acc = 0.9800 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 15:58:46.594217: step 87750, loss = 0.1303, acc = 0.9680 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 15:58:55.643691: step 87760, loss = 0.1175, acc = 0.9640 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 15:59:04.734539: step 87770, loss = 0.1078, acc = 0.9740 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 15:59:13.916922: step 87780, loss = 0.0981, acc = 0.9700 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 15:59:22.814297: step 87790, loss = 0.1238, acc = 0.9640 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 15:59:31.840243: step 87800, loss = 0.0993, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 15:59:40.954637: step 87810, loss = 0.1297, acc = 0.9660 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 15:59:50.274666: step 87820, loss = 0.1169, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 15:59:59.336751: step 87830, loss = 0.1224, acc = 0.9680 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 16:00:08.535173: step 87840, loss = 0.1241, acc = 0.9680 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 16:00:17.715908: step 87850, loss = 0.1093, acc = 0.9640 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 16:00:26.901666: step 87860, loss = 0.1260, acc = 0.9560 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:00:36.057819: step 87870, loss = 0.1228, acc = 0.9640 (270.8 examples/sec; 0.236 sec/batch)
2017-05-03 16:00:45.025450: step 87880, loss = 0.1352, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 16:00:54.693879: step 87890, loss = 0.1247, acc = 0.9740 (207.8 examples/sec; 0.308 sec/batch)
2017-05-03 16:01:03.640144: step 87900, loss = 0.1386, acc = 0.9580 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 16:01:12.752609: step 87910, loss = 0.1375, acc = 0.9580 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 16:01:21.782196: step 87920, loss = 0.1224, acc = 0.9580 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 16:01:30.891410: step 87930, loss = 0.1475, acc = 0.9540 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 16:01:39.874401: step 87940, loss = 0.1211, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 16:01:48.912538: step 87950, loss = 0.1283, acc = 0.9600 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 16:01:57.869792: step 87960, loss = 0.1249, acc = 0.9540 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 16:02:06.915084: step 87970, loss = 0.1196, acc = 0.9660 (298.0 examples/sec; 0.215 sec/batch)
2017-05-03 16:02:15.916044: step 87980, loss = 0.1158, acc = 0.9640 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 16:02:24.970902: step 87990, loss = 0.1174, acc = 0.9560 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 16:02:33.930661: step 88000, loss = 0.1206, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 16:02:34.433097: step 88000, loss = 0.1035, acc = 0.9630, f1neg = 0.9595, f1pos = 0.9660, f1 = 0.9627
[Eval_batch(1)(2000,4000)] 2017-05-03 16:02:34.930644: step 88000, loss = 0.1085, acc = 0.9680, f1neg = 0.9628, f1pos = 0.9719, f1 = 0.9674
[Eval_batch(2)(2000,6000)] 2017-05-03 16:02:35.395202: step 88000, loss = 0.1192, acc = 0.9615, f1neg = 0.9592, f1pos = 0.9636, f1 = 0.9614
[Eval_batch(3)(2000,8000)] 2017-05-03 16:02:35.863331: step 88000, loss = 0.1229, acc = 0.9605, f1neg = 0.9606, f1pos = 0.9604, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 16:02:36.372992: step 88000, loss = 0.1247, acc = 0.9585, f1neg = 0.9598, f1pos = 0.9571, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 16:02:36.847471: step 88000, loss = 0.1227, acc = 0.9545, f1neg = 0.9513, f1pos = 0.9573, f1 = 0.9543
[Eval_batch(6)(2000,14000)] 2017-05-03 16:02:37.322831: step 88000, loss = 0.1136, acc = 0.9690, f1neg = 0.9685, f1pos = 0.9695, f1 = 0.9690
[Eval_batch(7)(2000,16000)] 2017-05-03 16:02:37.787130: step 88000, loss = 0.1139, acc = 0.9615, f1neg = 0.9541, f1pos = 0.9668, f1 = 0.9605
[Eval_batch(8)(2000,18000)] 2017-05-03 16:02:38.249623: step 88000, loss = 0.1064, acc = 0.9635, f1neg = 0.9585, f1pos = 0.9674, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 16:02:38.713484: step 88000, loss = 0.1119, acc = 0.9650, f1neg = 0.9625, f1pos = 0.9672, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 16:02:39.190221: step 88000, loss = 0.1183, acc = 0.9625, f1neg = 0.9600, f1pos = 0.9647, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 16:02:39.658772: step 88000, loss = 0.1197, acc = 0.9615, f1neg = 0.9557, f1pos = 0.9659, f1 = 0.9608
[Eval_batch(12)(2000,26000)] 2017-05-03 16:02:40.119916: step 88000, loss = 0.1156, acc = 0.9575, f1neg = 0.9563, f1pos = 0.9587, f1 = 0.9575
[Eval_batch(13)(2000,28000)] 2017-05-03 16:02:40.598250: step 88000, loss = 0.1073, acc = 0.9665, f1neg = 0.9581, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 16:02:41.070628: step 88000, loss = 0.1341, acc = 0.9570, f1neg = 0.9490, f1pos = 0.9628, f1 = 0.9559
[Eval_batch(15)(2000,32000)] 2017-05-03 16:02:41.566685: step 88000, loss = 0.1084, acc = 0.9695, f1neg = 0.9672, f1pos = 0.9715, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 16:02:42.036626: step 88000, loss = 0.1029, acc = 0.9700, f1neg = 0.9688, f1pos = 0.9711, f1 = 0.9700
[Eval_batch(17)(2000,36000)] 2017-05-03 16:02:42.532869: step 88000, loss = 0.1327, acc = 0.9590, f1neg = 0.9594, f1pos = 0.9586, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 16:02:42.996311: step 88000, loss = 0.1253, acc = 0.9595, f1neg = 0.9612, f1pos = 0.9576, f1 = 0.9594
[Eval_batch(19)(2000,40000)] 2017-05-03 16:02:43.468950: step 88000, loss = 0.1089, acc = 0.9685, f1neg = 0.9637, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 16:02:43.939333: step 88000, loss = 0.1116, acc = 0.9625, f1neg = 0.9668, f1pos = 0.9570, f1 = 0.9619
[Eval_batch(21)(2000,44000)] 2017-05-03 16:02:44.410752: step 88000, loss = 0.1038, acc = 0.9640, f1neg = 0.9609, f1pos = 0.9667, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 16:02:44.884291: step 88000, loss = 0.1127, acc = 0.9605, f1neg = 0.9609, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 16:02:45.359845: step 88000, loss = 0.1185, acc = 0.9605, f1neg = 0.9555, f1pos = 0.9645, f1 = 0.9600
[Eval_batch(24)(2000,50000)] 2017-05-03 16:02:45.829310: step 88000, loss = 0.1019, acc = 0.9680, f1neg = 0.9635, f1pos = 0.9715, f1 = 0.9675
[Eval_batch(25)(2000,52000)] 2017-05-03 16:02:46.310196: step 88000, loss = 0.0940, acc = 0.9735, f1neg = 0.9711, f1pos = 0.9756, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 16:02:46.766012: step 88000, loss = 0.1087, acc = 0.9635, f1neg = 0.9656, f1pos = 0.9611, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 16:02:47.335922: step 88000, loss = 0.0940, acc = 0.9720, f1neg = 0.9707, f1pos = 0.9732, f1 = 0.9719
[Eval] 2017-05-03 16:02:47.336013: step 88000, acc = 0.9636, f1 = 0.9633
[Test_batch(0)(2000,2000)] 2017-05-03 16:02:47.823964: step 88000, loss = 0.1431, acc = 0.9540, f1neg = 0.9564, f1pos = 0.9513, f1 = 0.9539
[Test_batch(1)(2000,4000)] 2017-05-03 16:02:48.290445: step 88000, loss = 0.1226, acc = 0.9625, f1neg = 0.9667, f1pos = 0.9571, f1 = 0.9619
[Test_batch(2)(2000,6000)] 2017-05-03 16:02:48.753681: step 88000, loss = 0.1393, acc = 0.9585, f1neg = 0.9619, f1pos = 0.9544, f1 = 0.9582
[Test_batch(3)(2000,8000)] 2017-05-03 16:02:49.235828: step 88000, loss = 0.1452, acc = 0.9530, f1neg = 0.9560, f1pos = 0.9496, f1 = 0.9528
[Test_batch(4)(2000,10000)] 2017-05-03 16:02:49.714701: step 88000, loss = 0.1405, acc = 0.9525, f1neg = 0.9529, f1pos = 0.9520, f1 = 0.9525
[Test_batch(5)(2000,12000)] 2017-05-03 16:02:50.192355: step 88000, loss = 0.1545, acc = 0.9465, f1neg = 0.9473, f1pos = 0.9457, f1 = 0.9465
[Test_batch(6)(2000,14000)] 2017-05-03 16:02:50.698941: step 88000, loss = 0.1402, acc = 0.9510, f1neg = 0.9494, f1pos = 0.9525, f1 = 0.9509
[Test_batch(7)(2000,16000)] 2017-05-03 16:02:51.164973: step 88000, loss = 0.1314, acc = 0.9535, f1neg = 0.9578, f1pos = 0.9482, f1 = 0.9530
[Test_batch(8)(2000,18000)] 2017-05-03 16:02:51.667291: step 88000, loss = 0.1315, acc = 0.9500, f1neg = 0.9495, f1pos = 0.9505, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 16:02:52.133026: step 88000, loss = 0.1608, acc = 0.9380, f1neg = 0.9349, f1pos = 0.9408, f1 = 0.9379
[Test_batch(10)(2000,22000)] 2017-05-03 16:02:52.623949: step 88000, loss = 0.1393, acc = 0.9545, f1neg = 0.9488, f1pos = 0.9590, f1 = 0.9539
[Test_batch(11)(2000,24000)] 2017-05-03 16:02:53.082691: step 88000, loss = 0.1419, acc = 0.9540, f1neg = 0.9554, f1pos = 0.9525, f1 = 0.9540
[Test_batch(12)(2000,26000)] 2017-05-03 16:02:53.586165: step 88000, loss = 0.1160, acc = 0.9655, f1neg = 0.9668, f1pos = 0.9641, f1 = 0.9654
[Test_batch(13)(2000,28000)] 2017-05-03 16:02:54.094620: step 88000, loss = 0.1385, acc = 0.9535, f1neg = 0.9486, f1pos = 0.9576, f1 = 0.9531
[Test_batch(14)(2000,30000)] 2017-05-03 16:02:54.575277: step 88000, loss = 0.1167, acc = 0.9600, f1neg = 0.9541, f1pos = 0.9646, f1 = 0.9593
[Test_batch(15)(2000,32000)] 2017-05-03 16:02:55.054536: step 88000, loss = 0.1401, acc = 0.9570, f1neg = 0.9565, f1pos = 0.9575, f1 = 0.9570
[Test_batch(16)(2000,34000)] 2017-05-03 16:02:55.516954: step 88000, loss = 0.1214, acc = 0.9600, f1neg = 0.9591, f1pos = 0.9609, f1 = 0.9600
[Test_batch(17)(2000,36000)] 2017-05-03 16:02:56.022646: step 88000, loss = 0.1121, acc = 0.9680, f1neg = 0.9650, f1pos = 0.9705, f1 = 0.9678
[Test_batch(18)(2000,38000)] 2017-05-03 16:02:56.584338: step 88000, loss = 0.1073, acc = 0.9645, f1neg = 0.9638, f1pos = 0.9652, f1 = 0.9645
[Test] 2017-05-03 16:02:56.584402: step 88000, acc = 0.9556, f1 = 0.9554
[Status] 2017-05-03 16:02:56.584422: step 88000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 16:03:08.929870: step 88010, loss = 0.1079, acc = 0.9720 (272.7 examples/sec; 0.235 sec/batch)
2017-05-03 16:03:17.820298: step 88020, loss = 0.1376, acc = 0.9440 (303.8 examples/sec; 0.211 sec/batch)
2017-05-03 16:03:26.998171: step 88030, loss = 0.1161, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 16:03:36.098213: step 88040, loss = 0.1307, acc = 0.9500 (298.3 examples/sec; 0.215 sec/batch)
2017-05-03 16:03:45.289347: step 88050, loss = 0.1180, acc = 0.9560 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 16:03:54.457508: step 88060, loss = 0.0996, acc = 0.9660 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 16:04:03.529015: step 88070, loss = 0.1102, acc = 0.9680 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 16:04:12.840840: step 88080, loss = 0.1019, acc = 0.9700 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 16:04:22.037327: step 88090, loss = 0.0971, acc = 0.9740 (254.1 examples/sec; 0.252 sec/batch)
2017-05-03 16:04:31.043552: step 88100, loss = 0.1062, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 16:04:40.046146: step 88110, loss = 0.1172, acc = 0.9660 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 16:04:49.054156: step 88120, loss = 0.1052, acc = 0.9700 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 16:04:58.191850: step 88130, loss = 0.0947, acc = 0.9760 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 16:05:07.289721: step 88140, loss = 0.0803, acc = 0.9820 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 16:05:16.196106: step 88150, loss = 0.1102, acc = 0.9640 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 16:05:25.326519: step 88160, loss = 0.1238, acc = 0.9640 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 16:05:34.366329: step 88170, loss = 0.0944, acc = 0.9740 (274.1 examples/sec; 0.234 sec/batch)
2017-05-03 16:05:43.319906: step 88180, loss = 0.1361, acc = 0.9540 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 16:05:52.257603: step 88190, loss = 0.0985, acc = 0.9680 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 16:06:01.291621: step 88200, loss = 0.1045, acc = 0.9720 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 16:06:10.304839: step 88210, loss = 0.1072, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 16:06:19.556353: step 88220, loss = 0.0964, acc = 0.9720 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 16:06:28.481346: step 88230, loss = 0.1233, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 16:06:37.424816: step 88240, loss = 0.1333, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 16:06:46.377747: step 88250, loss = 0.1039, acc = 0.9700 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 16:06:55.507821: step 88260, loss = 0.0929, acc = 0.9700 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 16:07:04.475154: step 88270, loss = 0.1075, acc = 0.9700 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 16:07:13.383511: step 88280, loss = 0.1532, acc = 0.9520 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 16:07:22.438238: step 88290, loss = 0.1289, acc = 0.9580 (260.5 examples/sec; 0.246 sec/batch)
2017-05-03 16:07:31.586726: step 88300, loss = 0.1034, acc = 0.9660 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 16:07:40.708796: step 88310, loss = 0.1230, acc = 0.9640 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 16:07:49.752973: step 88320, loss = 0.1147, acc = 0.9600 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 16:07:58.747120: step 88330, loss = 0.0946, acc = 0.9760 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 16:08:07.769708: step 88340, loss = 0.1019, acc = 0.9700 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 16:08:16.721631: step 88350, loss = 0.1066, acc = 0.9700 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 16:08:25.716999: step 88360, loss = 0.1106, acc = 0.9660 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 16:08:34.793855: step 88370, loss = 0.1572, acc = 0.9460 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 16:08:43.999807: step 88380, loss = 0.0937, acc = 0.9780 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 16:08:53.115366: step 88390, loss = 0.1198, acc = 0.9520 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 16:09:02.235344: step 88400, loss = 0.1049, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 16:09:11.351353: step 88410, loss = 0.1069, acc = 0.9660 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 16:09:20.267073: step 88420, loss = 0.1229, acc = 0.9560 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 16:09:29.370167: step 88430, loss = 0.1134, acc = 0.9620 (309.7 examples/sec; 0.207 sec/batch)
2017-05-03 16:09:38.330031: step 88440, loss = 0.1215, acc = 0.9680 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 16:09:47.382161: step 88450, loss = 0.1116, acc = 0.9720 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 16:09:56.462292: step 88460, loss = 0.1460, acc = 0.9500 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 16:10:05.606952: step 88470, loss = 0.1152, acc = 0.9600 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 16:10:14.682137: step 88480, loss = 0.1063, acc = 0.9640 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 16:10:23.532768: step 88490, loss = 0.1034, acc = 0.9700 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 16:10:32.509388: step 88500, loss = 0.1043, acc = 0.9660 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 16:10:41.469628: step 88510, loss = 0.0874, acc = 0.9720 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 16:10:50.594173: step 88520, loss = 0.1143, acc = 0.9660 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 16:10:59.643394: step 88530, loss = 0.0886, acc = 0.9780 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 16:11:08.613207: step 88540, loss = 0.1289, acc = 0.9520 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 16:11:17.659916: step 88550, loss = 0.1061, acc = 0.9640 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 16:11:26.775155: step 88560, loss = 0.1384, acc = 0.9540 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 16:11:35.853284: step 88570, loss = 0.1140, acc = 0.9680 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 16:11:44.888155: step 88580, loss = 0.1192, acc = 0.9700 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 16:11:53.900765: step 88590, loss = 0.1265, acc = 0.9680 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 16:12:02.939324: step 88600, loss = 0.1024, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 16:12:11.978820: step 88610, loss = 0.1076, acc = 0.9700 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 16:12:20.975830: step 88620, loss = 0.1552, acc = 0.9460 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 16:12:29.922418: step 88630, loss = 0.1084, acc = 0.9660 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 16:12:38.867750: step 88640, loss = 0.1144, acc = 0.9640 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 16:12:47.919257: step 88650, loss = 0.1390, acc = 0.9460 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 16:12:57.065995: step 88660, loss = 0.1036, acc = 0.9820 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 16:13:06.136701: step 88670, loss = 0.1142, acc = 0.9640 (235.5 examples/sec; 0.272 sec/batch)
2017-05-03 16:13:15.134534: step 88680, loss = 0.1024, acc = 0.9660 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 16:13:24.247993: step 88690, loss = 0.1045, acc = 0.9660 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 16:13:33.237551: step 88700, loss = 0.1140, acc = 0.9640 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 16:13:42.997761: step 88710, loss = 0.1210, acc = 0.9640 (271.8 examples/sec; 0.235 sec/batch)
2017-05-03 16:13:52.168687: step 88720, loss = 0.1106, acc = 0.9660 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 16:14:01.206961: step 88730, loss = 0.0994, acc = 0.9720 (278.9 examples/sec; 0.230 sec/batch)
2017-05-03 16:14:10.643196: step 88740, loss = 0.0934, acc = 0.9700 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 16:14:19.842387: step 88750, loss = 0.1097, acc = 0.9540 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 16:14:28.903434: step 88760, loss = 0.1049, acc = 0.9640 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 16:14:38.089708: step 88770, loss = 0.1324, acc = 0.9580 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 16:14:47.130907: step 88780, loss = 0.1012, acc = 0.9740 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 16:14:56.258374: step 88790, loss = 0.1366, acc = 0.9500 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 16:15:05.267394: step 88800, loss = 0.1328, acc = 0.9520 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 16:15:14.519988: step 88810, loss = 0.1079, acc = 0.9660 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 16:15:23.517445: step 88820, loss = 0.1114, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 16:15:32.686758: step 88830, loss = 0.1214, acc = 0.9680 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 16:15:41.946833: step 88840, loss = 0.1109, acc = 0.9700 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 16:15:50.899028: step 88850, loss = 0.1157, acc = 0.9720 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:15:59.882139: step 88860, loss = 0.1366, acc = 0.9600 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 16:16:08.768924: step 88870, loss = 0.1109, acc = 0.9620 (305.5 examples/sec; 0.209 sec/batch)
2017-05-03 16:16:17.741522: step 88880, loss = 0.1311, acc = 0.9600 (299.6 examples/sec; 0.214 sec/batch)
2017-05-03 16:16:26.771747: step 88890, loss = 0.0865, acc = 0.9800 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 16:16:35.843997: step 88900, loss = 0.1088, acc = 0.9600 (257.4 examples/sec; 0.249 sec/batch)
2017-05-03 16:16:44.766794: step 88910, loss = 0.1043, acc = 0.9760 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 16:16:53.727440: step 88920, loss = 0.1463, acc = 0.9540 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 16:17:02.775465: step 88930, loss = 0.1387, acc = 0.9520 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 16:17:11.809740: step 88940, loss = 0.1119, acc = 0.9580 (297.3 examples/sec; 0.215 sec/batch)
2017-05-03 16:17:20.815619: step 88950, loss = 0.1116, acc = 0.9640 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 16:17:30.020766: step 88960, loss = 0.1539, acc = 0.9500 (269.4 examples/sec; 0.238 sec/batch)
2017-05-03 16:17:38.966818: step 88970, loss = 0.1234, acc = 0.9600 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 16:17:47.987972: step 88980, loss = 0.1286, acc = 0.9520 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 16:17:56.888742: step 88990, loss = 0.1100, acc = 0.9620 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 16:18:05.827717: step 89000, loss = 0.1129, acc = 0.9680 (280.1 examples/sec; 0.228 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 16:18:06.304297: step 89000, loss = 0.1029, acc = 0.9615, f1neg = 0.9579, f1pos = 0.9645, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 16:18:06.773731: step 89000, loss = 0.1102, acc = 0.9640, f1neg = 0.9582, f1pos = 0.9684, f1 = 0.9633
[Eval_batch(2)(2000,6000)] 2017-05-03 16:18:07.251228: step 89000, loss = 0.1191, acc = 0.9605, f1neg = 0.9581, f1pos = 0.9626, f1 = 0.9604
[Eval_batch(3)(2000,8000)] 2017-05-03 16:18:07.751801: step 89000, loss = 0.1217, acc = 0.9605, f1neg = 0.9606, f1pos = 0.9604, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 16:18:08.259676: step 89000, loss = 0.1242, acc = 0.9580, f1neg = 0.9593, f1pos = 0.9566, f1 = 0.9580
[Eval_batch(5)(2000,12000)] 2017-05-03 16:18:08.734724: step 89000, loss = 0.1230, acc = 0.9560, f1neg = 0.9530, f1pos = 0.9586, f1 = 0.9558
[Eval_batch(6)(2000,14000)] 2017-05-03 16:18:09.240362: step 89000, loss = 0.1137, acc = 0.9675, f1neg = 0.9670, f1pos = 0.9680, f1 = 0.9675
[Eval_batch(7)(2000,16000)] 2017-05-03 16:18:09.719032: step 89000, loss = 0.1152, acc = 0.9600, f1neg = 0.9524, f1pos = 0.9655, f1 = 0.9589
[Eval_batch(8)(2000,18000)] 2017-05-03 16:18:10.176053: step 89000, loss = 0.1059, acc = 0.9645, f1neg = 0.9598, f1pos = 0.9682, f1 = 0.9640
[Eval_batch(9)(2000,20000)] 2017-05-03 16:18:10.654259: step 89000, loss = 0.1123, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9667, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 16:18:11.158052: step 89000, loss = 0.1168, acc = 0.9640, f1neg = 0.9618, f1pos = 0.9660, f1 = 0.9639
[Eval_batch(11)(2000,24000)] 2017-05-03 16:18:11.667313: step 89000, loss = 0.1198, acc = 0.9630, f1neg = 0.9574, f1pos = 0.9673, f1 = 0.9624
[Eval_batch(12)(2000,26000)] 2017-05-03 16:18:12.181856: step 89000, loss = 0.1159, acc = 0.9555, f1neg = 0.9545, f1pos = 0.9565, f1 = 0.9555
[Eval_batch(13)(2000,28000)] 2017-05-03 16:18:12.650429: step 89000, loss = 0.1085, acc = 0.9655, f1neg = 0.9570, f1pos = 0.9712, f1 = 0.9641
[Eval_batch(14)(2000,30000)] 2017-05-03 16:18:13.160877: step 89000, loss = 0.1348, acc = 0.9555, f1neg = 0.9473, f1pos = 0.9615, f1 = 0.9544
[Eval_batch(15)(2000,32000)] 2017-05-03 16:18:13.634581: step 89000, loss = 0.1085, acc = 0.9680, f1neg = 0.9655, f1pos = 0.9701, f1 = 0.9678
[Eval_batch(16)(2000,34000)] 2017-05-03 16:18:14.140877: step 89000, loss = 0.1031, acc = 0.9680, f1neg = 0.9669, f1pos = 0.9691, f1 = 0.9680
[Eval_batch(17)(2000,36000)] 2017-05-03 16:18:14.648743: step 89000, loss = 0.1340, acc = 0.9590, f1neg = 0.9594, f1pos = 0.9586, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 16:18:15.115137: step 89000, loss = 0.1243, acc = 0.9615, f1neg = 0.9632, f1pos = 0.9596, f1 = 0.9614
[Eval_batch(19)(2000,40000)] 2017-05-03 16:18:15.590045: step 89000, loss = 0.1091, acc = 0.9685, f1neg = 0.9637, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 16:18:16.101365: step 89000, loss = 0.1109, acc = 0.9640, f1neg = 0.9682, f1pos = 0.9585, f1 = 0.9634
[Eval_batch(21)(2000,44000)] 2017-05-03 16:18:16.612429: step 89000, loss = 0.1042, acc = 0.9655, f1neg = 0.9626, f1pos = 0.9680, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 16:18:17.116319: step 89000, loss = 0.1139, acc = 0.9605, f1neg = 0.9610, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 16:18:17.604679: step 89000, loss = 0.1182, acc = 0.9635, f1neg = 0.9591, f1pos = 0.9671, f1 = 0.9631
[Eval_batch(24)(2000,50000)] 2017-05-03 16:18:18.087671: step 89000, loss = 0.1024, acc = 0.9665, f1neg = 0.9618, f1pos = 0.9702, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 16:18:18.555643: step 89000, loss = 0.0935, acc = 0.9730, f1neg = 0.9705, f1pos = 0.9751, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 16:18:19.020673: step 89000, loss = 0.1081, acc = 0.9635, f1neg = 0.9656, f1pos = 0.9611, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 16:18:19.587680: step 89000, loss = 0.0941, acc = 0.9720, f1neg = 0.9708, f1pos = 0.9731, f1 = 0.9720
[Eval] 2017-05-03 16:18:19.587770: step 89000, acc = 0.9634, f1 = 0.9630
[Test_batch(0)(2000,2000)] 2017-05-03 16:18:20.100956: step 89000, loss = 0.1434, acc = 0.9515, f1neg = 0.9541, f1pos = 0.9486, f1 = 0.9513
[Test_batch(1)(2000,4000)] 2017-05-03 16:18:20.611360: step 89000, loss = 0.1213, acc = 0.9625, f1neg = 0.9667, f1pos = 0.9570, f1 = 0.9619
[Test_batch(2)(2000,6000)] 2017-05-03 16:18:21.052232: step 89000, loss = 0.1398, acc = 0.9590, f1neg = 0.9625, f1pos = 0.9547, f1 = 0.9586
[Test_batch(3)(2000,8000)] 2017-05-03 16:18:21.545998: step 89000, loss = 0.1453, acc = 0.9520, f1neg = 0.9552, f1pos = 0.9483, f1 = 0.9517
[Test_batch(4)(2000,10000)] 2017-05-03 16:18:22.056010: step 89000, loss = 0.1410, acc = 0.9515, f1neg = 0.9520, f1pos = 0.9510, f1 = 0.9515
[Test_batch(5)(2000,12000)] 2017-05-03 16:18:22.568779: step 89000, loss = 0.1571, acc = 0.9460, f1neg = 0.9468, f1pos = 0.9452, f1 = 0.9460
[Test_batch(6)(2000,14000)] 2017-05-03 16:18:23.078156: step 89000, loss = 0.1411, acc = 0.9485, f1neg = 0.9468, f1pos = 0.9501, f1 = 0.9484
[Test_batch(7)(2000,16000)] 2017-05-03 16:18:23.557946: step 89000, loss = 0.1315, acc = 0.9520, f1neg = 0.9566, f1pos = 0.9462, f1 = 0.9514
[Test_batch(8)(2000,18000)] 2017-05-03 16:18:24.018032: step 89000, loss = 0.1322, acc = 0.9505, f1neg = 0.9500, f1pos = 0.9510, f1 = 0.9505
[Test_batch(9)(2000,20000)] 2017-05-03 16:18:24.492635: step 89000, loss = 0.1611, acc = 0.9385, f1neg = 0.9356, f1pos = 0.9412, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 16:18:24.945945: step 89000, loss = 0.1409, acc = 0.9545, f1neg = 0.9489, f1pos = 0.9590, f1 = 0.9539
[Test_batch(11)(2000,24000)] 2017-05-03 16:18:25.412642: step 89000, loss = 0.1435, acc = 0.9535, f1neg = 0.9549, f1pos = 0.9520, f1 = 0.9535
[Test_batch(12)(2000,26000)] 2017-05-03 16:18:25.888603: step 89000, loss = 0.1172, acc = 0.9645, f1neg = 0.9660, f1pos = 0.9629, f1 = 0.9644
[Test_batch(13)(2000,28000)] 2017-05-03 16:18:26.353196: step 89000, loss = 0.1394, acc = 0.9545, f1neg = 0.9501, f1pos = 0.9582, f1 = 0.9541
[Test_batch(14)(2000,30000)] 2017-05-03 16:18:26.801224: step 89000, loss = 0.1169, acc = 0.9590, f1neg = 0.9531, f1pos = 0.9636, f1 = 0.9583
[Test_batch(15)(2000,32000)] 2017-05-03 16:18:27.310697: step 89000, loss = 0.1407, acc = 0.9580, f1neg = 0.9576, f1pos = 0.9584, f1 = 0.9580
[Test_batch(16)(2000,34000)] 2017-05-03 16:18:27.820472: step 89000, loss = 0.1213, acc = 0.9585, f1neg = 0.9577, f1pos = 0.9593, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 16:18:28.331281: step 89000, loss = 0.1126, acc = 0.9665, f1neg = 0.9635, f1pos = 0.9691, f1 = 0.9663
[Test_batch(18)(2000,38000)] 2017-05-03 16:18:28.883524: step 89000, loss = 0.1076, acc = 0.9660, f1neg = 0.9654, f1pos = 0.9666, f1 = 0.9660
[Test] 2017-05-03 16:18:28.883588: step 89000, acc = 0.9551, f1 = 0.9549
[Status] 2017-05-03 16:18:28.883601: step 89000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 16:18:37.844751: step 89010, loss = 0.1071, acc = 0.9640 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 16:18:46.887076: step 89020, loss = 0.0924, acc = 0.9720 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 16:18:55.995215: step 89030, loss = 0.1211, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 16:19:05.972798: step 89040, loss = 0.1107, acc = 0.9600 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 16:19:15.054264: step 89050, loss = 0.0983, acc = 0.9800 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 16:19:24.072889: step 89060, loss = 0.1212, acc = 0.9640 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 16:19:32.987007: step 89070, loss = 0.0989, acc = 0.9740 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 16:19:42.139962: step 89080, loss = 0.1140, acc = 0.9640 (267.6 examples/sec; 0.239 sec/batch)
2017-05-03 16:19:51.055896: step 89090, loss = 0.1401, acc = 0.9440 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 16:20:00.495770: step 89100, loss = 0.1204, acc = 0.9700 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 16:20:09.433210: step 89110, loss = 0.1386, acc = 0.9560 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 16:20:18.306015: step 89120, loss = 0.1101, acc = 0.9680 (297.3 examples/sec; 0.215 sec/batch)
2017-05-03 16:20:27.293743: step 89130, loss = 0.1235, acc = 0.9700 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 16:20:36.319049: step 89140, loss = 0.1086, acc = 0.9760 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 16:20:45.252009: step 89150, loss = 0.1264, acc = 0.9520 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 16:20:54.130665: step 89160, loss = 0.0992, acc = 0.9740 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 16:21:03.395740: step 89170, loss = 0.0984, acc = 0.9680 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 16:21:12.232961: step 89180, loss = 0.1109, acc = 0.9680 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 16:21:21.332126: step 89190, loss = 0.1189, acc = 0.9640 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 16:21:30.393634: step 89200, loss = 0.1194, acc = 0.9600 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 16:21:39.392952: step 89210, loss = 0.1193, acc = 0.9680 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 16:21:48.429852: step 89220, loss = 0.1089, acc = 0.9700 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 16:21:57.345576: step 89230, loss = 0.1252, acc = 0.9560 (295.1 examples/sec; 0.217 sec/batch)
2017-05-03 16:22:06.446060: step 89240, loss = 0.1370, acc = 0.9520 (272.4 examples/sec; 0.235 sec/batch)
2017-05-03 16:22:15.440469: step 89250, loss = 0.1017, acc = 0.9660 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 16:22:24.458671: step 89260, loss = 0.1154, acc = 0.9640 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 16:22:33.414576: step 89270, loss = 0.1035, acc = 0.9740 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 16:22:42.384160: step 89280, loss = 0.1315, acc = 0.9620 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 16:22:51.397359: step 89290, loss = 0.1405, acc = 0.9540 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 16:23:00.317347: step 89300, loss = 0.1152, acc = 0.9540 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 16:23:09.303395: step 89310, loss = 0.1284, acc = 0.9640 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 16:23:18.480730: step 89320, loss = 0.1244, acc = 0.9600 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 16:23:27.491177: step 89330, loss = 0.1100, acc = 0.9700 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:23:36.381317: step 89340, loss = 0.1318, acc = 0.9580 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 16:23:45.327025: step 89350, loss = 0.1319, acc = 0.9520 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 16:23:54.364672: step 89360, loss = 0.1214, acc = 0.9560 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 16:24:03.463077: step 89370, loss = 0.1025, acc = 0.9700 (295.4 examples/sec; 0.217 sec/batch)
2017-05-03 16:24:12.380059: step 89380, loss = 0.1468, acc = 0.9660 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 16:24:21.611283: step 89390, loss = 0.1460, acc = 0.9600 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 16:24:30.614626: step 89400, loss = 0.1134, acc = 0.9680 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 16:24:39.726584: step 89410, loss = 0.1008, acc = 0.9720 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 16:24:48.718498: step 89420, loss = 0.1208, acc = 0.9580 (268.7 examples/sec; 0.238 sec/batch)
2017-05-03 16:24:57.720621: step 89430, loss = 0.1285, acc = 0.9580 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 16:25:06.836687: step 89440, loss = 0.1153, acc = 0.9600 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 16:25:15.986556: step 89450, loss = 0.1331, acc = 0.9540 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 16:25:24.970238: step 89460, loss = 0.0914, acc = 0.9780 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 16:25:33.974859: step 89470, loss = 0.1226, acc = 0.9660 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 16:25:42.967543: step 89480, loss = 0.1352, acc = 0.9500 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 16:25:52.001296: step 89490, loss = 0.0991, acc = 0.9780 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 16:26:00.879879: step 89500, loss = 0.1000, acc = 0.9660 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 16:26:09.818780: step 89510, loss = 0.1318, acc = 0.9680 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 16:26:18.755015: step 89520, loss = 0.1161, acc = 0.9680 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 16:26:27.888259: step 89530, loss = 0.1233, acc = 0.9580 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 16:26:36.806158: step 89540, loss = 0.1089, acc = 0.9700 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 16:26:45.908494: step 89550, loss = 0.1355, acc = 0.9500 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 16:26:54.831619: step 89560, loss = 0.1152, acc = 0.9700 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 16:27:03.773235: step 89570, loss = 0.1407, acc = 0.9520 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 16:27:12.953656: step 89580, loss = 0.0994, acc = 0.9700 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 16:27:21.891029: step 89590, loss = 0.1348, acc = 0.9560 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 16:27:30.865367: step 89600, loss = 0.1069, acc = 0.9800 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 16:27:39.772834: step 89610, loss = 0.1073, acc = 0.9640 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 16:27:48.738488: step 89620, loss = 0.1152, acc = 0.9700 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 16:27:57.759620: step 89630, loss = 0.1298, acc = 0.9520 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 16:28:06.769052: step 89640, loss = 0.1139, acc = 0.9640 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 16:28:15.624200: step 89650, loss = 0.1242, acc = 0.9620 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 16:28:24.627974: step 89660, loss = 0.0905, acc = 0.9760 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 16:28:33.583705: step 89670, loss = 0.1029, acc = 0.9640 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 16:28:42.654071: step 89680, loss = 0.1143, acc = 0.9700 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 16:28:51.809202: step 89690, loss = 0.0852, acc = 0.9860 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 16:29:01.045209: step 89700, loss = 0.1178, acc = 0.9680 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 16:29:10.032698: step 89710, loss = 0.1318, acc = 0.9600 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 16:29:20.224087: step 89720, loss = 0.1153, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 16:29:29.353647: step 89730, loss = 0.1278, acc = 0.9640 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 16:29:38.387982: step 89740, loss = 0.1377, acc = 0.9520 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 16:29:47.492711: step 89750, loss = 0.1133, acc = 0.9780 (305.1 examples/sec; 0.210 sec/batch)
2017-05-03 16:29:56.473887: step 89760, loss = 0.1481, acc = 0.9600 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 16:30:05.373512: step 89770, loss = 0.1237, acc = 0.9660 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 16:30:14.511933: step 89780, loss = 0.1201, acc = 0.9560 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 16:30:23.511116: step 89790, loss = 0.1038, acc = 0.9680 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 16:30:32.809053: step 89800, loss = 0.1055, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 16:30:41.866779: step 89810, loss = 0.0969, acc = 0.9700 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 16:30:50.857670: step 89820, loss = 0.1207, acc = 0.9580 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 16:30:59.839842: step 89830, loss = 0.0979, acc = 0.9680 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 16:31:08.896182: step 89840, loss = 0.0902, acc = 0.9740 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 16:31:17.802065: step 89850, loss = 0.1298, acc = 0.9580 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 16:31:26.698700: step 89860, loss = 0.1089, acc = 0.9700 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 16:31:35.906000: step 89870, loss = 0.0955, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 16:31:44.894805: step 89880, loss = 0.1109, acc = 0.9640 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 16:31:53.925829: step 89890, loss = 0.0861, acc = 0.9800 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 16:32:02.935150: step 89900, loss = 0.1028, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 16:32:11.945726: step 89910, loss = 0.0979, acc = 0.9800 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 16:32:20.861499: step 89920, loss = 0.1025, acc = 0.9680 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 16:32:29.922919: step 89930, loss = 0.0992, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 16:32:38.924218: step 89940, loss = 0.0924, acc = 0.9720 (276.2 examples/sec; 0.232 sec/batch)
2017-05-03 16:32:47.980921: step 89950, loss = 0.1216, acc = 0.9660 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 16:32:57.392803: step 89960, loss = 0.0907, acc = 0.9800 (274.7 examples/sec; 0.233 sec/batch)
2017-05-03 16:33:06.522748: step 89970, loss = 0.1176, acc = 0.9600 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 16:33:15.522026: step 89980, loss = 0.1148, acc = 0.9620 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 16:33:24.840179: step 89990, loss = 0.1097, acc = 0.9660 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 16:33:33.801657: step 90000, loss = 0.1300, acc = 0.9600 (284.7 examples/sec; 0.225 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 16:33:34.306663: step 90000, loss = 0.1059, acc = 0.9600, f1neg = 0.9560, f1pos = 0.9633, f1 = 0.9597
[Eval_batch(1)(2000,4000)] 2017-05-03 16:33:34.791398: step 90000, loss = 0.1082, acc = 0.9660, f1neg = 0.9602, f1pos = 0.9703, f1 = 0.9653
[Eval_batch(2)(2000,6000)] 2017-05-03 16:33:35.279457: step 90000, loss = 0.1203, acc = 0.9610, f1neg = 0.9584, f1pos = 0.9633, f1 = 0.9608
[Eval_batch(3)(2000,8000)] 2017-05-03 16:33:35.753357: step 90000, loss = 0.1265, acc = 0.9575, f1neg = 0.9573, f1pos = 0.9577, f1 = 0.9575
[Eval_batch(4)(2000,10000)] 2017-05-03 16:33:36.221496: step 90000, loss = 0.1271, acc = 0.9620, f1neg = 0.9629, f1pos = 0.9611, f1 = 0.9620
[Eval_batch(5)(2000,12000)] 2017-05-03 16:33:36.697163: step 90000, loss = 0.1247, acc = 0.9555, f1neg = 0.9519, f1pos = 0.9586, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 16:33:37.178012: step 90000, loss = 0.1167, acc = 0.9660, f1neg = 0.9653, f1pos = 0.9667, f1 = 0.9660
[Eval_batch(7)(2000,16000)] 2017-05-03 16:33:37.651681: step 90000, loss = 0.1151, acc = 0.9615, f1neg = 0.9538, f1pos = 0.9670, f1 = 0.9604
[Eval_batch(8)(2000,18000)] 2017-05-03 16:33:38.119712: step 90000, loss = 0.1069, acc = 0.9610, f1neg = 0.9554, f1pos = 0.9653, f1 = 0.9604
[Eval_batch(9)(2000,20000)] 2017-05-03 16:33:38.621589: step 90000, loss = 0.1122, acc = 0.9645, f1neg = 0.9618, f1pos = 0.9669, f1 = 0.9643
[Eval_batch(10)(2000,22000)] 2017-05-03 16:33:39.067317: step 90000, loss = 0.1197, acc = 0.9595, f1neg = 0.9565, f1pos = 0.9621, f1 = 0.9593
[Eval_batch(11)(2000,24000)] 2017-05-03 16:33:39.541730: step 90000, loss = 0.1191, acc = 0.9620, f1neg = 0.9561, f1pos = 0.9665, f1 = 0.9613
[Eval_batch(12)(2000,26000)] 2017-05-03 16:33:40.008840: step 90000, loss = 0.1183, acc = 0.9560, f1neg = 0.9545, f1pos = 0.9574, f1 = 0.9560
[Eval_batch(13)(2000,28000)] 2017-05-03 16:33:40.484220: step 90000, loss = 0.1088, acc = 0.9660, f1neg = 0.9573, f1pos = 0.9718, f1 = 0.9645
[Eval_batch(14)(2000,30000)] 2017-05-03 16:33:40.962114: step 90000, loss = 0.1355, acc = 0.9525, f1neg = 0.9433, f1pos = 0.9591, f1 = 0.9512
[Eval_batch(15)(2000,32000)] 2017-05-03 16:33:41.449143: step 90000, loss = 0.1090, acc = 0.9695, f1neg = 0.9670, f1pos = 0.9717, f1 = 0.9693
[Eval_batch(16)(2000,34000)] 2017-05-03 16:33:41.927556: step 90000, loss = 0.1034, acc = 0.9675, f1neg = 0.9660, f1pos = 0.9689, f1 = 0.9674
[Eval_batch(17)(2000,36000)] 2017-05-03 16:33:42.403623: step 90000, loss = 0.1331, acc = 0.9575, f1neg = 0.9577, f1pos = 0.9573, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 16:33:42.879905: step 90000, loss = 0.1285, acc = 0.9565, f1neg = 0.9581, f1pos = 0.9548, f1 = 0.9564
[Eval_batch(19)(2000,40000)] 2017-05-03 16:33:43.384181: step 90000, loss = 0.1105, acc = 0.9685, f1neg = 0.9636, f1pos = 0.9723, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 16:33:43.886412: step 90000, loss = 0.1142, acc = 0.9635, f1neg = 0.9675, f1pos = 0.9583, f1 = 0.9629
[Eval_batch(21)(2000,44000)] 2017-05-03 16:33:44.387465: step 90000, loss = 0.1015, acc = 0.9670, f1neg = 0.9640, f1pos = 0.9695, f1 = 0.9668
[Eval_batch(22)(2000,46000)] 2017-05-03 16:33:44.861443: step 90000, loss = 0.1135, acc = 0.9605, f1neg = 0.9608, f1pos = 0.9602, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 16:33:45.367317: step 90000, loss = 0.1176, acc = 0.9600, f1neg = 0.9547, f1pos = 0.9642, f1 = 0.9594
[Eval_batch(24)(2000,50000)] 2017-05-03 16:33:45.877016: step 90000, loss = 0.1025, acc = 0.9660, f1neg = 0.9609, f1pos = 0.9699, f1 = 0.9654
[Eval_batch(25)(2000,52000)] 2017-05-03 16:33:46.380670: step 90000, loss = 0.0950, acc = 0.9730, f1neg = 0.9703, f1pos = 0.9752, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 16:33:46.891091: step 90000, loss = 0.1113, acc = 0.9630, f1neg = 0.9649, f1pos = 0.9609, f1 = 0.9629
[Eval_batch(27)(2000,56000)] 2017-05-03 16:33:47.510145: step 90000, loss = 0.0964, acc = 0.9690, f1neg = 0.9673, f1pos = 0.9705, f1 = 0.9689
[Eval] 2017-05-03 16:33:47.510228: step 90000, acc = 0.9626, f1 = 0.9622
[Test_batch(0)(2000,2000)] 2017-05-03 16:33:48.021910: step 90000, loss = 0.1458, acc = 0.9535, f1neg = 0.9557, f1pos = 0.9511, f1 = 0.9534
[Test_batch(1)(2000,4000)] 2017-05-03 16:33:48.527381: step 90000, loss = 0.1248, acc = 0.9620, f1neg = 0.9661, f1pos = 0.9567, f1 = 0.9614
[Test_batch(2)(2000,6000)] 2017-05-03 16:33:49.008917: step 90000, loss = 0.1432, acc = 0.9565, f1neg = 0.9599, f1pos = 0.9525, f1 = 0.9562
[Test_batch(3)(2000,8000)] 2017-05-03 16:33:49.479558: step 90000, loss = 0.1485, acc = 0.9525, f1neg = 0.9551, f1pos = 0.9495, f1 = 0.9523
[Test_batch(4)(2000,10000)] 2017-05-03 16:33:49.980028: step 90000, loss = 0.1404, acc = 0.9515, f1neg = 0.9515, f1pos = 0.9515, f1 = 0.9515
[Test_batch(5)(2000,12000)] 2017-05-03 16:33:50.488998: step 90000, loss = 0.1572, acc = 0.9410, f1neg = 0.9413, f1pos = 0.9407, f1 = 0.9410
[Test_batch(6)(2000,14000)] 2017-05-03 16:33:50.970409: step 90000, loss = 0.1429, acc = 0.9480, f1neg = 0.9459, f1pos = 0.9499, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 16:33:51.450837: step 90000, loss = 0.1345, acc = 0.9545, f1neg = 0.9585, f1pos = 0.9496, f1 = 0.9541
[Test_batch(8)(2000,18000)] 2017-05-03 16:33:51.921367: step 90000, loss = 0.1327, acc = 0.9495, f1neg = 0.9487, f1pos = 0.9503, f1 = 0.9495
[Test_batch(9)(2000,20000)] 2017-05-03 16:33:52.427664: step 90000, loss = 0.1600, acc = 0.9365, f1neg = 0.9331, f1pos = 0.9396, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-03 16:33:52.918867: step 90000, loss = 0.1381, acc = 0.9530, f1neg = 0.9468, f1pos = 0.9579, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 16:33:53.427455: step 90000, loss = 0.1447, acc = 0.9510, f1neg = 0.9522, f1pos = 0.9497, f1 = 0.9510
[Test_batch(12)(2000,26000)] 2017-05-03 16:33:53.922441: step 90000, loss = 0.1156, acc = 0.9640, f1neg = 0.9653, f1pos = 0.9627, f1 = 0.9640
[Test_batch(13)(2000,28000)] 2017-05-03 16:33:54.423109: step 90000, loss = 0.1382, acc = 0.9540, f1neg = 0.9487, f1pos = 0.9583, f1 = 0.9535
[Test_batch(14)(2000,30000)] 2017-05-03 16:33:54.930285: step 90000, loss = 0.1161, acc = 0.9605, f1neg = 0.9543, f1pos = 0.9652, f1 = 0.9598
[Test_batch(15)(2000,32000)] 2017-05-03 16:33:55.425533: step 90000, loss = 0.1403, acc = 0.9550, f1neg = 0.9542, f1pos = 0.9558, f1 = 0.9550
[Test_batch(16)(2000,34000)] 2017-05-03 16:33:55.939674: step 90000, loss = 0.1215, acc = 0.9580, f1neg = 0.9567, f1pos = 0.9592, f1 = 0.9580
[Test_batch(17)(2000,36000)] 2017-05-03 16:33:56.438120: step 90000, loss = 0.1142, acc = 0.9670, f1neg = 0.9637, f1pos = 0.9697, f1 = 0.9667
[Test_batch(18)(2000,38000)] 2017-05-03 16:33:56.966861: step 90000, loss = 0.1104, acc = 0.9630, f1neg = 0.9621, f1pos = 0.9639, f1 = 0.9630
[Test] 2017-05-03 16:33:56.966960: step 90000, acc = 0.9543, f1 = 0.9540
[Status] 2017-05-03 16:33:56.966987: step 90000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 16:34:05.992142: step 90010, loss = 0.0898, acc = 0.9720 (278.8 examples/sec; 0.230 sec/batch)
2017-05-03 16:34:15.025298: step 90020, loss = 0.1512, acc = 0.9540 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 16:34:23.948056: step 90030, loss = 0.1058, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 16:34:32.908580: step 90040, loss = 0.1150, acc = 0.9620 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 16:34:42.047202: step 90050, loss = 0.1094, acc = 0.9820 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 16:34:50.969533: step 90060, loss = 0.1297, acc = 0.9700 (256.6 examples/sec; 0.249 sec/batch)
2017-05-03 16:35:00.144794: step 90070, loss = 0.1186, acc = 0.9640 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 16:35:09.074051: step 90080, loss = 0.1105, acc = 0.9720 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 16:35:18.092875: step 90090, loss = 0.1087, acc = 0.9800 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 16:35:27.070250: step 90100, loss = 0.1080, acc = 0.9660 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 16:35:35.985248: step 90110, loss = 0.1298, acc = 0.9580 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 16:35:45.073412: step 90120, loss = 0.0962, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 16:35:54.068472: step 90130, loss = 0.1058, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 16:36:03.063928: step 90140, loss = 0.1250, acc = 0.9580 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 16:36:12.051295: step 90150, loss = 0.1470, acc = 0.9560 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 16:36:21.103679: step 90160, loss = 0.1107, acc = 0.9660 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 16:36:30.128158: step 90170, loss = 0.1295, acc = 0.9580 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 16:36:39.084031: step 90180, loss = 0.0975, acc = 0.9720 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 16:36:48.067563: step 90190, loss = 0.1431, acc = 0.9600 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 16:36:57.103597: step 90200, loss = 0.1013, acc = 0.9780 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 16:37:06.052061: step 90210, loss = 0.0831, acc = 0.9820 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 16:37:15.027194: step 90220, loss = 0.1124, acc = 0.9600 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 16:37:23.957143: step 90230, loss = 0.1307, acc = 0.9520 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 16:37:33.128353: step 90240, loss = 0.0970, acc = 0.9740 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 16:37:42.159871: step 90250, loss = 0.1146, acc = 0.9720 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 16:37:51.225864: step 90260, loss = 0.0882, acc = 0.9760 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 16:38:00.276201: step 90270, loss = 0.1250, acc = 0.9540 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 16:38:10.120034: step 90280, loss = 0.1320, acc = 0.9500 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 16:38:19.184029: step 90290, loss = 0.0983, acc = 0.9800 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:38:28.108593: step 90300, loss = 0.1112, acc = 0.9680 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 16:38:37.142732: step 90310, loss = 0.1276, acc = 0.9480 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 16:38:46.237772: step 90320, loss = 0.1080, acc = 0.9620 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 16:38:55.206467: step 90330, loss = 0.1265, acc = 0.9580 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 16:39:04.259711: step 90340, loss = 0.1148, acc = 0.9620 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 16:39:13.263990: step 90350, loss = 0.0943, acc = 0.9780 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 16:39:22.270329: step 90360, loss = 0.1037, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 16:39:31.399084: step 90370, loss = 0.1176, acc = 0.9700 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 16:39:40.323093: step 90380, loss = 0.1194, acc = 0.9700 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 16:39:49.416781: step 90390, loss = 0.1253, acc = 0.9540 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 16:39:58.344929: step 90400, loss = 0.0942, acc = 0.9720 (304.1 examples/sec; 0.210 sec/batch)
2017-05-03 16:40:07.353627: step 90410, loss = 0.1331, acc = 0.9620 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 16:40:16.357085: step 90420, loss = 0.1269, acc = 0.9460 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 16:40:25.294391: step 90430, loss = 0.1283, acc = 0.9600 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 16:40:34.358165: step 90440, loss = 0.1380, acc = 0.9580 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 16:40:43.292302: step 90450, loss = 0.1139, acc = 0.9580 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 16:40:52.271853: step 90460, loss = 0.0875, acc = 0.9820 (276.3 examples/sec; 0.232 sec/batch)
2017-05-03 16:41:01.375579: step 90470, loss = 0.0922, acc = 0.9740 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 16:41:10.349786: step 90480, loss = 0.1332, acc = 0.9560 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 16:41:19.346205: step 90490, loss = 0.1312, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 16:41:28.374226: step 90500, loss = 0.1258, acc = 0.9680 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 16:41:37.573481: step 90510, loss = 0.1202, acc = 0.9660 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:41:46.734016: step 90520, loss = 0.1304, acc = 0.9600 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 16:41:55.797606: step 90530, loss = 0.1164, acc = 0.9620 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 16:42:04.838978: step 90540, loss = 0.1212, acc = 0.9660 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 16:42:13.768811: step 90550, loss = 0.1092, acc = 0.9660 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 16:42:22.840541: step 90560, loss = 0.1089, acc = 0.9640 (248.5 examples/sec; 0.258 sec/batch)
2017-05-03 16:42:31.966953: step 90570, loss = 0.1509, acc = 0.9380 (275.9 examples/sec; 0.232 sec/batch)
2017-05-03 16:42:41.049812: step 90580, loss = 0.1140, acc = 0.9640 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 16:42:50.057690: step 90590, loss = 0.1141, acc = 0.9620 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 16:42:59.158303: step 90600, loss = 0.1176, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 16:43:08.007188: step 90610, loss = 0.0900, acc = 0.9840 (305.7 examples/sec; 0.209 sec/batch)
2017-05-03 16:43:17.338048: step 90620, loss = 0.1107, acc = 0.9640 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 16:43:26.477319: step 90630, loss = 0.0998, acc = 0.9720 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 16:43:35.549949: step 90640, loss = 0.1221, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 16:43:44.617103: step 90650, loss = 0.1369, acc = 0.9540 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 16:43:54.204300: step 90660, loss = 0.1429, acc = 0.9580 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:44:03.232233: step 90670, loss = 0.1459, acc = 0.9480 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 16:44:12.260711: step 90680, loss = 0.1296, acc = 0.9620 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 16:44:21.840781: step 90690, loss = 0.0817, acc = 0.9780 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 16:44:30.847213: step 90700, loss = 0.1101, acc = 0.9680 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 16:44:39.857921: step 90710, loss = 0.1216, acc = 0.9540 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 16:44:49.573667: step 90720, loss = 0.1393, acc = 0.9560 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 16:44:58.566880: step 90730, loss = 0.1002, acc = 0.9640 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 16:45:07.546545: step 90740, loss = 0.1229, acc = 0.9580 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 16:45:16.573982: step 90750, loss = 0.1222, acc = 0.9620 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 16:45:25.778786: step 90760, loss = 0.1064, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 16:45:34.956369: step 90770, loss = 0.1007, acc = 0.9700 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 16:45:44.028564: step 90780, loss = 0.1036, acc = 0.9680 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 16:45:53.086901: step 90790, loss = 0.1299, acc = 0.9640 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 16:46:02.027985: step 90800, loss = 0.0901, acc = 0.9760 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 16:46:11.105127: step 90810, loss = 0.1492, acc = 0.9420 (275.5 examples/sec; 0.232 sec/batch)
2017-05-03 16:46:20.106893: step 90820, loss = 0.1023, acc = 0.9740 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 16:46:29.133610: step 90830, loss = 0.1262, acc = 0.9680 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 16:46:38.059934: step 90840, loss = 0.1127, acc = 0.9660 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:46:47.118933: step 90850, loss = 0.1098, acc = 0.9680 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 16:46:56.182191: step 90860, loss = 0.0992, acc = 0.9680 (266.7 examples/sec; 0.240 sec/batch)
2017-05-03 16:47:05.379188: step 90870, loss = 0.1132, acc = 0.9680 (272.9 examples/sec; 0.235 sec/batch)
2017-05-03 16:47:14.405181: step 90880, loss = 0.1232, acc = 0.9600 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 16:47:23.810814: step 90890, loss = 0.1132, acc = 0.9660 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 16:47:33.992782: step 90900, loss = 0.1319, acc = 0.9620 (194.1 examples/sec; 0.330 sec/batch)
2017-05-03 16:47:42.894174: step 90910, loss = 0.1197, acc = 0.9560 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 16:47:51.841047: step 90920, loss = 0.1139, acc = 0.9580 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 16:48:00.811986: step 90930, loss = 0.1214, acc = 0.9680 (270.3 examples/sec; 0.237 sec/batch)
2017-05-03 16:48:09.938414: step 90940, loss = 0.1157, acc = 0.9660 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 16:48:18.947418: step 90950, loss = 0.1265, acc = 0.9600 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 16:48:27.934538: step 90960, loss = 0.1053, acc = 0.9760 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 16:48:36.984895: step 90970, loss = 0.0838, acc = 0.9800 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 16:48:46.025369: step 90980, loss = 0.1037, acc = 0.9780 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 16:48:55.098471: step 90990, loss = 0.0782, acc = 0.9780 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 16:49:04.030203: step 91000, loss = 0.1136, acc = 0.9640 (291.7 examples/sec; 0.219 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 16:49:04.538367: step 91000, loss = 0.1028, acc = 0.9615, f1neg = 0.9579, f1pos = 0.9646, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 16:49:05.052845: step 91000, loss = 0.1090, acc = 0.9655, f1neg = 0.9599, f1pos = 0.9698, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 16:49:05.555111: step 91000, loss = 0.1193, acc = 0.9610, f1neg = 0.9587, f1pos = 0.9631, f1 = 0.9609
[Eval_batch(3)(2000,8000)] 2017-05-03 16:49:06.056215: step 91000, loss = 0.1209, acc = 0.9600, f1neg = 0.9601, f1pos = 0.9599, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 16:49:06.560894: step 91000, loss = 0.1236, acc = 0.9575, f1neg = 0.9588, f1pos = 0.9561, f1 = 0.9575
[Eval_batch(5)(2000,12000)] 2017-05-03 16:49:07.059359: step 91000, loss = 0.1236, acc = 0.9545, f1neg = 0.9514, f1pos = 0.9573, f1 = 0.9543
[Eval_batch(6)(2000,14000)] 2017-05-03 16:49:07.571734: step 91000, loss = 0.1136, acc = 0.9675, f1neg = 0.9670, f1pos = 0.9680, f1 = 0.9675
[Eval_batch(7)(2000,16000)] 2017-05-03 16:49:08.076584: step 91000, loss = 0.1142, acc = 0.9620, f1neg = 0.9549, f1pos = 0.9672, f1 = 0.9610
[Eval_batch(8)(2000,18000)] 2017-05-03 16:49:08.518472: step 91000, loss = 0.1050, acc = 0.9635, f1neg = 0.9585, f1pos = 0.9674, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 16:49:08.969838: step 91000, loss = 0.1123, acc = 0.9665, f1neg = 0.9641, f1pos = 0.9686, f1 = 0.9663
[Eval_batch(10)(2000,22000)] 2017-05-03 16:49:09.449212: step 91000, loss = 0.1172, acc = 0.9635, f1neg = 0.9611, f1pos = 0.9656, f1 = 0.9634
[Eval_batch(11)(2000,24000)] 2017-05-03 16:49:09.921088: step 91000, loss = 0.1197, acc = 0.9610, f1neg = 0.9552, f1pos = 0.9655, f1 = 0.9603
[Eval_batch(12)(2000,26000)] 2017-05-03 16:49:10.433486: step 91000, loss = 0.1156, acc = 0.9545, f1neg = 0.9533, f1pos = 0.9556, f1 = 0.9545
[Eval_batch(13)(2000,28000)] 2017-05-03 16:49:10.899638: step 91000, loss = 0.1076, acc = 0.9655, f1neg = 0.9569, f1pos = 0.9712, f1 = 0.9641
[Eval_batch(14)(2000,30000)] 2017-05-03 16:49:11.369978: step 91000, loss = 0.1337, acc = 0.9545, f1neg = 0.9461, f1pos = 0.9606, f1 = 0.9534
[Eval_batch(15)(2000,32000)] 2017-05-03 16:49:11.877205: step 91000, loss = 0.1077, acc = 0.9685, f1neg = 0.9660, f1pos = 0.9707, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 16:49:12.347203: step 91000, loss = 0.1031, acc = 0.9690, f1neg = 0.9678, f1pos = 0.9701, f1 = 0.9690
[Eval_batch(17)(2000,36000)] 2017-05-03 16:49:12.819205: step 91000, loss = 0.1338, acc = 0.9595, f1neg = 0.9598, f1pos = 0.9592, f1 = 0.9595
[Eval_batch(18)(2000,38000)] 2017-05-03 16:49:13.324431: step 91000, loss = 0.1240, acc = 0.9605, f1neg = 0.9622, f1pos = 0.9587, f1 = 0.9604
[Eval_batch(19)(2000,40000)] 2017-05-03 16:49:13.816800: step 91000, loss = 0.1094, acc = 0.9685, f1neg = 0.9637, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 16:49:14.296216: step 91000, loss = 0.1111, acc = 0.9630, f1neg = 0.9672, f1pos = 0.9575, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 16:49:14.769097: step 91000, loss = 0.1040, acc = 0.9645, f1neg = 0.9614, f1pos = 0.9671, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 16:49:15.241150: step 91000, loss = 0.1132, acc = 0.9585, f1neg = 0.9590, f1pos = 0.9580, f1 = 0.9585
[Eval_batch(23)(2000,48000)] 2017-05-03 16:49:15.707387: step 91000, loss = 0.1169, acc = 0.9610, f1neg = 0.9561, f1pos = 0.9649, f1 = 0.9605
[Eval_batch(24)(2000,50000)] 2017-05-03 16:49:16.180022: step 91000, loss = 0.1012, acc = 0.9680, f1neg = 0.9635, f1pos = 0.9715, f1 = 0.9675
[Eval_batch(25)(2000,52000)] 2017-05-03 16:49:16.674007: step 91000, loss = 0.0935, acc = 0.9735, f1neg = 0.9711, f1pos = 0.9756, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 16:49:17.182756: step 91000, loss = 0.1089, acc = 0.9620, f1neg = 0.9642, f1pos = 0.9595, f1 = 0.9619
[Eval_batch(27)(2000,56000)] 2017-05-03 16:49:17.807946: step 91000, loss = 0.0947, acc = 0.9705, f1neg = 0.9691, f1pos = 0.9718, f1 = 0.9704
[Eval] 2017-05-03 16:49:17.808015: step 91000, acc = 0.9631, f1 = 0.9627
[Test_batch(0)(2000,2000)] 2017-05-03 16:49:18.288388: step 91000, loss = 0.1427, acc = 0.9530, f1neg = 0.9555, f1pos = 0.9503, f1 = 0.9529
[Test_batch(1)(2000,4000)] 2017-05-03 16:49:18.795326: step 91000, loss = 0.1217, acc = 0.9620, f1neg = 0.9663, f1pos = 0.9565, f1 = 0.9614
[Test_batch(2)(2000,6000)] 2017-05-03 16:49:19.256451: step 91000, loss = 0.1398, acc = 0.9575, f1neg = 0.9610, f1pos = 0.9533, f1 = 0.9572
[Test_batch(3)(2000,8000)] 2017-05-03 16:49:19.758679: step 91000, loss = 0.1454, acc = 0.9520, f1neg = 0.9551, f1pos = 0.9484, f1 = 0.9518
[Test_batch(4)(2000,10000)] 2017-05-03 16:49:20.252072: step 91000, loss = 0.1398, acc = 0.9530, f1neg = 0.9533, f1pos = 0.9527, f1 = 0.9530
[Test_batch(5)(2000,12000)] 2017-05-03 16:49:20.758773: step 91000, loss = 0.1545, acc = 0.9465, f1neg = 0.9474, f1pos = 0.9456, f1 = 0.9465
[Test_batch(6)(2000,14000)] 2017-05-03 16:49:21.256594: step 91000, loss = 0.1399, acc = 0.9500, f1neg = 0.9483, f1pos = 0.9516, f1 = 0.9499
[Test_batch(7)(2000,16000)] 2017-05-03 16:49:21.755595: step 91000, loss = 0.1300, acc = 0.9520, f1neg = 0.9565, f1pos = 0.9465, f1 = 0.9515
[Test_batch(8)(2000,18000)] 2017-05-03 16:49:22.223267: step 91000, loss = 0.1319, acc = 0.9500, f1neg = 0.9494, f1pos = 0.9505, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 16:49:22.709654: step 91000, loss = 0.1599, acc = 0.9380, f1neg = 0.9348, f1pos = 0.9409, f1 = 0.9379
[Test_batch(10)(2000,22000)] 2017-05-03 16:49:23.216241: step 91000, loss = 0.1398, acc = 0.9530, f1neg = 0.9472, f1pos = 0.9577, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 16:49:23.722897: step 91000, loss = 0.1426, acc = 0.9530, f1neg = 0.9544, f1pos = 0.9515, f1 = 0.9530
[Test_batch(12)(2000,26000)] 2017-05-03 16:49:24.224153: step 91000, loss = 0.1159, acc = 0.9630, f1neg = 0.9645, f1pos = 0.9614, f1 = 0.9629
[Test_batch(13)(2000,28000)] 2017-05-03 16:49:24.718272: step 91000, loss = 0.1382, acc = 0.9550, f1neg = 0.9502, f1pos = 0.9590, f1 = 0.9546
[Test_batch(14)(2000,30000)] 2017-05-03 16:49:25.189854: step 91000, loss = 0.1156, acc = 0.9590, f1neg = 0.9529, f1pos = 0.9637, f1 = 0.9583
[Test_batch(15)(2000,32000)] 2017-05-03 16:49:25.694096: step 91000, loss = 0.1400, acc = 0.9575, f1neg = 0.9570, f1pos = 0.9580, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 16:49:26.147512: step 91000, loss = 0.1212, acc = 0.9585, f1neg = 0.9576, f1pos = 0.9594, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 16:49:26.644567: step 91000, loss = 0.1117, acc = 0.9660, f1neg = 0.9628, f1pos = 0.9687, f1 = 0.9658
[Test_batch(18)(2000,38000)] 2017-05-03 16:49:27.219210: step 91000, loss = 0.1076, acc = 0.9645, f1neg = 0.9638, f1pos = 0.9652, f1 = 0.9645
[Test] 2017-05-03 16:49:27.219274: step 91000, acc = 0.9549, f1 = 0.9547
[Status] 2017-05-03 16:49:27.219286: step 91000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 16:49:36.319717: step 91010, loss = 0.1079, acc = 0.9700 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 16:49:45.431347: step 91020, loss = 0.1039, acc = 0.9600 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 16:49:54.564278: step 91030, loss = 0.1431, acc = 0.9600 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 16:50:03.664388: step 91040, loss = 0.0945, acc = 0.9740 (264.6 examples/sec; 0.242 sec/batch)
2017-05-03 16:50:12.612037: step 91050, loss = 0.1030, acc = 0.9640 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 16:50:21.978146: step 91060, loss = 0.1160, acc = 0.9620 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 16:50:30.975143: step 91070, loss = 0.1041, acc = 0.9680 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 16:50:39.938658: step 91080, loss = 0.1054, acc = 0.9680 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 16:50:48.998312: step 91090, loss = 0.1253, acc = 0.9640 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 16:50:57.998622: step 91100, loss = 0.1096, acc = 0.9680 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 16:51:07.092617: step 91110, loss = 0.1009, acc = 0.9740 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 16:51:16.095921: step 91120, loss = 0.0943, acc = 0.9820 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 16:51:25.151309: step 91130, loss = 0.1093, acc = 0.9660 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 16:51:34.178960: step 91140, loss = 0.1415, acc = 0.9560 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 16:51:43.178230: step 91150, loss = 0.1153, acc = 0.9680 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 16:51:52.199205: step 91160, loss = 0.1058, acc = 0.9640 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 16:52:01.258071: step 91170, loss = 0.0974, acc = 0.9740 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 16:52:10.228032: step 91180, loss = 0.1129, acc = 0.9620 (262.4 examples/sec; 0.244 sec/batch)
2017-05-03 16:52:19.126556: step 91190, loss = 0.0968, acc = 0.9720 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 16:52:28.327350: step 91200, loss = 0.1229, acc = 0.9660 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 16:52:37.661699: step 91210, loss = 0.1074, acc = 0.9700 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 16:52:46.700053: step 91220, loss = 0.1001, acc = 0.9780 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 16:52:55.784315: step 91230, loss = 0.0954, acc = 0.9760 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 16:53:04.876005: step 91240, loss = 0.1299, acc = 0.9520 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 16:53:13.935340: step 91250, loss = 0.0984, acc = 0.9700 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 16:53:23.016186: step 91260, loss = 0.1251, acc = 0.9520 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 16:53:32.072328: step 91270, loss = 0.1234, acc = 0.9700 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 16:53:41.109678: step 91280, loss = 0.0932, acc = 0.9760 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 16:53:50.076806: step 91290, loss = 0.1098, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 16:53:59.194727: step 91300, loss = 0.1046, acc = 0.9680 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 16:54:08.274704: step 91310, loss = 0.1084, acc = 0.9680 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 16:54:17.439054: step 91320, loss = 0.1153, acc = 0.9620 (280.6 examples/sec; 0.228 sec/batch)
2017-05-03 16:54:26.340366: step 91330, loss = 0.1064, acc = 0.9700 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 16:54:35.381718: step 91340, loss = 0.1382, acc = 0.9520 (267.6 examples/sec; 0.239 sec/batch)
2017-05-03 16:54:44.469856: step 91350, loss = 0.1265, acc = 0.9580 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 16:54:53.674382: step 91360, loss = 0.0918, acc = 0.9760 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 16:55:02.694704: step 91370, loss = 0.1243, acc = 0.9580 (262.0 examples/sec; 0.244 sec/batch)
2017-05-03 16:55:11.849406: step 91380, loss = 0.1207, acc = 0.9680 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 16:55:20.937045: step 91390, loss = 0.1029, acc = 0.9680 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 16:55:29.950375: step 91400, loss = 0.1081, acc = 0.9680 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 16:55:39.061276: step 91410, loss = 0.1414, acc = 0.9520 (296.7 examples/sec; 0.216 sec/batch)
2017-05-03 16:55:48.216211: step 91420, loss = 0.1353, acc = 0.9580 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 16:55:57.239998: step 91430, loss = 0.1229, acc = 0.9580 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 16:56:06.334053: step 91440, loss = 0.1156, acc = 0.9560 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 16:56:15.379927: step 91450, loss = 0.1277, acc = 0.9560 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 16:56:24.368020: step 91460, loss = 0.1193, acc = 0.9700 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 16:56:33.402772: step 91470, loss = 0.1204, acc = 0.9620 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 16:56:42.530224: step 91480, loss = 0.1388, acc = 0.9540 (266.4 examples/sec; 0.240 sec/batch)
2017-05-03 16:56:51.556305: step 91490, loss = 0.1070, acc = 0.9640 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 16:57:00.490979: step 91500, loss = 0.1076, acc = 0.9700 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 16:57:09.515656: step 91510, loss = 0.1080, acc = 0.9660 (291.2 examples/sec; 0.220 sec/batch)
2017-05-03 16:57:18.581316: step 91520, loss = 0.1049, acc = 0.9640 (296.3 examples/sec; 0.216 sec/batch)
2017-05-03 16:57:27.538054: step 91530, loss = 0.0835, acc = 0.9800 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 16:57:36.431646: step 91540, loss = 0.1061, acc = 0.9700 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 16:57:45.454446: step 91550, loss = 0.1034, acc = 0.9640 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 16:57:54.437867: step 91560, loss = 0.1025, acc = 0.9660 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 16:58:03.346000: step 91570, loss = 0.1199, acc = 0.9600 (301.0 examples/sec; 0.213 sec/batch)
2017-05-03 16:58:12.252668: step 91580, loss = 0.1326, acc = 0.9640 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 16:58:21.270357: step 91590, loss = 0.1205, acc = 0.9660 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 16:58:30.206788: step 91600, loss = 0.1385, acc = 0.9480 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 16:58:39.159705: step 91610, loss = 0.0981, acc = 0.9700 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 16:58:48.382918: step 91620, loss = 0.1244, acc = 0.9720 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 16:58:57.432991: step 91630, loss = 0.1318, acc = 0.9580 (267.6 examples/sec; 0.239 sec/batch)
2017-05-03 16:59:06.618865: step 91640, loss = 0.1054, acc = 0.9720 (270.7 examples/sec; 0.236 sec/batch)
2017-05-03 16:59:15.592767: step 91650, loss = 0.1069, acc = 0.9680 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 16:59:24.596494: step 91660, loss = 0.1100, acc = 0.9600 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 16:59:33.882689: step 91670, loss = 0.1358, acc = 0.9600 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 16:59:42.933893: step 91680, loss = 0.1037, acc = 0.9760 (297.0 examples/sec; 0.215 sec/batch)
2017-05-03 16:59:52.112527: step 91690, loss = 0.1259, acc = 0.9600 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 17:00:01.189042: step 91700, loss = 0.0832, acc = 0.9760 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 17:00:10.178580: step 91710, loss = 0.1444, acc = 0.9560 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 17:00:19.235765: step 91720, loss = 0.1008, acc = 0.9740 (256.5 examples/sec; 0.249 sec/batch)
2017-05-03 17:00:29.107279: step 91730, loss = 0.0978, acc = 0.9700 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 17:00:38.011352: step 91740, loss = 0.1203, acc = 0.9600 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 17:00:47.034073: step 91750, loss = 0.1132, acc = 0.9720 (295.6 examples/sec; 0.217 sec/batch)
2017-05-03 17:00:56.153624: step 91760, loss = 0.1199, acc = 0.9600 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 17:01:05.402163: step 91770, loss = 0.1064, acc = 0.9660 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 17:01:14.417195: step 91780, loss = 0.1020, acc = 0.9740 (291.7 examples/sec; 0.219 sec/batch)
2017-05-03 17:01:23.292974: step 91790, loss = 0.1058, acc = 0.9680 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 17:01:32.336904: step 91800, loss = 0.1210, acc = 0.9620 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 17:01:41.523076: step 91810, loss = 0.1198, acc = 0.9620 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 17:01:50.596054: step 91820, loss = 0.1186, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 17:01:59.514591: step 91830, loss = 0.1127, acc = 0.9660 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 17:02:08.506700: step 91840, loss = 0.1604, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 17:02:17.560272: step 91850, loss = 0.1479, acc = 0.9540 (270.9 examples/sec; 0.236 sec/batch)
2017-05-03 17:02:26.737167: step 91860, loss = 0.1018, acc = 0.9740 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 17:02:35.753921: step 91870, loss = 0.0921, acc = 0.9780 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 17:02:44.758092: step 91880, loss = 0.0957, acc = 0.9760 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 17:02:53.755716: step 91890, loss = 0.1120, acc = 0.9660 (296.8 examples/sec; 0.216 sec/batch)
2017-05-03 17:03:02.786963: step 91900, loss = 0.1225, acc = 0.9660 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 17:03:11.739307: step 91910, loss = 0.1304, acc = 0.9600 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 17:03:20.800213: step 91920, loss = 0.1157, acc = 0.9640 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 17:03:29.849812: step 91930, loss = 0.1182, acc = 0.9600 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:03:38.891691: step 91940, loss = 0.0889, acc = 0.9840 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 17:03:47.942290: step 91950, loss = 0.1177, acc = 0.9660 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 17:03:57.075327: step 91960, loss = 0.1158, acc = 0.9600 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 17:04:06.383362: step 91970, loss = 0.1131, acc = 0.9640 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 17:04:15.633892: step 91980, loss = 0.0941, acc = 0.9760 (276.6 examples/sec; 0.231 sec/batch)
2017-05-03 17:04:24.859574: step 91990, loss = 0.1190, acc = 0.9560 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 17:04:34.171110: step 92000, loss = 0.0977, acc = 0.9800 (286.2 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 17:04:34.651643: step 92000, loss = 0.1056, acc = 0.9610, f1neg = 0.9570, f1pos = 0.9643, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 17:04:35.166516: step 92000, loss = 0.1083, acc = 0.9655, f1neg = 0.9596, f1pos = 0.9699, f1 = 0.9648
[Eval_batch(2)(2000,6000)] 2017-05-03 17:04:35.641773: step 92000, loss = 0.1209, acc = 0.9605, f1neg = 0.9580, f1pos = 0.9628, f1 = 0.9604
[Eval_batch(3)(2000,8000)] 2017-05-03 17:04:36.088447: step 92000, loss = 0.1250, acc = 0.9600, f1neg = 0.9600, f1pos = 0.9600, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 17:04:36.548430: step 92000, loss = 0.1258, acc = 0.9600, f1neg = 0.9609, f1pos = 0.9590, f1 = 0.9600
[Eval_batch(5)(2000,12000)] 2017-05-03 17:04:37.020500: step 92000, loss = 0.1248, acc = 0.9560, f1neg = 0.9525, f1pos = 0.9590, f1 = 0.9558
[Eval_batch(6)(2000,14000)] 2017-05-03 17:04:37.485658: step 92000, loss = 0.1170, acc = 0.9660, f1neg = 0.9653, f1pos = 0.9666, f1 = 0.9660
[Eval_batch(7)(2000,16000)] 2017-05-03 17:04:37.955896: step 92000, loss = 0.1147, acc = 0.9625, f1neg = 0.9550, f1pos = 0.9679, f1 = 0.9614
[Eval_batch(8)(2000,18000)] 2017-05-03 17:04:38.422768: step 92000, loss = 0.1072, acc = 0.9625, f1neg = 0.9572, f1pos = 0.9666, f1 = 0.9619
[Eval_batch(9)(2000,20000)] 2017-05-03 17:04:38.895496: step 92000, loss = 0.1131, acc = 0.9650, f1neg = 0.9623, f1pos = 0.9674, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 17:04:39.368944: step 92000, loss = 0.1189, acc = 0.9615, f1neg = 0.9587, f1pos = 0.9639, f1 = 0.9613
[Eval_batch(11)(2000,24000)] 2017-05-03 17:04:39.842828: step 92000, loss = 0.1200, acc = 0.9620, f1neg = 0.9562, f1pos = 0.9665, f1 = 0.9613
[Eval_batch(12)(2000,26000)] 2017-05-03 17:04:40.282651: step 92000, loss = 0.1174, acc = 0.9565, f1neg = 0.9549, f1pos = 0.9580, f1 = 0.9564
[Eval_batch(13)(2000,28000)] 2017-05-03 17:04:40.759049: step 92000, loss = 0.1084, acc = 0.9665, f1neg = 0.9579, f1pos = 0.9722, f1 = 0.9650
[Eval_batch(14)(2000,30000)] 2017-05-03 17:04:41.208796: step 92000, loss = 0.1360, acc = 0.9535, f1neg = 0.9445, f1pos = 0.9600, f1 = 0.9523
[Eval_batch(15)(2000,32000)] 2017-05-03 17:04:41.686333: step 92000, loss = 0.1083, acc = 0.9700, f1neg = 0.9675, f1pos = 0.9721, f1 = 0.9698
[Eval_batch(16)(2000,34000)] 2017-05-03 17:04:42.159103: step 92000, loss = 0.1031, acc = 0.9695, f1neg = 0.9682, f1pos = 0.9707, f1 = 0.9694
[Eval_batch(17)(2000,36000)] 2017-05-03 17:04:42.634637: step 92000, loss = 0.1329, acc = 0.9575, f1neg = 0.9578, f1pos = 0.9572, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 17:04:43.112440: step 92000, loss = 0.1279, acc = 0.9575, f1neg = 0.9591, f1pos = 0.9558, f1 = 0.9574
[Eval_batch(19)(2000,40000)] 2017-05-03 17:04:43.568182: step 92000, loss = 0.1103, acc = 0.9685, f1neg = 0.9636, f1pos = 0.9722, f1 = 0.9679
[Eval_batch(20)(2000,42000)] 2017-05-03 17:04:44.034416: step 92000, loss = 0.1141, acc = 0.9650, f1neg = 0.9689, f1pos = 0.9600, f1 = 0.9644
[Eval_batch(21)(2000,44000)] 2017-05-03 17:04:44.493135: step 92000, loss = 0.1019, acc = 0.9665, f1neg = 0.9634, f1pos = 0.9691, f1 = 0.9663
[Eval_batch(22)(2000,46000)] 2017-05-03 17:04:44.941273: step 92000, loss = 0.1145, acc = 0.9605, f1neg = 0.9608, f1pos = 0.9602, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 17:04:45.409798: step 92000, loss = 0.1171, acc = 0.9600, f1neg = 0.9547, f1pos = 0.9642, f1 = 0.9594
[Eval_batch(24)(2000,50000)] 2017-05-03 17:04:45.872757: step 92000, loss = 0.1017, acc = 0.9660, f1neg = 0.9610, f1pos = 0.9699, f1 = 0.9654
[Eval_batch(25)(2000,52000)] 2017-05-03 17:04:46.378305: step 92000, loss = 0.0949, acc = 0.9725, f1neg = 0.9698, f1pos = 0.9748, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 17:04:46.817995: step 92000, loss = 0.1110, acc = 0.9635, f1neg = 0.9654, f1pos = 0.9614, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 17:04:47.420825: step 92000, loss = 0.0960, acc = 0.9695, f1neg = 0.9679, f1pos = 0.9710, f1 = 0.9694
[Eval] 2017-05-03 17:04:47.420919: step 92000, acc = 0.9631, f1 = 0.9627
[Test_batch(0)(2000,2000)] 2017-05-03 17:04:47.898913: step 92000, loss = 0.1451, acc = 0.9520, f1neg = 0.9542, f1pos = 0.9495, f1 = 0.9519
[Test_batch(1)(2000,4000)] 2017-05-03 17:04:48.377385: step 92000, loss = 0.1239, acc = 0.9615, f1neg = 0.9657, f1pos = 0.9561, f1 = 0.9609
[Test_batch(2)(2000,6000)] 2017-05-03 17:04:48.852710: step 92000, loss = 0.1434, acc = 0.9540, f1neg = 0.9576, f1pos = 0.9498, f1 = 0.9537
[Test_batch(3)(2000,8000)] 2017-05-03 17:04:49.326417: step 92000, loss = 0.1479, acc = 0.9515, f1neg = 0.9542, f1pos = 0.9484, f1 = 0.9513
[Test_batch(4)(2000,10000)] 2017-05-03 17:04:49.804304: step 92000, loss = 0.1400, acc = 0.9495, f1neg = 0.9495, f1pos = 0.9495, f1 = 0.9495
[Test_batch(5)(2000,12000)] 2017-05-03 17:04:50.261906: step 92000, loss = 0.1562, acc = 0.9420, f1neg = 0.9424, f1pos = 0.9416, f1 = 0.9420
[Test_batch(6)(2000,14000)] 2017-05-03 17:04:50.731760: step 92000, loss = 0.1421, acc = 0.9480, f1neg = 0.9461, f1pos = 0.9498, f1 = 0.9479
[Test_batch(7)(2000,16000)] 2017-05-03 17:04:51.198782: step 92000, loss = 0.1328, acc = 0.9540, f1neg = 0.9581, f1pos = 0.9491, f1 = 0.9536
[Test_batch(8)(2000,18000)] 2017-05-03 17:04:51.665382: step 92000, loss = 0.1326, acc = 0.9490, f1neg = 0.9482, f1pos = 0.9498, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 17:04:52.170587: step 92000, loss = 0.1608, acc = 0.9365, f1neg = 0.9329, f1pos = 0.9397, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-03 17:04:52.666109: step 92000, loss = 0.1388, acc = 0.9515, f1neg = 0.9451, f1pos = 0.9566, f1 = 0.9508
[Test_batch(11)(2000,24000)] 2017-05-03 17:04:53.141887: step 92000, loss = 0.1441, acc = 0.9525, f1neg = 0.9537, f1pos = 0.9513, f1 = 0.9525
[Test_batch(12)(2000,26000)] 2017-05-03 17:04:53.615565: step 92000, loss = 0.1164, acc = 0.9640, f1neg = 0.9653, f1pos = 0.9626, f1 = 0.9640
[Test_batch(13)(2000,28000)] 2017-05-03 17:04:54.092655: step 92000, loss = 0.1386, acc = 0.9530, f1neg = 0.9475, f1pos = 0.9575, f1 = 0.9525
[Test_batch(14)(2000,30000)] 2017-05-03 17:04:54.573633: step 92000, loss = 0.1159, acc = 0.9620, f1neg = 0.9560, f1pos = 0.9665, f1 = 0.9613
[Test_batch(15)(2000,32000)] 2017-05-03 17:04:55.050865: step 92000, loss = 0.1414, acc = 0.9535, f1neg = 0.9526, f1pos = 0.9543, f1 = 0.9535
[Test_batch(16)(2000,34000)] 2017-05-03 17:04:55.523433: step 92000, loss = 0.1207, acc = 0.9605, f1neg = 0.9594, f1pos = 0.9616, f1 = 0.9605
[Test_batch(17)(2000,36000)] 2017-05-03 17:04:55.977324: step 92000, loss = 0.1126, acc = 0.9695, f1neg = 0.9665, f1pos = 0.9720, f1 = 0.9693
[Test_batch(18)(2000,38000)] 2017-05-03 17:04:56.508148: step 92000, loss = 0.1098, acc = 0.9635, f1neg = 0.9626, f1pos = 0.9643, f1 = 0.9635
[Test] 2017-05-03 17:04:56.508209: step 92000, acc = 0.9541, f1 = 0.9539
[Status] 2017-05-03 17:04:56.508234: step 92000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 17:05:05.333041: step 92010, loss = 0.1028, acc = 0.9780 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 17:05:14.382641: step 92020, loss = 0.0969, acc = 0.9720 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 17:05:23.394380: step 92030, loss = 0.1141, acc = 0.9580 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 17:05:32.531851: step 92040, loss = 0.1129, acc = 0.9660 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 17:05:41.612205: step 92050, loss = 0.1059, acc = 0.9700 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 17:05:50.638724: step 92060, loss = 0.1082, acc = 0.9660 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 17:05:59.820725: step 92070, loss = 0.1289, acc = 0.9680 (302.0 examples/sec; 0.212 sec/batch)
2017-05-03 17:06:08.962761: step 92080, loss = 0.1197, acc = 0.9640 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 17:06:18.215725: step 92090, loss = 0.1131, acc = 0.9720 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:06:27.260858: step 92100, loss = 0.1485, acc = 0.9400 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 17:06:36.328952: step 92110, loss = 0.1378, acc = 0.9600 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 17:06:45.532371: step 92120, loss = 0.1198, acc = 0.9560 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 17:06:54.457546: step 92130, loss = 0.1014, acc = 0.9740 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:07:03.428798: step 92140, loss = 0.1205, acc = 0.9720 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 17:07:12.545737: step 92150, loss = 0.1130, acc = 0.9660 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 17:07:21.795843: step 92160, loss = 0.1042, acc = 0.9640 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 17:07:30.725391: step 92170, loss = 0.0844, acc = 0.9780 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 17:07:39.842183: step 92180, loss = 0.1091, acc = 0.9660 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 17:07:48.926716: step 92190, loss = 0.1160, acc = 0.9560 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 17:07:57.949977: step 92200, loss = 0.1086, acc = 0.9640 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 17:08:07.186216: step 92210, loss = 0.1253, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 17:08:16.289616: step 92220, loss = 0.1120, acc = 0.9700 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 17:08:25.366435: step 92230, loss = 0.1392, acc = 0.9600 (296.8 examples/sec; 0.216 sec/batch)
2017-05-03 17:08:34.478614: step 92240, loss = 0.1509, acc = 0.9480 (270.4 examples/sec; 0.237 sec/batch)
2017-05-03 17:08:43.738061: step 92250, loss = 0.0878, acc = 0.9760 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 17:08:52.718041: step 92260, loss = 0.1311, acc = 0.9520 (276.5 examples/sec; 0.231 sec/batch)
2017-05-03 17:09:01.691209: step 92270, loss = 0.1198, acc = 0.9660 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 17:09:10.724017: step 92280, loss = 0.0877, acc = 0.9780 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 17:09:19.643474: step 92290, loss = 0.0929, acc = 0.9760 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 17:09:28.615208: step 92300, loss = 0.1075, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 17:09:37.689980: step 92310, loss = 0.1188, acc = 0.9720 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 17:09:46.618449: step 92320, loss = 0.1258, acc = 0.9660 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 17:09:55.733396: step 92330, loss = 0.1147, acc = 0.9680 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 17:10:04.780041: step 92340, loss = 0.1113, acc = 0.9620 (272.6 examples/sec; 0.235 sec/batch)
2017-05-03 17:10:13.886940: step 92350, loss = 0.1166, acc = 0.9660 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 17:10:22.972157: step 92360, loss = 0.1106, acc = 0.9680 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 17:10:32.045239: step 92370, loss = 0.0973, acc = 0.9660 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 17:10:41.103445: step 92380, loss = 0.0860, acc = 0.9780 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:10:50.081783: step 92390, loss = 0.1235, acc = 0.9600 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 17:10:59.018962: step 92400, loss = 0.1018, acc = 0.9700 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 17:11:07.985225: step 92410, loss = 0.0840, acc = 0.9820 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 17:11:17.116685: step 92420, loss = 0.1123, acc = 0.9580 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 17:11:26.397934: step 92430, loss = 0.1069, acc = 0.9720 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 17:11:35.387114: step 92440, loss = 0.1052, acc = 0.9680 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 17:11:44.490225: step 92450, loss = 0.0874, acc = 0.9820 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 17:11:53.408630: step 92460, loss = 0.1285, acc = 0.9540 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 17:12:02.419224: step 92470, loss = 0.1221, acc = 0.9640 (275.3 examples/sec; 0.232 sec/batch)
2017-05-03 17:12:11.400792: step 92480, loss = 0.1136, acc = 0.9680 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 17:12:20.594407: step 92490, loss = 0.1126, acc = 0.9620 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 17:12:29.633353: step 92500, loss = 0.1176, acc = 0.9600 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 17:12:38.816893: step 92510, loss = 0.1018, acc = 0.9760 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 17:12:47.893461: step 92520, loss = 0.1310, acc = 0.9600 (266.4 examples/sec; 0.240 sec/batch)
2017-05-03 17:12:57.191028: step 92530, loss = 0.1155, acc = 0.9620 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 17:13:06.363646: step 92540, loss = 0.1083, acc = 0.9680 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 17:13:15.607783: step 92550, loss = 0.1199, acc = 0.9600 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 17:13:24.679351: step 92560, loss = 0.1300, acc = 0.9600 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 17:13:33.664023: step 92570, loss = 0.1132, acc = 0.9720 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 17:13:42.629864: step 92580, loss = 0.1155, acc = 0.9700 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 17:13:51.745162: step 92590, loss = 0.1097, acc = 0.9700 (269.1 examples/sec; 0.238 sec/batch)
2017-05-03 17:14:00.753482: step 92600, loss = 0.1118, acc = 0.9680 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 17:14:09.717006: step 92610, loss = 0.1281, acc = 0.9560 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 17:14:18.738773: step 92620, loss = 0.1188, acc = 0.9700 (290.4 examples/sec; 0.220 sec/batch)
2017-05-03 17:14:27.859829: step 92630, loss = 0.1193, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 17:14:36.958568: step 92640, loss = 0.1165, acc = 0.9580 (242.6 examples/sec; 0.264 sec/batch)
2017-05-03 17:14:46.044590: step 92650, loss = 0.0942, acc = 0.9680 (275.1 examples/sec; 0.233 sec/batch)
2017-05-03 17:14:54.957186: step 92660, loss = 0.1299, acc = 0.9620 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 17:15:03.999226: step 92670, loss = 0.1002, acc = 0.9700 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 17:15:13.086542: step 92680, loss = 0.1075, acc = 0.9760 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 17:15:22.223645: step 92690, loss = 0.1192, acc = 0.9540 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 17:15:31.201054: step 92700, loss = 0.1471, acc = 0.9620 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 17:15:40.150515: step 92710, loss = 0.1066, acc = 0.9660 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 17:15:49.173452: step 92720, loss = 0.1223, acc = 0.9740 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 17:15:58.163959: step 92730, loss = 0.0968, acc = 0.9680 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 17:16:08.320559: step 92740, loss = 0.0932, acc = 0.9780 (260.0 examples/sec; 0.246 sec/batch)
2017-05-03 17:16:17.434010: step 92750, loss = 0.1153, acc = 0.9600 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 17:16:26.394748: step 92760, loss = 0.1299, acc = 0.9560 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 17:16:35.688665: step 92770, loss = 0.0944, acc = 0.9720 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 17:16:44.639154: step 92780, loss = 0.1166, acc = 0.9640 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 17:16:53.719933: step 92790, loss = 0.1015, acc = 0.9720 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 17:17:02.842507: step 92800, loss = 0.1020, acc = 0.9680 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 17:17:11.939702: step 92810, loss = 0.1065, acc = 0.9720 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 17:17:20.873281: step 92820, loss = 0.1451, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:17:30.004621: step 92830, loss = 0.1064, acc = 0.9800 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 17:17:39.014386: step 92840, loss = 0.0976, acc = 0.9780 (275.8 examples/sec; 0.232 sec/batch)
2017-05-03 17:17:48.090364: step 92850, loss = 0.1304, acc = 0.9580 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 17:17:57.051729: step 92860, loss = 0.1117, acc = 0.9600 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 17:18:06.017910: step 92870, loss = 0.1148, acc = 0.9660 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 17:18:15.119060: step 92880, loss = 0.0780, acc = 0.9840 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 17:18:24.168185: step 92890, loss = 0.1176, acc = 0.9580 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 17:18:33.039023: step 92900, loss = 0.1182, acc = 0.9580 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 17:18:41.930811: step 92910, loss = 0.0846, acc = 0.9760 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 17:18:50.899722: step 92920, loss = 0.1037, acc = 0.9760 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 17:18:59.919527: step 92930, loss = 0.1194, acc = 0.9720 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 17:19:08.949395: step 92940, loss = 0.1193, acc = 0.9640 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 17:19:17.926471: step 92950, loss = 0.1031, acc = 0.9680 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 17:19:26.810138: step 92960, loss = 0.1042, acc = 0.9700 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 17:19:35.799045: step 92970, loss = 0.0932, acc = 0.9760 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 17:19:44.820170: step 92980, loss = 0.1095, acc = 0.9700 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 17:19:53.805795: step 92990, loss = 0.0890, acc = 0.9740 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 17:20:02.812031: step 93000, loss = 0.1040, acc = 0.9620 (291.7 examples/sec; 0.219 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 17:20:03.313333: step 93000, loss = 0.1027, acc = 0.9615, f1neg = 0.9579, f1pos = 0.9645, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 17:20:03.827503: step 93000, loss = 0.1092, acc = 0.9645, f1neg = 0.9587, f1pos = 0.9689, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 17:20:04.339033: step 93000, loss = 0.1178, acc = 0.9610, f1neg = 0.9587, f1pos = 0.9631, f1 = 0.9609
[Eval_batch(3)(2000,8000)] 2017-05-03 17:20:04.851322: step 93000, loss = 0.1215, acc = 0.9605, f1neg = 0.9606, f1pos = 0.9604, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 17:20:05.369813: step 93000, loss = 0.1234, acc = 0.9575, f1neg = 0.9589, f1pos = 0.9560, f1 = 0.9574
[Eval_batch(5)(2000,12000)] 2017-05-03 17:20:05.882285: step 93000, loss = 0.1222, acc = 0.9540, f1neg = 0.9509, f1pos = 0.9568, f1 = 0.9538
[Eval_batch(6)(2000,14000)] 2017-05-03 17:20:06.381217: step 93000, loss = 0.1133, acc = 0.9685, f1neg = 0.9680, f1pos = 0.9690, f1 = 0.9685
[Eval_batch(7)(2000,16000)] 2017-05-03 17:20:06.868108: step 93000, loss = 0.1144, acc = 0.9590, f1neg = 0.9512, f1pos = 0.9647, f1 = 0.9579
[Eval_batch(8)(2000,18000)] 2017-05-03 17:20:07.374232: step 93000, loss = 0.1048, acc = 0.9635, f1neg = 0.9586, f1pos = 0.9673, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 17:20:07.879595: step 93000, loss = 0.1111, acc = 0.9660, f1neg = 0.9636, f1pos = 0.9681, f1 = 0.9659
[Eval_batch(10)(2000,22000)] 2017-05-03 17:20:08.384733: step 93000, loss = 0.1169, acc = 0.9640, f1neg = 0.9617, f1pos = 0.9660, f1 = 0.9639
[Eval_batch(11)(2000,24000)] 2017-05-03 17:20:08.882928: step 93000, loss = 0.1190, acc = 0.9625, f1neg = 0.9569, f1pos = 0.9668, f1 = 0.9619
[Eval_batch(12)(2000,26000)] 2017-05-03 17:20:09.391134: step 93000, loss = 0.1157, acc = 0.9550, f1neg = 0.9539, f1pos = 0.9561, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 17:20:09.892668: step 93000, loss = 0.1069, acc = 0.9670, f1neg = 0.9588, f1pos = 0.9725, f1 = 0.9656
[Eval_batch(14)(2000,30000)] 2017-05-03 17:20:10.400328: step 93000, loss = 0.1340, acc = 0.9570, f1neg = 0.9492, f1pos = 0.9627, f1 = 0.9560
[Eval_batch(15)(2000,32000)] 2017-05-03 17:20:10.900204: step 93000, loss = 0.1079, acc = 0.9685, f1neg = 0.9660, f1pos = 0.9706, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 17:20:11.384548: step 93000, loss = 0.1029, acc = 0.9695, f1neg = 0.9683, f1pos = 0.9706, f1 = 0.9695
[Eval_batch(17)(2000,36000)] 2017-05-03 17:20:11.878517: step 93000, loss = 0.1333, acc = 0.9585, f1neg = 0.9589, f1pos = 0.9581, f1 = 0.9585
[Eval_batch(18)(2000,38000)] 2017-05-03 17:20:12.383121: step 93000, loss = 0.1237, acc = 0.9610, f1neg = 0.9627, f1pos = 0.9591, f1 = 0.9609
[Eval_batch(19)(2000,40000)] 2017-05-03 17:20:12.873318: step 93000, loss = 0.1099, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9717, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 17:20:13.362971: step 93000, loss = 0.1097, acc = 0.9645, f1neg = 0.9686, f1pos = 0.9592, f1 = 0.9639
[Eval_batch(21)(2000,44000)] 2017-05-03 17:20:13.874250: step 93000, loss = 0.1033, acc = 0.9650, f1neg = 0.9621, f1pos = 0.9675, f1 = 0.9648
[Eval_batch(22)(2000,46000)] 2017-05-03 17:20:14.380091: step 93000, loss = 0.1129, acc = 0.9620, f1neg = 0.9624, f1pos = 0.9616, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 17:20:14.894707: step 93000, loss = 0.1188, acc = 0.9615, f1neg = 0.9568, f1pos = 0.9653, f1 = 0.9610
[Eval_batch(24)(2000,50000)] 2017-05-03 17:20:15.402930: step 93000, loss = 0.1016, acc = 0.9680, f1neg = 0.9636, f1pos = 0.9715, f1 = 0.9675
[Eval_batch(25)(2000,52000)] 2017-05-03 17:20:15.910686: step 93000, loss = 0.0931, acc = 0.9740, f1neg = 0.9716, f1pos = 0.9760, f1 = 0.9738
[Eval_batch(26)(2000,54000)] 2017-05-03 17:20:16.412085: step 93000, loss = 0.1085, acc = 0.9620, f1neg = 0.9643, f1pos = 0.9594, f1 = 0.9618
[Eval_batch(27)(2000,56000)] 2017-05-03 17:20:16.996728: step 93000, loss = 0.0930, acc = 0.9740, f1neg = 0.9728, f1pos = 0.9751, f1 = 0.9740
[Eval] 2017-05-03 17:20:16.996816: step 93000, acc = 0.9635, f1 = 0.9632
[Test_batch(0)(2000,2000)] 2017-05-03 17:20:17.513155: step 93000, loss = 0.1430, acc = 0.9510, f1neg = 0.9537, f1pos = 0.9480, f1 = 0.9508
[Test_batch(1)(2000,4000)] 2017-05-03 17:20:18.028629: step 93000, loss = 0.1209, acc = 0.9635, f1neg = 0.9676, f1pos = 0.9582, f1 = 0.9629
[Test_batch(2)(2000,6000)] 2017-05-03 17:20:18.507235: step 93000, loss = 0.1405, acc = 0.9575, f1neg = 0.9611, f1pos = 0.9532, f1 = 0.9571
[Test_batch(3)(2000,8000)] 2017-05-03 17:20:19.016336: step 93000, loss = 0.1453, acc = 0.9510, f1neg = 0.9542, f1pos = 0.9474, f1 = 0.9508
[Test_batch(4)(2000,10000)] 2017-05-03 17:20:19.523641: step 93000, loss = 0.1392, acc = 0.9500, f1neg = 0.9504, f1pos = 0.9495, f1 = 0.9500
[Test_batch(5)(2000,12000)] 2017-05-03 17:20:20.027488: step 93000, loss = 0.1552, acc = 0.9475, f1neg = 0.9484, f1pos = 0.9466, f1 = 0.9475
[Test_batch(6)(2000,14000)] 2017-05-03 17:20:20.468633: step 93000, loss = 0.1403, acc = 0.9490, f1neg = 0.9473, f1pos = 0.9506, f1 = 0.9489
[Test_batch(7)(2000,16000)] 2017-05-03 17:20:20.946019: step 93000, loss = 0.1311, acc = 0.9535, f1neg = 0.9579, f1pos = 0.9481, f1 = 0.9530
[Test_batch(8)(2000,18000)] 2017-05-03 17:20:21.423024: step 93000, loss = 0.1313, acc = 0.9515, f1neg = 0.9511, f1pos = 0.9519, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 17:20:21.927013: step 93000, loss = 0.1602, acc = 0.9385, f1neg = 0.9356, f1pos = 0.9411, f1 = 0.9384
[Test_batch(10)(2000,22000)] 2017-05-03 17:20:22.440452: step 93000, loss = 0.1398, acc = 0.9545, f1neg = 0.9490, f1pos = 0.9589, f1 = 0.9540
[Test_batch(11)(2000,24000)] 2017-05-03 17:20:22.947476: step 93000, loss = 0.1423, acc = 0.9540, f1neg = 0.9554, f1pos = 0.9525, f1 = 0.9540
[Test_batch(12)(2000,26000)] 2017-05-03 17:20:23.424005: step 93000, loss = 0.1156, acc = 0.9630, f1neg = 0.9645, f1pos = 0.9614, f1 = 0.9629
[Test_batch(13)(2000,28000)] 2017-05-03 17:20:23.900646: step 93000, loss = 0.1392, acc = 0.9555, f1neg = 0.9510, f1pos = 0.9592, f1 = 0.9551
[Test_batch(14)(2000,30000)] 2017-05-03 17:20:24.383159: step 93000, loss = 0.1159, acc = 0.9595, f1neg = 0.9536, f1pos = 0.9640, f1 = 0.9588
[Test_batch(15)(2000,32000)] 2017-05-03 17:20:24.862490: step 93000, loss = 0.1396, acc = 0.9600, f1neg = 0.9596, f1pos = 0.9604, f1 = 0.9600
[Test_batch(16)(2000,34000)] 2017-05-03 17:20:25.371278: step 93000, loss = 0.1208, acc = 0.9600, f1neg = 0.9591, f1pos = 0.9609, f1 = 0.9600
[Test_batch(17)(2000,36000)] 2017-05-03 17:20:25.848372: step 93000, loss = 0.1113, acc = 0.9670, f1neg = 0.9640, f1pos = 0.9695, f1 = 0.9668
[Test_batch(18)(2000,38000)] 2017-05-03 17:20:26.440688: step 93000, loss = 0.1071, acc = 0.9665, f1neg = 0.9659, f1pos = 0.9671, f1 = 0.9665
[Test] 2017-05-03 17:20:26.440777: step 93000, acc = 0.9554, f1 = 0.9552
[Status] 2017-05-03 17:20:26.440802: step 93000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 17:20:35.397771: step 93010, loss = 0.0843, acc = 0.9780 (301.9 examples/sec; 0.212 sec/batch)
2017-05-03 17:20:44.630848: step 93020, loss = 0.1402, acc = 0.9640 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 17:20:54.096340: step 93030, loss = 0.1230, acc = 0.9600 (250.3 examples/sec; 0.256 sec/batch)
2017-05-03 17:21:03.141548: step 93040, loss = 0.0915, acc = 0.9760 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 17:21:12.199428: step 93050, loss = 0.1323, acc = 0.9600 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 17:21:21.250885: step 93060, loss = 0.1000, acc = 0.9720 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 17:21:30.270381: step 93070, loss = 0.1113, acc = 0.9700 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 17:21:39.264730: step 93080, loss = 0.1223, acc = 0.9620 (301.6 examples/sec; 0.212 sec/batch)
2017-05-03 17:21:48.168636: step 93090, loss = 0.1161, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 17:21:57.153101: step 93100, loss = 0.1193, acc = 0.9720 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 17:22:06.302712: step 93110, loss = 0.0882, acc = 0.9780 (292.3 examples/sec; 0.219 sec/batch)
2017-05-03 17:22:15.263989: step 93120, loss = 0.1086, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 17:22:24.241440: step 93130, loss = 0.1130, acc = 0.9740 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 17:22:33.438601: step 93140, loss = 0.0940, acc = 0.9700 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 17:22:42.496348: step 93150, loss = 0.1518, acc = 0.9500 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:22:51.572185: step 93160, loss = 0.1185, acc = 0.9660 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 17:23:00.544066: step 93170, loss = 0.1148, acc = 0.9620 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 17:23:09.438326: step 93180, loss = 0.1181, acc = 0.9640 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 17:23:18.503337: step 93190, loss = 0.1232, acc = 0.9580 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 17:23:27.521087: step 93200, loss = 0.1304, acc = 0.9440 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 17:23:36.450607: step 93210, loss = 0.1132, acc = 0.9760 (302.9 examples/sec; 0.211 sec/batch)
2017-05-03 17:23:45.531879: step 93220, loss = 0.1064, acc = 0.9700 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 17:23:54.589269: step 93230, loss = 0.1165, acc = 0.9540 (295.2 examples/sec; 0.217 sec/batch)
2017-05-03 17:24:03.712439: step 93240, loss = 0.1463, acc = 0.9380 (273.6 examples/sec; 0.234 sec/batch)
2017-05-03 17:24:12.627771: step 93250, loss = 0.1002, acc = 0.9740 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 17:24:21.605483: step 93260, loss = 0.1322, acc = 0.9560 (272.0 examples/sec; 0.235 sec/batch)
2017-05-03 17:24:30.526235: step 93270, loss = 0.1119, acc = 0.9640 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 17:24:39.553471: step 93280, loss = 0.1480, acc = 0.9480 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 17:24:48.492242: step 93290, loss = 0.1328, acc = 0.9640 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 17:24:57.554306: step 93300, loss = 0.1068, acc = 0.9640 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 17:25:06.869007: step 93310, loss = 0.1068, acc = 0.9640 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 17:25:15.924067: step 93320, loss = 0.0902, acc = 0.9800 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 17:25:24.865464: step 93330, loss = 0.1161, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 17:25:33.829737: step 93340, loss = 0.1021, acc = 0.9720 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 17:25:42.823896: step 93350, loss = 0.1497, acc = 0.9520 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 17:25:51.798563: step 93360, loss = 0.1123, acc = 0.9680 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 17:26:01.053482: step 93370, loss = 0.0909, acc = 0.9760 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 17:26:10.108845: step 93380, loss = 0.1077, acc = 0.9660 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 17:26:19.068929: step 93390, loss = 0.1160, acc = 0.9680 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 17:26:28.110213: step 93400, loss = 0.1111, acc = 0.9680 (262.2 examples/sec; 0.244 sec/batch)
2017-05-03 17:26:36.983412: step 93410, loss = 0.0968, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 17:26:46.046636: step 93420, loss = 0.1279, acc = 0.9680 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 17:26:54.955247: step 93430, loss = 0.1167, acc = 0.9600 (288.9 examples/sec; 0.221 sec/batch)
2017-05-03 17:27:04.076541: step 93440, loss = 0.1247, acc = 0.9600 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 17:27:13.164909: step 93450, loss = 0.1403, acc = 0.9540 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:27:22.267669: step 93460, loss = 0.1173, acc = 0.9640 (274.5 examples/sec; 0.233 sec/batch)
2017-05-03 17:27:31.166789: step 93470, loss = 0.0861, acc = 0.9740 (297.9 examples/sec; 0.215 sec/batch)
2017-05-03 17:27:40.276925: step 93480, loss = 0.1273, acc = 0.9620 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 17:27:49.208037: step 93490, loss = 0.1072, acc = 0.9720 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 17:27:58.356532: step 93500, loss = 0.1151, acc = 0.9700 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 17:28:07.202443: step 93510, loss = 0.1052, acc = 0.9740 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 17:28:16.295846: step 93520, loss = 0.1197, acc = 0.9640 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 17:28:25.162405: step 93530, loss = 0.1023, acc = 0.9720 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 17:28:34.132799: step 93540, loss = 0.1203, acc = 0.9620 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 17:28:43.153780: step 93550, loss = 0.0984, acc = 0.9700 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 17:28:52.134607: step 93560, loss = 0.1346, acc = 0.9400 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 17:29:01.066932: step 93570, loss = 0.0876, acc = 0.9800 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 17:29:10.096859: step 93580, loss = 0.1015, acc = 0.9740 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 17:29:19.223586: step 93590, loss = 0.1337, acc = 0.9580 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 17:29:28.223847: step 93600, loss = 0.1296, acc = 0.9520 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 17:29:37.245172: step 93610, loss = 0.1197, acc = 0.9620 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 17:29:46.239882: step 93620, loss = 0.1071, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 17:29:55.384864: step 93630, loss = 0.1291, acc = 0.9520 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 17:30:04.493208: step 93640, loss = 0.1157, acc = 0.9620 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 17:30:13.584664: step 93650, loss = 0.1168, acc = 0.9520 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 17:30:22.491284: step 93660, loss = 0.1185, acc = 0.9720 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:30:31.390285: step 93670, loss = 0.0911, acc = 0.9820 (288.6 examples/sec; 0.222 sec/batch)
2017-05-03 17:30:40.453010: step 93680, loss = 0.1003, acc = 0.9760 (294.7 examples/sec; 0.217 sec/batch)
2017-05-03 17:30:49.383196: step 93690, loss = 0.1642, acc = 0.9460 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 17:30:58.403560: step 93700, loss = 0.1069, acc = 0.9680 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 17:31:07.426635: step 93710, loss = 0.1228, acc = 0.9660 (296.4 examples/sec; 0.216 sec/batch)
2017-05-03 17:31:16.488601: step 93720, loss = 0.1193, acc = 0.9540 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 17:31:25.478166: step 93730, loss = 0.1406, acc = 0.9580 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 17:31:34.570073: step 93740, loss = 0.1125, acc = 0.9700 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 17:31:44.488613: step 93750, loss = 0.1310, acc = 0.9560 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:31:53.451104: step 93760, loss = 0.1297, acc = 0.9640 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 17:32:02.911418: step 93770, loss = 0.1149, acc = 0.9660 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 17:32:11.901818: step 93780, loss = 0.0995, acc = 0.9660 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 17:32:20.885765: step 93790, loss = 0.1281, acc = 0.9560 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 17:32:30.221276: step 93800, loss = 0.0911, acc = 0.9760 (253.4 examples/sec; 0.253 sec/batch)
2017-05-03 17:32:39.180728: step 93810, loss = 0.1344, acc = 0.9440 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 17:32:48.238320: step 93820, loss = 0.0913, acc = 0.9700 (266.5 examples/sec; 0.240 sec/batch)
2017-05-03 17:32:57.201991: step 93830, loss = 0.1030, acc = 0.9740 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 17:33:06.271616: step 93840, loss = 0.0960, acc = 0.9660 (269.3 examples/sec; 0.238 sec/batch)
2017-05-03 17:33:15.365146: step 93850, loss = 0.1270, acc = 0.9620 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 17:33:24.324468: step 93860, loss = 0.1332, acc = 0.9480 (299.0 examples/sec; 0.214 sec/batch)
2017-05-03 17:33:33.511355: step 93870, loss = 0.0855, acc = 0.9740 (285.1 examples/sec; 0.224 sec/batch)
2017-05-03 17:33:42.522519: step 93880, loss = 0.1225, acc = 0.9660 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 17:33:51.415596: step 93890, loss = 0.1221, acc = 0.9680 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 17:34:00.387313: step 93900, loss = 0.1054, acc = 0.9620 (292.4 examples/sec; 0.219 sec/batch)
2017-05-03 17:34:09.390002: step 93910, loss = 0.1038, acc = 0.9680 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 17:34:18.415495: step 93920, loss = 0.1004, acc = 0.9800 (278.0 examples/sec; 0.230 sec/batch)
2017-05-03 17:34:27.399871: step 93930, loss = 0.1032, acc = 0.9680 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 17:34:36.328371: step 93940, loss = 0.0855, acc = 0.9800 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 17:34:45.346591: step 93950, loss = 0.1358, acc = 0.9560 (271.1 examples/sec; 0.236 sec/batch)
2017-05-03 17:34:54.562969: step 93960, loss = 0.1212, acc = 0.9620 (284.0 examples/sec; 0.225 sec/batch)
2017-05-03 17:35:03.598533: step 93970, loss = 0.1305, acc = 0.9680 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 17:35:12.641119: step 93980, loss = 0.1368, acc = 0.9580 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 17:35:21.851675: step 93990, loss = 0.1023, acc = 0.9700 (266.9 examples/sec; 0.240 sec/batch)
2017-05-03 17:35:30.937716: step 94000, loss = 0.1335, acc = 0.9660 (296.4 examples/sec; 0.216 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 17:35:31.407860: step 94000, loss = 0.1034, acc = 0.9625, f1neg = 0.9589, f1pos = 0.9655, f1 = 0.9622
[Eval_batch(1)(2000,4000)] 2017-05-03 17:35:31.880727: step 94000, loss = 0.1086, acc = 0.9635, f1neg = 0.9576, f1pos = 0.9679, f1 = 0.9628
[Eval_batch(2)(2000,6000)] 2017-05-03 17:35:32.362930: step 94000, loss = 0.1185, acc = 0.9595, f1neg = 0.9570, f1pos = 0.9617, f1 = 0.9594
[Eval_batch(3)(2000,8000)] 2017-05-03 17:35:32.868100: step 94000, loss = 0.1216, acc = 0.9595, f1neg = 0.9596, f1pos = 0.9594, f1 = 0.9595
[Eval_batch(4)(2000,10000)] 2017-05-03 17:35:33.364823: step 94000, loss = 0.1242, acc = 0.9585, f1neg = 0.9598, f1pos = 0.9571, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 17:35:33.871923: step 94000, loss = 0.1225, acc = 0.9550, f1neg = 0.9519, f1pos = 0.9577, f1 = 0.9548
[Eval_batch(6)(2000,14000)] 2017-05-03 17:35:34.381799: step 94000, loss = 0.1136, acc = 0.9675, f1neg = 0.9670, f1pos = 0.9680, f1 = 0.9675
[Eval_batch(7)(2000,16000)] 2017-05-03 17:35:34.858395: step 94000, loss = 0.1148, acc = 0.9600, f1neg = 0.9526, f1pos = 0.9654, f1 = 0.9590
[Eval_batch(8)(2000,18000)] 2017-05-03 17:35:35.364937: step 94000, loss = 0.1056, acc = 0.9630, f1neg = 0.9581, f1pos = 0.9669, f1 = 0.9625
[Eval_batch(9)(2000,20000)] 2017-05-03 17:35:35.828213: step 94000, loss = 0.1114, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9667, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 17:35:36.323565: step 94000, loss = 0.1173, acc = 0.9625, f1neg = 0.9601, f1pos = 0.9646, f1 = 0.9624
[Eval_batch(11)(2000,24000)] 2017-05-03 17:35:36.829055: step 94000, loss = 0.1196, acc = 0.9625, f1neg = 0.9569, f1pos = 0.9668, f1 = 0.9619
[Eval_batch(12)(2000,26000)] 2017-05-03 17:35:37.334531: step 94000, loss = 0.1149, acc = 0.9540, f1neg = 0.9527, f1pos = 0.9552, f1 = 0.9540
[Eval_batch(13)(2000,28000)] 2017-05-03 17:35:37.840999: step 94000, loss = 0.1070, acc = 0.9675, f1neg = 0.9594, f1pos = 0.9729, f1 = 0.9662
[Eval_batch(14)(2000,30000)] 2017-05-03 17:35:38.314508: step 94000, loss = 0.1343, acc = 0.9570, f1neg = 0.9491, f1pos = 0.9628, f1 = 0.9559
[Eval_batch(15)(2000,32000)] 2017-05-03 17:35:38.816589: step 94000, loss = 0.1078, acc = 0.9705, f1neg = 0.9682, f1pos = 0.9725, f1 = 0.9703
[Eval_batch(16)(2000,34000)] 2017-05-03 17:35:39.320935: step 94000, loss = 0.1021, acc = 0.9680, f1neg = 0.9668, f1pos = 0.9691, f1 = 0.9680
[Eval_batch(17)(2000,36000)] 2017-05-03 17:35:39.782746: step 94000, loss = 0.1325, acc = 0.9575, f1neg = 0.9579, f1pos = 0.9571, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 17:35:40.245641: step 94000, loss = 0.1252, acc = 0.9610, f1neg = 0.9627, f1pos = 0.9591, f1 = 0.9609
[Eval_batch(19)(2000,40000)] 2017-05-03 17:35:40.720939: step 94000, loss = 0.1083, acc = 0.9695, f1neg = 0.9649, f1pos = 0.9730, f1 = 0.9690
[Eval_batch(20)(2000,42000)] 2017-05-03 17:35:41.197117: step 94000, loss = 0.1100, acc = 0.9660, f1neg = 0.9699, f1pos = 0.9609, f1 = 0.9654
[Eval_batch(21)(2000,44000)] 2017-05-03 17:35:41.677947: step 94000, loss = 0.1031, acc = 0.9640, f1neg = 0.9609, f1pos = 0.9667, f1 = 0.9638
[Eval_batch(22)(2000,46000)] 2017-05-03 17:35:42.134526: step 94000, loss = 0.1130, acc = 0.9605, f1neg = 0.9609, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 17:35:42.612991: step 94000, loss = 0.1173, acc = 0.9610, f1neg = 0.9561, f1pos = 0.9649, f1 = 0.9605
[Eval_batch(24)(2000,50000)] 2017-05-03 17:35:43.088826: step 94000, loss = 0.1005, acc = 0.9680, f1neg = 0.9634, f1pos = 0.9716, f1 = 0.9675
[Eval_batch(25)(2000,52000)] 2017-05-03 17:35:43.590791: step 94000, loss = 0.0928, acc = 0.9735, f1neg = 0.9711, f1pos = 0.9755, f1 = 0.9733
[Eval_batch(26)(2000,54000)] 2017-05-03 17:35:44.098048: step 94000, loss = 0.1080, acc = 0.9635, f1neg = 0.9656, f1pos = 0.9611, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 17:35:44.666279: step 94000, loss = 0.0943, acc = 0.9725, f1neg = 0.9712, f1pos = 0.9736, f1 = 0.9724
[Eval] 2017-05-03 17:35:44.666345: step 94000, acc = 0.9633, f1 = 0.9630
[Test_batch(0)(2000,2000)] 2017-05-03 17:35:45.114417: step 94000, loss = 0.1429, acc = 0.9535, f1neg = 0.9560, f1pos = 0.9507, f1 = 0.9534
[Test_batch(1)(2000,4000)] 2017-05-03 17:35:45.620012: step 94000, loss = 0.1201, acc = 0.9640, f1neg = 0.9681, f1pos = 0.9587, f1 = 0.9634
[Test_batch(2)(2000,6000)] 2017-05-03 17:35:46.067700: step 94000, loss = 0.1387, acc = 0.9580, f1neg = 0.9615, f1pos = 0.9538, f1 = 0.9576
[Test_batch(3)(2000,8000)] 2017-05-03 17:35:46.536628: step 94000, loss = 0.1450, acc = 0.9510, f1neg = 0.9542, f1pos = 0.9474, f1 = 0.9508
[Test_batch(4)(2000,10000)] 2017-05-03 17:35:46.994521: step 94000, loss = 0.1395, acc = 0.9515, f1neg = 0.9520, f1pos = 0.9510, f1 = 0.9515
[Test_batch(5)(2000,12000)] 2017-05-03 17:35:47.475030: step 94000, loss = 0.1555, acc = 0.9470, f1neg = 0.9478, f1pos = 0.9462, f1 = 0.9470
[Test_batch(6)(2000,14000)] 2017-05-03 17:35:47.949504: step 94000, loss = 0.1403, acc = 0.9510, f1neg = 0.9494, f1pos = 0.9525, f1 = 0.9510
[Test_batch(7)(2000,16000)] 2017-05-03 17:35:48.426966: step 94000, loss = 0.1316, acc = 0.9525, f1neg = 0.9569, f1pos = 0.9471, f1 = 0.9520
[Test_batch(8)(2000,18000)] 2017-05-03 17:35:48.894427: step 94000, loss = 0.1311, acc = 0.9510, f1neg = 0.9506, f1pos = 0.9514, f1 = 0.9510
[Test_batch(9)(2000,20000)] 2017-05-03 17:35:49.355558: step 94000, loss = 0.1601, acc = 0.9370, f1neg = 0.9339, f1pos = 0.9398, f1 = 0.9369
[Test_batch(10)(2000,22000)] 2017-05-03 17:35:49.825053: step 94000, loss = 0.1405, acc = 0.9540, f1neg = 0.9484, f1pos = 0.9585, f1 = 0.9535
[Test_batch(11)(2000,24000)] 2017-05-03 17:35:50.322071: step 94000, loss = 0.1425, acc = 0.9525, f1neg = 0.9540, f1pos = 0.9509, f1 = 0.9524
[Test_batch(12)(2000,26000)] 2017-05-03 17:35:50.785904: step 94000, loss = 0.1160, acc = 0.9640, f1neg = 0.9655, f1pos = 0.9623, f1 = 0.9639
[Test_batch(13)(2000,28000)] 2017-05-03 17:35:51.263815: step 94000, loss = 0.1377, acc = 0.9565, f1neg = 0.9520, f1pos = 0.9603, f1 = 0.9561
[Test_batch(14)(2000,30000)] 2017-05-03 17:35:51.745760: step 94000, loss = 0.1164, acc = 0.9610, f1neg = 0.9553, f1pos = 0.9654, f1 = 0.9604
[Test_batch(15)(2000,32000)] 2017-05-03 17:35:52.233649: step 94000, loss = 0.1400, acc = 0.9575, f1neg = 0.9571, f1pos = 0.9579, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 17:35:52.710437: step 94000, loss = 0.1211, acc = 0.9610, f1neg = 0.9602, f1pos = 0.9618, f1 = 0.9610
[Test_batch(17)(2000,36000)] 2017-05-03 17:35:53.204149: step 94000, loss = 0.1117, acc = 0.9670, f1neg = 0.9640, f1pos = 0.9695, f1 = 0.9668
[Test_batch(18)(2000,38000)] 2017-05-03 17:35:53.743607: step 94000, loss = 0.1068, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 17:35:53.743695: step 94000, acc = 0.9555, f1 = 0.9553
[Status] 2017-05-03 17:35:53.743720: step 94000, maxindex = 88000, maxdev = 0.9636, maxtst = 0.9556
2017-05-03 17:36:02.782372: step 94010, loss = 0.1206, acc = 0.9600 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 17:36:11.803544: step 94020, loss = 0.1010, acc = 0.9780 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 17:36:20.822940: step 94030, loss = 0.1282, acc = 0.9580 (269.5 examples/sec; 0.237 sec/batch)
2017-05-03 17:36:29.793125: step 94040, loss = 0.1075, acc = 0.9600 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 17:36:39.184813: step 94050, loss = 0.1080, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 17:36:48.185956: step 94060, loss = 0.0900, acc = 0.9800 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 17:36:57.211556: step 94070, loss = 0.0976, acc = 0.9760 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 17:37:06.362102: step 94080, loss = 0.1291, acc = 0.9500 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 17:37:15.372305: step 94090, loss = 0.1252, acc = 0.9620 (297.0 examples/sec; 0.216 sec/batch)
2017-05-03 17:37:24.441896: step 94100, loss = 0.1244, acc = 0.9540 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 17:37:34.707454: step 94110, loss = 0.1354, acc = 0.9520 (227.6 examples/sec; 0.281 sec/batch)
2017-05-03 17:37:43.724543: step 94120, loss = 0.1152, acc = 0.9680 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 17:37:52.763881: step 94130, loss = 0.1317, acc = 0.9540 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 17:38:01.770189: step 94140, loss = 0.1358, acc = 0.9580 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 17:38:10.769075: step 94150, loss = 0.1104, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 17:38:19.726601: step 94160, loss = 0.1089, acc = 0.9760 (295.0 examples/sec; 0.217 sec/batch)
2017-05-03 17:38:28.790511: step 94170, loss = 0.1138, acc = 0.9700 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 17:38:38.027925: step 94180, loss = 0.1311, acc = 0.9600 (275.6 examples/sec; 0.232 sec/batch)
2017-05-03 17:38:47.105851: step 94190, loss = 0.1449, acc = 0.9500 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 17:38:56.491613: step 94200, loss = 0.0991, acc = 0.9660 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 17:39:05.537847: step 94210, loss = 0.1003, acc = 0.9780 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 17:39:14.501464: step 94220, loss = 0.1303, acc = 0.9560 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 17:39:23.614697: step 94230, loss = 0.1296, acc = 0.9580 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 17:39:32.901813: step 94240, loss = 0.1164, acc = 0.9620 (274.4 examples/sec; 0.233 sec/batch)
2017-05-03 17:39:41.949036: step 94250, loss = 0.1139, acc = 0.9600 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 17:39:51.049586: step 94260, loss = 0.1174, acc = 0.9600 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:40:00.213481: step 94270, loss = 0.1058, acc = 0.9760 (266.8 examples/sec; 0.240 sec/batch)
2017-05-03 17:40:09.354547: step 94280, loss = 0.1208, acc = 0.9620 (265.7 examples/sec; 0.241 sec/batch)
2017-05-03 17:40:18.424235: step 94290, loss = 0.1144, acc = 0.9540 (289.0 examples/sec; 0.221 sec/batch)
2017-05-03 17:40:27.503450: step 94300, loss = 0.1373, acc = 0.9580 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 17:40:36.672587: step 94310, loss = 0.1112, acc = 0.9680 (276.5 examples/sec; 0.232 sec/batch)
2017-05-03 17:40:45.719265: step 94320, loss = 0.1433, acc = 0.9520 (264.7 examples/sec; 0.242 sec/batch)
2017-05-03 17:40:54.773678: step 94330, loss = 0.1181, acc = 0.9560 (297.0 examples/sec; 0.216 sec/batch)
2017-05-03 17:41:03.955117: step 94340, loss = 0.1111, acc = 0.9660 (264.0 examples/sec; 0.242 sec/batch)
2017-05-03 17:41:12.991489: step 94350, loss = 0.0903, acc = 0.9720 (276.4 examples/sec; 0.232 sec/batch)
2017-05-03 17:41:22.141255: step 94360, loss = 0.1118, acc = 0.9720 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 17:41:31.300871: step 94370, loss = 0.1360, acc = 0.9500 (285.1 examples/sec; 0.225 sec/batch)
2017-05-03 17:41:40.498768: step 94380, loss = 0.1109, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 17:41:49.516520: step 94390, loss = 0.1177, acc = 0.9640 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 17:41:58.563650: step 94400, loss = 0.1129, acc = 0.9620 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 17:42:07.666505: step 94410, loss = 0.1128, acc = 0.9680 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 17:42:16.981243: step 94420, loss = 0.1112, acc = 0.9620 (294.6 examples/sec; 0.217 sec/batch)
2017-05-03 17:42:26.002971: step 94430, loss = 0.1115, acc = 0.9660 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:42:35.075914: step 94440, loss = 0.1115, acc = 0.9560 (269.0 examples/sec; 0.238 sec/batch)
2017-05-03 17:42:44.136190: step 94450, loss = 0.0858, acc = 0.9840 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 17:42:53.272049: step 94460, loss = 0.1108, acc = 0.9680 (292.0 examples/sec; 0.219 sec/batch)
2017-05-03 17:43:02.388866: step 94470, loss = 0.1410, acc = 0.9540 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 17:43:11.388024: step 94480, loss = 0.1044, acc = 0.9680 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 17:43:20.395101: step 94490, loss = 0.1026, acc = 0.9680 (299.2 examples/sec; 0.214 sec/batch)
2017-05-03 17:43:29.390699: step 94500, loss = 0.1084, acc = 0.9640 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 17:43:38.484174: step 94510, loss = 0.1076, acc = 0.9640 (250.6 examples/sec; 0.255 sec/batch)
2017-05-03 17:43:47.646344: step 94520, loss = 0.1323, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 17:43:56.651542: step 94530, loss = 0.0929, acc = 0.9720 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 17:44:05.763502: step 94540, loss = 0.1174, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 17:44:14.890361: step 94550, loss = 0.1179, acc = 0.9700 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 17:44:23.982084: step 94560, loss = 0.1259, acc = 0.9640 (282.3 examples/sec; 0.227 sec/batch)
2017-05-03 17:44:32.995585: step 94570, loss = 0.1143, acc = 0.9720 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 17:44:42.098912: step 94580, loss = 0.1106, acc = 0.9600 (282.6 examples/sec; 0.226 sec/batch)
2017-05-03 17:44:51.155761: step 94590, loss = 0.1425, acc = 0.9480 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 17:45:00.334360: step 94600, loss = 0.1353, acc = 0.9580 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 17:45:09.450330: step 94610, loss = 0.1074, acc = 0.9640 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 17:45:18.577206: step 94620, loss = 0.0988, acc = 0.9720 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 17:45:27.725882: step 94630, loss = 0.1329, acc = 0.9540 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 17:45:36.802766: step 94640, loss = 0.0957, acc = 0.9780 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:45:45.875538: step 94650, loss = 0.1214, acc = 0.9620 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 17:45:54.929692: step 94660, loss = 0.0953, acc = 0.9780 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 17:46:03.962537: step 94670, loss = 0.1378, acc = 0.9600 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 17:46:12.959354: step 94680, loss = 0.1095, acc = 0.9700 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 17:46:22.333516: step 94690, loss = 0.1199, acc = 0.9500 (270.0 examples/sec; 0.237 sec/batch)
2017-05-03 17:46:31.463316: step 94700, loss = 0.1084, acc = 0.9660 (297.5 examples/sec; 0.215 sec/batch)
2017-05-03 17:46:40.718993: step 94710, loss = 0.1097, acc = 0.9720 (298.4 examples/sec; 0.214 sec/batch)
2017-05-03 17:46:49.709596: step 94720, loss = 0.1055, acc = 0.9640 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 17:46:58.617619: step 94730, loss = 0.1318, acc = 0.9640 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 17:47:07.629550: step 94740, loss = 0.1147, acc = 0.9680 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 17:47:16.481411: step 94750, loss = 0.1213, acc = 0.9600 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 17:47:25.999354: step 94760, loss = 0.1173, acc = 0.9620 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 17:47:34.932434: step 94770, loss = 0.0883, acc = 0.9740 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 17:47:43.973861: step 94780, loss = 0.1189, acc = 0.9600 (278.2 examples/sec; 0.230 sec/batch)
2017-05-03 17:47:53.299290: step 94790, loss = 0.0928, acc = 0.9700 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 17:48:02.333812: step 94800, loss = 0.1304, acc = 0.9460 (277.0 examples/sec; 0.231 sec/batch)
2017-05-03 17:48:11.349194: step 94810, loss = 0.1520, acc = 0.9500 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 17:48:20.456881: step 94820, loss = 0.1164, acc = 0.9580 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 17:48:29.413082: step 94830, loss = 0.0972, acc = 0.9740 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 17:48:38.327032: step 94840, loss = 0.1427, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:48:47.446998: step 94850, loss = 0.1139, acc = 0.9620 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 17:48:56.398646: step 94860, loss = 0.1045, acc = 0.9720 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 17:49:05.408682: step 94870, loss = 0.0920, acc = 0.9720 (300.9 examples/sec; 0.213 sec/batch)
2017-05-03 17:49:14.329379: step 94880, loss = 0.1410, acc = 0.9400 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 17:49:23.344759: step 94890, loss = 0.1162, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 17:49:32.416004: step 94900, loss = 0.1025, acc = 0.9660 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 17:49:41.479236: step 94910, loss = 0.1030, acc = 0.9620 (293.7 examples/sec; 0.218 sec/batch)
2017-05-03 17:49:50.425457: step 94920, loss = 0.1086, acc = 0.9740 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 17:49:59.486917: step 94930, loss = 0.0970, acc = 0.9640 (264.1 examples/sec; 0.242 sec/batch)
2017-05-03 17:50:08.668737: step 94940, loss = 0.0976, acc = 0.9740 (268.7 examples/sec; 0.238 sec/batch)
2017-05-03 17:50:17.678252: step 94950, loss = 0.1183, acc = 0.9600 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 17:50:26.705883: step 94960, loss = 0.1115, acc = 0.9780 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 17:50:35.718900: step 94970, loss = 0.1319, acc = 0.9560 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 17:50:44.800429: step 94980, loss = 0.1017, acc = 0.9660 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 17:50:53.917837: step 94990, loss = 0.1336, acc = 0.9480 (267.0 examples/sec; 0.240 sec/batch)
2017-05-03 17:51:03.144574: step 95000, loss = 0.1025, acc = 0.9760 (270.7 examples/sec; 0.236 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 17:51:03.620201: step 95000, loss = 0.1036, acc = 0.9615, f1neg = 0.9578, f1pos = 0.9646, f1 = 0.9612
[Eval_batch(1)(2000,4000)] 2017-05-03 17:51:04.097964: step 95000, loss = 0.1078, acc = 0.9685, f1neg = 0.9633, f1pos = 0.9724, f1 = 0.9679
[Eval_batch(2)(2000,6000)] 2017-05-03 17:51:04.575837: step 95000, loss = 0.1186, acc = 0.9610, f1neg = 0.9586, f1pos = 0.9631, f1 = 0.9609
[Eval_batch(3)(2000,8000)] 2017-05-03 17:51:05.078817: step 95000, loss = 0.1220, acc = 0.9600, f1neg = 0.9600, f1pos = 0.9600, f1 = 0.9600
[Eval_batch(4)(2000,10000)] 2017-05-03 17:51:05.542112: step 95000, loss = 0.1239, acc = 0.9595, f1neg = 0.9607, f1pos = 0.9582, f1 = 0.9595
[Eval_batch(5)(2000,12000)] 2017-05-03 17:51:06.054867: step 95000, loss = 0.1230, acc = 0.9560, f1neg = 0.9528, f1pos = 0.9588, f1 = 0.9558
[Eval_batch(6)(2000,14000)] 2017-05-03 17:51:06.564269: step 95000, loss = 0.1140, acc = 0.9675, f1neg = 0.9669, f1pos = 0.9681, f1 = 0.9675
[Eval_batch(7)(2000,16000)] 2017-05-03 17:51:07.068238: step 95000, loss = 0.1143, acc = 0.9625, f1neg = 0.9553, f1pos = 0.9677, f1 = 0.9615
[Eval_batch(8)(2000,18000)] 2017-05-03 17:51:07.579874: step 95000, loss = 0.1056, acc = 0.9635, f1neg = 0.9586, f1pos = 0.9674, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 17:51:08.044879: step 95000, loss = 0.1116, acc = 0.9660, f1neg = 0.9635, f1pos = 0.9682, f1 = 0.9658
[Eval_batch(10)(2000,22000)] 2017-05-03 17:51:08.543147: step 95000, loss = 0.1168, acc = 0.9635, f1neg = 0.9611, f1pos = 0.9656, f1 = 0.9634
[Eval_batch(11)(2000,24000)] 2017-05-03 17:51:09.009289: step 95000, loss = 0.1190, acc = 0.9615, f1neg = 0.9556, f1pos = 0.9660, f1 = 0.9608
[Eval_batch(12)(2000,26000)] 2017-05-03 17:51:09.469338: step 95000, loss = 0.1147, acc = 0.9550, f1neg = 0.9537, f1pos = 0.9563, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 17:51:09.921651: step 95000, loss = 0.1067, acc = 0.9670, f1neg = 0.9587, f1pos = 0.9725, f1 = 0.9656
[Eval_batch(14)(2000,30000)] 2017-05-03 17:51:10.426403: step 95000, loss = 0.1342, acc = 0.9575, f1neg = 0.9496, f1pos = 0.9633, f1 = 0.9564
[Eval_batch(15)(2000,32000)] 2017-05-03 17:51:10.931370: step 95000, loss = 0.1081, acc = 0.9685, f1neg = 0.9660, f1pos = 0.9707, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 17:51:11.395907: step 95000, loss = 0.1023, acc = 0.9690, f1neg = 0.9678, f1pos = 0.9701, f1 = 0.9690
[Eval_batch(17)(2000,36000)] 2017-05-03 17:51:11.865110: step 95000, loss = 0.1341, acc = 0.9575, f1neg = 0.9579, f1pos = 0.9571, f1 = 0.9575
[Eval_batch(18)(2000,38000)] 2017-05-03 17:51:12.362044: step 95000, loss = 0.1252, acc = 0.9595, f1neg = 0.9613, f1pos = 0.9576, f1 = 0.9594
[Eval_batch(19)(2000,40000)] 2017-05-03 17:51:12.844106: step 95000, loss = 0.1091, acc = 0.9680, f1neg = 0.9631, f1pos = 0.9718, f1 = 0.9674
[Eval_batch(20)(2000,42000)] 2017-05-03 17:51:13.318313: step 95000, loss = 0.1121, acc = 0.9635, f1neg = 0.9677, f1pos = 0.9581, f1 = 0.9629
[Eval_batch(21)(2000,44000)] 2017-05-03 17:51:13.818276: step 95000, loss = 0.1020, acc = 0.9670, f1neg = 0.9641, f1pos = 0.9695, f1 = 0.9668
[Eval_batch(22)(2000,46000)] 2017-05-03 17:51:14.288902: step 95000, loss = 0.1130, acc = 0.9595, f1neg = 0.9599, f1pos = 0.9591, f1 = 0.9595
[Eval_batch(23)(2000,48000)] 2017-05-03 17:51:14.795285: step 95000, loss = 0.1176, acc = 0.9620, f1neg = 0.9573, f1pos = 0.9658, f1 = 0.9615
[Eval_batch(24)(2000,50000)] 2017-05-03 17:51:15.268460: step 95000, loss = 0.1006, acc = 0.9680, f1neg = 0.9634, f1pos = 0.9716, f1 = 0.9675
[Eval_batch(25)(2000,52000)] 2017-05-03 17:51:15.745829: step 95000, loss = 0.0929, acc = 0.9730, f1neg = 0.9705, f1pos = 0.9751, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 17:51:16.227129: step 95000, loss = 0.1084, acc = 0.9655, f1neg = 0.9674, f1pos = 0.9634, f1 = 0.9654
[Eval_batch(27)(2000,56000)] 2017-05-03 17:51:16.862757: step 95000, loss = 0.0950, acc = 0.9730, f1neg = 0.9717, f1pos = 0.9742, f1 = 0.9729
[Eval] 2017-05-03 17:51:16.862840: step 95000, acc = 0.9637, f1 = 0.9634
[Test_batch(0)(2000,2000)] 2017-05-03 17:51:17.371043: step 95000, loss = 0.1438, acc = 0.9520, f1neg = 0.9544, f1pos = 0.9493, f1 = 0.9519
[Test_batch(1)(2000,4000)] 2017-05-03 17:51:17.871014: step 95000, loss = 0.1216, acc = 0.9635, f1neg = 0.9676, f1pos = 0.9582, f1 = 0.9629
[Test_batch(2)(2000,6000)] 2017-05-03 17:51:18.347258: step 95000, loss = 0.1400, acc = 0.9560, f1neg = 0.9596, f1pos = 0.9518, f1 = 0.9557
[Test_batch(3)(2000,8000)] 2017-05-03 17:51:18.822777: step 95000, loss = 0.1461, acc = 0.9520, f1neg = 0.9550, f1pos = 0.9486, f1 = 0.9518
[Test_batch(4)(2000,10000)] 2017-05-03 17:51:19.298116: step 95000, loss = 0.1394, acc = 0.9515, f1neg = 0.9518, f1pos = 0.9512, f1 = 0.9515
[Test_batch(5)(2000,12000)] 2017-05-03 17:51:19.755058: step 95000, loss = 0.1562, acc = 0.9445, f1neg = 0.9451, f1pos = 0.9439, f1 = 0.9445
[Test_batch(6)(2000,14000)] 2017-05-03 17:51:20.229242: step 95000, loss = 0.1411, acc = 0.9500, f1neg = 0.9482, f1pos = 0.9516, f1 = 0.9499
[Test_batch(7)(2000,16000)] 2017-05-03 17:51:20.693923: step 95000, loss = 0.1309, acc = 0.9540, f1neg = 0.9582, f1pos = 0.9488, f1 = 0.9535
[Test_batch(8)(2000,18000)] 2017-05-03 17:51:21.163212: step 95000, loss = 0.1317, acc = 0.9490, f1neg = 0.9483, f1pos = 0.9497, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 17:51:21.641084: step 95000, loss = 0.1598, acc = 0.9365, f1neg = 0.9331, f1pos = 0.9396, f1 = 0.9363
[Test_batch(10)(2000,22000)] 2017-05-03 17:51:22.123622: step 95000, loss = 0.1396, acc = 0.9530, f1neg = 0.9471, f1pos = 0.9577, f1 = 0.9524
[Test_batch(11)(2000,24000)] 2017-05-03 17:51:22.603532: step 95000, loss = 0.1430, acc = 0.9535, f1neg = 0.9549, f1pos = 0.9520, f1 = 0.9535
[Test_batch(12)(2000,26000)] 2017-05-03 17:51:23.084741: step 95000, loss = 0.1155, acc = 0.9640, f1neg = 0.9654, f1pos = 0.9625, f1 = 0.9639
[Test_batch(13)(2000,28000)] 2017-05-03 17:51:23.558588: step 95000, loss = 0.1380, acc = 0.9540, f1neg = 0.9491, f1pos = 0.9581, f1 = 0.9536
[Test_batch(14)(2000,30000)] 2017-05-03 17:51:24.045907: step 95000, loss = 0.1157, acc = 0.9590, f1neg = 0.9528, f1pos = 0.9637, f1 = 0.9583
[Test_batch(15)(2000,32000)] 2017-05-03 17:51:24.519052: step 95000, loss = 0.1402, acc = 0.9575, f1neg = 0.9570, f1pos = 0.9580, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 17:51:24.996956: step 95000, loss = 0.1208, acc = 0.9585, f1neg = 0.9575, f1pos = 0.9595, f1 = 0.9585
[Test_batch(17)(2000,36000)] 2017-05-03 17:51:25.472771: step 95000, loss = 0.1117, acc = 0.9705, f1neg = 0.9677, f1pos = 0.9729, f1 = 0.9703
[Test_batch(18)(2000,38000)] 2017-05-03 17:51:26.005230: step 95000, loss = 0.1065, acc = 0.9660, f1neg = 0.9653, f1pos = 0.9666, f1 = 0.9660
[Test] 2017-05-03 17:51:26.005312: step 95000, acc = 0.9550, f1 = 0.9548
[Status] 2017-05-03 17:51:26.005334: step 95000, maxindex = 95000, maxdev = 0.9637, maxtst = 0.9550
2017-05-03 17:51:38.204388: step 95010, loss = 0.1056, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 17:51:47.506063: step 95020, loss = 0.1271, acc = 0.9520 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 17:51:56.556134: step 95030, loss = 0.1147, acc = 0.9600 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 17:52:05.631237: step 95040, loss = 0.1108, acc = 0.9600 (292.6 examples/sec; 0.219 sec/batch)
2017-05-03 17:52:15.142527: step 95050, loss = 0.0946, acc = 0.9780 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 17:52:24.040568: step 95060, loss = 0.1137, acc = 0.9720 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 17:52:32.975448: step 95070, loss = 0.1019, acc = 0.9660 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 17:52:42.149641: step 95080, loss = 0.1079, acc = 0.9720 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 17:52:51.198882: step 95090, loss = 0.1186, acc = 0.9620 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 17:53:00.134359: step 95100, loss = 0.0998, acc = 0.9720 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 17:53:09.161402: step 95110, loss = 0.1225, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 17:53:18.268885: step 95120, loss = 0.1076, acc = 0.9620 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 17:53:27.321950: step 95130, loss = 0.1048, acc = 0.9680 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 17:53:36.443248: step 95140, loss = 0.0913, acc = 0.9760 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 17:53:45.289126: step 95150, loss = 0.1210, acc = 0.9720 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 17:53:54.333375: step 95160, loss = 0.1096, acc = 0.9640 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 17:54:03.362856: step 95170, loss = 0.1171, acc = 0.9700 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 17:54:12.302008: step 95180, loss = 0.1018, acc = 0.9740 (292.8 examples/sec; 0.219 sec/batch)
2017-05-03 17:54:21.381833: step 95190, loss = 0.0985, acc = 0.9720 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 17:54:30.398384: step 95200, loss = 0.1057, acc = 0.9680 (244.0 examples/sec; 0.262 sec/batch)
2017-05-03 17:54:39.464321: step 95210, loss = 0.0834, acc = 0.9820 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 17:54:48.596534: step 95220, loss = 0.1304, acc = 0.9580 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 17:54:57.528917: step 95230, loss = 0.1162, acc = 0.9600 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 17:55:06.614077: step 95240, loss = 0.1108, acc = 0.9680 (264.6 examples/sec; 0.242 sec/batch)
2017-05-03 17:55:15.605396: step 95250, loss = 0.1191, acc = 0.9740 (290.9 examples/sec; 0.220 sec/batch)
2017-05-03 17:55:24.564977: step 95260, loss = 0.1119, acc = 0.9720 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 17:55:33.674719: step 95270, loss = 0.1323, acc = 0.9660 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 17:55:42.674671: step 95280, loss = 0.1195, acc = 0.9700 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 17:55:51.619019: step 95290, loss = 0.1192, acc = 0.9660 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 17:56:00.739387: step 95300, loss = 0.0865, acc = 0.9740 (295.4 examples/sec; 0.217 sec/batch)
2017-05-03 17:56:09.820859: step 95310, loss = 0.0998, acc = 0.9680 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 17:56:18.745901: step 95320, loss = 0.1201, acc = 0.9600 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 17:56:27.707446: step 95330, loss = 0.1160, acc = 0.9640 (286.4 examples/sec; 0.224 sec/batch)
2017-05-03 17:56:36.814493: step 95340, loss = 0.1287, acc = 0.9620 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 17:56:45.904474: step 95350, loss = 0.1407, acc = 0.9520 (268.0 examples/sec; 0.239 sec/batch)
2017-05-03 17:56:54.904059: step 95360, loss = 0.1346, acc = 0.9540 (273.3 examples/sec; 0.234 sec/batch)
2017-05-03 17:57:03.834503: step 95370, loss = 0.0887, acc = 0.9740 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 17:57:12.771071: step 95380, loss = 0.0979, acc = 0.9660 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 17:57:21.737058: step 95390, loss = 0.1382, acc = 0.9520 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 17:57:30.752140: step 95400, loss = 0.1196, acc = 0.9660 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 17:57:39.897635: step 95410, loss = 0.1197, acc = 0.9660 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 17:57:48.968718: step 95420, loss = 0.1317, acc = 0.9560 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:57:58.050627: step 95430, loss = 0.1289, acc = 0.9440 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 17:58:07.082656: step 95440, loss = 0.1347, acc = 0.9500 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 17:58:16.235251: step 95450, loss = 0.0967, acc = 0.9680 (260.0 examples/sec; 0.246 sec/batch)
2017-05-03 17:58:25.201251: step 95460, loss = 0.0965, acc = 0.9680 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 17:58:34.305526: step 95470, loss = 0.1081, acc = 0.9700 (274.1 examples/sec; 0.233 sec/batch)
2017-05-03 17:58:43.398389: step 95480, loss = 0.1119, acc = 0.9720 (265.6 examples/sec; 0.241 sec/batch)
2017-05-03 17:58:52.625113: step 95490, loss = 0.1315, acc = 0.9480 (240.6 examples/sec; 0.266 sec/batch)
2017-05-03 17:59:01.751289: step 95500, loss = 0.1131, acc = 0.9600 (267.9 examples/sec; 0.239 sec/batch)
2017-05-03 17:59:10.812343: step 95510, loss = 0.1352, acc = 0.9540 (296.0 examples/sec; 0.216 sec/batch)
2017-05-03 17:59:19.773609: step 95520, loss = 0.1031, acc = 0.9600 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 17:59:28.895375: step 95530, loss = 0.1347, acc = 0.9540 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 17:59:37.874679: step 95540, loss = 0.1201, acc = 0.9620 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 17:59:46.926930: step 95550, loss = 0.1115, acc = 0.9660 (286.8 examples/sec; 0.223 sec/batch)
2017-05-03 17:59:56.066464: step 95560, loss = 0.1066, acc = 0.9760 (279.3 examples/sec; 0.229 sec/batch)
2017-05-03 18:00:04.938874: step 95570, loss = 0.1032, acc = 0.9600 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 18:00:14.278196: step 95580, loss = 0.1350, acc = 0.9600 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 18:00:23.271910: step 95590, loss = 0.1137, acc = 0.9600 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 18:00:32.298232: step 95600, loss = 0.1196, acc = 0.9560 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 18:00:41.189128: step 95610, loss = 0.1460, acc = 0.9480 (288.0 examples/sec; 0.222 sec/batch)
2017-05-03 18:00:50.308124: step 95620, loss = 0.1233, acc = 0.9640 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 18:00:59.368748: step 95630, loss = 0.1054, acc = 0.9620 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 18:01:08.307203: step 95640, loss = 0.1287, acc = 0.9560 (284.1 examples/sec; 0.225 sec/batch)
2017-05-03 18:01:17.262848: step 95650, loss = 0.1184, acc = 0.9700 (291.6 examples/sec; 0.220 sec/batch)
2017-05-03 18:01:26.324487: step 95660, loss = 0.0789, acc = 0.9860 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 18:01:35.394578: step 95670, loss = 0.1398, acc = 0.9520 (297.9 examples/sec; 0.215 sec/batch)
2017-05-03 18:01:44.407848: step 95680, loss = 0.1089, acc = 0.9720 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 18:01:53.273179: step 95690, loss = 0.1174, acc = 0.9620 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 18:02:02.364906: step 95700, loss = 0.1137, acc = 0.9700 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 18:02:11.572315: step 95710, loss = 0.0984, acc = 0.9680 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 18:02:20.699193: step 95720, loss = 0.1112, acc = 0.9660 (279.8 examples/sec; 0.229 sec/batch)
2017-05-03 18:02:29.792388: step 95730, loss = 0.0983, acc = 0.9740 (268.1 examples/sec; 0.239 sec/batch)
2017-05-03 18:02:39.008065: step 95740, loss = 0.1339, acc = 0.9480 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 18:02:47.961513: step 95750, loss = 0.1028, acc = 0.9800 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 18:02:57.898017: step 95760, loss = 0.1033, acc = 0.9660 (283.3 examples/sec; 0.226 sec/batch)
2017-05-03 18:03:06.952817: step 95770, loss = 0.1329, acc = 0.9520 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 18:03:16.113600: step 95780, loss = 0.1151, acc = 0.9660 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 18:03:25.148899: step 95790, loss = 0.1123, acc = 0.9640 (278.6 examples/sec; 0.230 sec/batch)
2017-05-03 18:03:34.020242: step 95800, loss = 0.1125, acc = 0.9640 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 18:03:43.072033: step 95810, loss = 0.0980, acc = 0.9700 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 18:03:52.215812: step 95820, loss = 0.1106, acc = 0.9600 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 18:04:01.335406: step 95830, loss = 0.1008, acc = 0.9760 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 18:04:10.359486: step 95840, loss = 0.1499, acc = 0.9560 (293.4 examples/sec; 0.218 sec/batch)
2017-05-03 18:04:19.383236: step 95850, loss = 0.1506, acc = 0.9400 (281.7 examples/sec; 0.227 sec/batch)
2017-05-03 18:04:28.669389: step 95860, loss = 0.0882, acc = 0.9820 (268.3 examples/sec; 0.239 sec/batch)
2017-05-03 18:04:37.598962: step 95870, loss = 0.1260, acc = 0.9560 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 18:04:46.572401: step 95880, loss = 0.1184, acc = 0.9640 (294.4 examples/sec; 0.217 sec/batch)
2017-05-03 18:04:55.957079: step 95890, loss = 0.1309, acc = 0.9560 (220.2 examples/sec; 0.291 sec/batch)
2017-05-03 18:05:04.835715: step 95900, loss = 0.1082, acc = 0.9740 (298.1 examples/sec; 0.215 sec/batch)
2017-05-03 18:05:14.096135: step 95910, loss = 0.0832, acc = 0.9860 (228.0 examples/sec; 0.281 sec/batch)
2017-05-03 18:05:23.208971: step 95920, loss = 0.0951, acc = 0.9720 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 18:05:32.228435: step 95930, loss = 0.1067, acc = 0.9660 (297.9 examples/sec; 0.215 sec/batch)
2017-05-03 18:05:41.413050: step 95940, loss = 0.1053, acc = 0.9660 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 18:05:50.465939: step 95950, loss = 0.1151, acc = 0.9580 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 18:05:59.391164: step 95960, loss = 0.1206, acc = 0.9620 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 18:06:08.565077: step 95970, loss = 0.1190, acc = 0.9660 (287.2 examples/sec; 0.223 sec/batch)
2017-05-03 18:06:17.546682: step 95980, loss = 0.1288, acc = 0.9640 (294.8 examples/sec; 0.217 sec/batch)
2017-05-03 18:06:26.568778: step 95990, loss = 0.1046, acc = 0.9740 (272.5 examples/sec; 0.235 sec/batch)
2017-05-03 18:06:35.585872: step 96000, loss = 0.1051, acc = 0.9680 (291.9 examples/sec; 0.219 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 18:06:36.037149: step 96000, loss = 0.1047, acc = 0.9605, f1neg = 0.9566, f1pos = 0.9638, f1 = 0.9602
[Eval_batch(1)(2000,4000)] 2017-05-03 18:06:36.484626: step 96000, loss = 0.1075, acc = 0.9670, f1neg = 0.9614, f1pos = 0.9712, f1 = 0.9663
[Eval_batch(2)(2000,6000)] 2017-05-03 18:06:36.962808: step 96000, loss = 0.1198, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9599
[Eval_batch(3)(2000,8000)] 2017-05-03 18:06:37.436924: step 96000, loss = 0.1227, acc = 0.9615, f1neg = 0.9614, f1pos = 0.9616, f1 = 0.9615
[Eval_batch(4)(2000,10000)] 2017-05-03 18:06:37.882537: step 96000, loss = 0.1250, acc = 0.9585, f1neg = 0.9595, f1pos = 0.9574, f1 = 0.9585
[Eval_batch(5)(2000,12000)] 2017-05-03 18:06:38.364180: step 96000, loss = 0.1231, acc = 0.9560, f1neg = 0.9526, f1pos = 0.9590, f1 = 0.9558
[Eval_batch(6)(2000,14000)] 2017-05-03 18:06:38.830213: step 96000, loss = 0.1151, acc = 0.9655, f1neg = 0.9648, f1pos = 0.9662, f1 = 0.9655
[Eval_batch(7)(2000,16000)] 2017-05-03 18:06:39.309027: step 96000, loss = 0.1138, acc = 0.9640, f1neg = 0.9569, f1pos = 0.9691, f1 = 0.9630
[Eval_batch(8)(2000,18000)] 2017-05-03 18:06:39.775542: step 96000, loss = 0.1053, acc = 0.9625, f1neg = 0.9572, f1pos = 0.9666, f1 = 0.9619
[Eval_batch(9)(2000,20000)] 2017-05-03 18:06:40.244120: step 96000, loss = 0.1117, acc = 0.9655, f1neg = 0.9629, f1pos = 0.9677, f1 = 0.9653
[Eval_batch(10)(2000,22000)] 2017-05-03 18:06:40.715284: step 96000, loss = 0.1173, acc = 0.9595, f1neg = 0.9567, f1pos = 0.9620, f1 = 0.9593
[Eval_batch(11)(2000,24000)] 2017-05-03 18:06:41.175508: step 96000, loss = 0.1195, acc = 0.9630, f1neg = 0.9574, f1pos = 0.9673, f1 = 0.9623
[Eval_batch(12)(2000,26000)] 2017-05-03 18:06:41.640670: step 96000, loss = 0.1168, acc = 0.9555, f1neg = 0.9541, f1pos = 0.9568, f1 = 0.9555
[Eval_batch(13)(2000,28000)] 2017-05-03 18:06:42.083100: step 96000, loss = 0.1069, acc = 0.9675, f1neg = 0.9592, f1pos = 0.9730, f1 = 0.9661
[Eval_batch(14)(2000,30000)] 2017-05-03 18:06:42.543471: step 96000, loss = 0.1343, acc = 0.9565, f1neg = 0.9482, f1pos = 0.9625, f1 = 0.9554
[Eval_batch(15)(2000,32000)] 2017-05-03 18:06:43.015044: step 96000, loss = 0.1078, acc = 0.9690, f1neg = 0.9665, f1pos = 0.9712, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 18:06:43.458843: step 96000, loss = 0.1023, acc = 0.9690, f1neg = 0.9676, f1pos = 0.9702, f1 = 0.9689
[Eval_batch(17)(2000,36000)] 2017-05-03 18:06:43.937387: step 96000, loss = 0.1332, acc = 0.9590, f1neg = 0.9593, f1pos = 0.9587, f1 = 0.9590
[Eval_batch(18)(2000,38000)] 2017-05-03 18:06:44.413414: step 96000, loss = 0.1260, acc = 0.9595, f1neg = 0.9611, f1pos = 0.9577, f1 = 0.9594
[Eval_batch(19)(2000,40000)] 2017-05-03 18:06:44.879332: step 96000, loss = 0.1099, acc = 0.9690, f1neg = 0.9642, f1pos = 0.9726, f1 = 0.9684
[Eval_batch(20)(2000,42000)] 2017-05-03 18:06:45.356118: step 96000, loss = 0.1130, acc = 0.9630, f1neg = 0.9671, f1pos = 0.9577, f1 = 0.9624
[Eval_batch(21)(2000,44000)] 2017-05-03 18:06:45.821070: step 96000, loss = 0.1018, acc = 0.9670, f1neg = 0.9640, f1pos = 0.9696, f1 = 0.9668
[Eval_batch(22)(2000,46000)] 2017-05-03 18:06:46.263789: step 96000, loss = 0.1137, acc = 0.9610, f1neg = 0.9613, f1pos = 0.9607, f1 = 0.9610
[Eval_batch(23)(2000,48000)] 2017-05-03 18:06:46.742097: step 96000, loss = 0.1167, acc = 0.9605, f1neg = 0.9552, f1pos = 0.9647, f1 = 0.9599
[Eval_batch(24)(2000,50000)] 2017-05-03 18:06:47.224068: step 96000, loss = 0.1000, acc = 0.9680, f1neg = 0.9633, f1pos = 0.9717, f1 = 0.9675
[Eval_batch(25)(2000,52000)] 2017-05-03 18:06:47.684342: step 96000, loss = 0.0937, acc = 0.9725, f1neg = 0.9698, f1pos = 0.9747, f1 = 0.9723
[Eval_batch(26)(2000,54000)] 2017-05-03 18:06:48.147377: step 96000, loss = 0.1098, acc = 0.9635, f1neg = 0.9655, f1pos = 0.9613, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 18:06:48.746722: step 96000, loss = 0.0951, acc = 0.9695, f1neg = 0.9679, f1pos = 0.9710, f1 = 0.9694
[Eval] 2017-05-03 18:06:48.746812: step 96000, acc = 0.9633, f1 = 0.9630
[Test_batch(0)(2000,2000)] 2017-05-03 18:06:49.259194: step 96000, loss = 0.1441, acc = 0.9525, f1neg = 0.9548, f1pos = 0.9500, f1 = 0.9524
[Test_batch(1)(2000,4000)] 2017-05-03 18:06:49.732921: step 96000, loss = 0.1226, acc = 0.9650, f1neg = 0.9689, f1pos = 0.9600, f1 = 0.9644
[Test_batch(2)(2000,6000)] 2017-05-03 18:06:50.201098: step 96000, loss = 0.1419, acc = 0.9555, f1neg = 0.9590, f1pos = 0.9513, f1 = 0.9552
[Test_batch(3)(2000,8000)] 2017-05-03 18:06:50.675921: step 96000, loss = 0.1466, acc = 0.9540, f1neg = 0.9566, f1pos = 0.9510, f1 = 0.9538
[Test_batch(4)(2000,10000)] 2017-05-03 18:06:51.161751: step 96000, loss = 0.1394, acc = 0.9510, f1neg = 0.9511, f1pos = 0.9509, f1 = 0.9510
[Test_batch(5)(2000,12000)] 2017-05-03 18:06:51.642405: step 96000, loss = 0.1562, acc = 0.9425, f1neg = 0.9430, f1pos = 0.9420, f1 = 0.9425
[Test_batch(6)(2000,14000)] 2017-05-03 18:06:52.107679: step 96000, loss = 0.1413, acc = 0.9490, f1neg = 0.9470, f1pos = 0.9508, f1 = 0.9489
[Test_batch(7)(2000,16000)] 2017-05-03 18:06:52.577027: step 96000, loss = 0.1317, acc = 0.9550, f1neg = 0.9590, f1pos = 0.9502, f1 = 0.9546
[Test_batch(8)(2000,18000)] 2017-05-03 18:06:53.051829: step 96000, loss = 0.1312, acc = 0.9490, f1neg = 0.9482, f1pos = 0.9498, f1 = 0.9490
[Test_batch(9)(2000,20000)] 2017-05-03 18:06:53.520921: step 96000, loss = 0.1595, acc = 0.9385, f1neg = 0.9350, f1pos = 0.9417, f1 = 0.9383
[Test_batch(10)(2000,22000)] 2017-05-03 18:06:54.019679: step 96000, loss = 0.1387, acc = 0.9520, f1neg = 0.9458, f1pos = 0.9570, f1 = 0.9514
[Test_batch(11)(2000,24000)] 2017-05-03 18:06:54.460177: step 96000, loss = 0.1427, acc = 0.9515, f1neg = 0.9527, f1pos = 0.9502, f1 = 0.9515
[Test_batch(12)(2000,26000)] 2017-05-03 18:06:54.956172: step 96000, loss = 0.1144, acc = 0.9660, f1neg = 0.9672, f1pos = 0.9647, f1 = 0.9660
[Test_batch(13)(2000,28000)] 2017-05-03 18:06:55.440689: step 96000, loss = 0.1385, acc = 0.9530, f1neg = 0.9477, f1pos = 0.9574, f1 = 0.9525
[Test_batch(14)(2000,30000)] 2017-05-03 18:06:55.883812: step 96000, loss = 0.1156, acc = 0.9610, f1neg = 0.9551, f1pos = 0.9655, f1 = 0.9603
[Test_batch(15)(2000,32000)] 2017-05-03 18:06:56.347647: step 96000, loss = 0.1399, acc = 0.9560, f1neg = 0.9553, f1pos = 0.9567, f1 = 0.9560
[Test_batch(16)(2000,34000)] 2017-05-03 18:06:56.825046: step 96000, loss = 0.1207, acc = 0.9590, f1neg = 0.9579, f1pos = 0.9600, f1 = 0.9590
[Test_batch(17)(2000,36000)] 2017-05-03 18:06:57.328870: step 96000, loss = 0.1126, acc = 0.9680, f1neg = 0.9649, f1pos = 0.9706, f1 = 0.9678
[Test_batch(18)(2000,38000)] 2017-05-03 18:06:57.838031: step 96000, loss = 0.1096, acc = 0.9625, f1neg = 0.9616, f1pos = 0.9633, f1 = 0.9625
[Test] 2017-05-03 18:06:57.838123: step 96000, acc = 0.9548, f1 = 0.9546
[Status] 2017-05-03 18:06:57.838150: step 96000, maxindex = 95000, maxdev = 0.9637, maxtst = 0.9550
2017-05-03 18:07:06.775103: step 96010, loss = 0.1082, acc = 0.9660 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 18:07:15.702809: step 96020, loss = 0.1236, acc = 0.9640 (296.1 examples/sec; 0.216 sec/batch)
2017-05-03 18:07:24.688726: step 96030, loss = 0.0980, acc = 0.9760 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 18:07:33.671561: step 96040, loss = 0.1186, acc = 0.9580 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 18:07:42.805397: step 96050, loss = 0.0938, acc = 0.9720 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 18:07:51.715836: step 96060, loss = 0.1026, acc = 0.9620 (296.6 examples/sec; 0.216 sec/batch)
2017-05-03 18:08:00.765743: step 96070, loss = 0.1319, acc = 0.9640 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 18:08:09.910608: step 96080, loss = 0.1161, acc = 0.9720 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 18:08:18.991441: step 96090, loss = 0.1049, acc = 0.9660 (292.5 examples/sec; 0.219 sec/batch)
2017-05-03 18:08:28.125483: step 96100, loss = 0.1221, acc = 0.9660 (270.2 examples/sec; 0.237 sec/batch)
2017-05-03 18:08:37.222499: step 96110, loss = 0.1029, acc = 0.9720 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 18:08:46.269282: step 96120, loss = 0.1153, acc = 0.9660 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 18:08:55.408812: step 96130, loss = 0.1089, acc = 0.9640 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 18:09:04.539337: step 96140, loss = 0.1192, acc = 0.9620 (263.2 examples/sec; 0.243 sec/batch)
2017-05-03 18:09:13.554757: step 96150, loss = 0.1246, acc = 0.9640 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 18:09:23.362186: step 96160, loss = 0.1246, acc = 0.9540 (280.0 examples/sec; 0.229 sec/batch)
2017-05-03 18:09:32.355399: step 96170, loss = 0.1394, acc = 0.9540 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 18:09:41.427853: step 96180, loss = 0.1026, acc = 0.9700 (267.7 examples/sec; 0.239 sec/batch)
2017-05-03 18:09:50.452049: step 96190, loss = 0.1122, acc = 0.9680 (277.4 examples/sec; 0.231 sec/batch)
2017-05-03 18:09:59.362564: step 96200, loss = 0.1402, acc = 0.9520 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 18:10:08.401220: step 96210, loss = 0.0913, acc = 0.9740 (283.9 examples/sec; 0.225 sec/batch)
2017-05-03 18:10:17.436218: step 96220, loss = 0.0899, acc = 0.9800 (271.9 examples/sec; 0.235 sec/batch)
2017-05-03 18:10:26.389002: step 96230, loss = 0.1084, acc = 0.9660 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 18:10:35.755590: step 96240, loss = 0.1098, acc = 0.9680 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 18:10:44.827037: step 96250, loss = 0.1171, acc = 0.9560 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 18:10:53.905540: step 96260, loss = 0.1034, acc = 0.9760 (269.5 examples/sec; 0.237 sec/batch)
2017-05-03 18:11:03.015374: step 96270, loss = 0.1130, acc = 0.9620 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 18:11:12.054902: step 96280, loss = 0.1068, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 18:11:21.210575: step 96290, loss = 0.1212, acc = 0.9680 (242.3 examples/sec; 0.264 sec/batch)
2017-05-03 18:11:30.260404: step 96300, loss = 0.1025, acc = 0.9700 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 18:11:39.276284: step 96310, loss = 0.1277, acc = 0.9600 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 18:11:48.257293: step 96320, loss = 0.1170, acc = 0.9540 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 18:11:57.088109: step 96330, loss = 0.1267, acc = 0.9580 (295.9 examples/sec; 0.216 sec/batch)
2017-05-03 18:12:06.031852: step 96340, loss = 0.0976, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 18:12:15.116783: step 96350, loss = 0.0995, acc = 0.9720 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 18:12:24.085531: step 96360, loss = 0.1290, acc = 0.9640 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 18:12:33.220842: step 96370, loss = 0.1035, acc = 0.9660 (277.7 examples/sec; 0.230 sec/batch)
2017-05-03 18:12:42.254025: step 96380, loss = 0.0972, acc = 0.9700 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 18:12:51.158225: step 96390, loss = 0.1198, acc = 0.9620 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 18:13:00.139765: step 96400, loss = 0.1138, acc = 0.9660 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 18:13:09.108951: step 96410, loss = 0.1120, acc = 0.9660 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 18:13:17.986440: step 96420, loss = 0.1075, acc = 0.9680 (274.6 examples/sec; 0.233 sec/batch)
2017-05-03 18:13:27.005977: step 96430, loss = 0.1141, acc = 0.9740 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 18:13:36.026285: step 96440, loss = 0.0997, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 18:13:45.108080: step 96450, loss = 0.1528, acc = 0.9440 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 18:13:54.237915: step 96460, loss = 0.1168, acc = 0.9600 (288.1 examples/sec; 0.222 sec/batch)
2017-05-03 18:14:03.206134: step 96470, loss = 0.1413, acc = 0.9520 (274.8 examples/sec; 0.233 sec/batch)
2017-05-03 18:14:12.251695: step 96480, loss = 0.1067, acc = 0.9660 (293.8 examples/sec; 0.218 sec/batch)
2017-05-03 18:14:21.252000: step 96490, loss = 0.1366, acc = 0.9560 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 18:14:30.217240: step 96500, loss = 0.1240, acc = 0.9540 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 18:14:39.273110: step 96510, loss = 0.1323, acc = 0.9500 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 18:14:48.376115: step 96520, loss = 0.1007, acc = 0.9680 (263.9 examples/sec; 0.243 sec/batch)
2017-05-03 18:14:57.484203: step 96530, loss = 0.1325, acc = 0.9560 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 18:15:06.522213: step 96540, loss = 0.1270, acc = 0.9620 (271.6 examples/sec; 0.236 sec/batch)
2017-05-03 18:15:15.622488: step 96550, loss = 0.1151, acc = 0.9620 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 18:15:24.754300: step 96560, loss = 0.1205, acc = 0.9720 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 18:15:33.721770: step 96570, loss = 0.1312, acc = 0.9600 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 18:15:42.774876: step 96580, loss = 0.0892, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 18:15:51.631904: step 96590, loss = 0.1197, acc = 0.9620 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 18:16:00.482952: step 96600, loss = 0.1018, acc = 0.9660 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 18:16:09.306853: step 96610, loss = 0.1058, acc = 0.9820 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 18:16:18.185471: step 96620, loss = 0.1277, acc = 0.9660 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 18:16:27.026210: step 96630, loss = 0.1286, acc = 0.9540 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 18:16:35.935859: step 96640, loss = 0.1250, acc = 0.9680 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:16:44.777739: step 96650, loss = 0.1090, acc = 0.9600 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:16:53.703117: step 96660, loss = 0.1070, acc = 0.9680 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 18:17:02.763748: step 96670, loss = 0.1215, acc = 0.9580 (293.9 examples/sec; 0.218 sec/batch)
2017-05-03 18:17:11.818872: step 96680, loss = 0.0928, acc = 0.9760 (290.8 examples/sec; 0.220 sec/batch)
2017-05-03 18:17:20.749479: step 96690, loss = 0.1233, acc = 0.9660 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:17:29.644871: step 96700, loss = 0.0733, acc = 0.9840 (293.5 examples/sec; 0.218 sec/batch)
2017-05-03 18:17:38.663428: step 96710, loss = 0.1400, acc = 0.9600 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 18:17:47.597238: step 96720, loss = 0.1177, acc = 0.9660 (285.5 examples/sec; 0.224 sec/batch)
2017-05-03 18:17:56.694782: step 96730, loss = 0.1284, acc = 0.9580 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 18:18:05.698472: step 96740, loss = 0.1188, acc = 0.9680 (291.0 examples/sec; 0.220 sec/batch)
2017-05-03 18:18:14.811347: step 96750, loss = 0.1217, acc = 0.9600 (269.5 examples/sec; 0.238 sec/batch)
2017-05-03 18:18:23.820049: step 96760, loss = 0.1222, acc = 0.9580 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 18:18:33.675583: step 96770, loss = 0.0969, acc = 0.9680 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 18:18:42.706936: step 96780, loss = 0.1342, acc = 0.9560 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 18:18:51.707167: step 96790, loss = 0.1126, acc = 0.9720 (298.2 examples/sec; 0.215 sec/batch)
2017-05-03 18:19:00.801173: step 96800, loss = 0.0826, acc = 0.9780 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 18:19:09.815089: step 96810, loss = 0.1091, acc = 0.9720 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 18:19:18.952352: step 96820, loss = 0.1170, acc = 0.9700 (265.4 examples/sec; 0.241 sec/batch)
2017-05-03 18:19:27.836008: step 96830, loss = 0.0908, acc = 0.9760 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 18:19:36.913477: step 96840, loss = 0.0897, acc = 0.9720 (276.1 examples/sec; 0.232 sec/batch)
2017-05-03 18:19:45.820330: step 96850, loss = 0.1147, acc = 0.9720 (292.7 examples/sec; 0.219 sec/batch)
2017-05-03 18:19:54.823961: step 96860, loss = 0.1146, acc = 0.9640 (280.5 examples/sec; 0.228 sec/batch)
2017-05-03 18:20:04.057390: step 96870, loss = 0.1173, acc = 0.9660 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 18:20:12.890859: step 96880, loss = 0.1494, acc = 0.9480 (296.9 examples/sec; 0.216 sec/batch)
2017-05-03 18:20:22.034534: step 96890, loss = 0.1016, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 18:20:31.070528: step 96900, loss = 0.1509, acc = 0.9540 (256.3 examples/sec; 0.250 sec/batch)
2017-05-03 18:20:40.199282: step 96910, loss = 0.1106, acc = 0.9700 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 18:20:49.297702: step 96920, loss = 0.0801, acc = 0.9800 (280.1 examples/sec; 0.229 sec/batch)
2017-05-03 18:20:58.341179: step 96930, loss = 0.1306, acc = 0.9480 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 18:21:07.286686: step 96940, loss = 0.1133, acc = 0.9620 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 18:21:16.210523: step 96950, loss = 0.1109, acc = 0.9600 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 18:21:25.302686: step 96960, loss = 0.1154, acc = 0.9620 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 18:21:34.279968: step 96970, loss = 0.0920, acc = 0.9660 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 18:21:43.187484: step 96980, loss = 0.0959, acc = 0.9800 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 18:21:52.127669: step 96990, loss = 0.1073, acc = 0.9740 (286.0 examples/sec; 0.224 sec/batch)
2017-05-03 18:22:01.566425: step 97000, loss = 0.1176, acc = 0.9660 (286.0 examples/sec; 0.224 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 18:22:02.038985: step 97000, loss = 0.1025, acc = 0.9610, f1neg = 0.9576, f1pos = 0.9639, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 18:22:02.497729: step 97000, loss = 0.1108, acc = 0.9645, f1neg = 0.9590, f1pos = 0.9687, f1 = 0.9638
[Eval_batch(2)(2000,6000)] 2017-05-03 18:22:02.961705: step 97000, loss = 0.1197, acc = 0.9605, f1neg = 0.9584, f1pos = 0.9624, f1 = 0.9604
[Eval_batch(3)(2000,8000)] 2017-05-03 18:22:03.467565: step 97000, loss = 0.1205, acc = 0.9605, f1neg = 0.9608, f1pos = 0.9602, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 18:22:03.978720: step 97000, loss = 0.1235, acc = 0.9570, f1neg = 0.9586, f1pos = 0.9553, f1 = 0.9569
[Eval_batch(5)(2000,12000)] 2017-05-03 18:22:04.421379: step 97000, loss = 0.1230, acc = 0.9545, f1neg = 0.9517, f1pos = 0.9570, f1 = 0.9543
[Eval_batch(6)(2000,14000)] 2017-05-03 18:22:04.896674: step 97000, loss = 0.1131, acc = 0.9695, f1neg = 0.9691, f1pos = 0.9699, f1 = 0.9695
[Eval_batch(7)(2000,16000)] 2017-05-03 18:22:05.372711: step 97000, loss = 0.1149, acc = 0.9605, f1neg = 0.9532, f1pos = 0.9658, f1 = 0.9595
[Eval_batch(8)(2000,18000)] 2017-05-03 18:22:05.834172: step 97000, loss = 0.1062, acc = 0.9635, f1neg = 0.9588, f1pos = 0.9672, f1 = 0.9630
[Eval_batch(9)(2000,20000)] 2017-05-03 18:22:06.282709: step 97000, loss = 0.1118, acc = 0.9640, f1neg = 0.9617, f1pos = 0.9661, f1 = 0.9639
[Eval_batch(10)(2000,22000)] 2017-05-03 18:22:06.722081: step 97000, loss = 0.1170, acc = 0.9635, f1neg = 0.9615, f1pos = 0.9653, f1 = 0.9634
[Eval_batch(11)(2000,24000)] 2017-05-03 18:22:07.198140: step 97000, loss = 0.1206, acc = 0.9615, f1neg = 0.9559, f1pos = 0.9659, f1 = 0.9609
[Eval_batch(12)(2000,26000)] 2017-05-03 18:22:07.669298: step 97000, loss = 0.1156, acc = 0.9550, f1neg = 0.9540, f1pos = 0.9559, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 18:22:08.178384: step 97000, loss = 0.1076, acc = 0.9695, f1neg = 0.9621, f1pos = 0.9745, f1 = 0.9683
[Eval_batch(14)(2000,30000)] 2017-05-03 18:22:08.686475: step 97000, loss = 0.1350, acc = 0.9550, f1neg = 0.9469, f1pos = 0.9609, f1 = 0.9539
[Eval_batch(15)(2000,32000)] 2017-05-03 18:22:09.194180: step 97000, loss = 0.1100, acc = 0.9670, f1neg = 0.9645, f1pos = 0.9692, f1 = 0.9668
[Eval_batch(16)(2000,34000)] 2017-05-03 18:22:09.667958: step 97000, loss = 0.1030, acc = 0.9685, f1neg = 0.9676, f1pos = 0.9694, f1 = 0.9685
[Eval_batch(17)(2000,36000)] 2017-05-03 18:22:10.163046: step 97000, loss = 0.1340, acc = 0.9585, f1neg = 0.9591, f1pos = 0.9579, f1 = 0.9585
[Eval_batch(18)(2000,38000)] 2017-05-03 18:22:10.667932: step 97000, loss = 0.1236, acc = 0.9605, f1neg = 0.9624, f1pos = 0.9584, f1 = 0.9604
[Eval_batch(19)(2000,40000)] 2017-05-03 18:22:11.143522: step 97000, loss = 0.1092, acc = 0.9695, f1neg = 0.9650, f1pos = 0.9730, f1 = 0.9690
[Eval_batch(20)(2000,42000)] 2017-05-03 18:22:11.645665: step 97000, loss = 0.1094, acc = 0.9655, f1neg = 0.9695, f1pos = 0.9603, f1 = 0.9649
[Eval_batch(21)(2000,44000)] 2017-05-03 18:22:12.148361: step 97000, loss = 0.1061, acc = 0.9655, f1neg = 0.9627, f1pos = 0.9679, f1 = 0.9653
[Eval_batch(22)(2000,46000)] 2017-05-03 18:22:12.657960: step 97000, loss = 0.1140, acc = 0.9620, f1neg = 0.9626, f1pos = 0.9613, f1 = 0.9620
[Eval_batch(23)(2000,48000)] 2017-05-03 18:22:13.165329: step 97000, loss = 0.1197, acc = 0.9610, f1neg = 0.9564, f1pos = 0.9647, f1 = 0.9606
[Eval_batch(24)(2000,50000)] 2017-05-03 18:22:13.659049: step 97000, loss = 0.1026, acc = 0.9665, f1neg = 0.9620, f1pos = 0.9701, f1 = 0.9660
[Eval_batch(25)(2000,52000)] 2017-05-03 18:22:14.130462: step 97000, loss = 0.0942, acc = 0.9730, f1neg = 0.9706, f1pos = 0.9750, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 18:22:14.609427: step 97000, loss = 0.1076, acc = 0.9645, f1neg = 0.9667, f1pos = 0.9620, f1 = 0.9643
[Eval_batch(27)(2000,56000)] 2017-05-03 18:22:15.172757: step 97000, loss = 0.0930, acc = 0.9735, f1neg = 0.9724, f1pos = 0.9745, f1 = 0.9735
[Eval] 2017-05-03 18:22:15.172834: step 97000, acc = 0.9634, f1 = 0.9631
[Test_batch(0)(2000,2000)] 2017-05-03 18:22:15.644796: step 97000, loss = 0.1418, acc = 0.9535, f1neg = 0.9562, f1pos = 0.9504, f1 = 0.9533
[Test_batch(1)(2000,4000)] 2017-05-03 18:22:16.110674: step 97000, loss = 0.1203, acc = 0.9620, f1neg = 0.9665, f1pos = 0.9562, f1 = 0.9613
[Test_batch(2)(2000,6000)] 2017-05-03 18:22:16.568967: step 97000, loss = 0.1378, acc = 0.9565, f1neg = 0.9603, f1pos = 0.9519, f1 = 0.9561
[Test_batch(3)(2000,8000)] 2017-05-03 18:22:17.073354: step 97000, loss = 0.1447, acc = 0.9520, f1neg = 0.9554, f1pos = 0.9480, f1 = 0.9517
[Test_batch(4)(2000,10000)] 2017-05-03 18:22:17.575868: step 97000, loss = 0.1398, acc = 0.9535, f1neg = 0.9542, f1pos = 0.9528, f1 = 0.9535
[Test_batch(5)(2000,12000)] 2017-05-03 18:22:18.071244: step 97000, loss = 0.1560, acc = 0.9440, f1neg = 0.9453, f1pos = 0.9427, f1 = 0.9440
[Test_batch(6)(2000,14000)] 2017-05-03 18:22:18.569365: step 97000, loss = 0.1382, acc = 0.9500, f1neg = 0.9488, f1pos = 0.9512, f1 = 0.9500
[Test_batch(7)(2000,16000)] 2017-05-03 18:22:19.047769: step 97000, loss = 0.1306, acc = 0.9515, f1neg = 0.9563, f1pos = 0.9455, f1 = 0.9509
[Test_batch(8)(2000,18000)] 2017-05-03 18:22:19.554876: step 97000, loss = 0.1317, acc = 0.9515, f1neg = 0.9514, f1pos = 0.9516, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 18:22:20.048217: step 97000, loss = 0.1628, acc = 0.9400, f1neg = 0.9373, f1pos = 0.9425, f1 = 0.9399
[Test_batch(10)(2000,22000)] 2017-05-03 18:22:20.521450: step 97000, loss = 0.1428, acc = 0.9520, f1neg = 0.9464, f1pos = 0.9565, f1 = 0.9515
[Test_batch(11)(2000,24000)] 2017-05-03 18:22:20.993376: step 97000, loss = 0.1417, acc = 0.9535, f1neg = 0.9552, f1pos = 0.9516, f1 = 0.9534
[Test_batch(12)(2000,26000)] 2017-05-03 18:22:21.466659: step 97000, loss = 0.1164, acc = 0.9615, f1neg = 0.9633, f1pos = 0.9595, f1 = 0.9614
[Test_batch(13)(2000,28000)] 2017-05-03 18:22:21.945412: step 97000, loss = 0.1394, acc = 0.9565, f1neg = 0.9523, f1pos = 0.9600, f1 = 0.9562
[Test_batch(14)(2000,30000)] 2017-05-03 18:22:22.458175: step 97000, loss = 0.1187, acc = 0.9605, f1neg = 0.9551, f1pos = 0.9647, f1 = 0.9599
[Test_batch(15)(2000,32000)] 2017-05-03 18:22:22.935833: step 97000, loss = 0.1414, acc = 0.9575, f1neg = 0.9573, f1pos = 0.9577, f1 = 0.9575
[Test_batch(16)(2000,34000)] 2017-05-03 18:22:23.437488: step 97000, loss = 0.1218, acc = 0.9575, f1neg = 0.9568, f1pos = 0.9581, f1 = 0.9575
[Test_batch(17)(2000,36000)] 2017-05-03 18:22:23.941620: step 97000, loss = 0.1118, acc = 0.9650, f1neg = 0.9620, f1pos = 0.9676, f1 = 0.9648
[Test_batch(18)(2000,38000)] 2017-05-03 18:22:24.486153: step 97000, loss = 0.1062, acc = 0.9665, f1neg = 0.9659, f1pos = 0.9670, f1 = 0.9665
[Test] 2017-05-03 18:22:24.486217: step 97000, acc = 0.9550, f1 = 0.9548
[Status] 2017-05-03 18:22:24.486230: step 97000, maxindex = 95000, maxdev = 0.9637, maxtst = 0.9550
2017-05-03 18:22:33.619451: step 97010, loss = 0.1094, acc = 0.9600 (263.8 examples/sec; 0.243 sec/batch)
2017-05-03 18:22:42.605064: step 97020, loss = 0.1384, acc = 0.9580 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 18:22:51.545527: step 97030, loss = 0.1443, acc = 0.9560 (294.1 examples/sec; 0.218 sec/batch)
2017-05-03 18:23:00.523797: step 97040, loss = 0.1458, acc = 0.9500 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 18:23:09.502088: step 97050, loss = 0.1020, acc = 0.9620 (282.0 examples/sec; 0.227 sec/batch)
2017-05-03 18:23:18.594256: step 97060, loss = 0.1207, acc = 0.9620 (288.7 examples/sec; 0.222 sec/batch)
2017-05-03 18:23:27.527342: step 97070, loss = 0.1038, acc = 0.9720 (281.3 examples/sec; 0.228 sec/batch)
2017-05-03 18:23:36.496487: step 97080, loss = 0.1238, acc = 0.9540 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 18:23:45.828230: step 97090, loss = 0.0957, acc = 0.9740 (296.5 examples/sec; 0.216 sec/batch)
2017-05-03 18:23:54.894954: step 97100, loss = 0.0981, acc = 0.9720 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 18:24:03.972105: step 97110, loss = 0.1345, acc = 0.9660 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 18:24:13.102407: step 97120, loss = 0.0892, acc = 0.9800 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 18:24:22.142289: step 97130, loss = 0.1247, acc = 0.9580 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 18:24:31.317533: step 97140, loss = 0.0945, acc = 0.9760 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 18:24:40.312257: step 97150, loss = 0.1002, acc = 0.9640 (294.5 examples/sec; 0.217 sec/batch)
2017-05-03 18:24:49.169982: step 97160, loss = 0.1217, acc = 0.9660 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 18:24:58.103175: step 97170, loss = 0.1277, acc = 0.9700 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 18:25:07.092662: step 97180, loss = 0.1152, acc = 0.9680 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 18:25:16.321008: step 97190, loss = 0.1271, acc = 0.9600 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 18:25:25.336518: step 97200, loss = 0.1056, acc = 0.9700 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 18:25:34.400528: step 97210, loss = 0.1328, acc = 0.9540 (257.3 examples/sec; 0.249 sec/batch)
2017-05-03 18:25:43.540297: step 97220, loss = 0.1226, acc = 0.9560 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 18:25:52.986523: step 97230, loss = 0.1304, acc = 0.9620 (270.1 examples/sec; 0.237 sec/batch)
2017-05-03 18:26:01.962110: step 97240, loss = 0.0882, acc = 0.9720 (273.0 examples/sec; 0.234 sec/batch)
2017-05-03 18:26:11.027626: step 97250, loss = 0.0978, acc = 0.9740 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 18:26:20.147071: step 97260, loss = 0.1158, acc = 0.9620 (257.5 examples/sec; 0.249 sec/batch)
2017-05-03 18:26:29.182598: step 97270, loss = 0.1331, acc = 0.9660 (267.1 examples/sec; 0.240 sec/batch)
2017-05-03 18:26:38.322164: step 97280, loss = 0.0966, acc = 0.9720 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 18:26:47.346960: step 97290, loss = 0.1410, acc = 0.9640 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 18:26:56.307704: step 97300, loss = 0.1097, acc = 0.9740 (289.3 examples/sec; 0.221 sec/batch)
2017-05-03 18:27:05.233572: step 97310, loss = 0.0972, acc = 0.9740 (297.2 examples/sec; 0.215 sec/batch)
2017-05-03 18:27:14.257433: step 97320, loss = 0.1355, acc = 0.9500 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 18:27:23.521792: step 97330, loss = 0.1027, acc = 0.9720 (208.8 examples/sec; 0.307 sec/batch)
2017-05-03 18:27:32.494993: step 97340, loss = 0.1310, acc = 0.9520 (284.4 examples/sec; 0.225 sec/batch)
2017-05-03 18:27:41.458852: step 97350, loss = 0.1077, acc = 0.9600 (301.0 examples/sec; 0.213 sec/batch)
2017-05-03 18:27:50.472517: step 97360, loss = 0.0912, acc = 0.9780 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 18:27:59.457920: step 97370, loss = 0.1085, acc = 0.9680 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 18:28:08.490897: step 97380, loss = 0.0970, acc = 0.9720 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 18:28:17.474743: step 97390, loss = 0.1114, acc = 0.9720 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 18:28:26.437611: step 97400, loss = 0.1455, acc = 0.9600 (272.1 examples/sec; 0.235 sec/batch)
2017-05-03 18:28:35.444920: step 97410, loss = 0.1293, acc = 0.9620 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 18:28:44.403636: step 97420, loss = 0.1050, acc = 0.9640 (281.5 examples/sec; 0.227 sec/batch)
2017-05-03 18:28:53.746469: step 97430, loss = 0.0999, acc = 0.9720 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 18:29:02.685516: step 97440, loss = 0.1139, acc = 0.9660 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 18:29:11.716950: step 97450, loss = 0.1071, acc = 0.9740 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 18:29:21.127605: step 97460, loss = 0.1163, acc = 0.9540 (219.6 examples/sec; 0.291 sec/batch)
2017-05-03 18:29:30.112589: step 97470, loss = 0.1024, acc = 0.9720 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 18:29:39.005709: step 97480, loss = 0.0920, acc = 0.9780 (286.1 examples/sec; 0.224 sec/batch)
2017-05-03 18:29:48.042407: step 97490, loss = 0.0899, acc = 0.9760 (276.0 examples/sec; 0.232 sec/batch)
2017-05-03 18:29:57.083389: step 97500, loss = 0.1300, acc = 0.9520 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 18:30:06.160216: step 97510, loss = 0.1167, acc = 0.9640 (295.3 examples/sec; 0.217 sec/batch)
2017-05-03 18:30:15.256723: step 97520, loss = 0.0966, acc = 0.9680 (266.1 examples/sec; 0.241 sec/batch)
2017-05-03 18:30:24.264085: step 97530, loss = 0.1220, acc = 0.9640 (283.7 examples/sec; 0.226 sec/batch)
2017-05-03 18:30:33.226951: step 97540, loss = 0.0937, acc = 0.9740 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 18:30:42.216342: step 97550, loss = 0.1289, acc = 0.9560 (290.3 examples/sec; 0.220 sec/batch)
2017-05-03 18:30:51.263466: step 97560, loss = 0.1060, acc = 0.9660 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 18:31:00.302562: step 97570, loss = 0.1106, acc = 0.9700 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 18:31:09.432151: step 97580, loss = 0.1055, acc = 0.9620 (282.7 examples/sec; 0.226 sec/batch)
2017-05-03 18:31:18.542008: step 97590, loss = 0.1262, acc = 0.9640 (268.5 examples/sec; 0.238 sec/batch)
2017-05-03 18:31:27.389666: step 97600, loss = 0.1188, acc = 0.9620 (304.4 examples/sec; 0.210 sec/batch)
2017-05-03 18:31:36.291389: step 97610, loss = 0.1158, acc = 0.9620 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 18:31:45.410662: step 97620, loss = 0.0890, acc = 0.9780 (295.5 examples/sec; 0.217 sec/batch)
2017-05-03 18:31:54.556228: step 97630, loss = 0.0940, acc = 0.9600 (276.7 examples/sec; 0.231 sec/batch)
2017-05-03 18:32:03.601468: step 97640, loss = 0.1085, acc = 0.9640 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 18:32:12.610065: step 97650, loss = 0.0974, acc = 0.9600 (288.4 examples/sec; 0.222 sec/batch)
2017-05-03 18:32:21.658659: step 97660, loss = 0.1295, acc = 0.9540 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 18:32:30.696506: step 97670, loss = 0.1166, acc = 0.9640 (257.9 examples/sec; 0.248 sec/batch)
2017-05-03 18:32:39.644878: step 97680, loss = 0.1255, acc = 0.9500 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 18:32:48.671008: step 97690, loss = 0.1008, acc = 0.9660 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 18:32:57.579260: step 97700, loss = 0.0865, acc = 0.9840 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 18:33:06.573198: step 97710, loss = 0.1440, acc = 0.9520 (293.6 examples/sec; 0.218 sec/batch)
2017-05-03 18:33:15.581656: step 97720, loss = 0.1202, acc = 0.9680 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 18:33:24.795313: step 97730, loss = 0.1175, acc = 0.9580 (284.3 examples/sec; 0.225 sec/batch)
2017-05-03 18:33:33.728725: step 97740, loss = 0.0988, acc = 0.9740 (285.9 examples/sec; 0.224 sec/batch)
2017-05-03 18:33:42.688276: step 97750, loss = 0.0836, acc = 0.9720 (272.2 examples/sec; 0.235 sec/batch)
2017-05-03 18:33:51.644745: step 97760, loss = 0.1206, acc = 0.9620 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 18:34:00.593791: step 97770, loss = 0.1315, acc = 0.9600 (264.4 examples/sec; 0.242 sec/batch)
2017-05-03 18:34:10.553393: step 97780, loss = 0.1285, acc = 0.9600 (295.8 examples/sec; 0.216 sec/batch)
2017-05-03 18:34:19.470898: step 97790, loss = 0.1196, acc = 0.9600 (286.5 examples/sec; 0.223 sec/batch)
2017-05-03 18:34:28.470764: step 97800, loss = 0.1336, acc = 0.9540 (278.5 examples/sec; 0.230 sec/batch)
2017-05-03 18:34:37.318570: step 97810, loss = 0.1149, acc = 0.9600 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 18:34:46.386872: step 97820, loss = 0.1252, acc = 0.9580 (275.4 examples/sec; 0.232 sec/batch)
2017-05-03 18:34:55.372328: step 97830, loss = 0.0967, acc = 0.9760 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 18:35:04.487447: step 97840, loss = 0.0880, acc = 0.9840 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 18:35:13.638469: step 97850, loss = 0.1278, acc = 0.9580 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 18:35:23.021755: step 97860, loss = 0.0892, acc = 0.9780 (277.1 examples/sec; 0.231 sec/batch)
2017-05-03 18:35:32.109782: step 97870, loss = 0.0895, acc = 0.9740 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 18:35:41.285380: step 97880, loss = 0.1327, acc = 0.9580 (291.8 examples/sec; 0.219 sec/batch)
2017-05-03 18:35:50.243475: step 97890, loss = 0.0916, acc = 0.9780 (292.1 examples/sec; 0.219 sec/batch)
2017-05-03 18:35:59.278018: step 97900, loss = 0.0932, acc = 0.9840 (273.8 examples/sec; 0.234 sec/batch)
2017-05-03 18:36:08.120494: step 97910, loss = 0.1143, acc = 0.9620 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 18:36:17.198211: step 97920, loss = 0.1088, acc = 0.9600 (279.9 examples/sec; 0.229 sec/batch)
2017-05-03 18:36:26.241985: step 97930, loss = 0.0984, acc = 0.9740 (297.1 examples/sec; 0.215 sec/batch)
2017-05-03 18:36:35.119191: step 97940, loss = 0.1058, acc = 0.9600 (286.9 examples/sec; 0.223 sec/batch)
2017-05-03 18:36:44.144669: step 97950, loss = 0.1029, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 18:36:53.158969: step 97960, loss = 0.0903, acc = 0.9780 (274.0 examples/sec; 0.234 sec/batch)
2017-05-03 18:37:02.104047: step 97970, loss = 0.0822, acc = 0.9800 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 18:37:11.239057: step 97980, loss = 0.1252, acc = 0.9640 (271.3 examples/sec; 0.236 sec/batch)
2017-05-03 18:37:20.290437: step 97990, loss = 0.1101, acc = 0.9740 (286.6 examples/sec; 0.223 sec/batch)
2017-05-03 18:37:29.386232: step 98000, loss = 0.0867, acc = 0.9820 (298.0 examples/sec; 0.215 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 18:37:29.874367: step 98000, loss = 0.1034, acc = 0.9610, f1neg = 0.9571, f1pos = 0.9642, f1 = 0.9607
[Eval_batch(1)(2000,4000)] 2017-05-03 18:37:30.339242: step 98000, loss = 0.1083, acc = 0.9665, f1neg = 0.9609, f1pos = 0.9707, f1 = 0.9658
[Eval_batch(2)(2000,6000)] 2017-05-03 18:37:30.802390: step 98000, loss = 0.1191, acc = 0.9600, f1neg = 0.9574, f1pos = 0.9623, f1 = 0.9599
[Eval_batch(3)(2000,8000)] 2017-05-03 18:37:31.309174: step 98000, loss = 0.1239, acc = 0.9605, f1neg = 0.9605, f1pos = 0.9605, f1 = 0.9605
[Eval_batch(4)(2000,10000)] 2017-05-03 18:37:31.815037: step 98000, loss = 0.1246, acc = 0.9605, f1neg = 0.9615, f1pos = 0.9594, f1 = 0.9605
[Eval_batch(5)(2000,12000)] 2017-05-03 18:37:32.311263: step 98000, loss = 0.1228, acc = 0.9555, f1neg = 0.9521, f1pos = 0.9584, f1 = 0.9553
[Eval_batch(6)(2000,14000)] 2017-05-03 18:37:32.772565: step 98000, loss = 0.1142, acc = 0.9675, f1neg = 0.9669, f1pos = 0.9681, f1 = 0.9675
[Eval_batch(7)(2000,16000)] 2017-05-03 18:37:33.238503: step 98000, loss = 0.1149, acc = 0.9625, f1neg = 0.9552, f1pos = 0.9677, f1 = 0.9615
[Eval_batch(8)(2000,18000)] 2017-05-03 18:37:33.702333: step 98000, loss = 0.1058, acc = 0.9620, f1neg = 0.9567, f1pos = 0.9661, f1 = 0.9614
[Eval_batch(9)(2000,20000)] 2017-05-03 18:37:34.165010: step 98000, loss = 0.1113, acc = 0.9650, f1neg = 0.9623, f1pos = 0.9674, f1 = 0.9648
[Eval_batch(10)(2000,22000)] 2017-05-03 18:37:34.642812: step 98000, loss = 0.1184, acc = 0.9615, f1neg = 0.9588, f1pos = 0.9638, f1 = 0.9613
[Eval_batch(11)(2000,24000)] 2017-05-03 18:37:35.120932: step 98000, loss = 0.1180, acc = 0.9620, f1neg = 0.9562, f1pos = 0.9664, f1 = 0.9613
[Eval_batch(12)(2000,26000)] 2017-05-03 18:37:35.632578: step 98000, loss = 0.1157, acc = 0.9550, f1neg = 0.9536, f1pos = 0.9564, f1 = 0.9550
[Eval_batch(13)(2000,28000)] 2017-05-03 18:37:36.103066: step 98000, loss = 0.1078, acc = 0.9665, f1neg = 0.9580, f1pos = 0.9721, f1 = 0.9651
[Eval_batch(14)(2000,30000)] 2017-05-03 18:37:36.581681: step 98000, loss = 0.1352, acc = 0.9530, f1neg = 0.9440, f1pos = 0.9595, f1 = 0.9518
[Eval_batch(15)(2000,32000)] 2017-05-03 18:37:37.051362: step 98000, loss = 0.1086, acc = 0.9690, f1neg = 0.9665, f1pos = 0.9712, f1 = 0.9688
[Eval_batch(16)(2000,34000)] 2017-05-03 18:37:37.524794: step 98000, loss = 0.1023, acc = 0.9700, f1neg = 0.9687, f1pos = 0.9712, f1 = 0.9699
[Eval_batch(17)(2000,36000)] 2017-05-03 18:37:38.029617: step 98000, loss = 0.1331, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(18)(2000,38000)] 2017-05-03 18:37:38.531718: step 98000, loss = 0.1251, acc = 0.9605, f1neg = 0.9621, f1pos = 0.9587, f1 = 0.9604
[Eval_batch(19)(2000,40000)] 2017-05-03 18:37:38.996561: step 98000, loss = 0.1090, acc = 0.9695, f1neg = 0.9648, f1pos = 0.9731, f1 = 0.9689
[Eval_batch(20)(2000,42000)] 2017-05-03 18:37:39.473318: step 98000, loss = 0.1136, acc = 0.9635, f1neg = 0.9676, f1pos = 0.9582, f1 = 0.9629
[Eval_batch(21)(2000,44000)] 2017-05-03 18:37:39.942757: step 98000, loss = 0.1017, acc = 0.9670, f1neg = 0.9640, f1pos = 0.9695, f1 = 0.9668
[Eval_batch(22)(2000,46000)] 2017-05-03 18:37:40.447304: step 98000, loss = 0.1138, acc = 0.9600, f1neg = 0.9603, f1pos = 0.9597, f1 = 0.9600
[Eval_batch(23)(2000,48000)] 2017-05-03 18:37:40.937760: step 98000, loss = 0.1181, acc = 0.9605, f1neg = 0.9554, f1pos = 0.9645, f1 = 0.9600
[Eval_batch(24)(2000,50000)] 2017-05-03 18:37:41.403269: step 98000, loss = 0.1013, acc = 0.9660, f1neg = 0.9611, f1pos = 0.9698, f1 = 0.9655
[Eval_batch(25)(2000,52000)] 2017-05-03 18:37:41.864943: step 98000, loss = 0.0945, acc = 0.9745, f1neg = 0.9721, f1pos = 0.9765, f1 = 0.9743
[Eval_batch(26)(2000,54000)] 2017-05-03 18:37:42.332378: step 98000, loss = 0.1089, acc = 0.9630, f1neg = 0.9650, f1pos = 0.9608, f1 = 0.9629
[Eval_batch(27)(2000,56000)] 2017-05-03 18:37:42.882816: step 98000, loss = 0.0951, acc = 0.9695, f1neg = 0.9679, f1pos = 0.9709, f1 = 0.9694
[Eval] 2017-05-03 18:37:42.882914: step 98000, acc = 0.9633, f1 = 0.9629
[Test_batch(0)(2000,2000)] 2017-05-03 18:37:43.351801: step 98000, loss = 0.1427, acc = 0.9545, f1neg = 0.9567, f1pos = 0.9520, f1 = 0.9544
[Test_batch(1)(2000,4000)] 2017-05-03 18:37:43.811009: step 98000, loss = 0.1234, acc = 0.9630, f1neg = 0.9671, f1pos = 0.9578, f1 = 0.9624
[Test_batch(2)(2000,6000)] 2017-05-03 18:37:44.285106: step 98000, loss = 0.1410, acc = 0.9560, f1neg = 0.9595, f1pos = 0.9518, f1 = 0.9557
[Test_batch(3)(2000,8000)] 2017-05-03 18:37:44.801884: step 98000, loss = 0.1466, acc = 0.9525, f1neg = 0.9553, f1pos = 0.9493, f1 = 0.9523
[Test_batch(4)(2000,10000)] 2017-05-03 18:37:45.299839: step 98000, loss = 0.1390, acc = 0.9520, f1neg = 0.9522, f1pos = 0.9518, f1 = 0.9520
[Test_batch(5)(2000,12000)] 2017-05-03 18:37:45.769299: step 98000, loss = 0.1547, acc = 0.9460, f1neg = 0.9466, f1pos = 0.9453, f1 = 0.9460
[Test_batch(6)(2000,14000)] 2017-05-03 18:37:46.274771: step 98000, loss = 0.1413, acc = 0.9485, f1neg = 0.9465, f1pos = 0.9503, f1 = 0.9484
[Test_batch(7)(2000,16000)] 2017-05-03 18:37:46.741837: step 98000, loss = 0.1319, acc = 0.9550, f1neg = 0.9591, f1pos = 0.9500, f1 = 0.9545
[Test_batch(8)(2000,18000)] 2017-05-03 18:37:47.245718: step 98000, loss = 0.1306, acc = 0.9515, f1neg = 0.9509, f1pos = 0.9521, f1 = 0.9515
[Test_batch(9)(2000,20000)] 2017-05-03 18:37:47.749128: step 98000, loss = 0.1601, acc = 0.9375, f1neg = 0.9341, f1pos = 0.9406, f1 = 0.9373
[Test_batch(10)(2000,22000)] 2017-05-03 18:37:48.216171: step 98000, loss = 0.1381, acc = 0.9515, f1neg = 0.9454, f1pos = 0.9564, f1 = 0.9509
[Test_batch(11)(2000,24000)] 2017-05-03 18:37:48.693401: step 98000, loss = 0.1429, acc = 0.9520, f1neg = 0.9532, f1pos = 0.9507, f1 = 0.9520
[Test_batch(12)(2000,26000)] 2017-05-03 18:37:49.190033: step 98000, loss = 0.1154, acc = 0.9655, f1neg = 0.9668, f1pos = 0.9641, f1 = 0.9654
[Test_batch(13)(2000,28000)] 2017-05-03 18:37:49.697215: step 98000, loss = 0.1376, acc = 0.9540, f1neg = 0.9489, f1pos = 0.9581, f1 = 0.9535
[Test_batch(14)(2000,30000)] 2017-05-03 18:37:50.169173: step 98000, loss = 0.1157, acc = 0.9580, f1neg = 0.9516, f1pos = 0.9629, f1 = 0.9573
[Test_batch(15)(2000,32000)] 2017-05-03 18:37:50.633753: step 98000, loss = 0.1402, acc = 0.9560, f1neg = 0.9553, f1pos = 0.9567, f1 = 0.9560
[Test_batch(16)(2000,34000)] 2017-05-03 18:37:51.138261: step 98000, loss = 0.1201, acc = 0.9620, f1neg = 0.9611, f1pos = 0.9629, f1 = 0.9620
[Test_batch(17)(2000,36000)] 2017-05-03 18:37:51.607757: step 98000, loss = 0.1120, acc = 0.9680, f1neg = 0.9650, f1pos = 0.9705, f1 = 0.9678
[Test_batch(18)(2000,38000)] 2017-05-03 18:37:52.128987: step 98000, loss = 0.1083, acc = 0.9640, f1neg = 0.9632, f1pos = 0.9648, f1 = 0.9640
[Test] 2017-05-03 18:37:52.129073: step 98000, acc = 0.9551, f1 = 0.9549
[Status] 2017-05-03 18:37:52.129096: step 98000, maxindex = 95000, maxdev = 0.9637, maxtst = 0.9550
2017-05-03 18:38:01.165854: step 98010, loss = 0.0962, acc = 0.9720 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 18:38:10.216411: step 98020, loss = 0.1185, acc = 0.9620 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 18:38:19.250173: step 98030, loss = 0.1198, acc = 0.9620 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 18:38:28.156260: step 98040, loss = 0.1366, acc = 0.9520 (287.0 examples/sec; 0.223 sec/batch)
2017-05-03 18:38:37.214284: step 98050, loss = 0.1236, acc = 0.9660 (277.6 examples/sec; 0.231 sec/batch)
2017-05-03 18:38:46.187076: step 98060, loss = 0.1094, acc = 0.9660 (264.7 examples/sec; 0.242 sec/batch)
2017-05-03 18:38:55.236829: step 98070, loss = 0.1026, acc = 0.9700 (283.6 examples/sec; 0.226 sec/batch)
2017-05-03 18:39:04.382540: step 98080, loss = 0.1377, acc = 0.9680 (279.7 examples/sec; 0.229 sec/batch)
2017-05-03 18:39:13.243557: step 98090, loss = 0.0958, acc = 0.9760 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 18:39:22.147488: step 98100, loss = 0.1145, acc = 0.9660 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:39:31.099996: step 98110, loss = 0.1170, acc = 0.9560 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 18:39:40.243431: step 98120, loss = 0.1011, acc = 0.9800 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 18:39:49.220123: step 98130, loss = 0.0902, acc = 0.9680 (284.8 examples/sec; 0.225 sec/batch)
2017-05-03 18:39:58.326667: step 98140, loss = 0.1231, acc = 0.9560 (293.2 examples/sec; 0.218 sec/batch)
2017-05-03 18:40:07.336185: step 98150, loss = 0.1104, acc = 0.9540 (293.0 examples/sec; 0.218 sec/batch)
2017-05-03 18:40:16.376970: step 98160, loss = 0.0988, acc = 0.9720 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 18:40:25.387296: step 98170, loss = 0.1289, acc = 0.9540 (286.3 examples/sec; 0.224 sec/batch)
2017-05-03 18:40:34.374819: step 98180, loss = 0.1015, acc = 0.9740 (285.7 examples/sec; 0.224 sec/batch)
2017-05-03 18:40:43.378832: step 98190, loss = 0.0974, acc = 0.9740 (283.8 examples/sec; 0.226 sec/batch)
2017-05-03 18:40:52.513905: step 98200, loss = 0.0956, acc = 0.9660 (282.4 examples/sec; 0.227 sec/batch)
2017-05-03 18:41:01.560621: step 98210, loss = 0.1508, acc = 0.9540 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 18:41:10.523367: step 98220, loss = 0.1253, acc = 0.9640 (287.4 examples/sec; 0.223 sec/batch)
2017-05-03 18:41:19.465733: step 98230, loss = 0.1188, acc = 0.9580 (281.4 examples/sec; 0.227 sec/batch)
2017-05-03 18:41:28.485153: step 98240, loss = 0.1173, acc = 0.9560 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 18:41:37.255236: step 98250, loss = 0.1170, acc = 0.9580 (283.2 examples/sec; 0.226 sec/batch)
2017-05-03 18:41:46.464849: step 98260, loss = 0.1040, acc = 0.9720 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:41:55.424347: step 98270, loss = 0.1099, acc = 0.9740 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 18:42:04.479092: step 98280, loss = 0.1097, acc = 0.9720 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 18:42:14.039528: step 98290, loss = 0.1010, acc = 0.9580 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 18:42:23.104126: step 98300, loss = 0.0921, acc = 0.9720 (287.1 examples/sec; 0.223 sec/batch)
2017-05-03 18:42:32.159734: step 98310, loss = 0.0918, acc = 0.9760 (271.5 examples/sec; 0.236 sec/batch)
2017-05-03 18:42:41.387658: step 98320, loss = 0.1321, acc = 0.9600 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 18:42:50.477703: step 98330, loss = 0.0999, acc = 0.9780 (277.9 examples/sec; 0.230 sec/batch)
2017-05-03 18:42:59.664861: step 98340, loss = 0.1033, acc = 0.9700 (281.3 examples/sec; 0.227 sec/batch)
2017-05-03 18:43:08.574955: step 98350, loss = 0.1029, acc = 0.9700 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 18:43:17.683617: step 98360, loss = 0.1045, acc = 0.9700 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 18:43:26.568915: step 98370, loss = 0.1154, acc = 0.9700 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 18:43:35.625232: step 98380, loss = 0.1545, acc = 0.9540 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 18:43:44.650664: step 98390, loss = 0.1199, acc = 0.9720 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 18:43:53.599085: step 98400, loss = 0.1163, acc = 0.9600 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 18:44:02.628963: step 98410, loss = 0.1323, acc = 0.9600 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 18:44:11.721823: step 98420, loss = 0.0999, acc = 0.9680 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 18:44:20.745972: step 98430, loss = 0.1262, acc = 0.9640 (302.7 examples/sec; 0.211 sec/batch)
2017-05-03 18:44:29.751686: step 98440, loss = 0.0958, acc = 0.9680 (283.8 examples/sec; 0.225 sec/batch)
2017-05-03 18:44:38.935311: step 98450, loss = 0.1255, acc = 0.9580 (291.5 examples/sec; 0.220 sec/batch)
2017-05-03 18:44:47.840115: step 98460, loss = 0.0889, acc = 0.9840 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 18:44:56.947912: step 98470, loss = 0.1407, acc = 0.9580 (294.9 examples/sec; 0.217 sec/batch)
2017-05-03 18:45:06.093515: step 98480, loss = 0.1066, acc = 0.9760 (284.5 examples/sec; 0.225 sec/batch)
2017-05-03 18:45:15.233670: step 98490, loss = 0.1152, acc = 0.9620 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 18:45:24.265660: step 98500, loss = 0.1284, acc = 0.9480 (281.2 examples/sec; 0.228 sec/batch)
2017-05-03 18:45:33.388942: step 98510, loss = 0.1227, acc = 0.9600 (281.8 examples/sec; 0.227 sec/batch)
2017-05-03 18:45:42.376042: step 98520, loss = 0.1075, acc = 0.9660 (269.8 examples/sec; 0.237 sec/batch)
2017-05-03 18:45:51.371607: step 98530, loss = 0.1022, acc = 0.9700 (279.5 examples/sec; 0.229 sec/batch)
2017-05-03 18:46:00.491121: step 98540, loss = 0.1131, acc = 0.9620 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 18:46:09.519230: step 98550, loss = 0.1328, acc = 0.9560 (289.4 examples/sec; 0.221 sec/batch)
2017-05-03 18:46:18.622083: step 98560, loss = 0.0959, acc = 0.9780 (260.9 examples/sec; 0.245 sec/batch)
2017-05-03 18:46:27.698638: step 98570, loss = 0.1346, acc = 0.9500 (278.4 examples/sec; 0.230 sec/batch)
2017-05-03 18:46:37.067223: step 98580, loss = 0.1097, acc = 0.9680 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 18:46:46.084157: step 98590, loss = 0.0841, acc = 0.9820 (298.8 examples/sec; 0.214 sec/batch)
2017-05-03 18:46:55.062274: step 98600, loss = 0.1062, acc = 0.9700 (305.3 examples/sec; 0.210 sec/batch)
2017-05-03 18:47:04.147139: step 98610, loss = 0.1129, acc = 0.9680 (266.3 examples/sec; 0.240 sec/batch)
2017-05-03 18:47:13.252337: step 98620, loss = 0.1134, acc = 0.9560 (267.7 examples/sec; 0.239 sec/batch)
2017-05-03 18:47:22.307957: step 98630, loss = 0.1002, acc = 0.9760 (283.4 examples/sec; 0.226 sec/batch)
2017-05-03 18:47:31.299304: step 98640, loss = 0.1310, acc = 0.9520 (279.2 examples/sec; 0.229 sec/batch)
2017-05-03 18:47:40.372341: step 98650, loss = 0.1188, acc = 0.9760 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 18:47:49.507067: step 98660, loss = 0.1110, acc = 0.9660 (281.1 examples/sec; 0.228 sec/batch)
2017-05-03 18:47:58.572954: step 98670, loss = 0.1062, acc = 0.9740 (266.9 examples/sec; 0.240 sec/batch)
2017-05-03 18:48:07.522905: step 98680, loss = 0.1011, acc = 0.9700 (293.3 examples/sec; 0.218 sec/batch)
2017-05-03 18:48:16.477606: step 98690, loss = 0.1452, acc = 0.9480 (290.6 examples/sec; 0.220 sec/batch)
2017-05-03 18:48:25.414314: step 98700, loss = 0.0985, acc = 0.9700 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 18:48:34.805381: step 98710, loss = 0.0948, acc = 0.9740 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 18:48:43.826104: step 98720, loss = 0.1045, acc = 0.9740 (289.5 examples/sec; 0.221 sec/batch)
2017-05-03 18:48:52.843322: step 98730, loss = 0.0958, acc = 0.9700 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 18:49:01.931856: step 98740, loss = 0.0937, acc = 0.9720 (263.0 examples/sec; 0.243 sec/batch)
2017-05-03 18:49:10.971200: step 98750, loss = 0.1090, acc = 0.9740 (284.9 examples/sec; 0.225 sec/batch)
2017-05-03 18:49:20.021227: step 98760, loss = 0.1183, acc = 0.9620 (297.7 examples/sec; 0.215 sec/batch)
2017-05-03 18:49:29.053511: step 98770, loss = 0.1108, acc = 0.9620 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 18:49:38.159213: step 98780, loss = 0.1066, acc = 0.9580 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 18:49:47.893578: step 98790, loss = 0.1144, acc = 0.9620 (268.4 examples/sec; 0.238 sec/batch)
2017-05-03 18:49:56.953927: step 98800, loss = 0.1174, acc = 0.9740 (277.5 examples/sec; 0.231 sec/batch)
2017-05-03 18:50:05.887863: step 98810, loss = 0.1214, acc = 0.9560 (289.9 examples/sec; 0.221 sec/batch)
2017-05-03 18:50:14.856262: step 98820, loss = 0.1092, acc = 0.9660 (280.9 examples/sec; 0.228 sec/batch)
2017-05-03 18:50:23.762438: step 98830, loss = 0.1075, acc = 0.9620 (290.1 examples/sec; 0.221 sec/batch)
2017-05-03 18:50:32.933526: step 98840, loss = 0.1073, acc = 0.9620 (279.0 examples/sec; 0.229 sec/batch)
2017-05-03 18:50:41.818517: step 98850, loss = 0.1101, acc = 0.9780 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 18:50:50.859406: step 98860, loss = 0.0896, acc = 0.9740 (289.7 examples/sec; 0.221 sec/batch)
2017-05-03 18:50:59.863591: step 98870, loss = 0.1673, acc = 0.9560 (280.2 examples/sec; 0.228 sec/batch)
2017-05-03 18:51:08.839735: step 98880, loss = 0.1133, acc = 0.9740 (265.9 examples/sec; 0.241 sec/batch)
2017-05-03 18:51:18.065378: step 98890, loss = 0.1323, acc = 0.9660 (280.3 examples/sec; 0.228 sec/batch)
2017-05-03 18:51:27.131336: step 98900, loss = 0.0892, acc = 0.9800 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 18:51:36.080251: step 98910, loss = 0.0972, acc = 0.9720 (295.4 examples/sec; 0.217 sec/batch)
2017-05-03 18:51:45.039584: step 98920, loss = 0.1131, acc = 0.9660 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 18:51:54.071558: step 98930, loss = 0.1075, acc = 0.9620 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 18:52:03.292079: step 98940, loss = 0.1017, acc = 0.9720 (289.6 examples/sec; 0.221 sec/batch)
2017-05-03 18:52:12.271419: step 98950, loss = 0.1008, acc = 0.9760 (290.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:52:21.259779: step 98960, loss = 0.1011, acc = 0.9720 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 18:52:30.277107: step 98970, loss = 0.1214, acc = 0.9620 (280.4 examples/sec; 0.228 sec/batch)
2017-05-03 18:52:39.283609: step 98980, loss = 0.1130, acc = 0.9600 (285.0 examples/sec; 0.225 sec/batch)
2017-05-03 18:52:48.341258: step 98990, loss = 0.1117, acc = 0.9620 (294.2 examples/sec; 0.218 sec/batch)
2017-05-03 18:52:57.418075: step 99000, loss = 0.0994, acc = 0.9740 (287.5 examples/sec; 0.223 sec/batch)
[Eval_batch(0)(2000,2000)] 2017-05-03 18:52:57.923028: step 99000, loss = 0.1021, acc = 0.9625, f1neg = 0.9590, f1pos = 0.9655, f1 = 0.9622
[Eval_batch(1)(2000,4000)] 2017-05-03 18:52:58.435630: step 99000, loss = 0.1085, acc = 0.9650, f1neg = 0.9594, f1pos = 0.9692, f1 = 0.9643
[Eval_batch(2)(2000,6000)] 2017-05-03 18:52:58.916408: step 99000, loss = 0.1175, acc = 0.9615, f1neg = 0.9593, f1pos = 0.9635, f1 = 0.9614
[Eval_batch(3)(2000,8000)] 2017-05-03 18:52:59.386951: step 99000, loss = 0.1204, acc = 0.9610, f1neg = 0.9612, f1pos = 0.9608, f1 = 0.9610
[Eval_batch(4)(2000,10000)] 2017-05-03 18:52:59.860036: step 99000, loss = 0.1218, acc = 0.9570, f1neg = 0.9586, f1pos = 0.9553, f1 = 0.9569
[Eval_batch(5)(2000,12000)] 2017-05-03 18:53:00.372918: step 99000, loss = 0.1213, acc = 0.9580, f1neg = 0.9552, f1pos = 0.9605, f1 = 0.9578
[Eval_batch(6)(2000,14000)] 2017-05-03 18:53:00.877612: step 99000, loss = 0.1128, acc = 0.9685, f1neg = 0.9680, f1pos = 0.9690, f1 = 0.9685
[Eval_batch(7)(2000,16000)] 2017-05-03 18:53:01.313324: step 99000, loss = 0.1146, acc = 0.9620, f1neg = 0.9549, f1pos = 0.9672, f1 = 0.9610
[Eval_batch(8)(2000,18000)] 2017-05-03 18:53:01.755837: step 99000, loss = 0.1060, acc = 0.9645, f1neg = 0.9600, f1pos = 0.9681, f1 = 0.9640
[Eval_batch(9)(2000,20000)] 2017-05-03 18:53:02.255382: step 99000, loss = 0.1109, acc = 0.9645, f1neg = 0.9621, f1pos = 0.9666, f1 = 0.9644
[Eval_batch(10)(2000,22000)] 2017-05-03 18:53:02.724247: step 99000, loss = 0.1161, acc = 0.9645, f1neg = 0.9623, f1pos = 0.9665, f1 = 0.9644
[Eval_batch(11)(2000,24000)] 2017-05-03 18:53:03.193796: step 99000, loss = 0.1197, acc = 0.9620, f1neg = 0.9564, f1pos = 0.9663, f1 = 0.9614
[Eval_batch(12)(2000,26000)] 2017-05-03 18:53:03.700371: step 99000, loss = 0.1144, acc = 0.9540, f1neg = 0.9529, f1pos = 0.9551, f1 = 0.9540
[Eval_batch(13)(2000,28000)] 2017-05-03 18:53:04.208705: step 99000, loss = 0.1073, acc = 0.9670, f1neg = 0.9589, f1pos = 0.9724, f1 = 0.9657
[Eval_batch(14)(2000,30000)] 2017-05-03 18:53:04.722243: step 99000, loss = 0.1332, acc = 0.9555, f1neg = 0.9474, f1pos = 0.9614, f1 = 0.9544
[Eval_batch(15)(2000,32000)] 2017-05-03 18:53:05.185517: step 99000, loss = 0.1078, acc = 0.9685, f1neg = 0.9661, f1pos = 0.9705, f1 = 0.9683
[Eval_batch(16)(2000,34000)] 2017-05-03 18:53:05.624658: step 99000, loss = 0.1021, acc = 0.9705, f1neg = 0.9695, f1pos = 0.9715, f1 = 0.9705
[Eval_batch(17)(2000,36000)] 2017-05-03 18:53:06.099742: step 99000, loss = 0.1326, acc = 0.9585, f1neg = 0.9590, f1pos = 0.9580, f1 = 0.9585
[Eval_batch(18)(2000,38000)] 2017-05-03 18:53:06.602952: step 99000, loss = 0.1236, acc = 0.9630, f1neg = 0.9648, f1pos = 0.9611, f1 = 0.9629
[Eval_batch(19)(2000,40000)] 2017-05-03 18:53:07.077050: step 99000, loss = 0.1091, acc = 0.9685, f1neg = 0.9638, f1pos = 0.9721, f1 = 0.9680
[Eval_batch(20)(2000,42000)] 2017-05-03 18:53:07.555467: step 99000, loss = 0.1085, acc = 0.9660, f1neg = 0.9699, f1pos = 0.9609, f1 = 0.9654
[Eval_batch(21)(2000,44000)] 2017-05-03 18:53:08.020414: step 99000, loss = 0.1034, acc = 0.9645, f1neg = 0.9615, f1pos = 0.9671, f1 = 0.9643
[Eval_batch(22)(2000,46000)] 2017-05-03 18:53:08.489382: step 99000, loss = 0.1128, acc = 0.9605, f1neg = 0.9610, f1pos = 0.9600, f1 = 0.9605
[Eval_batch(23)(2000,48000)] 2017-05-03 18:53:08.950526: step 99000, loss = 0.1176, acc = 0.9625, f1neg = 0.9579, f1pos = 0.9662, f1 = 0.9621
[Eval_batch(24)(2000,50000)] 2017-05-03 18:53:09.415720: step 99000, loss = 0.1013, acc = 0.9695, f1neg = 0.9652, f1pos = 0.9728, f1 = 0.9690
[Eval_batch(25)(2000,52000)] 2017-05-03 18:53:09.885744: step 99000, loss = 0.0926, acc = 0.9730, f1neg = 0.9706, f1pos = 0.9751, f1 = 0.9728
[Eval_batch(26)(2000,54000)] 2017-05-03 18:53:10.364125: step 99000, loss = 0.1073, acc = 0.9635, f1neg = 0.9656, f1pos = 0.9611, f1 = 0.9634
[Eval_batch(27)(2000,56000)] 2017-05-03 18:53:10.923209: step 99000, loss = 0.0937, acc = 0.9740, f1neg = 0.9729, f1pos = 0.9750, f1 = 0.9740
[Eval] 2017-05-03 18:53:10.923300: step 99000, acc = 0.9639, f1 = 0.9636
[Test_batch(0)(2000,2000)] 2017-05-03 18:53:11.397933: step 99000, loss = 0.1425, acc = 0.9520, f1neg = 0.9547, f1pos = 0.9490, f1 = 0.9518
[Test_batch(1)(2000,4000)] 2017-05-03 18:53:11.902702: step 99000, loss = 0.1202, acc = 0.9645, f1neg = 0.9686, f1pos = 0.9592, f1 = 0.9639
[Test_batch(2)(2000,6000)] 2017-05-03 18:53:12.400443: step 99000, loss = 0.1384, acc = 0.9575, f1neg = 0.9611, f1pos = 0.9532, f1 = 0.9571
[Test_batch(3)(2000,8000)] 2017-05-03 18:53:12.912942: step 99000, loss = 0.1447, acc = 0.9515, f1neg = 0.9548, f1pos = 0.9477, f1 = 0.9512
[Test_batch(4)(2000,10000)] 2017-05-03 18:53:13.417467: step 99000, loss = 0.1376, acc = 0.9510, f1neg = 0.9515, f1pos = 0.9505, f1 = 0.9510
[Test_batch(5)(2000,12000)] 2017-05-03 18:53:13.902966: step 99000, loss = 0.1534, acc = 0.9475, f1neg = 0.9485, f1pos = 0.9465, f1 = 0.9475
[Test_batch(6)(2000,14000)] 2017-05-03 18:53:14.407556: step 99000, loss = 0.1385, acc = 0.9485, f1neg = 0.9470, f1pos = 0.9499, f1 = 0.9485
[Test_batch(7)(2000,16000)] 2017-05-03 18:53:14.883796: step 99000, loss = 0.1306, acc = 0.9515, f1neg = 0.9561, f1pos = 0.9458, f1 = 0.9510
[Test_batch(8)(2000,18000)] 2017-05-03 18:53:15.356727: step 99000, loss = 0.1299, acc = 0.9500, f1neg = 0.9497, f1pos = 0.9503, f1 = 0.9500
[Test_batch(9)(2000,20000)] 2017-05-03 18:53:15.825320: step 99000, loss = 0.1600, acc = 0.9360, f1neg = 0.9331, f1pos = 0.9387, f1 = 0.9359
[Test_batch(10)(2000,22000)] 2017-05-03 18:53:16.304566: step 99000, loss = 0.1402, acc = 0.9535, f1neg = 0.9479, f1pos = 0.9580, f1 = 0.9530
[Test_batch(11)(2000,24000)] 2017-05-03 18:53:16.783600: step 99000, loss = 0.1416, acc = 0.9555, f1neg = 0.9570, f1pos = 0.9539, f1 = 0.9554
[Test_batch(12)(2000,26000)] 2017-05-03 18:53:17.256545: step 99000, loss = 0.1153, acc = 0.9635, f1neg = 0.9650, f1pos = 0.9619, f1 = 0.9634
[Test_batch(13)(2000,28000)] 2017-05-03 18:53:17.727339: step 99000, loss = 0.1389, acc = 0.9545, f1neg = 0.9499, f1pos = 0.9583, f1 = 0.9541
[Test_batch(14)(2000,30000)] 2017-05-03 18:53:18.199895: step 99000, loss = 0.1157, acc = 0.9610, f1neg = 0.9554, f1pos = 0.9654, f1 = 0.9604
[Test_batch(15)(2000,32000)] 2017-05-03 18:53:18.679042: step 99000, loss = 0.1395, acc = 0.9580, f1neg = 0.9577, f1pos = 0.9583, f1 = 0.9580
[Test_batch(16)(2000,34000)] 2017-05-03 18:53:19.189473: step 99000, loss = 0.1203, acc = 0.9610, f1neg = 0.9602, f1pos = 0.9618, f1 = 0.9610
[Test_batch(17)(2000,36000)] 2017-05-03 18:53:19.663777: step 99000, loss = 0.1112, acc = 0.9670, f1neg = 0.9640, f1pos = 0.9695, f1 = 0.9668
[Test_batch(18)(2000,38000)] 2017-05-03 18:53:20.198862: step 99000, loss = 0.1062, acc = 0.9650, f1neg = 0.9644, f1pos = 0.9656, f1 = 0.9650
[Test] 2017-05-03 18:53:20.198958: step 99000, acc = 0.9552, f1 = 0.9550
[Status] 2017-05-03 18:53:20.198988: step 99000, maxindex = 99000, maxdev = 0.9639, maxtst = 0.9552
2017-05-03 18:53:32.337218: step 99010, loss = 0.1424, acc = 0.9520 (289.2 examples/sec; 0.221 sec/batch)
2017-05-03 18:53:41.426067: step 99020, loss = 0.1103, acc = 0.9720 (288.8 examples/sec; 0.222 sec/batch)
2017-05-03 18:53:50.509783: step 99030, loss = 0.0791, acc = 0.9860 (284.2 examples/sec; 0.225 sec/batch)
2017-05-03 18:53:59.428029: step 99040, loss = 0.0793, acc = 0.9780 (285.2 examples/sec; 0.224 sec/batch)
2017-05-03 18:54:08.632066: step 99050, loss = 0.1425, acc = 0.9500 (287.5 examples/sec; 0.223 sec/batch)
2017-05-03 18:54:17.683631: step 99060, loss = 0.1242, acc = 0.9600 (273.7 examples/sec; 0.234 sec/batch)
2017-05-03 18:54:26.899576: step 99070, loss = 0.0963, acc = 0.9720 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 18:54:35.961827: step 99080, loss = 0.0993, acc = 0.9740 (279.1 examples/sec; 0.229 sec/batch)
2017-05-03 18:54:45.021915: step 99090, loss = 0.1124, acc = 0.9680 (278.1 examples/sec; 0.230 sec/batch)
2017-05-03 18:54:54.137000: step 99100, loss = 0.0946, acc = 0.9660 (288.5 examples/sec; 0.222 sec/batch)
2017-05-03 18:55:03.106056: step 99110, loss = 0.1215, acc = 0.9620 (291.9 examples/sec; 0.219 sec/batch)
2017-05-03 18:55:12.249440: step 99120, loss = 0.1122, acc = 0.9740 (285.4 examples/sec; 0.224 sec/batch)
2017-05-03 18:55:21.623029: step 99130, loss = 0.1145, acc = 0.9640 (272.8 examples/sec; 0.235 sec/batch)
2017-05-03 18:55:30.948284: step 99140, loss = 0.0859, acc = 0.9840 (273.9 examples/sec; 0.234 sec/batch)
2017-05-03 18:55:40.015531: step 99150, loss = 0.1443, acc = 0.9520 (283.0 examples/sec; 0.226 sec/batch)
2017-05-03 18:55:48.894417: step 99160, loss = 0.1105, acc = 0.9740 (296.2 examples/sec; 0.216 sec/batch)
2017-05-03 18:55:57.915954: step 99170, loss = 0.1281, acc = 0.9500 (281.6 examples/sec; 0.227 sec/batch)
2017-05-03 18:56:06.855635: step 99180, loss = 0.1035, acc = 0.9680 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 18:56:15.737268: step 99190, loss = 0.0829, acc = 0.9820 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 18:56:24.734557: step 99200, loss = 0.1003, acc = 0.9620 (278.3 examples/sec; 0.230 sec/batch)
2017-05-03 18:56:34.057643: step 99210, loss = 0.1097, acc = 0.9720 (286.7 examples/sec; 0.223 sec/batch)
2017-05-03 18:56:43.078357: step 99220, loss = 0.1118, acc = 0.9660 (280.7 examples/sec; 0.228 sec/batch)
2017-05-03 18:56:52.074394: step 99230, loss = 0.1451, acc = 0.9500 (278.9 examples/sec; 0.229 sec/batch)
2017-05-03 18:57:01.186573: step 99240, loss = 0.1004, acc = 0.9700 (252.5 examples/sec; 0.253 sec/batch)
2017-05-03 18:57:10.296833: step 99250, loss = 0.1143, acc = 0.9640 (301.6 examples/sec; 0.212 sec/batch)
2017-05-03 18:57:19.344657: step 99260, loss = 0.0867, acc = 0.9780 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 18:57:28.344815: step 99270, loss = 0.0865, acc = 0.9720 (273.5 examples/sec; 0.234 sec/batch)
2017-05-03 18:57:37.366355: step 99280, loss = 0.0891, acc = 0.9760 (300.8 examples/sec; 0.213 sec/batch)
2017-05-03 18:57:46.428919: step 99290, loss = 0.1249, acc = 0.9600 (291.3 examples/sec; 0.220 sec/batch)
2017-05-03 18:57:55.638210: step 99300, loss = 0.1037, acc = 0.9700 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 18:58:04.693803: step 99310, loss = 0.1113, acc = 0.9620 (281.0 examples/sec; 0.228 sec/batch)
2017-05-03 18:58:13.758571: step 99320, loss = 0.1162, acc = 0.9700 (280.8 examples/sec; 0.228 sec/batch)
2017-05-03 18:58:22.881070: step 99330, loss = 0.1105, acc = 0.9660 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 18:58:32.177694: step 99340, loss = 0.1090, acc = 0.9700 (268.9 examples/sec; 0.238 sec/batch)
2017-05-03 18:58:41.179881: step 99350, loss = 0.1094, acc = 0.9700 (291.6 examples/sec; 0.219 sec/batch)
2017-05-03 18:58:50.238772: step 99360, loss = 0.1263, acc = 0.9620 (289.1 examples/sec; 0.221 sec/batch)
2017-05-03 18:58:59.350581: step 99370, loss = 0.1538, acc = 0.9500 (284.7 examples/sec; 0.225 sec/batch)
2017-05-03 18:59:08.370950: step 99380, loss = 0.1124, acc = 0.9720 (268.6 examples/sec; 0.238 sec/batch)
2017-05-03 18:59:17.374041: step 99390, loss = 0.1085, acc = 0.9660 (277.2 examples/sec; 0.231 sec/batch)
2017-05-03 18:59:26.484510: step 99400, loss = 0.0910, acc = 0.9780 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 18:59:35.384583: step 99410, loss = 0.1041, acc = 0.9680 (288.2 examples/sec; 0.222 sec/batch)
2017-05-03 18:59:44.428692: step 99420, loss = 0.1116, acc = 0.9700 (285.8 examples/sec; 0.224 sec/batch)
2017-05-03 18:59:53.424617: step 99430, loss = 0.1085, acc = 0.9680 (271.4 examples/sec; 0.236 sec/batch)
2017-05-03 19:00:02.564103: step 99440, loss = 0.1298, acc = 0.9700 (280.1 examples/sec; 0.228 sec/batch)
2017-05-03 19:00:11.558813: step 99450, loss = 0.1065, acc = 0.9720 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 19:00:20.762551: step 99460, loss = 0.1136, acc = 0.9720 (282.5 examples/sec; 0.227 sec/batch)
2017-05-03 19:00:29.667298: step 99470, loss = 0.1340, acc = 0.9540 (288.9 examples/sec; 0.222 sec/batch)
2017-05-03 19:00:38.664654: step 99480, loss = 0.1417, acc = 0.9620 (300.5 examples/sec; 0.213 sec/batch)
2017-05-03 19:00:47.715545: step 99490, loss = 0.1029, acc = 0.9700 (279.4 examples/sec; 0.229 sec/batch)
2017-05-03 19:00:56.627670: step 99500, loss = 0.1183, acc = 0.9640 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 19:01:05.644972: step 99510, loss = 0.0929, acc = 0.9740 (275.0 examples/sec; 0.233 sec/batch)
2017-05-03 19:01:14.656744: step 99520, loss = 0.1140, acc = 0.9620 (271.0 examples/sec; 0.236 sec/batch)
2017-05-03 19:01:23.542470: step 99530, loss = 0.1084, acc = 0.9680 (285.3 examples/sec; 0.224 sec/batch)
2017-05-03 19:01:32.662521: step 99540, loss = 0.1175, acc = 0.9680 (278.7 examples/sec; 0.230 sec/batch)
2017-05-03 19:01:41.769478: step 99550, loss = 0.0970, acc = 0.9720 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 19:01:50.964207: step 99560, loss = 0.1022, acc = 0.9720 (281.9 examples/sec; 0.227 sec/batch)
2017-05-03 19:01:59.992450: step 99570, loss = 0.1329, acc = 0.9540 (284.6 examples/sec; 0.225 sec/batch)
2017-05-03 19:02:08.985782: step 99580, loss = 0.1227, acc = 0.9680 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 19:02:17.962715: step 99590, loss = 0.1252, acc = 0.9620 (282.2 examples/sec; 0.227 sec/batch)
2017-05-03 19:02:26.837957: step 99600, loss = 0.1181, acc = 0.9680 (290.0 examples/sec; 0.221 sec/batch)
2017-05-03 19:02:35.952309: step 99610, loss = 0.0895, acc = 0.9780 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 19:02:44.975167: step 99620, loss = 0.1014, acc = 0.9680 (294.0 examples/sec; 0.218 sec/batch)
2017-05-03 19:02:54.002714: step 99630, loss = 0.0943, acc = 0.9760 (263.5 examples/sec; 0.243 sec/batch)
2017-05-03 19:03:03.071787: step 99640, loss = 0.1313, acc = 0.9600 (299.2 examples/sec; 0.214 sec/batch)
2017-05-03 19:03:12.210113: step 99650, loss = 0.1116, acc = 0.9600 (277.3 examples/sec; 0.231 sec/batch)
2017-05-03 19:03:21.203592: step 99660, loss = 0.0947, acc = 0.9740 (287.8 examples/sec; 0.222 sec/batch)
2017-05-03 19:03:30.456677: step 99670, loss = 0.1159, acc = 0.9540 (274.3 examples/sec; 0.233 sec/batch)
2017-05-03 19:03:39.651253: step 99680, loss = 0.0967, acc = 0.9740 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 19:03:48.617499: step 99690, loss = 0.1286, acc = 0.9580 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 19:03:57.639461: step 99700, loss = 0.0887, acc = 0.9760 (288.3 examples/sec; 0.222 sec/batch)
2017-05-03 19:04:06.571416: step 99710, loss = 0.1298, acc = 0.9580 (290.7 examples/sec; 0.220 sec/batch)
2017-05-03 19:04:15.779516: step 99720, loss = 0.1037, acc = 0.9680 (273.2 examples/sec; 0.234 sec/batch)
2017-05-03 19:04:24.742112: step 99730, loss = 0.1141, acc = 0.9600 (291.4 examples/sec; 0.220 sec/batch)
2017-05-03 19:04:33.834286: step 99740, loss = 0.1427, acc = 0.9500 (276.8 examples/sec; 0.231 sec/batch)
2017-05-03 19:04:42.849179: step 99750, loss = 0.0918, acc = 0.9700 (279.6 examples/sec; 0.229 sec/batch)
2017-05-03 19:04:51.909193: step 99760, loss = 0.0903, acc = 0.9720 (282.9 examples/sec; 0.226 sec/batch)
2017-05-03 19:05:00.814248: step 99770, loss = 0.0999, acc = 0.9700 (292.9 examples/sec; 0.218 sec/batch)
2017-05-03 19:05:09.808126: step 99780, loss = 0.0999, acc = 0.9740 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 19:05:18.680377: step 99790, loss = 0.1033, acc = 0.9720 (286.2 examples/sec; 0.224 sec/batch)
2017-05-03 19:05:28.600364: step 99800, loss = 0.1114, acc = 0.9740 (282.8 examples/sec; 0.226 sec/batch)
2017-05-03 19:05:37.567213: step 99810, loss = 0.1413, acc = 0.9480 (286.4 examples/sec; 0.223 sec/batch)
2017-05-03 19:05:46.749407: step 99820, loss = 0.1059, acc = 0.9680 (287.6 examples/sec; 0.223 sec/batch)
2017-05-03 19:05:55.839340: step 99830, loss = 0.1147, acc = 0.9660 (315.1 examples/sec; 0.203 sec/batch)
2017-05-03 19:06:04.910138: step 99840, loss = 0.1066, acc = 0.9720 (285.6 examples/sec; 0.224 sec/batch)
2017-05-03 19:06:14.089796: step 99850, loss = 0.1293, acc = 0.9680 (292.2 examples/sec; 0.219 sec/batch)
2017-05-03 19:06:23.018135: step 99860, loss = 0.0826, acc = 0.9820 (287.7 examples/sec; 0.222 sec/batch)
2017-05-03 19:06:32.336105: step 99870, loss = 0.1188, acc = 0.9680 (282.1 examples/sec; 0.227 sec/batch)
2017-05-03 19:06:41.418484: step 99880, loss = 0.1173, acc = 0.9620 (298.9 examples/sec; 0.214 sec/batch)
2017-05-03 19:06:50.555140: step 99890, loss = 0.1366, acc = 0.9520 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 19:06:59.767463: step 99900, loss = 0.0964, acc = 0.9780 (291.1 examples/sec; 0.220 sec/batch)
2017-05-03 19:07:08.861324: step 99910, loss = 0.1233, acc = 0.9600 (283.1 examples/sec; 0.226 sec/batch)
2017-05-03 19:07:17.981824: step 99920, loss = 0.0881, acc = 0.9740 (283.5 examples/sec; 0.226 sec/batch)
2017-05-03 19:07:27.163930: step 99930, loss = 0.1299, acc = 0.9620 (289.8 examples/sec; 0.221 sec/batch)
2017-05-03 19:07:36.210285: step 99940, loss = 0.1176, acc = 0.9660 (287.9 examples/sec; 0.222 sec/batch)
2017-05-03 19:07:45.337812: step 99950, loss = 0.1105, acc = 0.9640 (274.9 examples/sec; 0.233 sec/batch)
2017-05-03 19:07:54.272586: step 99960, loss = 0.1009, acc = 0.9740 (287.3 examples/sec; 0.223 sec/batch)
2017-05-03 19:08:03.331406: step 99970, loss = 0.0972, acc = 0.9720 (294.3 examples/sec; 0.217 sec/batch)
2017-05-03 19:08:12.355131: step 99980, loss = 0.1161, acc = 0.9660 (276.9 examples/sec; 0.231 sec/batch)
2017-05-03 19:08:21.412217: step 99990, loss = 0.1161, acc = 0.9560 (287.1 examples/sec; 0.223 sec/batch)
