train_1
True
The vocabulary size is: 2665791
[w2v..]
Elapsed: 39.4461669922
[loading trn..]
Elapsed: 350.754899979
[loading dev..]
Elapsed: 38.1441750526
[loading tst..]
Elapsed: 26.0502641201
w2v is trainable
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0_dev/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:0)
h_pool Tensor("tower_0_dev/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:0)
h_pool_flat Tensor("tower_0_dev/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:0)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0_tst/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:1)
h_pool Tensor("tower_0_tst/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:1)
h_pool_flat Tensor("tower_0_tst/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:1)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:0)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_0/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:0)
h_pool Tensor("tower_0/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:0)
h_pool_flat Tensor("tower_0/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:0)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:1)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_1/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:1)
h_pool Tensor("tower_1/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:1)
h_pool_flat Tensor("tower_1/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:1)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:2)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_2/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:2)
h_pool Tensor("tower_2/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:2)
h_pool_flat Tensor("tower_2/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:2)
filter_size 2
ksize [1, 1199, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-2/conv-maxpool-2:0", shape=(?, 1199, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 3
ksize [1, 1198, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-3/conv-maxpool-3:0", shape=(?, 1198, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 4
ksize [1, 1197, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-4/conv-maxpool-4:0", shape=(?, 1197, 1, 64), dtype=float32, device=/device:GPU:3)
filter_size 5
ksize [1, 1196, 1, 1]
conv_out Tensor("tower_3/conv-maxpool-5/conv-maxpool-5:0", shape=(?, 1196, 1, 64), dtype=float32, device=/device:GPU:3)
h_pool Tensor("tower_3/concat_1:0", shape=(?, 1, 1, 256), dtype=float32, device=/device:GPU:3)
h_pool_flat Tensor("tower_3/Reshape:0", shape=(?, 256), dtype=float32, device=/device:GPU:3)
2017-05-09 01:21:16.104402: step 20, loss = 1.0774, acc = 0.4940 (16.6 examples/sec; 3.861 sec/batch)
2017-05-09 01:22:31.449317: step 40, loss = 0.9227, acc = 0.5700 (17.2 examples/sec; 3.724 sec/batch)
2017-05-09 01:23:45.805325: step 60, loss = 0.7846, acc = 0.7420 (17.4 examples/sec; 3.676 sec/batch)
2017-05-09 01:24:58.959600: step 80, loss = 0.6357, acc = 0.8660 (18.0 examples/sec; 3.548 sec/batch)
2017-05-09 01:26:13.029495: step 100, loss = 0.5620, acc = 0.8480 (17.0 examples/sec; 3.756 sec/batch)
2017-05-09 01:27:28.167706: step 120, loss = 0.4439, acc = 0.9080 (17.1 examples/sec; 3.732 sec/batch)
2017-05-09 01:28:44.646147: step 140, loss = 0.3913, acc = 0.9020 (17.1 examples/sec; 3.739 sec/batch)
2017-05-09 01:30:00.118973: step 160, loss = 0.3550, acc = 0.9120 (16.9 examples/sec; 3.788 sec/batch)
2017-05-09 01:31:15.377384: step 180, loss = 0.3500, acc = 0.9080 (16.6 examples/sec; 3.846 sec/batch)
2017-05-09 01:32:31.592748: step 200, loss = 0.2931, acc = 0.9240 (16.5 examples/sec; 3.870 sec/batch)
2017-05-09 01:33:48.332499: step 220, loss = 0.3315, acc = 0.8880 (16.7 examples/sec; 3.838 sec/batch)
2017-05-09 01:35:01.786984: step 240, loss = 0.2649, acc = 0.9320 (17.9 examples/sec; 3.577 sec/batch)
2017-05-09 01:36:15.869330: step 260, loss = 0.2492, acc = 0.9320 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 01:37:33.186207: step 280, loss = 0.2586, acc = 0.9260 (16.7 examples/sec; 3.830 sec/batch)
2017-05-09 01:38:49.186827: step 300, loss = 0.2432, acc = 0.9220 (16.9 examples/sec; 3.776 sec/batch)
2017-05-09 01:40:05.019573: step 320, loss = 0.2383, acc = 0.9260 (17.1 examples/sec; 3.739 sec/batch)
2017-05-09 01:41:19.484804: step 340, loss = 0.2211, acc = 0.9300 (16.5 examples/sec; 3.874 sec/batch)
2017-05-09 01:42:34.982551: step 360, loss = 0.2471, acc = 0.9180 (17.4 examples/sec; 3.683 sec/batch)
2017-05-09 01:43:50.814941: step 380, loss = 0.2061, acc = 0.9340 (16.3 examples/sec; 3.923 sec/batch)
2017-05-09 01:45:05.264875: step 400, loss = 0.2376, acc = 0.9240 (17.2 examples/sec; 3.727 sec/batch)
2017-05-09 01:46:21.880815: step 420, loss = 0.2302, acc = 0.9240 (16.6 examples/sec; 3.845 sec/batch)
2017-05-09 01:47:37.145846: step 440, loss = 0.1837, acc = 0.9380 (17.0 examples/sec; 3.762 sec/batch)
2017-05-09 01:48:52.463829: step 460, loss = 0.2293, acc = 0.9180 (17.7 examples/sec; 3.626 sec/batch)
2017-05-09 01:50:07.520015: step 480, loss = 0.2294, acc = 0.9200 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 01:51:22.104675: step 500, loss = 0.1911, acc = 0.9320 (17.8 examples/sec; 3.601 sec/batch)
2017-05-09 01:52:39.288599: step 520, loss = 0.2130, acc = 0.9300 (16.6 examples/sec; 3.847 sec/batch)
2017-05-09 01:53:53.922665: step 540, loss = 0.2096, acc = 0.9320 (16.7 examples/sec; 3.823 sec/batch)
2017-05-09 01:55:09.838016: step 560, loss = 0.2195, acc = 0.9200 (16.8 examples/sec; 3.811 sec/batch)
2017-05-09 01:56:25.992526: step 580, loss = 0.2006, acc = 0.9420 (16.9 examples/sec; 3.778 sec/batch)
2017-05-09 01:57:42.028008: step 600, loss = 0.1739, acc = 0.9400 (17.3 examples/sec; 3.699 sec/batch)
2017-05-09 01:58:58.145008: step 620, loss = 0.1706, acc = 0.9400 (16.8 examples/sec; 3.809 sec/batch)
2017-05-09 02:00:13.014733: step 640, loss = 0.2051, acc = 0.9260 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 02:01:29.027377: step 660, loss = 0.1768, acc = 0.9480 (16.5 examples/sec; 3.881 sec/batch)
2017-05-09 02:02:43.807420: step 680, loss = 0.1504, acc = 0.9560 (16.9 examples/sec; 3.796 sec/batch)
2017-05-09 02:03:59.871323: step 700, loss = 0.1733, acc = 0.9440 (16.8 examples/sec; 3.803 sec/batch)
2017-05-09 02:05:16.190531: step 720, loss = 0.1593, acc = 0.9600 (17.3 examples/sec; 3.695 sec/batch)
2017-05-09 02:06:32.976212: step 740, loss = 0.1645, acc = 0.9460 (17.1 examples/sec; 3.738 sec/batch)
2017-05-09 02:07:47.329386: step 760, loss = 0.1557, acc = 0.9440 (17.9 examples/sec; 3.568 sec/batch)
2017-05-09 02:09:03.107354: step 780, loss = 0.1905, acc = 0.9380 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 02:10:18.499154: step 800, loss = 0.1516, acc = 0.9500 (16.7 examples/sec; 3.843 sec/batch)
2017-05-09 02:11:33.445674: step 820, loss = 0.1814, acc = 0.9420 (17.2 examples/sec; 3.720 sec/batch)
2017-05-09 02:12:48.531854: step 840, loss = 0.2052, acc = 0.9220 (17.7 examples/sec; 3.618 sec/batch)
2017-05-09 02:14:03.569801: step 860, loss = 0.1398, acc = 0.9560 (16.8 examples/sec; 3.801 sec/batch)
2017-05-09 02:15:19.455915: step 880, loss = 0.1429, acc = 0.9580 (16.6 examples/sec; 3.857 sec/batch)
2017-05-09 02:16:34.606735: step 900, loss = 0.1615, acc = 0.9480 (16.6 examples/sec; 3.850 sec/batch)
2017-05-09 02:17:50.515615: step 920, loss = 0.1694, acc = 0.9360 (16.9 examples/sec; 3.781 sec/batch)
2017-05-09 02:19:06.962252: step 940, loss = 0.1694, acc = 0.9520 (16.6 examples/sec; 3.854 sec/batch)
2017-05-09 02:20:22.262218: step 960, loss = 0.1436, acc = 0.9560 (17.3 examples/sec; 3.699 sec/batch)
2017-05-09 02:21:37.699628: step 980, loss = 0.1562, acc = 0.9460 (17.1 examples/sec; 3.744 sec/batch)
2017-05-09 02:22:53.121881: step 1000, loss = 0.1953, acc = 0.9300 (17.1 examples/sec; 3.751 sec/batch)
[Eval] 2017-05-09 02:23:09.466466: step 1000, acc = 0.9596, f1 = 0.9583
[Test] 2017-05-09 02:23:20.405548: step 1000, acc = 0.9491, f1 = 0.9486
[Status] 2017-05-09 02:23:20.405679: step 1000, maxindex = 1000, maxdev = 0.9596, maxtst = 0.9491
2017-05-09 02:24:44.028238: step 1020, loss = 0.1428, acc = 0.9500 (17.2 examples/sec; 3.721 sec/batch)
2017-05-09 02:26:00.071491: step 1040, loss = 0.1891, acc = 0.9360 (16.7 examples/sec; 3.832 sec/batch)
2017-05-09 02:27:15.519311: step 1060, loss = 0.1623, acc = 0.9480 (16.7 examples/sec; 3.839 sec/batch)
2017-05-09 02:28:30.998624: step 1080, loss = 0.1567, acc = 0.9520 (16.6 examples/sec; 3.852 sec/batch)
2017-05-09 02:29:44.227747: step 1100, loss = 0.1369, acc = 0.9580 (16.5 examples/sec; 3.877 sec/batch)
2017-05-09 02:30:57.706763: step 1120, loss = 0.1552, acc = 0.9440 (16.7 examples/sec; 3.843 sec/batch)
2017-05-09 02:32:11.740217: step 1140, loss = 0.1412, acc = 0.9460 (17.5 examples/sec; 3.665 sec/batch)
2017-05-09 02:33:23.883326: step 1160, loss = 0.1863, acc = 0.9360 (18.5 examples/sec; 3.469 sec/batch)
2017-05-09 02:34:39.006640: step 1180, loss = 0.1481, acc = 0.9580 (17.4 examples/sec; 3.674 sec/batch)
2017-05-09 02:35:54.664959: step 1200, loss = 0.1680, acc = 0.9480 (17.0 examples/sec; 3.776 sec/batch)
2017-05-09 02:37:09.477632: step 1220, loss = 0.1548, acc = 0.9480 (17.1 examples/sec; 3.753 sec/batch)
2017-05-09 02:38:23.047238: step 1240, loss = 0.1578, acc = 0.9560 (17.1 examples/sec; 3.742 sec/batch)
2017-05-09 02:39:38.206287: step 1260, loss = 0.1442, acc = 0.9500 (16.8 examples/sec; 3.799 sec/batch)
2017-05-09 02:40:53.509084: step 1280, loss = 0.1362, acc = 0.9600 (16.7 examples/sec; 3.828 sec/batch)
2017-05-09 02:42:06.169370: step 1300, loss = 0.1284, acc = 0.9520 (16.7 examples/sec; 3.830 sec/batch)
2017-05-09 02:43:22.320275: step 1320, loss = 0.1320, acc = 0.9640 (16.7 examples/sec; 3.822 sec/batch)
2017-05-09 02:44:38.512833: step 1340, loss = 0.1123, acc = 0.9620 (16.8 examples/sec; 3.800 sec/batch)
2017-05-09 02:45:54.691159: step 1360, loss = 0.1009, acc = 0.9740 (16.8 examples/sec; 3.807 sec/batch)
2017-05-09 02:47:07.722975: step 1380, loss = 0.1225, acc = 0.9540 (17.2 examples/sec; 3.722 sec/batch)
2017-05-09 02:48:23.329026: step 1400, loss = 0.1496, acc = 0.9460 (17.5 examples/sec; 3.668 sec/batch)
2017-05-09 02:49:38.176980: step 1420, loss = 0.1165, acc = 0.9660 (17.8 examples/sec; 3.591 sec/batch)
2017-05-09 02:50:52.859023: step 1440, loss = 0.1498, acc = 0.9440 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 02:52:09.325523: step 1460, loss = 0.0950, acc = 0.9800 (17.1 examples/sec; 3.749 sec/batch)
2017-05-09 02:53:23.406444: step 1480, loss = 0.1342, acc = 0.9560 (16.9 examples/sec; 3.788 sec/batch)
2017-05-09 02:54:38.064907: step 1500, loss = 0.1603, acc = 0.9500 (17.1 examples/sec; 3.749 sec/batch)
2017-05-09 02:55:51.715394: step 1520, loss = 0.1325, acc = 0.9500 (17.2 examples/sec; 3.721 sec/batch)
2017-05-09 02:57:07.519278: step 1540, loss = 0.1205, acc = 0.9680 (17.3 examples/sec; 3.699 sec/batch)
2017-05-09 02:58:22.012824: step 1560, loss = 0.1384, acc = 0.9600 (16.6 examples/sec; 3.861 sec/batch)
2017-05-09 02:59:37.257972: step 1580, loss = 0.1231, acc = 0.9600 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 03:00:52.606646: step 1600, loss = 0.1190, acc = 0.9680 (16.9 examples/sec; 3.789 sec/batch)
2017-05-09 03:02:07.291646: step 1620, loss = 0.1110, acc = 0.9620 (16.7 examples/sec; 3.840 sec/batch)
2017-05-09 03:03:22.822689: step 1640, loss = 0.1045, acc = 0.9680 (17.2 examples/sec; 3.728 sec/batch)
2017-05-09 03:04:35.540082: step 1660, loss = 0.1056, acc = 0.9660 (17.5 examples/sec; 3.667 sec/batch)
2017-05-09 03:05:52.526709: step 1680, loss = 0.1632, acc = 0.9540 (16.5 examples/sec; 3.871 sec/batch)
2017-05-09 03:07:06.074371: step 1700, loss = 0.1250, acc = 0.9680 (17.4 examples/sec; 3.672 sec/batch)
2017-05-09 03:08:20.024307: step 1720, loss = 0.0953, acc = 0.9740 (16.9 examples/sec; 3.777 sec/batch)
2017-05-09 03:09:34.247526: step 1740, loss = 0.1278, acc = 0.9580 (17.2 examples/sec; 3.718 sec/batch)
2017-05-09 03:10:48.783486: step 1760, loss = 0.0931, acc = 0.9720 (17.0 examples/sec; 3.772 sec/batch)
2017-05-09 03:12:03.971959: step 1780, loss = 0.1059, acc = 0.9640 (17.0 examples/sec; 3.767 sec/batch)
2017-05-09 03:13:17.605506: step 1800, loss = 0.1026, acc = 0.9720 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 03:14:32.103008: step 1820, loss = 0.1020, acc = 0.9700 (16.4 examples/sec; 3.891 sec/batch)
2017-05-09 03:15:45.008934: step 1840, loss = 0.0836, acc = 0.9780 (17.3 examples/sec; 3.705 sec/batch)
2017-05-09 03:17:00.810342: step 1860, loss = 0.1349, acc = 0.9480 (16.7 examples/sec; 3.835 sec/batch)
2017-05-09 03:18:13.640396: step 1880, loss = 0.1373, acc = 0.9460 (16.9 examples/sec; 3.794 sec/batch)
2017-05-09 03:19:28.704804: step 1900, loss = 0.0894, acc = 0.9720 (16.6 examples/sec; 3.860 sec/batch)
2017-05-09 03:20:42.428923: step 1920, loss = 0.1064, acc = 0.9620 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 03:21:55.813672: step 1940, loss = 0.1429, acc = 0.9540 (16.8 examples/sec; 3.816 sec/batch)
2017-05-09 03:23:10.914008: step 1960, loss = 0.1161, acc = 0.9680 (16.7 examples/sec; 3.825 sec/batch)
2017-05-09 03:24:26.338818: step 1980, loss = 0.1197, acc = 0.9620 (16.9 examples/sec; 3.784 sec/batch)
2017-05-09 03:25:39.529708: step 2000, loss = 0.0811, acc = 0.9860 (19.1 examples/sec; 3.354 sec/batch)
[Eval] 2017-05-09 03:25:55.658112: step 2000, acc = 0.9637, f1 = 0.9625
[Test] 2017-05-09 03:26:06.634034: step 2000, acc = 0.9536, f1 = 0.9532
[Status] 2017-05-09 03:26:06.634143: step 2000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 03:27:31.369014: step 2020, loss = 0.0969, acc = 0.9760 (17.7 examples/sec; 3.615 sec/batch)
2017-05-09 03:28:45.615297: step 2040, loss = 0.1109, acc = 0.9600 (16.8 examples/sec; 3.814 sec/batch)
2017-05-09 03:29:59.067888: step 2060, loss = 0.1016, acc = 0.9740 (18.5 examples/sec; 3.468 sec/batch)
2017-05-09 03:31:14.289399: step 2080, loss = 0.0869, acc = 0.9820 (16.4 examples/sec; 3.908 sec/batch)
2017-05-09 03:32:29.943689: step 2100, loss = 0.0946, acc = 0.9700 (16.7 examples/sec; 3.839 sec/batch)
2017-05-09 03:33:44.275624: step 2120, loss = 0.0902, acc = 0.9720 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 03:34:58.160623: step 2140, loss = 0.0697, acc = 0.9860 (16.7 examples/sec; 3.824 sec/batch)
2017-05-09 03:36:11.990305: step 2160, loss = 0.1025, acc = 0.9800 (17.8 examples/sec; 3.595 sec/batch)
2017-05-09 03:37:26.515338: step 2180, loss = 0.0966, acc = 0.9760 (17.6 examples/sec; 3.637 sec/batch)
2017-05-09 03:38:38.925254: step 2200, loss = 0.0932, acc = 0.9800 (17.9 examples/sec; 3.583 sec/batch)
2017-05-09 03:39:51.963844: step 2220, loss = 0.1065, acc = 0.9580 (17.2 examples/sec; 3.723 sec/batch)
2017-05-09 03:41:05.791329: step 2240, loss = 0.0937, acc = 0.9820 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 03:42:21.353630: step 2260, loss = 0.0941, acc = 0.9700 (16.6 examples/sec; 3.863 sec/batch)
2017-05-09 03:43:34.606219: step 2280, loss = 0.0821, acc = 0.9780 (17.6 examples/sec; 3.640 sec/batch)
2017-05-09 03:44:47.898753: step 2300, loss = 0.1087, acc = 0.9700 (17.4 examples/sec; 3.668 sec/batch)
2017-05-09 03:46:02.176373: step 2320, loss = 0.0900, acc = 0.9760 (17.0 examples/sec; 3.762 sec/batch)
2017-05-09 03:47:14.929869: step 2340, loss = 0.0929, acc = 0.9720 (18.7 examples/sec; 3.426 sec/batch)
2017-05-09 03:48:29.316121: step 2360, loss = 0.0811, acc = 0.9780 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 03:49:43.816901: step 2380, loss = 0.1282, acc = 0.9620 (17.1 examples/sec; 3.735 sec/batch)
2017-05-09 03:50:59.621814: step 2400, loss = 0.0985, acc = 0.9640 (16.9 examples/sec; 3.797 sec/batch)
2017-05-09 03:52:14.212588: step 2420, loss = 0.1270, acc = 0.9580 (16.9 examples/sec; 3.794 sec/batch)
2017-05-09 03:53:28.438265: step 2440, loss = 0.0723, acc = 0.9800 (17.3 examples/sec; 3.697 sec/batch)
2017-05-09 03:54:43.065214: step 2460, loss = 0.0908, acc = 0.9660 (17.1 examples/sec; 3.732 sec/batch)
2017-05-09 03:55:55.266412: step 2480, loss = 0.0809, acc = 0.9780 (19.3 examples/sec; 3.320 sec/batch)
2017-05-09 03:57:06.799488: step 2500, loss = 0.1062, acc = 0.9700 (17.8 examples/sec; 3.593 sec/batch)
2017-05-09 03:58:21.823070: step 2520, loss = 0.0935, acc = 0.9740 (16.8 examples/sec; 3.805 sec/batch)
2017-05-09 03:59:36.136186: step 2540, loss = 0.0996, acc = 0.9740 (17.1 examples/sec; 3.732 sec/batch)
2017-05-09 04:00:49.139802: step 2560, loss = 0.0821, acc = 0.9820 (17.0 examples/sec; 3.755 sec/batch)
2017-05-09 04:02:04.312257: step 2580, loss = 0.0914, acc = 0.9700 (17.4 examples/sec; 3.684 sec/batch)
2017-05-09 04:03:19.766684: step 2600, loss = 0.0943, acc = 0.9780 (16.7 examples/sec; 3.838 sec/batch)
2017-05-09 04:04:32.277785: step 2620, loss = 0.1206, acc = 0.9620 (17.9 examples/sec; 3.584 sec/batch)
2017-05-09 04:05:47.558406: step 2640, loss = 0.0918, acc = 0.9700 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 04:07:02.425681: step 2660, loss = 0.1170, acc = 0.9660 (17.8 examples/sec; 3.595 sec/batch)
2017-05-09 04:08:17.889545: step 2680, loss = 0.1291, acc = 0.9440 (17.6 examples/sec; 3.633 sec/batch)
2017-05-09 04:09:30.686834: step 2700, loss = 0.0773, acc = 0.9820 (16.4 examples/sec; 3.900 sec/batch)
2017-05-09 04:10:44.779723: step 2720, loss = 0.1142, acc = 0.9700 (17.4 examples/sec; 3.668 sec/batch)
2017-05-09 04:11:59.324878: step 2740, loss = 0.1315, acc = 0.9540 (17.0 examples/sec; 3.761 sec/batch)
2017-05-09 04:13:13.243108: step 2760, loss = 0.1084, acc = 0.9600 (17.3 examples/sec; 3.699 sec/batch)
2017-05-09 04:14:29.571661: step 2780, loss = 0.0796, acc = 0.9740 (17.0 examples/sec; 3.772 sec/batch)
2017-05-09 04:15:45.110011: step 2800, loss = 0.0893, acc = 0.9740 (17.9 examples/sec; 3.573 sec/batch)
2017-05-09 04:16:57.298549: step 2820, loss = 0.1172, acc = 0.9620 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 04:18:12.113552: step 2840, loss = 0.0867, acc = 0.9740 (16.9 examples/sec; 3.784 sec/batch)
2017-05-09 04:19:27.122117: step 2860, loss = 0.0906, acc = 0.9700 (17.2 examples/sec; 3.726 sec/batch)
2017-05-09 04:20:42.397665: step 2880, loss = 0.0974, acc = 0.9720 (16.7 examples/sec; 3.823 sec/batch)
2017-05-09 04:21:54.945779: step 2900, loss = 0.0945, acc = 0.9700 (17.4 examples/sec; 3.674 sec/batch)
2017-05-09 04:23:08.222232: step 2920, loss = 0.1128, acc = 0.9640 (18.2 examples/sec; 3.523 sec/batch)
2017-05-09 04:24:21.636683: step 2940, loss = 0.1017, acc = 0.9760 (17.0 examples/sec; 3.763 sec/batch)
2017-05-09 04:25:34.538236: step 2960, loss = 0.1288, acc = 0.9600 (17.6 examples/sec; 3.635 sec/batch)
2017-05-09 04:26:46.967395: step 2980, loss = 0.0823, acc = 0.9800 (17.6 examples/sec; 3.637 sec/batch)
2017-05-09 04:28:01.749046: step 3000, loss = 0.0967, acc = 0.9640 (16.9 examples/sec; 3.777 sec/batch)
[Eval] 2017-05-09 04:28:17.950399: step 3000, acc = 0.9603, f1 = 0.9591
[Test] 2017-05-09 04:28:28.873768: step 3000, acc = 0.9499, f1 = 0.9495
[Status] 2017-05-09 04:28:28.873835: step 3000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 04:29:43.195417: step 3020, loss = 0.0930, acc = 0.9620 (15.9 examples/sec; 4.025 sec/batch)
2017-05-09 04:30:55.430168: step 3040, loss = 0.0772, acc = 0.9860 (19.1 examples/sec; 3.355 sec/batch)
2017-05-09 04:32:09.942182: step 3060, loss = 0.0719, acc = 0.9780 (16.8 examples/sec; 3.801 sec/batch)
2017-05-09 04:33:23.862157: step 3080, loss = 0.0614, acc = 0.9900 (18.0 examples/sec; 3.558 sec/batch)
2017-05-09 04:34:38.143548: step 3100, loss = 0.0745, acc = 0.9820 (18.1 examples/sec; 3.538 sec/batch)
2017-05-09 04:35:52.769989: step 3120, loss = 0.0524, acc = 0.9920 (16.7 examples/sec; 3.839 sec/batch)
2017-05-09 04:37:07.035137: step 3140, loss = 0.0702, acc = 0.9820 (16.8 examples/sec; 3.801 sec/batch)
2017-05-09 04:38:22.379793: step 3160, loss = 0.0492, acc = 0.9900 (17.7 examples/sec; 3.610 sec/batch)
2017-05-09 04:39:36.437132: step 3180, loss = 0.0808, acc = 0.9740 (16.6 examples/sec; 3.851 sec/batch)
2017-05-09 04:40:51.762943: step 3200, loss = 0.0701, acc = 0.9880 (16.7 examples/sec; 3.833 sec/batch)
2017-05-09 04:42:07.132031: step 3220, loss = 0.0639, acc = 0.9800 (17.6 examples/sec; 3.646 sec/batch)
2017-05-09 04:43:21.657794: step 3240, loss = 0.0646, acc = 0.9860 (18.4 examples/sec; 3.473 sec/batch)
2017-05-09 04:44:35.629537: step 3260, loss = 0.0744, acc = 0.9840 (16.9 examples/sec; 3.790 sec/batch)
2017-05-09 04:45:51.748512: step 3280, loss = 0.0605, acc = 0.9880 (16.7 examples/sec; 3.822 sec/batch)
2017-05-09 04:47:05.528680: step 3300, loss = 0.0652, acc = 0.9860 (16.7 examples/sec; 3.832 sec/batch)
2017-05-09 04:48:19.488884: step 3320, loss = 0.0644, acc = 0.9860 (16.7 examples/sec; 3.821 sec/batch)
2017-05-09 04:49:32.268990: step 3340, loss = 0.0697, acc = 0.9840 (19.0 examples/sec; 3.372 sec/batch)
2017-05-09 04:50:46.905932: step 3360, loss = 0.0542, acc = 0.9860 (17.0 examples/sec; 3.762 sec/batch)
2017-05-09 04:52:02.261001: step 3380, loss = 0.0591, acc = 0.9880 (17.9 examples/sec; 3.570 sec/batch)
2017-05-09 04:53:17.853960: step 3400, loss = 0.0670, acc = 0.9840 (17.0 examples/sec; 3.769 sec/batch)
2017-05-09 04:54:32.743885: step 3420, loss = 0.0577, acc = 0.9860 (16.6 examples/sec; 3.853 sec/batch)
2017-05-09 04:55:47.728821: step 3440, loss = 0.0638, acc = 0.9920 (17.3 examples/sec; 3.706 sec/batch)
2017-05-09 04:57:00.518740: step 3460, loss = 0.0721, acc = 0.9800 (17.5 examples/sec; 3.666 sec/batch)
2017-05-09 04:58:15.283914: step 3480, loss = 0.0724, acc = 0.9740 (17.0 examples/sec; 3.766 sec/batch)
2017-05-09 04:59:29.072722: step 3500, loss = 0.0802, acc = 0.9780 (17.0 examples/sec; 3.769 sec/batch)
2017-05-09 05:00:43.584162: step 3520, loss = 0.0667, acc = 0.9820 (17.4 examples/sec; 3.671 sec/batch)
2017-05-09 05:01:57.143278: step 3540, loss = 0.0710, acc = 0.9820 (18.6 examples/sec; 3.444 sec/batch)
2017-05-09 05:03:10.568277: step 3560, loss = 0.0610, acc = 0.9800 (17.3 examples/sec; 3.703 sec/batch)
2017-05-09 05:04:24.104532: step 3580, loss = 0.0680, acc = 0.9820 (16.8 examples/sec; 3.804 sec/batch)
2017-05-09 05:05:35.827142: step 3600, loss = 0.0761, acc = 0.9820 (18.4 examples/sec; 3.486 sec/batch)
2017-05-09 05:06:50.164950: step 3620, loss = 0.0682, acc = 0.9800 (16.7 examples/sec; 3.839 sec/batch)
2017-05-09 05:08:02.067520: step 3640, loss = 0.0612, acc = 0.9820 (17.9 examples/sec; 3.566 sec/batch)
2017-05-09 05:09:16.230350: step 3660, loss = 0.0701, acc = 0.9840 (17.3 examples/sec; 3.699 sec/batch)
2017-05-09 05:10:29.102369: step 3680, loss = 0.0814, acc = 0.9820 (18.0 examples/sec; 3.553 sec/batch)
2017-05-09 05:11:44.402536: step 3700, loss = 0.0585, acc = 0.9880 (17.5 examples/sec; 3.651 sec/batch)
2017-05-09 05:13:00.170258: step 3720, loss = 0.0772, acc = 0.9820 (16.9 examples/sec; 3.794 sec/batch)
2017-05-09 05:14:12.640689: step 3740, loss = 0.0899, acc = 0.9720 (17.9 examples/sec; 3.571 sec/batch)
2017-05-09 05:15:27.174697: step 3760, loss = 0.0687, acc = 0.9760 (17.4 examples/sec; 3.679 sec/batch)
2017-05-09 05:16:42.027395: step 3780, loss = 0.1084, acc = 0.9680 (17.5 examples/sec; 3.666 sec/batch)
2017-05-09 05:17:54.570394: step 3800, loss = 0.0776, acc = 0.9780 (17.7 examples/sec; 3.610 sec/batch)
2017-05-09 05:19:09.201367: step 3820, loss = 0.0641, acc = 0.9860 (17.1 examples/sec; 3.752 sec/batch)
2017-05-09 05:20:24.112789: step 3840, loss = 0.0726, acc = 0.9780 (17.2 examples/sec; 3.714 sec/batch)
2017-05-09 05:21:39.157763: step 3860, loss = 0.0708, acc = 0.9820 (17.4 examples/sec; 3.687 sec/batch)
2017-05-09 05:22:53.095479: step 3880, loss = 0.0842, acc = 0.9760 (16.9 examples/sec; 3.795 sec/batch)
2017-05-09 05:24:07.224127: step 3900, loss = 0.0717, acc = 0.9800 (16.7 examples/sec; 3.829 sec/batch)
2017-05-09 05:25:21.608804: step 3920, loss = 0.0731, acc = 0.9780 (17.2 examples/sec; 3.716 sec/batch)
2017-05-09 05:26:33.468470: step 3940, loss = 0.0882, acc = 0.9760 (16.9 examples/sec; 3.777 sec/batch)
2017-05-09 05:27:46.969136: step 3960, loss = 0.0801, acc = 0.9760 (16.6 examples/sec; 3.847 sec/batch)
2017-05-09 05:29:00.139901: step 3980, loss = 0.0590, acc = 0.9780 (18.2 examples/sec; 3.521 sec/batch)
2017-05-09 05:30:14.711166: step 4000, loss = 0.0858, acc = 0.9780 (16.9 examples/sec; 3.786 sec/batch)
[Eval] 2017-05-09 05:30:31.102173: step 4000, acc = 0.9625, f1 = 0.9613
[Test] 2017-05-09 05:30:42.367105: step 4000, acc = 0.9528, f1 = 0.9524
[Status] 2017-05-09 05:30:42.367191: step 4000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 05:31:56.628443: step 4020, loss = 0.0842, acc = 0.9680 (18.1 examples/sec; 3.528 sec/batch)
2017-05-09 05:33:11.903796: step 4040, loss = 0.0386, acc = 0.9980 (17.3 examples/sec; 3.691 sec/batch)
2017-05-09 05:34:26.000568: step 4060, loss = 0.0531, acc = 0.9820 (17.0 examples/sec; 3.773 sec/batch)
2017-05-09 05:35:39.428423: step 4080, loss = 0.0586, acc = 0.9880 (18.3 examples/sec; 3.503 sec/batch)
2017-05-09 05:36:55.719608: step 4100, loss = 0.0424, acc = 0.9880 (16.8 examples/sec; 3.815 sec/batch)
2017-05-09 05:38:10.122667: step 4120, loss = 0.0573, acc = 0.9840 (17.6 examples/sec; 3.644 sec/batch)
2017-05-09 05:39:22.373243: step 4140, loss = 0.0690, acc = 0.9920 (17.8 examples/sec; 3.605 sec/batch)
2017-05-09 05:40:35.451379: step 4160, loss = 0.0504, acc = 0.9920 (17.1 examples/sec; 3.750 sec/batch)
2017-05-09 05:41:48.816826: step 4180, loss = 0.0640, acc = 0.9860 (17.5 examples/sec; 3.656 sec/batch)
2017-05-09 05:43:03.093023: step 4200, loss = 0.0443, acc = 0.9960 (17.6 examples/sec; 3.639 sec/batch)
2017-05-09 05:44:17.856445: step 4220, loss = 0.0380, acc = 0.9940 (16.9 examples/sec; 3.784 sec/batch)
2017-05-09 05:45:31.388819: step 4240, loss = 0.0578, acc = 0.9820 (17.4 examples/sec; 3.688 sec/batch)
2017-05-09 05:46:45.858834: step 4260, loss = 0.0425, acc = 0.9920 (17.6 examples/sec; 3.635 sec/batch)
2017-05-09 05:47:58.828104: step 4280, loss = 0.0602, acc = 0.9900 (17.2 examples/sec; 3.728 sec/batch)
2017-05-09 05:49:13.161384: step 4300, loss = 0.0740, acc = 0.9780 (17.3 examples/sec; 3.696 sec/batch)
2017-05-09 05:50:29.214095: step 4320, loss = 0.0492, acc = 0.9900 (16.7 examples/sec; 3.839 sec/batch)
2017-05-09 05:51:44.035209: step 4340, loss = 0.0498, acc = 0.9900 (17.3 examples/sec; 3.702 sec/batch)
2017-05-09 05:52:57.807064: step 4360, loss = 0.0602, acc = 0.9860 (16.8 examples/sec; 3.814 sec/batch)
2017-05-09 05:54:12.645965: step 4380, loss = 0.0627, acc = 0.9880 (17.2 examples/sec; 3.727 sec/batch)
2017-05-09 05:55:27.114241: step 4400, loss = 0.0463, acc = 0.9940 (18.1 examples/sec; 3.542 sec/batch)
2017-05-09 05:56:40.207003: step 4420, loss = 0.0590, acc = 0.9900 (16.9 examples/sec; 3.782 sec/batch)
2017-05-09 05:57:53.691976: step 4440, loss = 0.0611, acc = 0.9900 (18.0 examples/sec; 3.554 sec/batch)
2017-05-09 05:59:07.941196: step 4460, loss = 0.0629, acc = 0.9860 (17.7 examples/sec; 3.617 sec/batch)
2017-05-09 06:00:23.069757: step 4480, loss = 0.0559, acc = 0.9860 (17.8 examples/sec; 3.597 sec/batch)
2017-05-09 06:01:34.403919: step 4500, loss = 0.0615, acc = 0.9840 (17.5 examples/sec; 3.654 sec/batch)
2017-05-09 06:02:49.476804: step 4520, loss = 0.0445, acc = 0.9940 (16.7 examples/sec; 3.831 sec/batch)
2017-05-09 06:04:03.627868: step 4540, loss = 0.0656, acc = 0.9800 (17.3 examples/sec; 3.693 sec/batch)
2017-05-09 06:05:16.423468: step 4560, loss = 0.0541, acc = 0.9880 (16.8 examples/sec; 3.813 sec/batch)
2017-05-09 06:06:32.088277: step 4580, loss = 0.0566, acc = 0.9840 (17.0 examples/sec; 3.755 sec/batch)
2017-05-09 06:07:46.486710: step 4600, loss = 0.0741, acc = 0.9800 (17.2 examples/sec; 3.730 sec/batch)
2017-05-09 06:08:59.827209: step 4620, loss = 0.0553, acc = 0.9860 (18.0 examples/sec; 3.559 sec/batch)
2017-05-09 06:10:13.423489: step 4640, loss = 0.0549, acc = 0.9840 (17.4 examples/sec; 3.686 sec/batch)
2017-05-09 06:11:28.159885: step 4660, loss = 0.0570, acc = 0.9880 (17.3 examples/sec; 3.694 sec/batch)
2017-05-09 06:12:44.981497: step 4680, loss = 0.0443, acc = 0.9940 (16.6 examples/sec; 3.863 sec/batch)
2017-05-09 06:13:57.428358: step 4700, loss = 0.0653, acc = 0.9860 (17.0 examples/sec; 3.774 sec/batch)
2017-05-09 06:15:11.909264: step 4720, loss = 0.0621, acc = 0.9920 (16.9 examples/sec; 3.776 sec/batch)
2017-05-09 06:16:23.953625: step 4740, loss = 0.0516, acc = 0.9860 (18.2 examples/sec; 3.521 sec/batch)
2017-05-09 06:17:37.399880: step 4760, loss = 0.0595, acc = 0.9820 (18.5 examples/sec; 3.453 sec/batch)
2017-05-09 06:18:49.956527: step 4780, loss = 0.0786, acc = 0.9820 (19.0 examples/sec; 3.365 sec/batch)
2017-05-09 06:20:06.302079: step 4800, loss = 0.0415, acc = 0.9920 (16.6 examples/sec; 3.844 sec/batch)
2017-05-09 06:21:20.100747: step 4820, loss = 0.0565, acc = 0.9820 (17.4 examples/sec; 3.682 sec/batch)
2017-05-09 06:22:33.284011: step 4840, loss = 0.0437, acc = 0.9920 (16.8 examples/sec; 3.809 sec/batch)
2017-05-09 06:23:47.703694: step 4860, loss = 0.0515, acc = 0.9900 (17.4 examples/sec; 3.688 sec/batch)
2017-05-09 06:25:02.153768: step 4880, loss = 0.0510, acc = 0.9860 (16.7 examples/sec; 3.831 sec/batch)
2017-05-09 06:26:16.325052: step 4900, loss = 0.0448, acc = 0.9880 (18.6 examples/sec; 3.446 sec/batch)
2017-05-09 06:27:30.254084: step 4920, loss = 0.0802, acc = 0.9820 (17.1 examples/sec; 3.735 sec/batch)
2017-05-09 06:28:45.741662: step 4940, loss = 0.0506, acc = 0.9880 (17.0 examples/sec; 3.775 sec/batch)
2017-05-09 06:29:59.298834: step 4960, loss = 0.0675, acc = 0.9780 (17.1 examples/sec; 3.751 sec/batch)
2017-05-09 06:31:13.173992: step 4980, loss = 0.0638, acc = 0.9820 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 06:32:26.839207: step 5000, loss = 0.0421, acc = 0.9900 (17.6 examples/sec; 3.645 sec/batch)
[Eval] 2017-05-09 06:32:42.950343: step 5000, acc = 0.9619, f1 = 0.9605
[Test] 2017-05-09 06:32:53.915608: step 5000, acc = 0.9509, f1 = 0.9505
[Status] 2017-05-09 06:32:53.915713: step 5000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 06:34:06.891737: step 5020, loss = 0.0541, acc = 0.9900 (16.7 examples/sec; 3.826 sec/batch)
2017-05-09 06:35:19.193602: step 5040, loss = 0.0582, acc = 0.9920 (16.7 examples/sec; 3.830 sec/batch)
2017-05-09 06:36:35.080770: step 5060, loss = 0.0282, acc = 0.9960 (17.6 examples/sec; 3.630 sec/batch)
2017-05-09 06:37:49.131975: step 5080, loss = 0.0354, acc = 0.9960 (17.0 examples/sec; 3.757 sec/batch)
2017-05-09 06:39:02.615024: step 5100, loss = 0.0459, acc = 0.9920 (18.2 examples/sec; 3.516 sec/batch)
2017-05-09 06:40:17.070912: step 5120, loss = 0.0367, acc = 0.9940 (17.3 examples/sec; 3.693 sec/batch)
2017-05-09 06:41:33.176544: step 5140, loss = 0.0345, acc = 0.9980 (16.8 examples/sec; 3.807 sec/batch)
2017-05-09 06:42:49.775151: step 5160, loss = 0.0483, acc = 0.9840 (17.6 examples/sec; 3.634 sec/batch)
2017-05-09 06:44:01.487442: step 5180, loss = 0.0327, acc = 0.9940 (17.7 examples/sec; 3.618 sec/batch)
2017-05-09 06:45:17.656358: step 5200, loss = 0.0264, acc = 1.0000 (17.8 examples/sec; 3.598 sec/batch)
2017-05-09 06:46:32.679424: step 5220, loss = 0.0422, acc = 0.9940 (18.8 examples/sec; 3.410 sec/batch)
2017-05-09 06:47:46.446517: step 5240, loss = 0.0411, acc = 0.9920 (17.6 examples/sec; 3.631 sec/batch)
2017-05-09 06:49:00.492711: step 5260, loss = 0.0443, acc = 0.9940 (17.5 examples/sec; 3.651 sec/batch)
2017-05-09 06:50:13.212081: step 5280, loss = 0.0387, acc = 0.9900 (18.5 examples/sec; 3.458 sec/batch)
2017-05-09 06:51:28.585163: step 5300, loss = 0.0301, acc = 1.0000 (16.5 examples/sec; 3.878 sec/batch)
2017-05-09 06:52:43.831894: step 5320, loss = 0.0321, acc = 0.9960 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 06:53:58.726222: step 5340, loss = 0.0333, acc = 0.9940 (16.9 examples/sec; 3.785 sec/batch)
2017-05-09 06:55:14.197248: step 5360, loss = 0.0355, acc = 0.9940 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 06:56:26.326971: step 5380, loss = 0.0311, acc = 0.9980 (19.3 examples/sec; 3.313 sec/batch)
2017-05-09 06:57:40.834925: step 5400, loss = 0.0448, acc = 0.9860 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 06:58:54.575465: step 5420, loss = 0.0396, acc = 0.9920 (16.9 examples/sec; 3.781 sec/batch)
2017-05-09 07:00:09.551817: step 5440, loss = 0.0280, acc = 0.9960 (17.0 examples/sec; 3.756 sec/batch)
2017-05-09 07:01:23.764960: step 5460, loss = 0.0444, acc = 0.9900 (18.1 examples/sec; 3.528 sec/batch)
2017-05-09 07:02:38.448525: step 5480, loss = 0.0395, acc = 0.9940 (16.5 examples/sec; 3.884 sec/batch)
2017-05-09 07:03:51.786362: step 5500, loss = 0.0295, acc = 0.9980 (17.9 examples/sec; 3.571 sec/batch)
2017-05-09 07:05:02.902602: step 5520, loss = 0.0329, acc = 0.9960 (19.2 examples/sec; 3.326 sec/batch)
2017-05-09 07:06:17.523310: step 5540, loss = 0.0382, acc = 0.9920 (17.0 examples/sec; 3.770 sec/batch)
2017-05-09 07:07:30.964751: step 5560, loss = 0.0293, acc = 0.9980 (17.8 examples/sec; 3.590 sec/batch)
2017-05-09 07:08:46.602625: step 5580, loss = 0.0470, acc = 0.9860 (16.7 examples/sec; 3.823 sec/batch)
2017-05-09 07:10:00.876365: step 5600, loss = 0.0285, acc = 0.9980 (16.7 examples/sec; 3.832 sec/batch)
2017-05-09 07:11:14.362568: step 5620, loss = 0.0289, acc = 0.9960 (16.4 examples/sec; 3.893 sec/batch)
2017-05-09 07:12:27.980674: step 5640, loss = 0.0334, acc = 0.9980 (17.7 examples/sec; 3.620 sec/batch)
2017-05-09 07:13:41.448242: step 5660, loss = 0.0362, acc = 0.9940 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 07:14:54.343750: step 5680, loss = 0.0417, acc = 0.9900 (16.9 examples/sec; 3.783 sec/batch)
2017-05-09 07:16:09.243484: step 5700, loss = 0.0376, acc = 0.9920 (17.9 examples/sec; 3.571 sec/batch)
2017-05-09 07:17:23.340333: step 5720, loss = 0.0363, acc = 0.9920 (18.0 examples/sec; 3.551 sec/batch)
2017-05-09 07:18:38.611831: step 5740, loss = 0.0281, acc = 0.9980 (16.5 examples/sec; 3.877 sec/batch)
2017-05-09 07:19:52.421130: step 5760, loss = 0.0492, acc = 0.9920 (18.0 examples/sec; 3.546 sec/batch)
2017-05-09 07:21:06.719003: step 5780, loss = 0.0278, acc = 0.9960 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 07:22:20.917264: step 5800, loss = 0.0334, acc = 0.9920 (17.0 examples/sec; 3.765 sec/batch)
2017-05-09 07:23:35.241770: step 5820, loss = 0.0304, acc = 0.9960 (16.8 examples/sec; 3.803 sec/batch)
2017-05-09 07:24:49.613184: step 5840, loss = 0.0474, acc = 0.9860 (17.3 examples/sec; 3.694 sec/batch)
2017-05-09 07:26:02.559473: step 5860, loss = 0.0298, acc = 0.9960 (17.8 examples/sec; 3.604 sec/batch)
2017-05-09 07:27:17.456510: step 5880, loss = 0.0356, acc = 0.9960 (17.0 examples/sec; 3.767 sec/batch)
2017-05-09 07:28:33.766891: step 5900, loss = 0.0418, acc = 0.9920 (16.9 examples/sec; 3.791 sec/batch)
2017-05-09 07:29:49.802939: step 5920, loss = 0.0340, acc = 0.9960 (15.4 examples/sec; 4.152 sec/batch)
2017-05-09 07:31:02.077574: step 5940, loss = 0.0504, acc = 0.9900 (18.4 examples/sec; 3.488 sec/batch)
2017-05-09 07:32:16.086502: step 5960, loss = 0.0541, acc = 0.9900 (16.8 examples/sec; 3.809 sec/batch)
2017-05-09 07:33:32.306338: step 5980, loss = 0.0378, acc = 0.9940 (16.7 examples/sec; 3.822 sec/batch)
2017-05-09 07:34:45.380203: step 6000, loss = 0.0324, acc = 0.9960 (17.1 examples/sec; 3.747 sec/batch)
[Eval] 2017-05-09 07:35:03.010883: step 6000, acc = 0.9603, f1 = 0.9589
[Test] 2017-05-09 07:35:14.924236: step 6000, acc = 0.9499, f1 = 0.9495
[Status] 2017-05-09 07:35:14.924325: step 6000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 07:36:30.805157: step 6020, loss = 0.0368, acc = 0.9960 (16.9 examples/sec; 3.794 sec/batch)
2017-05-09 07:37:47.535486: step 6040, loss = 0.0343, acc = 0.9920 (16.6 examples/sec; 3.858 sec/batch)
2017-05-09 07:39:03.480220: step 6060, loss = 0.0336, acc = 0.9980 (16.6 examples/sec; 3.847 sec/batch)
2017-05-09 07:40:15.252378: step 6080, loss = 0.0338, acc = 0.9920 (18.3 examples/sec; 3.489 sec/batch)
2017-05-09 07:41:29.325222: step 6100, loss = 0.0286, acc = 0.9980 (17.9 examples/sec; 3.571 sec/batch)
2017-05-09 07:42:42.834045: step 6120, loss = 0.0226, acc = 1.0000 (17.4 examples/sec; 3.681 sec/batch)
2017-05-09 07:43:57.437616: step 6140, loss = 0.0283, acc = 0.9960 (17.0 examples/sec; 3.770 sec/batch)
2017-05-09 07:45:11.948527: step 6160, loss = 0.0200, acc = 1.0000 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 07:46:25.215741: step 6180, loss = 0.0299, acc = 0.9940 (17.0 examples/sec; 3.761 sec/batch)
2017-05-09 07:47:39.836810: step 6200, loss = 0.0222, acc = 0.9980 (18.2 examples/sec; 3.514 sec/batch)
2017-05-09 07:48:52.021671: step 6220, loss = 0.0300, acc = 0.9920 (17.3 examples/sec; 3.693 sec/batch)
2017-05-09 07:50:06.584228: step 6240, loss = 0.0362, acc = 0.9900 (16.9 examples/sec; 3.780 sec/batch)
2017-05-09 07:51:20.569622: step 6260, loss = 0.0321, acc = 0.9960 (17.5 examples/sec; 3.662 sec/batch)
2017-05-09 07:52:32.796762: step 6280, loss = 0.0283, acc = 0.9980 (17.5 examples/sec; 3.653 sec/batch)
2017-05-09 07:53:48.290647: step 6300, loss = 0.0419, acc = 0.9880 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 07:55:02.760469: step 6320, loss = 0.0280, acc = 0.9960 (16.9 examples/sec; 3.782 sec/batch)
2017-05-09 07:56:16.126929: step 6340, loss = 0.0240, acc = 0.9980 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 07:57:29.835119: step 6360, loss = 0.0432, acc = 0.9940 (16.9 examples/sec; 3.784 sec/batch)
2017-05-09 07:58:41.951503: step 6380, loss = 0.0257, acc = 1.0000 (17.1 examples/sec; 3.746 sec/batch)
2017-05-09 07:59:55.438736: step 6400, loss = 0.0431, acc = 0.9900 (17.0 examples/sec; 3.775 sec/batch)
2017-05-09 08:01:11.931482: step 6420, loss = 0.0327, acc = 0.9920 (16.4 examples/sec; 3.913 sec/batch)
2017-05-09 08:02:27.954430: step 6440, loss = 0.0375, acc = 0.9960 (15.8 examples/sec; 4.046 sec/batch)
2017-05-09 08:03:42.107616: step 6460, loss = 0.0333, acc = 0.9960 (16.7 examples/sec; 3.822 sec/batch)
2017-05-09 08:04:56.100234: step 6480, loss = 0.0375, acc = 0.9960 (17.3 examples/sec; 3.701 sec/batch)
2017-05-09 08:06:10.850221: step 6500, loss = 0.0260, acc = 0.9980 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 08:07:26.557696: step 6520, loss = 0.0269, acc = 0.9980 (17.0 examples/sec; 3.758 sec/batch)
2017-05-09 08:08:42.774457: step 6540, loss = 0.0287, acc = 0.9960 (16.0 examples/sec; 4.012 sec/batch)
2017-05-09 08:09:53.956927: step 6560, loss = 0.0286, acc = 0.9960 (17.8 examples/sec; 3.602 sec/batch)
2017-05-09 08:11:09.545050: step 6580, loss = 0.0433, acc = 0.9900 (16.8 examples/sec; 3.820 sec/batch)
2017-05-09 08:12:22.855281: step 6600, loss = 0.0341, acc = 0.9960 (17.0 examples/sec; 3.761 sec/batch)
2017-05-09 08:13:39.775824: step 6620, loss = 0.0269, acc = 0.9980 (16.2 examples/sec; 3.939 sec/batch)
2017-05-09 08:14:55.136609: step 6640, loss = 0.0297, acc = 0.9960 (17.0 examples/sec; 3.773 sec/batch)
2017-05-09 08:16:07.365630: step 6660, loss = 0.0241, acc = 1.0000 (17.8 examples/sec; 3.594 sec/batch)
2017-05-09 08:17:22.218703: step 6680, loss = 0.0376, acc = 0.9920 (16.9 examples/sec; 3.785 sec/batch)
2017-05-09 08:18:34.879227: step 6700, loss = 0.0361, acc = 0.9940 (17.5 examples/sec; 3.659 sec/batch)
2017-05-09 08:19:49.037449: step 6720, loss = 0.0292, acc = 0.9980 (18.6 examples/sec; 3.438 sec/batch)
2017-05-09 08:21:03.918741: step 6740, loss = 0.0428, acc = 0.9860 (16.6 examples/sec; 3.852 sec/batch)
2017-05-09 08:22:19.083005: step 6760, loss = 0.0355, acc = 0.9900 (16.2 examples/sec; 3.962 sec/batch)
2017-05-09 08:23:33.252136: step 6780, loss = 0.0319, acc = 0.9940 (17.2 examples/sec; 3.720 sec/batch)
2017-05-09 08:24:48.238087: step 6800, loss = 0.0356, acc = 0.9940 (17.7 examples/sec; 3.606 sec/batch)
2017-05-09 08:26:02.302891: step 6820, loss = 0.0246, acc = 0.9980 (17.1 examples/sec; 3.744 sec/batch)
2017-05-09 08:27:14.889882: step 6840, loss = 0.0463, acc = 0.9920 (17.9 examples/sec; 3.574 sec/batch)
2017-05-09 08:28:29.597973: step 6860, loss = 0.0235, acc = 0.9960 (16.7 examples/sec; 3.840 sec/batch)
2017-05-09 08:29:45.092012: step 6880, loss = 0.0287, acc = 0.9960 (17.7 examples/sec; 3.612 sec/batch)
2017-05-09 08:30:59.505451: step 6900, loss = 0.0348, acc = 0.9940 (18.4 examples/sec; 3.482 sec/batch)
2017-05-09 08:32:15.032680: step 6920, loss = 0.0240, acc = 0.9980 (17.2 examples/sec; 3.729 sec/batch)
2017-05-09 08:33:27.149729: step 6940, loss = 0.0268, acc = 1.0000 (17.2 examples/sec; 3.711 sec/batch)
2017-05-09 08:34:41.846794: step 6960, loss = 0.0350, acc = 0.9940 (17.3 examples/sec; 3.692 sec/batch)
2017-05-09 08:35:55.223520: step 6980, loss = 0.0284, acc = 0.9940 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 08:37:08.776168: step 7000, loss = 0.0309, acc = 0.9960 (17.3 examples/sec; 3.705 sec/batch)
[Eval] 2017-05-09 08:37:24.991995: step 7000, acc = 0.9586, f1 = 0.9572
[Test] 2017-05-09 08:37:35.895672: step 7000, acc = 0.9487, f1 = 0.9483
[Status] 2017-05-09 08:37:35.895756: step 7000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 08:38:51.161609: step 7020, loss = 0.0410, acc = 0.9920 (16.9 examples/sec; 3.797 sec/batch)
2017-05-09 08:40:04.925722: step 7040, loss = 0.0373, acc = 0.9960 (17.0 examples/sec; 3.769 sec/batch)
2017-05-09 08:41:22.406795: step 7060, loss = 0.0201, acc = 1.0000 (15.9 examples/sec; 4.031 sec/batch)
2017-05-09 08:42:36.933778: step 7080, loss = 0.0217, acc = 0.9980 (17.3 examples/sec; 3.692 sec/batch)
2017-05-09 08:43:52.845805: step 7100, loss = 0.0375, acc = 0.9960 (17.1 examples/sec; 3.743 sec/batch)
2017-05-09 08:45:09.105627: step 7120, loss = 0.0277, acc = 0.9920 (17.0 examples/sec; 3.769 sec/batch)
2017-05-09 08:46:24.243889: step 7140, loss = 0.0288, acc = 0.9960 (17.8 examples/sec; 3.605 sec/batch)
2017-05-09 08:47:40.154512: step 7160, loss = 0.0229, acc = 0.9980 (15.8 examples/sec; 4.043 sec/batch)
2017-05-09 08:48:54.334379: step 7180, loss = 0.0274, acc = 0.9960 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 08:50:09.210201: step 7200, loss = 0.0240, acc = 0.9960 (16.5 examples/sec; 3.880 sec/batch)
2017-05-09 08:51:23.852465: step 7220, loss = 0.0205, acc = 0.9980 (16.9 examples/sec; 3.790 sec/batch)
2017-05-09 08:52:35.762469: step 7240, loss = 0.0232, acc = 0.9980 (19.3 examples/sec; 3.319 sec/batch)
2017-05-09 08:53:51.760980: step 7260, loss = 0.0196, acc = 1.0000 (16.3 examples/sec; 3.931 sec/batch)
2017-05-09 08:55:07.038436: step 7280, loss = 0.0197, acc = 1.0000 (17.6 examples/sec; 3.626 sec/batch)
2017-05-09 08:56:23.624748: step 7300, loss = 0.0210, acc = 1.0000 (16.6 examples/sec; 3.853 sec/batch)
2017-05-09 08:57:37.394397: step 7320, loss = 0.0233, acc = 0.9980 (17.2 examples/sec; 3.722 sec/batch)
2017-05-09 08:58:53.051245: step 7340, loss = 0.0237, acc = 0.9960 (17.0 examples/sec; 3.756 sec/batch)
2017-05-09 09:00:08.193498: step 7360, loss = 0.0284, acc = 0.9980 (16.7 examples/sec; 3.833 sec/batch)
2017-05-09 09:01:22.415053: step 7380, loss = 0.0226, acc = 0.9960 (16.6 examples/sec; 3.857 sec/batch)
2017-05-09 09:02:37.730733: step 7400, loss = 0.0217, acc = 0.9980 (16.9 examples/sec; 3.776 sec/batch)
2017-05-09 09:03:53.492502: step 7420, loss = 0.0196, acc = 1.0000 (16.5 examples/sec; 3.881 sec/batch)
2017-05-09 09:05:10.703951: step 7440, loss = 0.0245, acc = 0.9980 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 09:06:25.455438: step 7460, loss = 0.0223, acc = 0.9980 (16.5 examples/sec; 3.869 sec/batch)
2017-05-09 09:07:40.182938: step 7480, loss = 0.0283, acc = 0.9960 (16.8 examples/sec; 3.815 sec/batch)
2017-05-09 09:08:55.029482: step 7500, loss = 0.0258, acc = 0.9980 (18.3 examples/sec; 3.505 sec/batch)
2017-05-09 09:10:08.920076: step 7520, loss = 0.0213, acc = 0.9980 (17.0 examples/sec; 3.758 sec/batch)
2017-05-09 09:11:22.602019: step 7540, loss = 0.0213, acc = 1.0000 (17.0 examples/sec; 3.755 sec/batch)
2017-05-09 09:12:37.598216: step 7560, loss = 0.0424, acc = 0.9920 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 09:13:51.923518: step 7580, loss = 0.0334, acc = 0.9960 (18.0 examples/sec; 3.558 sec/batch)
2017-05-09 09:15:06.583334: step 7600, loss = 0.0246, acc = 0.9940 (17.8 examples/sec; 3.602 sec/batch)
2017-05-09 09:16:22.314108: step 7620, loss = 0.0217, acc = 1.0000 (16.6 examples/sec; 3.844 sec/batch)
2017-05-09 09:17:38.052782: step 7640, loss = 0.0195, acc = 1.0000 (17.1 examples/sec; 3.751 sec/batch)
2017-05-09 09:18:52.755664: step 7660, loss = 0.0262, acc = 0.9940 (17.9 examples/sec; 3.578 sec/batch)
2017-05-09 09:20:08.775070: step 7680, loss = 0.0262, acc = 0.9940 (17.1 examples/sec; 3.736 sec/batch)
2017-05-09 09:21:24.376863: step 7700, loss = 0.0226, acc = 0.9980 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 09:22:38.545975: step 7720, loss = 0.0246, acc = 0.9980 (16.8 examples/sec; 3.810 sec/batch)
2017-05-09 09:23:53.388502: step 7740, loss = 0.0288, acc = 0.9960 (17.6 examples/sec; 3.629 sec/batch)
2017-05-09 09:25:08.443058: step 7760, loss = 0.0251, acc = 0.9960 (17.7 examples/sec; 3.615 sec/batch)
2017-05-09 09:26:21.486557: step 7780, loss = 0.0222, acc = 0.9980 (16.3 examples/sec; 3.931 sec/batch)
2017-05-09 09:27:37.433145: step 7800, loss = 0.0249, acc = 0.9960 (17.1 examples/sec; 3.733 sec/batch)
2017-05-09 09:28:52.618570: step 7820, loss = 0.0352, acc = 0.9940 (17.0 examples/sec; 3.756 sec/batch)
2017-05-09 09:30:07.727440: step 7840, loss = 0.0341, acc = 0.9960 (16.7 examples/sec; 3.837 sec/batch)
2017-05-09 09:31:22.154742: step 7860, loss = 0.0244, acc = 1.0000 (17.0 examples/sec; 3.772 sec/batch)
2017-05-09 09:32:37.949542: step 7880, loss = 0.0200, acc = 1.0000 (16.7 examples/sec; 3.842 sec/batch)
2017-05-09 09:33:53.100744: step 7900, loss = 0.0284, acc = 0.9940 (16.8 examples/sec; 3.821 sec/batch)
2017-05-09 09:35:09.092349: step 7920, loss = 0.0313, acc = 0.9960 (16.7 examples/sec; 3.821 sec/batch)
2017-05-09 09:36:22.759516: step 7940, loss = 0.0250, acc = 0.9980 (16.6 examples/sec; 3.867 sec/batch)
2017-05-09 09:37:37.490275: step 7960, loss = 0.0338, acc = 0.9960 (17.0 examples/sec; 3.766 sec/batch)
2017-05-09 09:38:51.746673: step 7980, loss = 0.0265, acc = 0.9980 (16.9 examples/sec; 3.795 sec/batch)
2017-05-09 09:40:07.010796: step 8000, loss = 0.0229, acc = 0.9960 (16.7 examples/sec; 3.834 sec/batch)
[Eval] 2017-05-09 09:40:23.346968: step 8000, acc = 0.9580, f1 = 0.9565
[Test] 2017-05-09 09:40:34.198687: step 8000, acc = 0.9465, f1 = 0.9461
[Status] 2017-05-09 09:40:34.198805: step 8000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 09:41:50.059075: step 8020, loss = 0.0218, acc = 1.0000 (16.4 examples/sec; 3.901 sec/batch)
2017-05-09 09:43:05.644934: step 8040, loss = 0.0358, acc = 0.9940 (16.8 examples/sec; 3.804 sec/batch)
2017-05-09 09:44:20.933130: step 8060, loss = 0.0268, acc = 0.9940 (16.1 examples/sec; 3.964 sec/batch)
2017-05-09 09:45:36.166890: step 8080, loss = 0.0278, acc = 0.9980 (17.7 examples/sec; 3.608 sec/batch)
2017-05-09 09:46:50.366460: step 8100, loss = 0.0203, acc = 0.9980 (18.4 examples/sec; 3.487 sec/batch)
2017-05-09 09:48:04.304429: step 8120, loss = 0.0192, acc = 0.9980 (17.4 examples/sec; 3.679 sec/batch)
2017-05-09 09:49:19.779182: step 8140, loss = 0.0160, acc = 1.0000 (16.5 examples/sec; 3.876 sec/batch)
2017-05-09 09:50:34.461383: step 8160, loss = 0.0182, acc = 0.9980 (18.1 examples/sec; 3.539 sec/batch)
2017-05-09 09:51:49.291561: step 8180, loss = 0.0170, acc = 1.0000 (17.9 examples/sec; 3.578 sec/batch)
2017-05-09 09:53:02.915258: step 8200, loss = 0.0241, acc = 0.9980 (16.8 examples/sec; 3.821 sec/batch)
2017-05-09 09:54:17.637959: step 8220, loss = 0.0198, acc = 1.0000 (17.2 examples/sec; 3.712 sec/batch)
2017-05-09 09:55:31.569941: step 8240, loss = 0.0355, acc = 0.9940 (17.1 examples/sec; 3.749 sec/batch)
2017-05-09 09:56:45.347723: step 8260, loss = 0.0181, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-09 09:57:59.776440: step 8280, loss = 0.0197, acc = 0.9980 (16.7 examples/sec; 3.835 sec/batch)
2017-05-09 09:59:14.019318: step 8300, loss = 0.0200, acc = 0.9980 (16.9 examples/sec; 3.793 sec/batch)
2017-05-09 10:00:28.887404: step 8320, loss = 0.0186, acc = 1.0000 (16.4 examples/sec; 3.909 sec/batch)
2017-05-09 10:01:41.357528: step 8340, loss = 0.0301, acc = 0.9960 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 10:02:52.029208: step 8360, loss = 0.0208, acc = 0.9980 (19.2 examples/sec; 3.334 sec/batch)
2017-05-09 10:04:05.525765: step 8380, loss = 0.0188, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-09 10:05:18.511080: step 8400, loss = 0.0234, acc = 0.9980 (18.9 examples/sec; 3.386 sec/batch)
2017-05-09 10:06:32.800652: step 8420, loss = 0.0180, acc = 0.9980 (18.1 examples/sec; 3.528 sec/batch)
2017-05-09 10:07:49.107127: step 8440, loss = 0.0211, acc = 0.9960 (16.2 examples/sec; 3.962 sec/batch)
2017-05-09 10:09:05.092327: step 8460, loss = 0.0211, acc = 1.0000 (17.0 examples/sec; 3.765 sec/batch)
2017-05-09 10:10:18.717683: step 8480, loss = 0.0202, acc = 0.9980 (17.2 examples/sec; 3.731 sec/batch)
2017-05-09 10:11:34.554386: step 8500, loss = 0.0169, acc = 1.0000 (16.5 examples/sec; 3.870 sec/batch)
2017-05-09 10:12:47.890237: step 8520, loss = 0.0219, acc = 0.9960 (17.7 examples/sec; 3.608 sec/batch)
2017-05-09 10:14:02.023856: step 8540, loss = 0.0249, acc = 0.9940 (18.1 examples/sec; 3.532 sec/batch)
2017-05-09 10:15:17.337397: step 8560, loss = 0.0171, acc = 1.0000 (16.5 examples/sec; 3.872 sec/batch)
2017-05-09 10:16:33.459928: step 8580, loss = 0.0203, acc = 0.9980 (17.2 examples/sec; 3.731 sec/batch)
2017-05-09 10:17:49.696432: step 8600, loss = 0.0177, acc = 1.0000 (16.7 examples/sec; 3.837 sec/batch)
2017-05-09 10:19:04.501212: step 8620, loss = 0.0307, acc = 0.9960 (16.8 examples/sec; 3.799 sec/batch)
2017-05-09 10:20:19.094555: step 8640, loss = 0.0310, acc = 0.9960 (17.1 examples/sec; 3.752 sec/batch)
2017-05-09 10:21:33.585161: step 8660, loss = 0.0158, acc = 1.0000 (16.7 examples/sec; 3.825 sec/batch)
2017-05-09 10:22:47.781477: step 8680, loss = 0.0201, acc = 0.9980 (17.9 examples/sec; 3.583 sec/batch)
2017-05-09 10:24:01.076304: step 8700, loss = 0.0267, acc = 0.9960 (17.2 examples/sec; 3.718 sec/batch)
2017-05-09 10:25:15.755579: step 8720, loss = 0.0227, acc = 0.9980 (17.1 examples/sec; 3.746 sec/batch)
2017-05-09 10:26:30.938868: step 8740, loss = 0.0217, acc = 0.9980 (16.8 examples/sec; 3.809 sec/batch)
2017-05-09 10:27:46.070906: step 8760, loss = 0.0229, acc = 0.9980 (17.2 examples/sec; 3.721 sec/batch)
2017-05-09 10:29:01.983052: step 8780, loss = 0.0171, acc = 1.0000 (17.1 examples/sec; 3.739 sec/batch)
2017-05-09 10:30:16.793446: step 8800, loss = 0.0176, acc = 1.0000 (16.6 examples/sec; 3.850 sec/batch)
2017-05-09 10:31:30.436790: step 8820, loss = 0.0219, acc = 0.9980 (17.3 examples/sec; 3.702 sec/batch)
2017-05-09 10:32:45.884966: step 8840, loss = 0.0177, acc = 1.0000 (16.2 examples/sec; 3.958 sec/batch)
2017-05-09 10:34:01.693198: step 8860, loss = 0.0170, acc = 1.0000 (16.6 examples/sec; 3.853 sec/batch)
2017-05-09 10:35:17.219121: step 8880, loss = 0.0244, acc = 0.9980 (18.0 examples/sec; 3.562 sec/batch)
2017-05-09 10:36:31.272729: step 8900, loss = 0.0253, acc = 0.9960 (16.3 examples/sec; 3.916 sec/batch)
2017-05-09 10:37:45.470392: step 8920, loss = 0.0162, acc = 1.0000 (16.9 examples/sec; 3.785 sec/batch)
2017-05-09 10:39:00.758568: step 8940, loss = 0.0225, acc = 0.9960 (16.7 examples/sec; 3.821 sec/batch)
2017-05-09 10:40:15.868490: step 8960, loss = 0.0232, acc = 0.9980 (16.9 examples/sec; 3.796 sec/batch)
2017-05-09 10:41:31.360702: step 8980, loss = 0.0225, acc = 0.9980 (16.9 examples/sec; 3.793 sec/batch)
2017-05-09 10:42:44.856766: step 9000, loss = 0.0176, acc = 1.0000 (16.8 examples/sec; 3.817 sec/batch)
[Eval] 2017-05-09 10:43:00.772022: step 9000, acc = 0.9568, f1 = 0.9553
[Test] 2017-05-09 10:43:11.722112: step 9000, acc = 0.9461, f1 = 0.9457
[Status] 2017-05-09 10:43:11.722205: step 9000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 10:44:26.781579: step 9020, loss = 0.0204, acc = 1.0000 (16.9 examples/sec; 3.788 sec/batch)
2017-05-09 10:45:40.162481: step 9040, loss = 0.0227, acc = 0.9960 (17.0 examples/sec; 3.768 sec/batch)
2017-05-09 10:46:53.973355: step 9060, loss = 0.0179, acc = 0.9980 (16.9 examples/sec; 3.791 sec/batch)
2017-05-09 10:48:08.422895: step 9080, loss = 0.0148, acc = 1.0000 (17.0 examples/sec; 3.776 sec/batch)
2017-05-09 10:49:20.131909: step 9100, loss = 0.0146, acc = 1.0000 (17.0 examples/sec; 3.770 sec/batch)
2017-05-09 10:50:36.164633: step 9120, loss = 0.0139, acc = 1.0000 (16.6 examples/sec; 3.859 sec/batch)
2017-05-09 10:51:51.374054: step 9140, loss = 0.0198, acc = 0.9980 (17.7 examples/sec; 3.610 sec/batch)
2017-05-09 10:53:05.628523: step 9160, loss = 0.0166, acc = 1.0000 (17.3 examples/sec; 3.689 sec/batch)
2017-05-09 10:54:21.965411: step 9180, loss = 0.0214, acc = 0.9980 (16.5 examples/sec; 3.884 sec/batch)
2017-05-09 10:55:38.523630: step 9200, loss = 0.0153, acc = 1.0000 (16.7 examples/sec; 3.836 sec/batch)
2017-05-09 10:56:52.801714: step 9220, loss = 0.0169, acc = 1.0000 (16.8 examples/sec; 3.805 sec/batch)
2017-05-09 10:58:06.950971: step 9240, loss = 0.0193, acc = 0.9940 (17.0 examples/sec; 3.760 sec/batch)
2017-05-09 10:59:20.294179: step 9260, loss = 0.0166, acc = 0.9980 (17.2 examples/sec; 3.717 sec/batch)
2017-05-09 11:00:35.671513: step 9280, loss = 0.0151, acc = 0.9980 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 11:01:52.107521: step 9300, loss = 0.0130, acc = 1.0000 (16.8 examples/sec; 3.799 sec/batch)
2017-05-09 11:03:06.688660: step 9320, loss = 0.0310, acc = 0.9960 (17.5 examples/sec; 3.648 sec/batch)
2017-05-09 11:04:23.329472: step 9340, loss = 0.0185, acc = 0.9980 (16.6 examples/sec; 3.850 sec/batch)
2017-05-09 11:05:34.715068: step 9360, loss = 0.0151, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-09 11:06:49.590034: step 9380, loss = 0.0149, acc = 1.0000 (16.9 examples/sec; 3.783 sec/batch)
2017-05-09 11:08:05.585090: step 9400, loss = 0.0157, acc = 0.9980 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 11:09:21.137003: step 9420, loss = 0.0205, acc = 0.9960 (16.8 examples/sec; 3.811 sec/batch)
2017-05-09 11:10:34.351226: step 9440, loss = 0.0171, acc = 1.0000 (17.0 examples/sec; 3.766 sec/batch)
2017-05-09 11:11:50.375423: step 9460, loss = 0.0143, acc = 1.0000 (17.0 examples/sec; 3.766 sec/batch)
2017-05-09 11:13:05.141734: step 9480, loss = 0.0164, acc = 1.0000 (17.0 examples/sec; 3.773 sec/batch)
2017-05-09 11:14:18.671251: step 9500, loss = 0.0175, acc = 0.9980 (18.2 examples/sec; 3.510 sec/batch)
2017-05-09 11:15:32.296738: step 9520, loss = 0.0144, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-09 11:16:47.417372: step 9540, loss = 0.0148, acc = 1.0000 (16.5 examples/sec; 3.886 sec/batch)
2017-05-09 11:18:01.785287: step 9560, loss = 0.0152, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-09 11:19:17.065830: step 9580, loss = 0.0171, acc = 0.9980 (17.1 examples/sec; 3.751 sec/batch)
2017-05-09 11:20:32.142341: step 9600, loss = 0.0172, acc = 1.0000 (17.3 examples/sec; 3.696 sec/batch)
2017-05-09 11:21:47.434724: step 9620, loss = 0.0135, acc = 1.0000 (17.4 examples/sec; 3.672 sec/batch)
2017-05-09 11:23:02.727470: step 9640, loss = 0.0183, acc = 0.9980 (16.5 examples/sec; 3.890 sec/batch)
2017-05-09 11:24:17.064132: step 9660, loss = 0.0142, acc = 1.0000 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 11:25:27.987485: step 9680, loss = 0.0150, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-09 11:26:42.716698: step 9700, loss = 0.0165, acc = 0.9980 (16.9 examples/sec; 3.779 sec/batch)
2017-05-09 11:27:55.439276: step 9720, loss = 0.0421, acc = 0.9960 (16.7 examples/sec; 3.840 sec/batch)
2017-05-09 11:29:09.497273: step 9740, loss = 0.0156, acc = 1.0000 (17.0 examples/sec; 3.770 sec/batch)
2017-05-09 11:30:24.710255: step 9760, loss = 0.0195, acc = 0.9960 (17.2 examples/sec; 3.712 sec/batch)
2017-05-09 11:31:38.224001: step 9780, loss = 0.0193, acc = 0.9980 (16.6 examples/sec; 3.849 sec/batch)
2017-05-09 11:32:54.880676: step 9800, loss = 0.0157, acc = 0.9980 (16.4 examples/sec; 3.912 sec/batch)
2017-05-09 11:34:10.328364: step 9820, loss = 0.0183, acc = 0.9980 (17.6 examples/sec; 3.644 sec/batch)
2017-05-09 11:35:24.829589: step 9840, loss = 0.0149, acc = 1.0000 (17.0 examples/sec; 3.767 sec/batch)
2017-05-09 11:36:39.537240: step 9860, loss = 0.0167, acc = 1.0000 (16.7 examples/sec; 3.831 sec/batch)
2017-05-09 11:37:55.919235: step 9880, loss = 0.0170, acc = 0.9980 (17.2 examples/sec; 3.721 sec/batch)
2017-05-09 11:39:11.537420: step 9900, loss = 0.0201, acc = 0.9960 (16.8 examples/sec; 3.812 sec/batch)
2017-05-09 11:40:24.620200: step 9920, loss = 0.0217, acc = 0.9960 (17.5 examples/sec; 3.667 sec/batch)
2017-05-09 11:41:38.562053: step 9940, loss = 0.0402, acc = 0.9940 (17.8 examples/sec; 3.605 sec/batch)
2017-05-09 11:42:53.300874: step 9960, loss = 0.0167, acc = 0.9980 (17.1 examples/sec; 3.737 sec/batch)
2017-05-09 11:44:07.528958: step 9980, loss = 0.0173, acc = 0.9980 (18.2 examples/sec; 3.521 sec/batch)
2017-05-09 11:45:19.924685: step 10000, loss = 0.0172, acc = 0.9980 (17.6 examples/sec; 3.636 sec/batch)
[Eval] 2017-05-09 11:45:36.097361: step 10000, acc = 0.9567, f1 = 0.9553
[Test] 2017-05-09 11:45:46.934856: step 10000, acc = 0.9456, f1 = 0.9452
[Status] 2017-05-09 11:45:46.934933: step 10000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 11:47:02.885530: step 10020, loss = 0.0149, acc = 1.0000 (16.9 examples/sec; 3.778 sec/batch)
2017-05-09 11:48:17.543018: step 10040, loss = 0.0162, acc = 1.0000 (16.6 examples/sec; 3.862 sec/batch)
2017-05-09 11:49:33.221904: step 10060, loss = 0.0155, acc = 1.0000 (17.0 examples/sec; 3.754 sec/batch)
2017-05-09 11:50:48.551775: step 10080, loss = 0.0122, acc = 1.0000 (16.9 examples/sec; 3.780 sec/batch)
2017-05-09 11:52:05.622187: step 10100, loss = 0.0135, acc = 1.0000 (16.6 examples/sec; 3.851 sec/batch)
2017-05-09 11:53:18.995466: step 10120, loss = 0.0144, acc = 1.0000 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 11:54:33.855946: step 10140, loss = 0.0124, acc = 1.0000 (17.3 examples/sec; 3.705 sec/batch)
2017-05-09 11:55:46.688153: step 10160, loss = 0.0136, acc = 1.0000 (17.5 examples/sec; 3.649 sec/batch)
2017-05-09 11:57:00.721900: step 10180, loss = 0.0133, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-09 11:58:14.867994: step 10200, loss = 0.0156, acc = 0.9960 (17.2 examples/sec; 3.711 sec/batch)
2017-05-09 11:59:29.542568: step 10220, loss = 0.0210, acc = 0.9980 (17.4 examples/sec; 3.680 sec/batch)
2017-05-09 12:00:44.752653: step 10240, loss = 0.0136, acc = 1.0000 (16.6 examples/sec; 3.864 sec/batch)
2017-05-09 12:01:59.569240: step 10260, loss = 0.0154, acc = 0.9980 (16.9 examples/sec; 3.789 sec/batch)
2017-05-09 12:03:15.203946: step 10280, loss = 0.0132, acc = 1.0000 (17.5 examples/sec; 3.647 sec/batch)
2017-05-09 12:04:29.633087: step 10300, loss = 0.0136, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-09 12:05:43.606226: step 10320, loss = 0.0128, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-09 12:06:59.047909: step 10340, loss = 0.0156, acc = 0.9980 (17.5 examples/sec; 3.660 sec/batch)
2017-05-09 12:08:13.725700: step 10360, loss = 0.0165, acc = 0.9960 (17.4 examples/sec; 3.677 sec/batch)
2017-05-09 12:09:28.894738: step 10380, loss = 0.0134, acc = 1.0000 (17.4 examples/sec; 3.683 sec/batch)
2017-05-09 12:10:42.068211: step 10400, loss = 0.0155, acc = 0.9960 (16.6 examples/sec; 3.856 sec/batch)
2017-05-09 12:11:56.350583: step 10420, loss = 0.0127, acc = 1.0000 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 12:13:09.893770: step 10440, loss = 0.0148, acc = 1.0000 (17.0 examples/sec; 3.772 sec/batch)
2017-05-09 12:14:22.160439: step 10460, loss = 0.0249, acc = 0.9960 (16.5 examples/sec; 3.869 sec/batch)
2017-05-09 12:15:37.228728: step 10480, loss = 0.0176, acc = 0.9980 (16.8 examples/sec; 3.803 sec/batch)
2017-05-09 12:16:52.765667: step 10500, loss = 0.0150, acc = 1.0000 (16.9 examples/sec; 3.796 sec/batch)
2017-05-09 12:18:07.527469: step 10520, loss = 0.0164, acc = 0.9980 (16.7 examples/sec; 3.844 sec/batch)
2017-05-09 12:19:23.003542: step 10540, loss = 0.0196, acc = 0.9980 (17.4 examples/sec; 3.682 sec/batch)
2017-05-09 12:20:38.589225: step 10560, loss = 0.0124, acc = 1.0000 (16.7 examples/sec; 3.824 sec/batch)
2017-05-09 12:21:55.811119: step 10580, loss = 0.0178, acc = 0.9980 (16.8 examples/sec; 3.815 sec/batch)
2017-05-09 12:23:10.017430: step 10600, loss = 0.0127, acc = 1.0000 (18.0 examples/sec; 3.549 sec/batch)
2017-05-09 12:24:25.768347: step 10620, loss = 0.0143, acc = 1.0000 (16.6 examples/sec; 3.858 sec/batch)
2017-05-09 12:25:38.460568: step 10640, loss = 0.0159, acc = 0.9960 (19.1 examples/sec; 3.346 sec/batch)
2017-05-09 12:26:52.117515: step 10660, loss = 0.0158, acc = 0.9980 (17.8 examples/sec; 3.593 sec/batch)
2017-05-09 12:28:06.406049: step 10680, loss = 0.0210, acc = 0.9980 (17.1 examples/sec; 3.738 sec/batch)
2017-05-09 12:29:20.891104: step 10700, loss = 0.0167, acc = 0.9960 (16.6 examples/sec; 3.861 sec/batch)
2017-05-09 12:30:37.215134: step 10720, loss = 0.0268, acc = 0.9980 (16.4 examples/sec; 3.913 sec/batch)
2017-05-09 12:31:52.470756: step 10740, loss = 0.0128, acc = 1.0000 (16.4 examples/sec; 3.903 sec/batch)
2017-05-09 12:33:07.922380: step 10760, loss = 0.0119, acc = 1.0000 (17.1 examples/sec; 3.751 sec/batch)
2017-05-09 12:34:22.268124: step 10780, loss = 0.0159, acc = 1.0000 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 12:35:36.177237: step 10800, loss = 0.0117, acc = 1.0000 (17.1 examples/sec; 3.750 sec/batch)
2017-05-09 12:36:51.121514: step 10820, loss = 0.0178, acc = 0.9960 (17.2 examples/sec; 3.723 sec/batch)
2017-05-09 12:38:05.003002: step 10840, loss = 0.0271, acc = 0.9960 (17.1 examples/sec; 3.747 sec/batch)
2017-05-09 12:39:20.592322: step 10860, loss = 0.0256, acc = 0.9940 (17.0 examples/sec; 3.773 sec/batch)
2017-05-09 12:40:35.707516: step 10880, loss = 0.0131, acc = 1.0000 (16.5 examples/sec; 3.889 sec/batch)
2017-05-09 12:41:49.526067: step 10900, loss = 0.0116, acc = 1.0000 (16.9 examples/sec; 3.785 sec/batch)
2017-05-09 12:43:04.083034: step 10920, loss = 0.0141, acc = 1.0000 (17.0 examples/sec; 3.769 sec/batch)
2017-05-09 12:44:18.987877: step 10940, loss = 0.0127, acc = 1.0000 (16.9 examples/sec; 3.783 sec/batch)
2017-05-09 12:45:33.960250: step 10960, loss = 0.0148, acc = 0.9980 (16.7 examples/sec; 3.824 sec/batch)
2017-05-09 12:46:48.162946: step 10980, loss = 0.0142, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-09 12:48:02.900119: step 11000, loss = 0.0153, acc = 1.0000 (16.8 examples/sec; 3.813 sec/batch)
[Eval] 2017-05-09 12:48:19.045015: step 11000, acc = 0.9562, f1 = 0.9547
[Test] 2017-05-09 12:48:29.824823: step 11000, acc = 0.9456, f1 = 0.9451
[Status] 2017-05-09 12:48:29.824910: step 11000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 12:49:43.453887: step 11020, loss = 0.0261, acc = 0.9980 (16.4 examples/sec; 3.894 sec/batch)
2017-05-09 12:50:58.827550: step 11040, loss = 0.0123, acc = 1.0000 (16.9 examples/sec; 3.779 sec/batch)
2017-05-09 12:52:14.439220: step 11060, loss = 0.0134, acc = 0.9980 (16.6 examples/sec; 3.858 sec/batch)
2017-05-09 12:53:29.009757: step 11080, loss = 0.0137, acc = 1.0000 (17.6 examples/sec; 3.629 sec/batch)
2017-05-09 12:54:46.378030: step 11100, loss = 0.0147, acc = 0.9980 (16.8 examples/sec; 3.814 sec/batch)
2017-05-09 12:56:01.671795: step 11120, loss = 0.0144, acc = 1.0000 (16.8 examples/sec; 3.803 sec/batch)
2017-05-09 12:57:15.643278: step 11140, loss = 0.0103, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-09 12:58:29.075102: step 11160, loss = 0.0151, acc = 0.9980 (17.7 examples/sec; 3.622 sec/batch)
2017-05-09 12:59:44.551084: step 11180, loss = 0.0126, acc = 1.0000 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 13:01:00.316060: step 11200, loss = 0.0127, acc = 1.0000 (16.4 examples/sec; 3.891 sec/batch)
2017-05-09 13:02:14.575406: step 11220, loss = 0.0115, acc = 1.0000 (17.1 examples/sec; 3.733 sec/batch)
2017-05-09 13:03:27.937032: step 11240, loss = 0.0148, acc = 0.9980 (18.2 examples/sec; 3.523 sec/batch)
2017-05-09 13:04:42.090604: step 11260, loss = 0.0124, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-09 13:05:56.075647: step 11280, loss = 0.0150, acc = 0.9980 (17.7 examples/sec; 3.618 sec/batch)
2017-05-09 13:07:12.500584: step 11300, loss = 0.0188, acc = 0.9980 (17.3 examples/sec; 3.699 sec/batch)
2017-05-09 13:08:25.553598: step 11320, loss = 0.0119, acc = 1.0000 (17.5 examples/sec; 3.667 sec/batch)
2017-05-09 13:09:40.209811: step 11340, loss = 0.0122, acc = 1.0000 (17.1 examples/sec; 3.749 sec/batch)
2017-05-09 13:10:53.537846: step 11360, loss = 0.0104, acc = 1.0000 (16.9 examples/sec; 3.780 sec/batch)
2017-05-09 13:12:07.373655: step 11380, loss = 0.0117, acc = 1.0000 (18.1 examples/sec; 3.545 sec/batch)
2017-05-09 13:13:22.486135: step 11400, loss = 0.0117, acc = 1.0000 (17.4 examples/sec; 3.676 sec/batch)
2017-05-09 13:14:38.123699: step 11420, loss = 0.0125, acc = 1.0000 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 13:15:51.455874: step 11440, loss = 0.0165, acc = 1.0000 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 13:17:06.286826: step 11460, loss = 0.0130, acc = 1.0000 (17.0 examples/sec; 3.755 sec/batch)
2017-05-09 13:18:20.507777: step 11480, loss = 0.0110, acc = 1.0000 (17.4 examples/sec; 3.673 sec/batch)
2017-05-09 13:19:32.947036: step 11500, loss = 0.0136, acc = 0.9980 (17.4 examples/sec; 3.685 sec/batch)
2017-05-09 13:20:48.587865: step 11520, loss = 0.0107, acc = 1.0000 (17.0 examples/sec; 3.756 sec/batch)
2017-05-09 13:22:03.206847: step 11540, loss = 0.0115, acc = 1.0000 (16.9 examples/sec; 3.789 sec/batch)
2017-05-09 13:23:17.009146: step 11560, loss = 0.0121, acc = 1.0000 (18.1 examples/sec; 3.545 sec/batch)
2017-05-09 13:24:33.307806: step 11580, loss = 0.0149, acc = 0.9980 (16.6 examples/sec; 3.864 sec/batch)
2017-05-09 13:25:49.441794: step 11600, loss = 0.0138, acc = 0.9980 (16.9 examples/sec; 3.785 sec/batch)
2017-05-09 13:27:06.041302: step 11620, loss = 0.0143, acc = 1.0000 (17.6 examples/sec; 3.637 sec/batch)
2017-05-09 13:28:18.008509: step 11640, loss = 0.0110, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-09 13:29:31.412377: step 11660, loss = 0.0120, acc = 1.0000 (17.4 examples/sec; 3.686 sec/batch)
2017-05-09 13:30:47.129672: step 11680, loss = 0.0108, acc = 1.0000 (16.4 examples/sec; 3.911 sec/batch)
2017-05-09 13:32:02.389166: step 11700, loss = 0.0125, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-09 13:33:19.279799: step 11720, loss = 0.0125, acc = 1.0000 (16.9 examples/sec; 3.787 sec/batch)
2017-05-09 13:34:34.604185: step 11740, loss = 0.0138, acc = 0.9980 (17.7 examples/sec; 3.610 sec/batch)
2017-05-09 13:35:47.392081: step 11760, loss = 0.0127, acc = 1.0000 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 13:37:00.205838: step 11780, loss = 0.0130, acc = 1.0000 (17.1 examples/sec; 3.747 sec/batch)
2017-05-09 13:38:14.476396: step 11800, loss = 0.0113, acc = 1.0000 (16.8 examples/sec; 3.821 sec/batch)
2017-05-09 13:39:30.607332: step 11820, loss = 0.0143, acc = 0.9980 (16.4 examples/sec; 3.894 sec/batch)
2017-05-09 13:40:43.478073: step 11840, loss = 0.0132, acc = 1.0000 (17.5 examples/sec; 3.665 sec/batch)
2017-05-09 13:41:57.952868: step 11860, loss = 0.0141, acc = 0.9980 (16.6 examples/sec; 3.850 sec/batch)
2017-05-09 13:43:14.211377: step 11880, loss = 0.0120, acc = 1.0000 (16.6 examples/sec; 3.866 sec/batch)
2017-05-09 13:44:30.072835: step 11900, loss = 0.0118, acc = 1.0000 (17.0 examples/sec; 3.768 sec/batch)
2017-05-09 13:45:44.730174: step 11920, loss = 0.0238, acc = 0.9980 (18.2 examples/sec; 3.521 sec/batch)
2017-05-09 13:46:58.956813: step 11940, loss = 0.0119, acc = 0.9980 (16.5 examples/sec; 3.889 sec/batch)
2017-05-09 13:48:13.049868: step 11960, loss = 0.0130, acc = 1.0000 (17.5 examples/sec; 3.654 sec/batch)
2017-05-09 13:49:28.454646: step 11980, loss = 0.0133, acc = 0.9980 (17.0 examples/sec; 3.771 sec/batch)
2017-05-09 13:50:43.223509: step 12000, loss = 0.0125, acc = 1.0000 (16.9 examples/sec; 3.777 sec/batch)
[Eval] 2017-05-09 13:50:59.431930: step 12000, acc = 0.9553, f1 = 0.9538
[Test] 2017-05-09 13:51:10.293336: step 12000, acc = 0.9444, f1 = 0.9439
[Status] 2017-05-09 13:51:10.293421: step 12000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 13:52:24.386864: step 12020, loss = 0.0116, acc = 1.0000 (17.0 examples/sec; 3.765 sec/batch)
2017-05-09 13:53:37.426513: step 12040, loss = 0.0133, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-09 13:54:51.882828: step 12060, loss = 0.0131, acc = 1.0000 (17.0 examples/sec; 3.764 sec/batch)
2017-05-09 13:56:07.143095: step 12080, loss = 0.0138, acc = 1.0000 (17.2 examples/sec; 3.728 sec/batch)
2017-05-09 13:57:23.195521: step 12100, loss = 0.0114, acc = 1.0000 (17.8 examples/sec; 3.601 sec/batch)
2017-05-09 13:58:38.083526: step 12120, loss = 0.0109, acc = 1.0000 (16.7 examples/sec; 3.842 sec/batch)
2017-05-09 13:59:53.065485: step 12140, loss = 0.0099, acc = 1.0000 (17.6 examples/sec; 3.639 sec/batch)
2017-05-09 14:01:07.230068: step 12160, loss = 0.0117, acc = 1.0000 (17.7 examples/sec; 3.615 sec/batch)
2017-05-09 14:02:21.547582: step 12180, loss = 0.0104, acc = 1.0000 (16.7 examples/sec; 3.831 sec/batch)
2017-05-09 14:03:35.393000: step 12200, loss = 0.0152, acc = 0.9960 (16.5 examples/sec; 3.872 sec/batch)
2017-05-09 14:04:50.755328: step 12220, loss = 0.0106, acc = 1.0000 (17.1 examples/sec; 3.735 sec/batch)
2017-05-09 14:06:04.644860: step 12240, loss = 0.0112, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-09 14:07:20.345363: step 12260, loss = 0.0170, acc = 0.9960 (17.4 examples/sec; 3.681 sec/batch)
2017-05-09 14:08:33.390249: step 12280, loss = 0.0112, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-09 14:09:47.741357: step 12300, loss = 0.0135, acc = 1.0000 (17.0 examples/sec; 3.768 sec/batch)
2017-05-09 14:11:02.011442: step 12320, loss = 0.0118, acc = 1.0000 (16.9 examples/sec; 3.778 sec/batch)
2017-05-09 14:12:18.722590: step 12340, loss = 0.0102, acc = 1.0000 (16.7 examples/sec; 3.822 sec/batch)
2017-05-09 14:13:34.868207: step 12360, loss = 0.0111, acc = 1.0000 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 14:14:48.604525: step 12380, loss = 0.0101, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-09 14:16:01.944859: step 12400, loss = 0.0111, acc = 1.0000 (17.3 examples/sec; 3.705 sec/batch)
2017-05-09 14:17:16.991860: step 12420, loss = 0.0104, acc = 1.0000 (16.8 examples/sec; 3.802 sec/batch)
2017-05-09 14:18:33.142202: step 12440, loss = 0.0122, acc = 0.9980 (16.4 examples/sec; 3.895 sec/batch)
2017-05-09 14:19:47.998487: step 12460, loss = 0.0114, acc = 1.0000 (16.5 examples/sec; 3.871 sec/batch)
2017-05-09 14:21:02.825061: step 12480, loss = 0.0111, acc = 1.0000 (17.4 examples/sec; 3.685 sec/batch)
2017-05-09 14:22:17.762970: step 12500, loss = 0.0103, acc = 1.0000 (17.6 examples/sec; 3.639 sec/batch)
2017-05-09 14:23:32.648259: step 12520, loss = 0.0129, acc = 0.9980 (17.1 examples/sec; 3.733 sec/batch)
2017-05-09 14:24:48.110754: step 12540, loss = 0.0109, acc = 1.0000 (17.3 examples/sec; 3.710 sec/batch)
2017-05-09 14:26:02.891645: step 12560, loss = 0.0102, acc = 1.0000 (17.1 examples/sec; 3.739 sec/batch)
2017-05-09 14:27:16.287631: step 12580, loss = 0.0119, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-09 14:28:30.278203: step 12600, loss = 0.0134, acc = 0.9980 (16.6 examples/sec; 3.857 sec/batch)
2017-05-09 14:29:44.658927: step 12620, loss = 0.0124, acc = 1.0000 (17.4 examples/sec; 3.678 sec/batch)
2017-05-09 14:30:59.440717: step 12640, loss = 0.0125, acc = 1.0000 (17.0 examples/sec; 3.765 sec/batch)
2017-05-09 14:32:13.738590: step 12660, loss = 0.0108, acc = 1.0000 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 14:33:26.821941: step 12680, loss = 0.0110, acc = 1.0000 (17.0 examples/sec; 3.759 sec/batch)
2017-05-09 14:34:42.974961: step 12700, loss = 0.0100, acc = 1.0000 (16.9 examples/sec; 3.790 sec/batch)
2017-05-09 14:35:57.913078: step 12720, loss = 0.0106, acc = 1.0000 (17.6 examples/sec; 3.633 sec/batch)
2017-05-09 14:37:12.951966: step 12740, loss = 0.0101, acc = 1.0000 (16.6 examples/sec; 3.845 sec/batch)
2017-05-09 14:38:26.398199: step 12760, loss = 0.0093, acc = 1.0000 (16.8 examples/sec; 3.821 sec/batch)
2017-05-09 14:39:41.820476: step 12780, loss = 0.0103, acc = 1.0000 (17.2 examples/sec; 3.721 sec/batch)
2017-05-09 14:40:57.565188: step 12800, loss = 0.0102, acc = 1.0000 (16.7 examples/sec; 3.827 sec/batch)
2017-05-09 14:42:11.432104: step 12820, loss = 0.0160, acc = 0.9980 (18.1 examples/sec; 3.535 sec/batch)
2017-05-09 14:43:26.112428: step 12840, loss = 0.0106, acc = 1.0000 (16.7 examples/sec; 3.829 sec/batch)
2017-05-09 14:44:42.596094: step 12860, loss = 0.0119, acc = 1.0000 (16.2 examples/sec; 3.952 sec/batch)
2017-05-09 14:45:57.284593: step 12880, loss = 0.0115, acc = 1.0000 (17.1 examples/sec; 3.735 sec/batch)
2017-05-09 14:47:13.327664: step 12900, loss = 0.0105, acc = 1.0000 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 14:48:27.567395: step 12920, loss = 0.0122, acc = 1.0000 (17.6 examples/sec; 3.642 sec/batch)
2017-05-09 14:49:43.156873: step 12940, loss = 0.0120, acc = 1.0000 (17.2 examples/sec; 3.726 sec/batch)
2017-05-09 14:50:58.165774: step 12960, loss = 0.0133, acc = 0.9980 (18.1 examples/sec; 3.540 sec/batch)
2017-05-09 14:52:12.074309: step 12980, loss = 0.0109, acc = 1.0000 (17.5 examples/sec; 3.667 sec/batch)
2017-05-09 14:53:26.312781: step 13000, loss = 0.0090, acc = 1.0000 (17.3 examples/sec; 3.694 sec/batch)
[Eval] 2017-05-09 14:53:41.975154: step 13000, acc = 0.9560, f1 = 0.9545
[Test] 2017-05-09 14:53:52.984726: step 13000, acc = 0.9453, f1 = 0.9449
[Status] 2017-05-09 14:53:52.984802: step 13000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 14:55:08.551871: step 13020, loss = 0.0121, acc = 1.0000 (17.5 examples/sec; 3.666 sec/batch)
2017-05-09 14:56:23.132044: step 13040, loss = 0.0103, acc = 1.0000 (17.3 examples/sec; 3.689 sec/batch)
2017-05-09 14:57:38.803448: step 13060, loss = 0.0131, acc = 1.0000 (16.2 examples/sec; 3.950 sec/batch)
2017-05-09 14:58:53.664053: step 13080, loss = 0.0178, acc = 0.9960 (16.4 examples/sec; 3.898 sec/batch)
2017-05-09 15:00:08.963422: step 13100, loss = 0.0182, acc = 1.0000 (15.9 examples/sec; 4.034 sec/batch)
2017-05-09 15:01:24.736902: step 13120, loss = 0.0897, acc = 0.9660 (17.0 examples/sec; 3.755 sec/batch)
2017-05-09 15:02:41.731642: step 13140, loss = 0.3646, acc = 0.9080 (15.6 examples/sec; 4.098 sec/batch)
2017-05-09 15:03:55.952941: step 13160, loss = 0.0819, acc = 0.9940 (17.6 examples/sec; 3.633 sec/batch)
2017-05-09 15:05:10.282542: step 13180, loss = 0.1217, acc = 0.9860 (16.9 examples/sec; 3.784 sec/batch)
2017-05-09 15:06:23.365810: step 13200, loss = 0.1603, acc = 0.9720 (17.8 examples/sec; 3.587 sec/batch)
2017-05-09 15:07:38.012157: step 13220, loss = 0.0892, acc = 0.9860 (17.5 examples/sec; 3.665 sec/batch)
2017-05-09 15:08:53.524877: step 13240, loss = 0.0855, acc = 0.9940 (16.6 examples/sec; 3.850 sec/batch)
2017-05-09 15:10:08.556686: step 13260, loss = 0.0537, acc = 0.9980 (17.4 examples/sec; 3.687 sec/batch)
2017-05-09 15:11:21.186500: step 13280, loss = 0.0530, acc = 0.9980 (17.5 examples/sec; 3.658 sec/batch)
2017-05-09 15:12:33.845671: step 13300, loss = 0.0867, acc = 0.9860 (17.4 examples/sec; 3.674 sec/batch)
2017-05-09 15:13:48.747064: step 13320, loss = 0.0337, acc = 0.9980 (17.1 examples/sec; 3.742 sec/batch)
2017-05-09 15:15:03.702876: step 13340, loss = 0.0333, acc = 0.9980 (16.7 examples/sec; 3.842 sec/batch)
2017-05-09 15:16:18.786373: step 13360, loss = 0.0329, acc = 0.9940 (17.1 examples/sec; 3.735 sec/batch)
2017-05-09 15:17:34.696111: step 13380, loss = 0.0225, acc = 1.0000 (16.9 examples/sec; 3.797 sec/batch)
2017-05-09 15:18:49.522074: step 13400, loss = 0.0249, acc = 0.9980 (17.6 examples/sec; 3.631 sec/batch)
2017-05-09 15:20:04.551839: step 13420, loss = 0.0214, acc = 0.9980 (16.9 examples/sec; 3.793 sec/batch)
2017-05-09 15:21:18.522320: step 13440, loss = 0.0199, acc = 1.0000 (17.3 examples/sec; 3.696 sec/batch)
2017-05-09 15:22:34.574711: step 13460, loss = 0.0366, acc = 0.9940 (16.9 examples/sec; 3.782 sec/batch)
2017-05-09 15:23:50.404029: step 13480, loss = 0.0201, acc = 0.9980 (17.4 examples/sec; 3.683 sec/batch)
2017-05-09 15:25:06.451750: step 13500, loss = 0.0262, acc = 0.9960 (16.5 examples/sec; 3.871 sec/batch)
2017-05-09 15:26:21.026831: step 13520, loss = 0.0255, acc = 0.9960 (17.3 examples/sec; 3.706 sec/batch)
2017-05-09 15:27:35.915121: step 13540, loss = 0.0175, acc = 1.0000 (16.6 examples/sec; 3.848 sec/batch)
2017-05-09 15:28:51.001526: step 13560, loss = 0.0225, acc = 0.9940 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 15:30:05.436618: step 13580, loss = 0.0171, acc = 0.9980 (16.7 examples/sec; 3.826 sec/batch)
2017-05-09 15:31:19.113769: step 13600, loss = 0.0162, acc = 1.0000 (17.8 examples/sec; 3.600 sec/batch)
2017-05-09 15:32:32.284203: step 13620, loss = 0.0277, acc = 0.9920 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 15:33:47.523900: step 13640, loss = 0.0186, acc = 0.9980 (16.6 examples/sec; 3.856 sec/batch)
2017-05-09 15:35:01.942504: step 13660, loss = 0.0186, acc = 0.9980 (17.8 examples/sec; 3.605 sec/batch)
2017-05-09 15:36:15.466828: step 13680, loss = 0.0161, acc = 0.9980 (17.2 examples/sec; 3.713 sec/batch)
2017-05-09 15:37:30.072211: step 13700, loss = 0.0205, acc = 0.9960 (16.8 examples/sec; 3.820 sec/batch)
2017-05-09 15:38:46.440509: step 13720, loss = 0.0135, acc = 1.0000 (16.4 examples/sec; 3.895 sec/batch)
2017-05-09 15:40:02.367453: step 13740, loss = 0.0157, acc = 1.0000 (16.8 examples/sec; 3.800 sec/batch)
2017-05-09 15:41:17.619311: step 13760, loss = 0.0227, acc = 0.9960 (17.2 examples/sec; 3.711 sec/batch)
2017-05-09 15:42:32.974983: step 13780, loss = 0.0185, acc = 0.9980 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 15:43:48.647344: step 13800, loss = 0.0148, acc = 0.9980 (16.7 examples/sec; 3.825 sec/batch)
2017-05-09 15:45:02.666029: step 13820, loss = 0.0126, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-09 15:46:18.087936: step 13840, loss = 0.0131, acc = 0.9980 (18.0 examples/sec; 3.565 sec/batch)
2017-05-09 15:47:33.278484: step 13860, loss = 0.0282, acc = 0.9940 (17.0 examples/sec; 3.757 sec/batch)
2017-05-09 15:48:48.245245: step 13880, loss = 0.0163, acc = 0.9980 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 15:50:04.054155: step 13900, loss = 0.0147, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-09 15:51:17.616299: step 13920, loss = 0.0179, acc = 0.9980 (16.8 examples/sec; 3.800 sec/batch)
2017-05-09 15:52:31.178623: step 13940, loss = 0.0165, acc = 0.9980 (18.0 examples/sec; 3.555 sec/batch)
2017-05-09 15:53:45.060371: step 13960, loss = 0.0134, acc = 1.0000 (16.4 examples/sec; 3.914 sec/batch)
2017-05-09 15:54:59.822634: step 13980, loss = 0.0172, acc = 0.9960 (17.2 examples/sec; 3.725 sec/batch)
2017-05-09 15:56:13.947129: step 14000, loss = 0.0181, acc = 0.9980 (17.2 examples/sec; 3.723 sec/batch)
[Eval] 2017-05-09 15:56:30.103716: step 14000, acc = 0.9551, f1 = 0.9537
[Test] 2017-05-09 15:56:41.081259: step 14000, acc = 0.9429, f1 = 0.9425
[Status] 2017-05-09 15:56:41.081331: step 14000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 15:57:56.540857: step 14020, loss = 0.0137, acc = 1.0000 (16.9 examples/sec; 3.783 sec/batch)
2017-05-09 15:59:11.280699: step 14040, loss = 0.0144, acc = 0.9980 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 16:00:25.739118: step 14060, loss = 0.0136, acc = 1.0000 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 16:01:42.082840: step 14080, loss = 0.0146, acc = 1.0000 (16.8 examples/sec; 3.817 sec/batch)
2017-05-09 16:02:54.357044: step 14100, loss = 0.0155, acc = 1.0000 (17.8 examples/sec; 3.591 sec/batch)
2017-05-09 16:04:09.276728: step 14120, loss = 0.0119, acc = 1.0000 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 16:05:22.415272: step 14140, loss = 0.0159, acc = 0.9960 (17.5 examples/sec; 3.651 sec/batch)
2017-05-09 16:06:34.896317: step 14160, loss = 0.0117, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-09 16:07:49.965814: step 14180, loss = 0.0136, acc = 1.0000 (17.8 examples/sec; 3.605 sec/batch)
2017-05-09 16:09:06.103006: step 14200, loss = 0.0109, acc = 1.0000 (16.7 examples/sec; 3.842 sec/batch)
2017-05-09 16:10:18.266316: step 14220, loss = 0.0109, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-09 16:11:32.558945: step 14240, loss = 0.0111, acc = 1.0000 (16.9 examples/sec; 3.791 sec/batch)
2017-05-09 16:12:46.814558: step 14260, loss = 0.0086, acc = 1.0000 (17.6 examples/sec; 3.644 sec/batch)
2017-05-09 16:14:03.797993: step 14280, loss = 0.0128, acc = 0.9960 (16.7 examples/sec; 3.835 sec/batch)
2017-05-09 16:15:16.819066: step 14300, loss = 0.0101, acc = 1.0000 (17.0 examples/sec; 3.772 sec/batch)
2017-05-09 16:16:32.917873: step 14320, loss = 0.0113, acc = 1.0000 (16.6 examples/sec; 3.867 sec/batch)
2017-05-09 16:17:47.578194: step 14340, loss = 0.0104, acc = 1.0000 (17.3 examples/sec; 3.710 sec/batch)
2017-05-09 16:19:01.886714: step 14360, loss = 0.0098, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-09 16:20:14.464080: step 14380, loss = 0.0114, acc = 1.0000 (17.4 examples/sec; 3.673 sec/batch)
2017-05-09 16:21:29.709929: step 14400, loss = 0.0112, acc = 1.0000 (15.8 examples/sec; 4.044 sec/batch)
2017-05-09 16:22:44.707098: step 14420, loss = 0.0127, acc = 1.0000 (17.1 examples/sec; 3.742 sec/batch)
2017-05-09 16:23:58.845762: step 14440, loss = 0.0142, acc = 0.9960 (17.6 examples/sec; 3.646 sec/batch)
2017-05-09 16:25:14.370997: step 14460, loss = 0.0111, acc = 1.0000 (16.9 examples/sec; 3.791 sec/batch)
2017-05-09 16:26:29.406204: step 14480, loss = 0.0171, acc = 0.9980 (17.4 examples/sec; 3.674 sec/batch)
2017-05-09 16:27:43.577836: step 14500, loss = 0.0113, acc = 1.0000 (17.7 examples/sec; 3.606 sec/batch)
2017-05-09 16:28:59.920833: step 14520, loss = 0.0119, acc = 0.9980 (16.7 examples/sec; 3.840 sec/batch)
2017-05-09 16:30:13.127794: step 14540, loss = 0.0106, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-09 16:31:28.442219: step 14560, loss = 0.0117, acc = 1.0000 (17.9 examples/sec; 3.580 sec/batch)
2017-05-09 16:32:40.727139: step 14580, loss = 0.0110, acc = 1.0000 (17.3 examples/sec; 3.706 sec/batch)
2017-05-09 16:33:57.382865: step 14600, loss = 0.0105, acc = 1.0000 (16.5 examples/sec; 3.875 sec/batch)
2017-05-09 16:35:12.646276: step 14620, loss = 0.0106, acc = 1.0000 (17.7 examples/sec; 3.625 sec/batch)
2017-05-09 16:36:26.407590: step 14640, loss = 0.0102, acc = 0.9980 (17.7 examples/sec; 3.617 sec/batch)
2017-05-09 16:37:39.898958: step 14660, loss = 0.0099, acc = 0.9980 (17.0 examples/sec; 3.766 sec/batch)
2017-05-09 16:38:55.010909: step 14680, loss = 0.0102, acc = 1.0000 (17.1 examples/sec; 3.751 sec/batch)
2017-05-09 16:40:10.987680: step 14700, loss = 0.0129, acc = 1.0000 (17.0 examples/sec; 3.763 sec/batch)
2017-05-09 16:41:25.697338: step 14720, loss = 0.0094, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-09 16:42:41.182188: step 14740, loss = 0.0110, acc = 0.9980 (17.1 examples/sec; 3.741 sec/batch)
2017-05-09 16:43:55.779459: step 14760, loss = 0.0110, acc = 1.0000 (17.6 examples/sec; 3.628 sec/batch)
2017-05-09 16:45:09.254550: step 14780, loss = 0.0104, acc = 1.0000 (17.4 examples/sec; 3.674 sec/batch)
2017-05-09 16:46:24.430473: step 14800, loss = 0.0100, acc = 1.0000 (16.8 examples/sec; 3.819 sec/batch)
2017-05-09 16:47:38.770299: step 14820, loss = 0.0151, acc = 0.9980 (16.9 examples/sec; 3.782 sec/batch)
2017-05-09 16:48:54.732373: step 14840, loss = 0.0130, acc = 0.9980 (17.0 examples/sec; 3.765 sec/batch)
2017-05-09 16:50:07.114173: step 14860, loss = 0.0113, acc = 1.0000 (17.4 examples/sec; 3.671 sec/batch)
2017-05-09 16:51:21.668634: step 14880, loss = 0.0099, acc = 1.0000 (17.8 examples/sec; 3.594 sec/batch)
2017-05-09 16:52:36.263314: step 14900, loss = 0.0106, acc = 1.0000 (16.9 examples/sec; 3.779 sec/batch)
2017-05-09 16:53:49.243795: step 14920, loss = 0.0139, acc = 0.9980 (17.8 examples/sec; 3.605 sec/batch)
2017-05-09 16:55:04.688431: step 14940, loss = 0.0099, acc = 1.0000 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 16:56:18.496791: step 14960, loss = 0.0097, acc = 1.0000 (17.4 examples/sec; 3.671 sec/batch)
2017-05-09 16:57:32.830898: step 14980, loss = 0.0107, acc = 1.0000 (17.4 examples/sec; 3.670 sec/batch)
2017-05-09 16:58:47.694007: step 15000, loss = 0.0103, acc = 1.0000 (16.8 examples/sec; 3.801 sec/batch)
[Eval] 2017-05-09 16:59:03.590290: step 15000, acc = 0.9548, f1 = 0.9533
[Test] 2017-05-09 16:59:14.445938: step 15000, acc = 0.9436, f1 = 0.9431
[Status] 2017-05-09 16:59:14.446009: step 15000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 17:00:30.070284: step 15020, loss = 0.0096, acc = 1.0000 (16.8 examples/sec; 3.803 sec/batch)
2017-05-09 17:01:44.362688: step 15040, loss = 0.0100, acc = 1.0000 (17.4 examples/sec; 3.686 sec/batch)
2017-05-09 17:02:58.549518: step 15060, loss = 0.0098, acc = 1.0000 (17.2 examples/sec; 3.723 sec/batch)
2017-05-09 17:04:13.450776: step 15080, loss = 0.0114, acc = 1.0000 (17.5 examples/sec; 3.660 sec/batch)
2017-05-09 17:05:28.391042: step 15100, loss = 0.0113, acc = 0.9980 (17.0 examples/sec; 3.775 sec/batch)
2017-05-09 17:06:43.975083: step 15120, loss = 0.0088, acc = 1.0000 (16.9 examples/sec; 3.795 sec/batch)
2017-05-09 17:07:56.899192: step 15140, loss = 0.0088, acc = 1.0000 (17.6 examples/sec; 3.631 sec/batch)
2017-05-09 17:09:12.668014: step 15160, loss = 0.0082, acc = 1.0000 (16.5 examples/sec; 3.874 sec/batch)
2017-05-09 17:10:25.900116: step 15180, loss = 0.0087, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-09 17:11:39.269553: step 15200, loss = 0.0086, acc = 1.0000 (17.3 examples/sec; 3.706 sec/batch)
2017-05-09 17:12:54.180672: step 15220, loss = 0.0096, acc = 1.0000 (16.8 examples/sec; 3.800 sec/batch)
2017-05-09 17:14:08.582455: step 15240, loss = 0.0082, acc = 1.0000 (17.3 examples/sec; 3.708 sec/batch)
2017-05-09 17:15:22.750905: step 15260, loss = 0.0076, acc = 1.0000 (17.5 examples/sec; 3.653 sec/batch)
2017-05-09 17:16:36.975323: step 15280, loss = 0.0090, acc = 1.0000 (16.9 examples/sec; 3.788 sec/batch)
2017-05-09 17:17:51.142594: step 15300, loss = 0.0099, acc = 1.0000 (17.9 examples/sec; 3.578 sec/batch)
2017-05-09 17:19:03.508743: step 15320, loss = 0.0095, acc = 1.0000 (17.2 examples/sec; 3.718 sec/batch)
2017-05-09 17:20:17.561313: step 15340, loss = 0.0098, acc = 1.0000 (16.6 examples/sec; 3.858 sec/batch)
2017-05-09 17:21:32.287483: step 15360, loss = 0.0085, acc = 1.0000 (16.4 examples/sec; 3.898 sec/batch)
2017-05-09 17:22:46.804927: step 15380, loss = 0.0082, acc = 1.0000 (16.8 examples/sec; 3.812 sec/batch)
2017-05-09 17:24:00.528643: step 15400, loss = 0.0089, acc = 1.0000 (17.3 examples/sec; 3.704 sec/batch)
2017-05-09 17:25:14.817776: step 15420, loss = 0.0085, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-09 17:26:30.351523: step 15440, loss = 0.0143, acc = 0.9980 (16.8 examples/sec; 3.806 sec/batch)
2017-05-09 17:27:42.402391: step 15460, loss = 0.0086, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-09 17:28:56.193244: step 15480, loss = 0.0111, acc = 0.9980 (17.6 examples/sec; 3.642 sec/batch)
2017-05-09 17:30:11.218056: step 15500, loss = 0.0087, acc = 1.0000 (17.9 examples/sec; 3.579 sec/batch)
2017-05-09 17:31:25.566336: step 15520, loss = 0.0110, acc = 0.9980 (17.0 examples/sec; 3.758 sec/batch)
2017-05-09 17:32:38.191505: step 15540, loss = 0.0102, acc = 1.0000 (17.4 examples/sec; 3.673 sec/batch)
2017-05-09 17:33:53.569730: step 15560, loss = 0.0087, acc = 1.0000 (17.2 examples/sec; 3.719 sec/batch)
2017-05-09 17:35:08.090962: step 15580, loss = 0.0084, acc = 1.0000 (16.7 examples/sec; 3.829 sec/batch)
2017-05-09 17:36:22.410104: step 15600, loss = 0.0093, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-09 17:37:35.747179: step 15620, loss = 0.0115, acc = 0.9980 (17.1 examples/sec; 3.733 sec/batch)
2017-05-09 17:38:51.259549: step 15640, loss = 0.0143, acc = 0.9960 (16.7 examples/sec; 3.823 sec/batch)
2017-05-09 17:40:05.325883: step 15660, loss = 0.0099, acc = 0.9980 (16.9 examples/sec; 3.798 sec/batch)
2017-05-09 17:41:18.941224: step 15680, loss = 0.0094, acc = 1.0000 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 17:42:34.593895: step 15700, loss = 0.0101, acc = 1.0000 (17.0 examples/sec; 3.757 sec/batch)
2017-05-09 17:43:48.047967: step 15720, loss = 0.0183, acc = 0.9980 (17.4 examples/sec; 3.674 sec/batch)
2017-05-09 17:45:02.570015: step 15740, loss = 0.0104, acc = 0.9980 (17.4 examples/sec; 3.679 sec/batch)
2017-05-09 17:46:17.532471: step 15760, loss = 0.0087, acc = 1.0000 (17.9 examples/sec; 3.583 sec/batch)
2017-05-09 17:47:34.005541: step 15780, loss = 0.0107, acc = 1.0000 (16.8 examples/sec; 3.817 sec/batch)
2017-05-09 17:48:48.977549: step 15800, loss = 0.0095, acc = 1.0000 (17.1 examples/sec; 3.738 sec/batch)
2017-05-09 17:50:04.273314: step 15820, loss = 0.0163, acc = 0.9980 (15.9 examples/sec; 4.016 sec/batch)
2017-05-09 17:51:19.696596: step 15840, loss = 0.0089, acc = 1.0000 (17.4 examples/sec; 3.683 sec/batch)
2017-05-09 17:52:35.047355: step 15860, loss = 0.0075, acc = 1.0000 (17.4 examples/sec; 3.677 sec/batch)
2017-05-09 17:53:48.672453: step 15880, loss = 0.0092, acc = 1.0000 (16.9 examples/sec; 3.792 sec/batch)
2017-05-09 17:55:04.416024: step 15900, loss = 0.0095, acc = 1.0000 (17.2 examples/sec; 3.716 sec/batch)
2017-05-09 17:56:19.973939: step 15920, loss = 0.0091, acc = 1.0000 (16.9 examples/sec; 3.797 sec/batch)
2017-05-09 17:57:32.832843: step 15940, loss = 0.0105, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-09 17:58:47.759953: step 15960, loss = 0.0110, acc = 0.9980 (17.8 examples/sec; 3.596 sec/batch)
2017-05-09 18:00:02.473270: step 15980, loss = 0.0099, acc = 0.9980 (17.9 examples/sec; 3.566 sec/batch)
2017-05-09 18:01:15.441891: step 16000, loss = 0.0084, acc = 1.0000 (16.8 examples/sec; 3.819 sec/batch)
[Eval] 2017-05-09 18:01:31.515141: step 16000, acc = 0.9551, f1 = 0.9537
[Test] 2017-05-09 18:01:42.004204: step 16000, acc = 0.9443, f1 = 0.9438
[Status] 2017-05-09 18:01:42.004280: step 16000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 18:02:54.838354: step 16020, loss = 0.0088, acc = 1.0000 (17.3 examples/sec; 3.692 sec/batch)
2017-05-09 18:04:08.691347: step 16040, loss = 0.0088, acc = 1.0000 (16.6 examples/sec; 3.849 sec/batch)
2017-05-09 18:05:25.341772: step 16060, loss = 0.0086, acc = 1.0000 (16.5 examples/sec; 3.877 sec/batch)
2017-05-09 18:06:39.114792: step 16080, loss = 0.0079, acc = 1.0000 (16.5 examples/sec; 3.872 sec/batch)
2017-05-09 18:07:54.441398: step 16100, loss = 0.0094, acc = 1.0000 (16.9 examples/sec; 3.791 sec/batch)
2017-05-09 18:09:09.566366: step 16120, loss = 0.0091, acc = 1.0000 (16.8 examples/sec; 3.816 sec/batch)
2017-05-09 18:10:22.932099: step 16140, loss = 0.0083, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-09 18:11:36.484566: step 16160, loss = 0.0083, acc = 1.0000 (17.6 examples/sec; 3.644 sec/batch)
2017-05-09 18:12:51.850001: step 16180, loss = 0.0085, acc = 1.0000 (16.5 examples/sec; 3.877 sec/batch)
2017-05-09 18:14:06.824710: step 16200, loss = 0.0075, acc = 1.0000 (17.2 examples/sec; 3.711 sec/batch)
2017-05-09 18:15:21.486157: step 16220, loss = 0.0096, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-09 18:16:37.398208: step 16240, loss = 0.0071, acc = 1.0000 (17.0 examples/sec; 3.758 sec/batch)
2017-05-09 18:17:51.734615: step 16260, loss = 0.0095, acc = 0.9980 (16.6 examples/sec; 3.867 sec/batch)
2017-05-09 18:19:03.566276: step 16280, loss = 0.0076, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-09 18:20:19.666047: step 16300, loss = 0.0081, acc = 1.0000 (17.3 examples/sec; 3.710 sec/batch)
2017-05-09 18:21:34.842606: step 16320, loss = 0.0076, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-09 18:22:47.203306: step 16340, loss = 0.0071, acc = 1.0000 (17.3 examples/sec; 3.695 sec/batch)
2017-05-09 18:24:02.937455: step 16360, loss = 0.0072, acc = 1.0000 (17.1 examples/sec; 3.738 sec/batch)
2017-05-09 18:25:19.316156: step 16380, loss = 0.0080, acc = 1.0000 (17.1 examples/sec; 3.747 sec/batch)
2017-05-09 18:26:34.491042: step 16400, loss = 0.0099, acc = 0.9980 (16.6 examples/sec; 3.854 sec/batch)
2017-05-09 18:27:47.882877: step 16420, loss = 0.0097, acc = 0.9980 (17.9 examples/sec; 3.568 sec/batch)
2017-05-09 18:29:02.406865: step 16440, loss = 0.0089, acc = 1.0000 (17.5 examples/sec; 3.664 sec/batch)
2017-05-09 18:30:17.437284: step 16460, loss = 0.0175, acc = 0.9980 (17.3 examples/sec; 3.701 sec/batch)
2017-05-09 18:31:31.147570: step 16480, loss = 0.0086, acc = 1.0000 (17.6 examples/sec; 3.641 sec/batch)
2017-05-09 18:32:44.822106: step 16500, loss = 0.0090, acc = 1.0000 (17.2 examples/sec; 3.726 sec/batch)
2017-05-09 18:34:00.361652: step 16520, loss = 0.0080, acc = 1.0000 (16.8 examples/sec; 3.817 sec/batch)
2017-05-09 18:35:16.031900: step 16540, loss = 0.0119, acc = 0.9980 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 18:36:28.609738: step 16560, loss = 0.0076, acc = 1.0000 (17.1 examples/sec; 3.743 sec/batch)
2017-05-09 18:37:42.700488: step 16580, loss = 0.0086, acc = 1.0000 (17.0 examples/sec; 3.767 sec/batch)
2017-05-09 18:38:56.449616: step 16600, loss = 0.0082, acc = 1.0000 (17.0 examples/sec; 3.767 sec/batch)
2017-05-09 18:40:11.607290: step 16620, loss = 0.0081, acc = 1.0000 (17.3 examples/sec; 3.703 sec/batch)
2017-05-09 18:41:25.264730: step 16640, loss = 0.0095, acc = 1.0000 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 18:42:43.158687: step 16660, loss = 0.0078, acc = 1.0000 (16.6 examples/sec; 3.857 sec/batch)
2017-05-09 18:43:59.411316: step 16680, loss = 0.0076, acc = 1.0000 (16.6 examples/sec; 3.845 sec/batch)
2017-05-09 18:45:14.659529: step 16700, loss = 0.0114, acc = 1.0000 (16.8 examples/sec; 3.806 sec/batch)
2017-05-09 18:46:29.415370: step 16720, loss = 0.0081, acc = 1.0000 (17.1 examples/sec; 3.747 sec/batch)
2017-05-09 18:47:44.730998: step 16740, loss = 0.0077, acc = 1.0000 (17.4 examples/sec; 3.681 sec/batch)
2017-05-09 18:48:57.631822: step 16760, loss = 0.0093, acc = 0.9980 (17.7 examples/sec; 3.619 sec/batch)
2017-05-09 18:50:11.286267: step 16780, loss = 0.0102, acc = 0.9980 (16.5 examples/sec; 3.878 sec/batch)
2017-05-09 18:51:25.851925: step 16800, loss = 0.0075, acc = 1.0000 (17.3 examples/sec; 3.706 sec/batch)
2017-05-09 18:52:41.889936: step 16820, loss = 0.0089, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-09 18:53:58.081308: step 16840, loss = 0.0082, acc = 1.0000 (17.2 examples/sec; 3.728 sec/batch)
2017-05-09 18:55:13.214043: step 16860, loss = 0.0085, acc = 1.0000 (16.9 examples/sec; 3.792 sec/batch)
2017-05-09 18:56:27.369293: step 16880, loss = 0.0085, acc = 1.0000 (17.3 examples/sec; 3.692 sec/batch)
2017-05-09 18:57:41.110960: step 16900, loss = 0.0074, acc = 1.0000 (17.1 examples/sec; 3.743 sec/batch)
2017-05-09 18:58:55.524702: step 16920, loss = 0.0085, acc = 1.0000 (16.7 examples/sec; 3.833 sec/batch)
2017-05-09 19:00:10.722926: step 16940, loss = 0.0094, acc = 1.0000 (17.4 examples/sec; 3.681 sec/batch)
2017-05-09 19:01:26.692778: step 16960, loss = 0.0078, acc = 1.0000 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 19:02:40.665228: step 16980, loss = 0.0088, acc = 1.0000 (17.1 examples/sec; 3.747 sec/batch)
2017-05-09 19:03:52.792673: step 17000, loss = 0.0082, acc = 1.0000 (17.8 examples/sec; 3.597 sec/batch)
[Eval] 2017-05-09 19:04:08.796023: step 17000, acc = 0.9544, f1 = 0.9529
[Test] 2017-05-09 19:04:19.762195: step 17000, acc = 0.9434, f1 = 0.9430
[Status] 2017-05-09 19:04:19.762295: step 17000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 19:05:36.087716: step 17020, loss = 0.0084, acc = 1.0000 (16.8 examples/sec; 3.815 sec/batch)
2017-05-09 19:06:49.572783: step 17040, loss = 0.0095, acc = 1.0000 (16.5 examples/sec; 3.882 sec/batch)
2017-05-09 19:08:05.006991: step 17060, loss = 0.0077, acc = 1.0000 (17.3 examples/sec; 3.693 sec/batch)
2017-05-09 19:09:19.986937: step 17080, loss = 0.0071, acc = 1.0000 (17.2 examples/sec; 3.717 sec/batch)
2017-05-09 19:10:34.358928: step 17100, loss = 0.0094, acc = 1.0000 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 19:11:47.687548: step 17120, loss = 0.0094, acc = 1.0000 (17.6 examples/sec; 3.632 sec/batch)
2017-05-09 19:13:03.680890: step 17140, loss = 0.0072, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-09 19:14:18.540613: step 17160, loss = 0.0078, acc = 1.0000 (17.6 examples/sec; 3.642 sec/batch)
2017-05-09 19:15:32.819546: step 17180, loss = 0.0081, acc = 1.0000 (16.6 examples/sec; 3.849 sec/batch)
2017-05-09 19:16:49.343349: step 17200, loss = 0.0076, acc = 1.0000 (16.7 examples/sec; 3.836 sec/batch)
2017-05-09 19:18:04.972584: step 17220, loss = 0.0077, acc = 1.0000 (16.8 examples/sec; 3.806 sec/batch)
2017-05-09 19:19:18.247462: step 17240, loss = 0.0085, acc = 1.0000 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 19:20:33.094715: step 17260, loss = 0.0076, acc = 1.0000 (16.7 examples/sec; 3.828 sec/batch)
2017-05-09 19:21:48.597200: step 17280, loss = 0.0085, acc = 1.0000 (16.5 examples/sec; 3.879 sec/batch)
2017-05-09 19:23:03.085638: step 17300, loss = 0.0084, acc = 1.0000 (17.8 examples/sec; 3.600 sec/batch)
2017-05-09 19:24:17.280855: step 17320, loss = 0.0070, acc = 1.0000 (16.4 examples/sec; 3.896 sec/batch)
2017-05-09 19:25:30.089627: step 17340, loss = 0.0090, acc = 0.9980 (17.9 examples/sec; 3.581 sec/batch)
2017-05-09 19:26:44.092251: step 17360, loss = 0.0074, acc = 1.0000 (16.9 examples/sec; 3.790 sec/batch)
2017-05-09 19:27:59.416611: step 17380, loss = 0.0073, acc = 1.0000 (16.7 examples/sec; 3.827 sec/batch)
2017-05-09 19:29:14.767961: step 17400, loss = 0.0081, acc = 1.0000 (17.1 examples/sec; 3.736 sec/batch)
2017-05-09 19:30:27.981988: step 17420, loss = 0.0083, acc = 1.0000 (17.8 examples/sec; 3.589 sec/batch)
2017-05-09 19:31:41.464083: step 17440, loss = 0.0071, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-09 19:32:55.581862: step 17460, loss = 0.0079, acc = 1.0000 (16.8 examples/sec; 3.799 sec/batch)
2017-05-09 19:34:09.008580: step 17480, loss = 0.0081, acc = 1.0000 (17.7 examples/sec; 3.606 sec/batch)
2017-05-09 19:35:23.487951: step 17500, loss = 0.0070, acc = 1.0000 (16.9 examples/sec; 3.796 sec/batch)
2017-05-09 19:36:35.944228: step 17520, loss = 0.0072, acc = 1.0000 (17.6 examples/sec; 3.626 sec/batch)
2017-05-09 19:37:48.930443: step 17540, loss = 0.0096, acc = 0.9980 (16.8 examples/sec; 3.809 sec/batch)
2017-05-09 19:39:04.378601: step 17560, loss = 0.0084, acc = 1.0000 (16.0 examples/sec; 4.000 sec/batch)
2017-05-09 19:40:20.368140: step 17580, loss = 0.0070, acc = 1.0000 (16.9 examples/sec; 3.780 sec/batch)
2017-05-09 19:41:35.484448: step 17600, loss = 0.0065, acc = 1.0000 (16.2 examples/sec; 3.940 sec/batch)
2017-05-09 19:42:49.486994: step 17620, loss = 0.0078, acc = 1.0000 (17.5 examples/sec; 3.647 sec/batch)
2017-05-09 19:44:04.602573: step 17640, loss = 0.0076, acc = 1.0000 (17.2 examples/sec; 3.722 sec/batch)
2017-05-09 19:45:18.532526: step 17660, loss = 0.0074, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-09 19:46:33.979841: step 17680, loss = 0.0070, acc = 1.0000 (17.6 examples/sec; 3.642 sec/batch)
2017-05-09 19:47:49.015011: step 17700, loss = 0.0079, acc = 1.0000 (16.6 examples/sec; 3.849 sec/batch)
2017-05-09 19:49:02.184367: step 17720, loss = 0.0116, acc = 0.9980 (17.2 examples/sec; 3.723 sec/batch)
2017-05-09 19:50:14.792514: step 17740, loss = 0.0073, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-09 19:51:29.600695: step 17760, loss = 0.0075, acc = 1.0000 (16.8 examples/sec; 3.813 sec/batch)
2017-05-09 19:52:45.376647: step 17780, loss = 0.0080, acc = 1.0000 (16.5 examples/sec; 3.872 sec/batch)
2017-05-09 19:54:00.730111: step 17800, loss = 0.0076, acc = 1.0000 (17.0 examples/sec; 3.756 sec/batch)
2017-05-09 19:55:15.538127: step 17820, loss = 0.0076, acc = 1.0000 (16.8 examples/sec; 3.806 sec/batch)
2017-05-09 19:56:29.946445: step 17840, loss = 0.0079, acc = 1.0000 (17.0 examples/sec; 3.764 sec/batch)
2017-05-09 19:57:44.225549: step 17860, loss = 0.0068, acc = 1.0000 (16.8 examples/sec; 3.810 sec/batch)
2017-05-09 19:59:00.079957: step 17880, loss = 0.0082, acc = 1.0000 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 20:00:15.640879: step 17900, loss = 0.0069, acc = 1.0000 (16.6 examples/sec; 3.847 sec/batch)
2017-05-09 20:01:28.765271: step 17920, loss = 0.0087, acc = 1.0000 (17.4 examples/sec; 3.672 sec/batch)
2017-05-09 20:02:42.051245: step 17940, loss = 0.0071, acc = 1.0000 (16.6 examples/sec; 3.846 sec/batch)
2017-05-09 20:03:56.981024: step 17960, loss = 0.0073, acc = 1.0000 (16.9 examples/sec; 3.797 sec/batch)
2017-05-09 20:05:12.301550: step 17980, loss = 0.0074, acc = 1.0000 (16.7 examples/sec; 3.832 sec/batch)
2017-05-09 20:06:27.914303: step 18000, loss = 0.0066, acc = 1.0000 (16.8 examples/sec; 3.810 sec/batch)
[Eval] 2017-05-09 20:06:43.950219: step 18000, acc = 0.9543, f1 = 0.9527
[Test] 2017-05-09 20:06:54.736880: step 18000, acc = 0.9433, f1 = 0.9428
[Status] 2017-05-09 20:06:54.736960: step 18000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 20:08:09.336379: step 18020, loss = 0.0100, acc = 0.9980 (17.4 examples/sec; 3.676 sec/batch)
2017-05-09 20:09:24.634026: step 18040, loss = 0.0133, acc = 0.9980 (16.3 examples/sec; 3.925 sec/batch)
2017-05-09 20:10:40.703371: step 18060, loss = 0.0073, acc = 1.0000 (16.9 examples/sec; 3.796 sec/batch)
2017-05-09 20:11:55.473382: step 18080, loss = 0.0085, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-09 20:13:12.153660: step 18100, loss = 0.0072, acc = 1.0000 (16.3 examples/sec; 3.915 sec/batch)
2017-05-09 20:14:26.021035: step 18120, loss = 0.0084, acc = 1.0000 (17.7 examples/sec; 3.608 sec/batch)
2017-05-09 20:15:42.034494: step 18140, loss = 0.0079, acc = 1.0000 (16.5 examples/sec; 3.878 sec/batch)
2017-05-09 20:16:58.407545: step 18160, loss = 0.0087, acc = 0.9980 (16.7 examples/sec; 3.837 sec/batch)
2017-05-09 20:18:11.775246: step 18180, loss = 0.0065, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-09 20:19:26.110693: step 18200, loss = 0.0070, acc = 1.0000 (17.2 examples/sec; 3.727 sec/batch)
2017-05-09 20:20:41.567020: step 18220, loss = 0.0066, acc = 1.0000 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 20:21:56.175124: step 18240, loss = 0.0065, acc = 1.0000 (16.6 examples/sec; 3.859 sec/batch)
2017-05-09 20:23:13.055854: step 18260, loss = 0.0063, acc = 1.0000 (17.2 examples/sec; 3.725 sec/batch)
2017-05-09 20:24:29.876254: step 18280, loss = 0.0068, acc = 1.0000 (16.8 examples/sec; 3.810 sec/batch)
2017-05-09 20:25:45.756159: step 18300, loss = 0.0068, acc = 1.0000 (16.6 examples/sec; 3.852 sec/batch)
2017-05-09 20:27:01.536210: step 18320, loss = 0.0073, acc = 1.0000 (16.9 examples/sec; 3.786 sec/batch)
2017-05-09 20:28:16.399407: step 18340, loss = 0.0072, acc = 1.0000 (17.9 examples/sec; 3.568 sec/batch)
2017-05-09 20:29:32.810669: step 18360, loss = 0.0061, acc = 1.0000 (16.4 examples/sec; 3.897 sec/batch)
2017-05-09 20:30:47.659678: step 18380, loss = 0.0083, acc = 1.0000 (18.0 examples/sec; 3.560 sec/batch)
2017-05-09 20:32:01.496668: step 18400, loss = 0.0064, acc = 1.0000 (17.1 examples/sec; 3.742 sec/batch)
2017-05-09 20:33:16.332131: step 18420, loss = 0.0107, acc = 0.9980 (17.9 examples/sec; 3.575 sec/batch)
2017-05-09 20:34:29.837283: step 18440, loss = 0.0074, acc = 1.0000 (16.7 examples/sec; 3.825 sec/batch)
2017-05-09 20:35:44.540786: step 18460, loss = 0.0068, acc = 1.0000 (17.4 examples/sec; 3.682 sec/batch)
2017-05-09 20:36:58.490245: step 18480, loss = 0.0079, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-09 20:38:13.010706: step 18500, loss = 0.0070, acc = 1.0000 (16.9 examples/sec; 3.789 sec/batch)
2017-05-09 20:39:25.729898: step 18520, loss = 0.0062, acc = 1.0000 (18.1 examples/sec; 3.543 sec/batch)
2017-05-09 20:40:39.676893: step 18540, loss = 0.0063, acc = 1.0000 (17.2 examples/sec; 3.729 sec/batch)
2017-05-09 20:41:53.826845: step 18560, loss = 0.0073, acc = 1.0000 (17.7 examples/sec; 3.608 sec/batch)
2017-05-09 20:43:09.021778: step 18580, loss = 0.0073, acc = 1.0000 (16.2 examples/sec; 3.956 sec/batch)
2017-05-09 20:44:24.419154: step 18600, loss = 0.0065, acc = 1.0000 (17.3 examples/sec; 3.694 sec/batch)
2017-05-09 20:45:36.453075: step 18620, loss = 0.0073, acc = 1.0000 (17.4 examples/sec; 3.681 sec/batch)
2017-05-09 20:46:48.483547: step 18640, loss = 0.0063, acc = 1.0000 (17.6 examples/sec; 3.637 sec/batch)
2017-05-09 20:48:03.507912: step 18660, loss = 0.0084, acc = 1.0000 (17.3 examples/sec; 3.707 sec/batch)
2017-05-09 20:49:17.935507: step 18680, loss = 0.0071, acc = 1.0000 (16.8 examples/sec; 3.810 sec/batch)
2017-05-09 20:50:33.178542: step 18700, loss = 0.0068, acc = 1.0000 (17.2 examples/sec; 3.712 sec/batch)
2017-05-09 20:51:49.842293: step 18720, loss = 0.0071, acc = 1.0000 (17.4 examples/sec; 3.683 sec/batch)
2017-05-09 20:53:05.919545: step 18740, loss = 0.0068, acc = 1.0000 (16.8 examples/sec; 3.817 sec/batch)
2017-05-09 20:54:21.096613: step 18760, loss = 0.0069, acc = 1.0000 (17.2 examples/sec; 3.719 sec/batch)
2017-05-09 20:55:36.328281: step 18780, loss = 0.0084, acc = 0.9980 (17.0 examples/sec; 3.774 sec/batch)
2017-05-09 20:56:51.406087: step 18800, loss = 0.0087, acc = 1.0000 (18.0 examples/sec; 3.559 sec/batch)
2017-05-09 20:58:07.045193: step 18820, loss = 0.0088, acc = 0.9980 (17.0 examples/sec; 3.760 sec/batch)
2017-05-09 20:59:22.228602: step 18840, loss = 0.0076, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-09 21:00:37.699093: step 18860, loss = 0.0073, acc = 1.0000 (17.2 examples/sec; 3.727 sec/batch)
2017-05-09 21:01:51.358574: step 18880, loss = 0.0068, acc = 1.0000 (18.1 examples/sec; 3.535 sec/batch)
2017-05-09 21:03:07.801351: step 18900, loss = 0.0084, acc = 1.0000 (16.8 examples/sec; 3.812 sec/batch)
2017-05-09 21:04:21.156247: step 18920, loss = 0.0065, acc = 1.0000 (18.0 examples/sec; 3.554 sec/batch)
2017-05-09 21:05:36.140496: step 18940, loss = 0.0066, acc = 1.0000 (17.3 examples/sec; 3.697 sec/batch)
2017-05-09 21:06:51.611956: step 18960, loss = 0.0062, acc = 1.0000 (17.3 examples/sec; 3.705 sec/batch)
2017-05-09 21:08:05.232388: step 18980, loss = 0.0063, acc = 1.0000 (16.8 examples/sec; 3.820 sec/batch)
2017-05-09 21:09:21.139706: step 19000, loss = 0.0069, acc = 1.0000 (16.7 examples/sec; 3.829 sec/batch)
[Eval] 2017-05-09 21:09:37.142703: step 19000, acc = 0.9544, f1 = 0.9529
[Test] 2017-05-09 21:09:48.045524: step 19000, acc = 0.9433, f1 = 0.9428
[Status] 2017-05-09 21:09:48.045616: step 19000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 21:11:03.314677: step 19020, loss = 0.0082, acc = 0.9980 (17.4 examples/sec; 3.680 sec/batch)
2017-05-09 21:12:19.372866: step 19040, loss = 0.0068, acc = 1.0000 (15.8 examples/sec; 4.055 sec/batch)
2017-05-09 21:13:33.689819: step 19060, loss = 0.0094, acc = 1.0000 (16.7 examples/sec; 3.840 sec/batch)
2017-05-09 21:14:47.296453: step 19080, loss = 0.0088, acc = 1.0000 (17.0 examples/sec; 3.762 sec/batch)
2017-05-09 21:16:01.629872: step 19100, loss = 0.0068, acc = 1.0000 (17.1 examples/sec; 3.740 sec/batch)
2017-05-09 21:17:16.059857: step 19120, loss = 0.0071, acc = 1.0000 (17.8 examples/sec; 3.595 sec/batch)
2017-05-09 21:18:29.913993: step 19140, loss = 0.0080, acc = 1.0000 (17.6 examples/sec; 3.642 sec/batch)
2017-05-09 21:19:46.289099: step 19160, loss = 0.0064, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-09 21:21:00.513597: step 19180, loss = 0.0080, acc = 1.0000 (17.6 examples/sec; 3.642 sec/batch)
2017-05-09 21:22:13.059156: step 19200, loss = 0.0062, acc = 1.0000 (17.3 examples/sec; 3.696 sec/batch)
2017-05-09 21:23:26.995634: step 19220, loss = 0.0070, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-09 21:24:42.098562: step 19240, loss = 0.0067, acc = 1.0000 (17.1 examples/sec; 3.734 sec/batch)
2017-05-09 21:25:57.315220: step 19260, loss = 0.0070, acc = 1.0000 (16.6 examples/sec; 3.852 sec/batch)
2017-05-09 21:27:11.472713: step 19280, loss = 0.0063, acc = 1.0000 (17.9 examples/sec; 3.582 sec/batch)
2017-05-09 21:28:27.030542: step 19300, loss = 0.0068, acc = 1.0000 (17.0 examples/sec; 3.764 sec/batch)
2017-05-09 21:29:42.664830: step 19320, loss = 0.0062, acc = 1.0000 (16.5 examples/sec; 3.881 sec/batch)
2017-05-09 21:30:56.507577: step 19340, loss = 0.0074, acc = 1.0000 (17.8 examples/sec; 3.597 sec/batch)
2017-05-09 21:32:12.727759: step 19360, loss = 0.0062, acc = 1.0000 (17.0 examples/sec; 3.772 sec/batch)
2017-05-09 21:33:27.042413: step 19380, loss = 0.0062, acc = 1.0000 (17.6 examples/sec; 3.643 sec/batch)
2017-05-09 21:34:43.552321: step 19400, loss = 0.0062, acc = 1.0000 (17.0 examples/sec; 3.766 sec/batch)
2017-05-09 21:35:58.639034: step 19420, loss = 0.0063, acc = 1.0000 (17.4 examples/sec; 3.677 sec/batch)
2017-05-09 21:37:12.965590: step 19440, loss = 0.0069, acc = 1.0000 (17.0 examples/sec; 3.768 sec/batch)
2017-05-09 21:38:28.279031: step 19460, loss = 0.0059, acc = 1.0000 (16.7 examples/sec; 3.830 sec/batch)
2017-05-09 21:39:40.444784: step 19480, loss = 0.0066, acc = 1.0000 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 21:40:52.172640: step 19500, loss = 0.0066, acc = 1.0000 (17.9 examples/sec; 3.584 sec/batch)
2017-05-09 21:42:05.876975: step 19520, loss = 0.0063, acc = 1.0000 (17.3 examples/sec; 3.702 sec/batch)
2017-05-09 21:43:18.988822: step 19540, loss = 0.0065, acc = 1.0000 (17.6 examples/sec; 3.629 sec/batch)
2017-05-09 21:44:34.619216: step 19560, loss = 0.0065, acc = 1.0000 (17.5 examples/sec; 3.667 sec/batch)
2017-05-09 21:45:49.029614: step 19580, loss = 0.0061, acc = 1.0000 (16.8 examples/sec; 3.813 sec/batch)
2017-05-09 21:47:03.777764: step 19600, loss = 0.0068, acc = 1.0000 (17.0 examples/sec; 3.761 sec/batch)
2017-05-09 21:48:17.395920: step 19620, loss = 0.0062, acc = 1.0000 (18.0 examples/sec; 3.558 sec/batch)
2017-05-09 21:49:33.752992: step 19640, loss = 0.0074, acc = 1.0000 (16.2 examples/sec; 3.957 sec/batch)
2017-05-09 21:50:47.127274: step 19660, loss = 0.0063, acc = 1.0000 (17.3 examples/sec; 3.703 sec/batch)
2017-05-09 21:52:00.067384: step 19680, loss = 0.0157, acc = 0.9980 (18.3 examples/sec; 3.493 sec/batch)
2017-05-09 21:53:13.838769: step 19700, loss = 0.0073, acc = 1.0000 (17.9 examples/sec; 3.579 sec/batch)
2017-05-09 21:54:29.362272: step 19720, loss = 0.0073, acc = 1.0000 (17.0 examples/sec; 3.758 sec/batch)
2017-05-09 21:55:43.891127: step 19740, loss = 0.0059, acc = 1.0000 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 21:56:59.844902: step 19760, loss = 0.0059, acc = 1.0000 (17.5 examples/sec; 3.657 sec/batch)
2017-05-09 21:58:14.360252: step 19780, loss = 0.0072, acc = 1.0000 (17.5 examples/sec; 3.652 sec/batch)
2017-05-09 21:59:27.935653: step 19800, loss = 0.0059, acc = 1.0000 (17.3 examples/sec; 3.691 sec/batch)
2017-05-09 22:00:44.195872: step 19820, loss = 0.0070, acc = 1.0000 (16.7 examples/sec; 3.838 sec/batch)
2017-05-09 22:01:58.933889: step 19840, loss = 0.0071, acc = 1.0000 (16.7 examples/sec; 3.839 sec/batch)
2017-05-09 22:03:14.680337: step 19860, loss = 0.0142, acc = 0.9980 (17.2 examples/sec; 3.711 sec/batch)
2017-05-09 22:04:29.078672: step 19880, loss = 0.0061, acc = 1.0000 (17.8 examples/sec; 3.589 sec/batch)
2017-05-09 22:05:45.409369: step 19900, loss = 0.0064, acc = 1.0000 (16.4 examples/sec; 3.909 sec/batch)
2017-05-09 22:07:00.308116: step 19920, loss = 0.0064, acc = 1.0000 (17.2 examples/sec; 3.726 sec/batch)
2017-05-09 22:08:14.293888: step 19940, loss = 0.0064, acc = 1.0000 (16.5 examples/sec; 3.874 sec/batch)
2017-05-09 22:09:29.071636: step 19960, loss = 0.0075, acc = 1.0000 (17.3 examples/sec; 3.696 sec/batch)
2017-05-09 22:10:42.848328: step 19980, loss = 0.0064, acc = 1.0000 (16.3 examples/sec; 3.926 sec/batch)
2017-05-09 22:11:56.130498: step 20000, loss = 0.0073, acc = 1.0000 (17.4 examples/sec; 3.680 sec/batch)
[Eval] 2017-05-09 22:12:12.258264: step 20000, acc = 0.9536, f1 = 0.9521
[Test] 2017-05-09 22:12:23.015903: step 20000, acc = 0.9424, f1 = 0.9419
[Status] 2017-05-09 22:12:23.015975: step 20000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 22:13:38.321609: step 20020, loss = 0.0079, acc = 1.0000 (17.4 examples/sec; 3.679 sec/batch)
2017-05-09 22:14:52.181510: step 20040, loss = 0.0084, acc = 1.0000 (16.7 examples/sec; 3.827 sec/batch)
2017-05-09 22:16:06.489919: step 20060, loss = 0.0073, acc = 1.0000 (17.3 examples/sec; 3.709 sec/batch)
2017-05-09 22:17:21.422631: step 20080, loss = 0.0059, acc = 1.0000 (17.5 examples/sec; 3.668 sec/batch)
2017-05-09 22:18:36.763913: step 20100, loss = 0.0064, acc = 1.0000 (17.9 examples/sec; 3.578 sec/batch)
2017-05-09 22:19:50.399442: step 20120, loss = 0.0068, acc = 1.0000 (17.5 examples/sec; 3.649 sec/batch)
2017-05-09 22:21:05.402926: step 20140, loss = 0.0093, acc = 0.9980 (17.3 examples/sec; 3.707 sec/batch)
2017-05-09 22:22:20.663962: step 20160, loss = 0.0065, acc = 1.0000 (17.5 examples/sec; 3.665 sec/batch)
2017-05-09 22:23:34.660941: step 20180, loss = 0.0070, acc = 1.0000 (17.2 examples/sec; 3.715 sec/batch)
2017-05-09 22:24:49.608787: step 20200, loss = 0.0060, acc = 1.0000 (16.8 examples/sec; 3.806 sec/batch)
2017-05-09 22:26:05.370483: step 20220, loss = 0.0064, acc = 1.0000 (16.9 examples/sec; 3.780 sec/batch)
2017-05-09 22:27:19.725246: step 20240, loss = 0.0059, acc = 1.0000 (18.1 examples/sec; 3.545 sec/batch)
2017-05-09 22:28:33.358353: step 20260, loss = 0.0064, acc = 1.0000 (17.8 examples/sec; 3.588 sec/batch)
2017-05-09 22:29:47.348241: step 20280, loss = 0.0063, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-09 22:31:02.598035: step 20300, loss = 0.0061, acc = 1.0000 (16.8 examples/sec; 3.801 sec/batch)
2017-05-09 22:32:15.512580: step 20320, loss = 0.0066, acc = 1.0000 (17.3 examples/sec; 3.702 sec/batch)
2017-05-09 22:33:30.294535: step 20340, loss = 0.0057, acc = 1.0000 (17.5 examples/sec; 3.665 sec/batch)
2017-05-09 22:34:45.265403: step 20360, loss = 0.0055, acc = 1.0000 (16.8 examples/sec; 3.818 sec/batch)
2017-05-09 22:35:59.638980: step 20380, loss = 0.0062, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-09 22:37:15.121576: step 20400, loss = 0.0069, acc = 0.9980 (17.3 examples/sec; 3.700 sec/batch)
2017-05-09 22:38:31.501684: step 20420, loss = 0.0063, acc = 1.0000 (16.1 examples/sec; 3.980 sec/batch)
2017-05-09 22:39:45.302300: step 20440, loss = 0.0072, acc = 1.0000 (16.8 examples/sec; 3.801 sec/batch)
2017-05-09 22:40:59.769509: step 20460, loss = 0.0057, acc = 1.0000 (17.2 examples/sec; 3.713 sec/batch)
2017-05-09 22:42:14.857520: step 20480, loss = 0.0058, acc = 1.0000 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 22:43:29.537034: step 20500, loss = 0.0059, acc = 1.0000 (16.7 examples/sec; 3.828 sec/batch)
2017-05-09 22:44:44.175926: step 20520, loss = 0.0078, acc = 1.0000 (17.5 examples/sec; 3.653 sec/batch)
2017-05-09 22:45:58.444097: step 20540, loss = 0.0066, acc = 1.0000 (16.7 examples/sec; 3.833 sec/batch)
2017-05-09 22:47:13.292935: step 20560, loss = 0.0062, acc = 1.0000 (17.4 examples/sec; 3.669 sec/batch)
2017-05-09 22:48:26.552966: step 20580, loss = 0.0096, acc = 0.9960 (17.8 examples/sec; 3.604 sec/batch)
2017-05-09 22:49:41.461059: step 20600, loss = 0.0058, acc = 1.0000 (16.8 examples/sec; 3.804 sec/batch)
2017-05-09 22:50:58.017977: step 20620, loss = 0.0065, acc = 1.0000 (16.1 examples/sec; 3.969 sec/batch)
2017-05-09 22:52:13.029849: step 20640, loss = 0.0065, acc = 1.0000 (17.4 examples/sec; 3.680 sec/batch)
2017-05-09 22:53:27.355476: step 20660, loss = 0.0065, acc = 1.0000 (16.8 examples/sec; 3.816 sec/batch)
2017-05-09 22:54:40.482793: step 20680, loss = 0.0057, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-09 22:55:54.748510: step 20700, loss = 0.0065, acc = 1.0000 (16.9 examples/sec; 3.797 sec/batch)
2017-05-09 22:57:08.765445: step 20720, loss = 0.0060, acc = 1.0000 (17.8 examples/sec; 3.588 sec/batch)
2017-05-09 22:58:23.715976: step 20740, loss = 0.0077, acc = 0.9980 (16.7 examples/sec; 3.838 sec/batch)
2017-05-09 22:59:37.626406: step 20760, loss = 0.0059, acc = 1.0000 (17.2 examples/sec; 3.727 sec/batch)
2017-05-09 23:00:52.000584: step 20780, loss = 0.0073, acc = 0.9980 (17.3 examples/sec; 3.705 sec/batch)
2017-05-09 23:02:04.720087: step 20800, loss = 0.0063, acc = 1.0000 (17.2 examples/sec; 3.710 sec/batch)
2017-05-09 23:03:19.422638: step 20820, loss = 0.0131, acc = 0.9980 (17.9 examples/sec; 3.570 sec/batch)
2017-05-09 23:04:31.342318: step 20840, loss = 0.0062, acc = 1.0000 (17.8 examples/sec; 3.590 sec/batch)
2017-05-09 23:05:45.862754: step 20860, loss = 0.0057, acc = 1.0000 (17.4 examples/sec; 3.680 sec/batch)
2017-05-09 23:07:00.800649: step 20880, loss = 0.0064, acc = 1.0000 (17.2 examples/sec; 3.727 sec/batch)
2017-05-09 23:08:14.491241: step 20900, loss = 0.0063, acc = 1.0000 (17.3 examples/sec; 3.697 sec/batch)
2017-05-09 23:09:27.802232: step 20920, loss = 0.0063, acc = 1.0000 (17.8 examples/sec; 3.602 sec/batch)
2017-05-09 23:10:41.868663: step 20940, loss = 0.0062, acc = 1.0000 (17.1 examples/sec; 3.741 sec/batch)
2017-05-09 23:11:58.055761: step 20960, loss = 0.0071, acc = 1.0000 (16.6 examples/sec; 3.863 sec/batch)
2017-05-09 23:13:13.597742: step 20980, loss = 0.0065, acc = 1.0000 (17.0 examples/sec; 3.771 sec/batch)
2017-05-09 23:14:30.301201: step 21000, loss = 0.0060, acc = 1.0000 (16.4 examples/sec; 3.893 sec/batch)
[Eval] 2017-05-09 23:14:45.770531: step 21000, acc = 0.9538, f1 = 0.9523
[Test] 2017-05-09 23:14:56.560511: step 21000, acc = 0.9428, f1 = 0.9423
[Status] 2017-05-09 23:14:56.560591: step 21000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-09 23:16:12.183732: step 21020, loss = 0.0069, acc = 1.0000 (16.8 examples/sec; 3.815 sec/batch)
2017-05-09 23:17:27.334221: step 21040, loss = 0.0077, acc = 0.9980 (17.5 examples/sec; 3.653 sec/batch)
2017-05-09 23:18:41.648679: step 21060, loss = 0.0071, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-09 23:19:56.606023: step 21080, loss = 0.0065, acc = 1.0000 (17.6 examples/sec; 3.627 sec/batch)
2017-05-09 23:21:09.568619: step 21100, loss = 0.0062, acc = 1.0000 (16.8 examples/sec; 3.808 sec/batch)
2017-05-09 23:22:24.409798: step 21120, loss = 0.0071, acc = 1.0000 (16.6 examples/sec; 3.855 sec/batch)
2017-05-09 23:23:39.786937: step 21140, loss = 0.0058, acc = 1.0000 (16.8 examples/sec; 3.804 sec/batch)
2017-05-09 23:24:54.320863: step 21160, loss = 0.0060, acc = 1.0000 (16.7 examples/sec; 3.831 sec/batch)
2017-05-09 23:26:11.011334: step 21180, loss = 0.0055, acc = 1.0000 (17.0 examples/sec; 3.757 sec/batch)
2017-05-09 23:27:25.887637: step 21200, loss = 0.0057, acc = 1.0000 (16.9 examples/sec; 3.785 sec/batch)
2017-05-09 23:28:40.198014: step 21220, loss = 0.0056, acc = 1.0000 (17.4 examples/sec; 3.675 sec/batch)
2017-05-09 23:29:54.175102: step 21240, loss = 0.0057, acc = 1.0000 (17.9 examples/sec; 3.571 sec/batch)
2017-05-09 23:31:10.320155: step 21260, loss = 0.0053, acc = 1.0000 (16.6 examples/sec; 3.848 sec/batch)
2017-05-09 23:32:24.607738: step 21280, loss = 0.0064, acc = 1.0000 (17.9 examples/sec; 3.581 sec/batch)
2017-05-09 23:33:40.009231: step 21300, loss = 0.0056, acc = 1.0000 (17.2 examples/sec; 3.729 sec/batch)
2017-05-09 23:34:55.268998: step 21320, loss = 0.0058, acc = 1.0000 (17.6 examples/sec; 3.632 sec/batch)
2017-05-09 23:36:11.721463: step 21340, loss = 0.0061, acc = 1.0000 (16.7 examples/sec; 3.824 sec/batch)
2017-05-09 23:37:23.992477: step 21360, loss = 0.0055, acc = 1.0000 (17.3 examples/sec; 3.701 sec/batch)
2017-05-09 23:38:39.167537: step 21380, loss = 0.0072, acc = 0.9980 (17.7 examples/sec; 3.609 sec/batch)
2017-05-09 23:39:54.147216: step 21400, loss = 0.0072, acc = 1.0000 (17.4 examples/sec; 3.668 sec/batch)
2017-05-09 23:41:09.509845: step 21420, loss = 0.0059, acc = 1.0000 (17.7 examples/sec; 3.622 sec/batch)
2017-05-09 23:42:24.280892: step 21440, loss = 0.0059, acc = 1.0000 (16.7 examples/sec; 3.831 sec/batch)
2017-05-09 23:43:39.231251: step 21460, loss = 0.0054, acc = 1.0000 (17.2 examples/sec; 3.716 sec/batch)
2017-05-09 23:44:53.384877: step 21480, loss = 0.0063, acc = 1.0000 (16.7 examples/sec; 3.838 sec/batch)
2017-05-09 23:46:07.459601: step 21500, loss = 0.0058, acc = 1.0000 (16.8 examples/sec; 3.800 sec/batch)
2017-05-09 23:47:20.341661: step 21520, loss = 0.0056, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-09 23:48:32.914422: step 21540, loss = 0.0063, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-09 23:49:47.199939: step 21560, loss = 0.0074, acc = 0.9980 (17.7 examples/sec; 3.624 sec/batch)
2017-05-09 23:51:02.364071: step 21580, loss = 0.0071, acc = 1.0000 (17.4 examples/sec; 3.673 sec/batch)
2017-05-09 23:52:16.579362: step 21600, loss = 0.0064, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-09 23:53:32.020844: step 21620, loss = 0.0063, acc = 1.0000 (17.1 examples/sec; 3.736 sec/batch)
2017-05-09 23:54:47.694263: step 21640, loss = 0.0058, acc = 1.0000 (16.6 examples/sec; 3.863 sec/batch)
2017-05-09 23:56:01.976458: step 21660, loss = 0.0057, acc = 1.0000 (16.9 examples/sec; 3.795 sec/batch)
2017-05-09 23:57:14.726303: step 21680, loss = 0.0055, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-09 23:58:30.150809: step 21700, loss = 0.0059, acc = 1.0000 (16.4 examples/sec; 3.910 sec/batch)
2017-05-09 23:59:45.725578: step 21720, loss = 0.0062, acc = 1.0000 (17.5 examples/sec; 3.655 sec/batch)
2017-05-10 00:01:00.226961: step 21740, loss = 0.0054, acc = 1.0000 (16.6 examples/sec; 3.847 sec/batch)
2017-05-10 00:02:17.808201: step 21760, loss = 0.0068, acc = 1.0000 (16.5 examples/sec; 3.887 sec/batch)
2017-05-10 00:03:31.815397: step 21780, loss = 0.0052, acc = 1.0000 (16.9 examples/sec; 3.782 sec/batch)
2017-05-10 00:04:46.612613: step 21800, loss = 0.0064, acc = 1.0000 (17.0 examples/sec; 3.764 sec/batch)
2017-05-10 00:05:59.896513: step 21820, loss = 0.0073, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-10 00:07:15.730067: step 21840, loss = 0.0060, acc = 1.0000 (16.7 examples/sec; 3.833 sec/batch)
2017-05-10 00:08:29.297811: step 21860, loss = 0.0060, acc = 1.0000 (17.5 examples/sec; 3.649 sec/batch)
2017-05-10 00:09:44.744281: step 21880, loss = 0.0054, acc = 1.0000 (17.3 examples/sec; 3.691 sec/batch)
2017-05-10 00:10:59.923502: step 21900, loss = 0.0055, acc = 1.0000 (16.7 examples/sec; 3.822 sec/batch)
2017-05-10 00:12:14.024497: step 21920, loss = 0.0061, acc = 1.0000 (17.4 examples/sec; 3.670 sec/batch)
2017-05-10 00:13:27.797375: step 21940, loss = 0.0058, acc = 1.0000 (16.6 examples/sec; 3.860 sec/batch)
2017-05-10 00:14:40.487299: step 21960, loss = 0.0082, acc = 0.9980 (16.8 examples/sec; 3.820 sec/batch)
2017-05-10 00:15:55.864780: step 21980, loss = 0.0060, acc = 1.0000 (16.8 examples/sec; 3.813 sec/batch)
2017-05-10 00:17:11.981976: step 22000, loss = 0.0052, acc = 1.0000 (17.2 examples/sec; 3.711 sec/batch)
[Eval] 2017-05-10 00:17:27.884308: step 22000, acc = 0.9537, f1 = 0.9521
[Test] 2017-05-10 00:17:38.818353: step 22000, acc = 0.9420, f1 = 0.9415
[Status] 2017-05-10 00:17:38.818444: step 22000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 00:18:52.679666: step 22020, loss = 0.0071, acc = 1.0000 (17.9 examples/sec; 3.580 sec/batch)
2017-05-10 00:20:06.107514: step 22040, loss = 0.0056, acc = 1.0000 (17.7 examples/sec; 3.623 sec/batch)
2017-05-10 00:21:23.036809: step 22060, loss = 0.0053, acc = 1.0000 (16.6 examples/sec; 3.853 sec/batch)
2017-05-10 00:22:36.435162: step 22080, loss = 0.0055, acc = 1.0000 (17.2 examples/sec; 3.715 sec/batch)
2017-05-10 00:23:51.069185: step 22100, loss = 0.0052, acc = 1.0000 (17.4 examples/sec; 3.686 sec/batch)
2017-05-10 00:25:04.777241: step 22120, loss = 0.0055, acc = 1.0000 (16.6 examples/sec; 3.863 sec/batch)
2017-05-10 00:26:18.708267: step 22140, loss = 0.0054, acc = 1.0000 (16.9 examples/sec; 3.789 sec/batch)
2017-05-10 00:27:33.203103: step 22160, loss = 0.0067, acc = 1.0000 (17.3 examples/sec; 3.690 sec/batch)
2017-05-10 00:28:49.558291: step 22180, loss = 0.0056, acc = 1.0000 (16.9 examples/sec; 3.794 sec/batch)
2017-05-10 00:30:04.111728: step 22200, loss = 0.0055, acc = 1.0000 (17.0 examples/sec; 3.759 sec/batch)
2017-05-10 00:31:18.169075: step 22220, loss = 0.0072, acc = 0.9980 (16.9 examples/sec; 3.791 sec/batch)
2017-05-10 00:32:31.340098: step 22240, loss = 0.0055, acc = 1.0000 (16.9 examples/sec; 3.784 sec/batch)
2017-05-10 00:33:45.469621: step 22260, loss = 0.0053, acc = 1.0000 (16.9 examples/sec; 3.797 sec/batch)
2017-05-10 00:35:00.258041: step 22280, loss = 0.0057, acc = 1.0000 (17.5 examples/sec; 3.657 sec/batch)
2017-05-10 00:36:16.245101: step 22300, loss = 0.0056, acc = 1.0000 (17.4 examples/sec; 3.670 sec/batch)
2017-05-10 00:37:31.563943: step 22320, loss = 0.0056, acc = 1.0000 (17.8 examples/sec; 3.587 sec/batch)
2017-05-10 00:38:47.429373: step 22340, loss = 0.0055, acc = 1.0000 (17.1 examples/sec; 3.751 sec/batch)
2017-05-10 00:40:00.861207: step 22360, loss = 0.0056, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-10 00:41:14.210666: step 22380, loss = 0.0054, acc = 1.0000 (16.7 examples/sec; 3.826 sec/batch)
2017-05-10 00:42:30.298156: step 22400, loss = 0.0056, acc = 1.0000 (16.8 examples/sec; 3.816 sec/batch)
2017-05-10 00:43:45.008888: step 22420, loss = 0.0057, acc = 1.0000 (17.5 examples/sec; 3.647 sec/batch)
2017-05-10 00:44:59.926960: step 22440, loss = 0.0047, acc = 1.0000 (17.4 examples/sec; 3.679 sec/batch)
2017-05-10 00:46:15.913524: step 22460, loss = 0.0056, acc = 1.0000 (16.6 examples/sec; 3.855 sec/batch)
2017-05-10 00:47:30.387339: step 22480, loss = 0.0057, acc = 1.0000 (16.8 examples/sec; 3.813 sec/batch)
2017-05-10 00:48:42.696778: step 22500, loss = 0.0066, acc = 1.0000 (17.6 examples/sec; 3.640 sec/batch)
2017-05-10 00:49:57.167566: step 22520, loss = 0.0049, acc = 1.0000 (16.5 examples/sec; 3.868 sec/batch)
2017-05-10 00:51:11.764467: step 22540, loss = 0.0051, acc = 1.0000 (17.5 examples/sec; 3.649 sec/batch)
2017-05-10 00:52:25.884803: step 22560, loss = 0.0073, acc = 0.9980 (17.9 examples/sec; 3.573 sec/batch)
2017-05-10 00:53:37.192985: step 22580, loss = 0.0058, acc = 1.0000 (17.6 examples/sec; 3.626 sec/batch)
2017-05-10 00:54:51.398411: step 22600, loss = 0.0059, acc = 1.0000 (17.8 examples/sec; 3.590 sec/batch)
2017-05-10 00:56:06.270495: step 22620, loss = 0.0054, acc = 1.0000 (17.1 examples/sec; 3.732 sec/batch)
2017-05-10 00:57:18.822002: step 22640, loss = 0.0053, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-10 00:58:33.593355: step 22660, loss = 0.0065, acc = 1.0000 (16.8 examples/sec; 3.805 sec/batch)
2017-05-10 00:59:48.649806: step 22680, loss = 0.0065, acc = 1.0000 (17.8 examples/sec; 3.595 sec/batch)
2017-05-10 01:01:03.695980: step 22700, loss = 0.0050, acc = 1.0000 (17.5 examples/sec; 3.650 sec/batch)
2017-05-10 01:02:16.028745: step 22720, loss = 0.0057, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-10 01:03:30.876477: step 22740, loss = 0.0080, acc = 0.9980 (17.0 examples/sec; 3.770 sec/batch)
2017-05-10 01:04:46.756010: step 22760, loss = 0.0072, acc = 0.9980 (16.7 examples/sec; 3.839 sec/batch)
2017-05-10 01:06:01.750089: step 22780, loss = 0.0053, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-10 01:07:16.031204: step 22800, loss = 0.0062, acc = 1.0000 (17.3 examples/sec; 3.692 sec/batch)
2017-05-10 01:08:28.679863: step 22820, loss = 0.0053, acc = 1.0000 (17.6 examples/sec; 3.637 sec/batch)
2017-05-10 01:09:43.166303: step 22840, loss = 0.0063, acc = 1.0000 (16.9 examples/sec; 3.798 sec/batch)
2017-05-10 01:10:57.852578: step 22860, loss = 0.0053, acc = 1.0000 (16.9 examples/sec; 3.776 sec/batch)
2017-05-10 01:12:10.906255: step 22880, loss = 0.0057, acc = 1.0000 (17.2 examples/sec; 3.717 sec/batch)
2017-05-10 01:13:24.836296: step 22900, loss = 0.0062, acc = 1.0000 (17.1 examples/sec; 3.753 sec/batch)
2017-05-10 01:14:40.309168: step 22920, loss = 0.0058, acc = 1.0000 (16.4 examples/sec; 3.910 sec/batch)
2017-05-10 01:15:55.097523: step 22940, loss = 0.0059, acc = 1.0000 (16.8 examples/sec; 3.801 sec/batch)
2017-05-10 01:17:09.539565: step 22960, loss = 0.0053, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-10 01:18:24.089427: step 22980, loss = 0.0050, acc = 1.0000 (17.6 examples/sec; 3.645 sec/batch)
2017-05-10 01:19:37.076294: step 23000, loss = 0.0062, acc = 1.0000 (16.7 examples/sec; 3.838 sec/batch)
[Eval] 2017-05-10 01:19:53.123205: step 23000, acc = 0.9529, f1 = 0.9514
[Test] 2017-05-10 01:20:03.825143: step 23000, acc = 0.9420, f1 = 0.9415
[Status] 2017-05-10 01:20:03.825218: step 23000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 01:21:19.549364: step 23020, loss = 0.0054, acc = 1.0000 (17.3 examples/sec; 3.705 sec/batch)
2017-05-10 01:22:33.855991: step 23040, loss = 0.0053, acc = 1.0000 (17.3 examples/sec; 3.702 sec/batch)
2017-05-10 01:23:48.151876: step 23060, loss = 0.0066, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-10 01:25:02.231845: step 23080, loss = 0.0054, acc = 1.0000 (16.8 examples/sec; 3.819 sec/batch)
2017-05-10 01:26:14.987890: step 23100, loss = 0.0055, acc = 1.0000 (17.6 examples/sec; 3.639 sec/batch)
2017-05-10 01:27:26.383012: step 23120, loss = 0.0066, acc = 0.9980 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 01:28:33.230031: step 23140, loss = 0.0063, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 01:29:39.722914: step 23160, loss = 0.0056, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 01:30:47.309119: step 23180, loss = 0.0051, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-10 01:31:53.818079: step 23200, loss = 0.0047, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-10 01:33:00.735807: step 23220, loss = 0.0067, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 01:34:06.951057: step 23240, loss = 0.0053, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 01:35:13.424477: step 23260, loss = 0.0050, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-10 01:36:19.647402: step 23280, loss = 0.0050, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-10 01:37:27.856611: step 23300, loss = 0.0064, acc = 1.0000 (17.7 examples/sec; 3.612 sec/batch)
2017-05-10 01:38:34.229972: step 23320, loss = 0.0047, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 01:39:41.092951: step 23340, loss = 0.0047, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-10 01:40:47.317162: step 23360, loss = 0.0059, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 01:41:53.940065: step 23380, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 01:43:00.434031: step 23400, loss = 0.0058, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 01:44:08.535380: step 23420, loss = 0.0073, acc = 0.9980 (18.1 examples/sec; 3.537 sec/batch)
2017-05-10 01:45:15.266756: step 23440, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 01:46:21.613413: step 23460, loss = 0.0066, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 01:47:28.157725: step 23480, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 01:48:34.082492: step 23500, loss = 0.0053, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 01:49:39.975909: step 23520, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-10 01:50:47.391932: step 23540, loss = 0.0053, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-10 01:51:54.628067: step 23560, loss = 0.0055, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 01:53:01.142312: step 23580, loss = 0.0062, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 01:54:07.656103: step 23600, loss = 0.0059, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 01:55:14.492865: step 23620, loss = 0.0062, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 01:56:21.200227: step 23640, loss = 0.0048, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 01:57:28.345911: step 23660, loss = 0.0057, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 01:58:34.911446: step 23680, loss = 0.0049, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-10 01:59:43.130931: step 23700, loss = 0.0054, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-10 02:00:49.862870: step 23720, loss = 0.0054, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 02:01:57.050239: step 23740, loss = 0.0055, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-10 02:03:03.707743: step 23760, loss = 0.0052, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-10 02:04:10.499748: step 23780, loss = 0.0059, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 02:05:16.397085: step 23800, loss = 0.0055, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 02:06:23.225468: step 23820, loss = 0.0051, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 02:07:29.854754: step 23840, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 02:08:37.193459: step 23860, loss = 0.0050, acc = 1.0000 (17.7 examples/sec; 3.608 sec/batch)
2017-05-10 02:09:43.933889: step 23880, loss = 0.0054, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-10 02:10:50.461821: step 23900, loss = 0.0058, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-10 02:11:56.984624: step 23920, loss = 0.0048, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-10 02:13:03.283896: step 23940, loss = 0.0073, acc = 0.9980 (19.1 examples/sec; 3.354 sec/batch)
2017-05-10 02:14:09.830215: step 23960, loss = 0.0049, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-10 02:15:16.891770: step 23980, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 02:16:22.504839: step 24000, loss = 0.0097, acc = 0.9980 (19.6 examples/sec; 3.267 sec/batch)
[Eval] 2017-05-10 02:16:37.944651: step 24000, acc = 0.9523, f1 = 0.9507
[Test] 2017-05-10 02:16:48.362602: step 24000, acc = 0.9408, f1 = 0.9403
[Status] 2017-05-10 02:16:48.362696: step 24000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 02:17:55.538359: step 24020, loss = 0.0073, acc = 0.9980 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 02:19:03.295485: step 24040, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 02:20:10.171268: step 24060, loss = 0.0052, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-10 02:21:16.708032: step 24080, loss = 0.0052, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 02:22:23.958575: step 24100, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 02:23:31.196873: step 24120, loss = 0.0048, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-10 02:24:38.102528: step 24140, loss = 0.0055, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-10 02:25:44.260550: step 24160, loss = 0.0054, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-10 02:26:50.768314: step 24180, loss = 0.0054, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-10 02:27:58.390926: step 24200, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-10 02:29:07.066786: step 24220, loss = 0.0047, acc = 1.0000 (18.1 examples/sec; 3.537 sec/batch)
2017-05-10 02:30:13.974448: step 24240, loss = 0.0048, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-10 02:31:20.109037: step 24260, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-10 02:32:27.082916: step 24280, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 02:33:34.139779: step 24300, loss = 0.0049, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 02:34:41.715041: step 24320, loss = 0.0050, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-10 02:35:48.454365: step 24340, loss = 0.0050, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 02:36:55.343399: step 24360, loss = 0.0049, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 02:38:02.157064: step 24380, loss = 0.0060, acc = 0.9980 (19.2 examples/sec; 3.336 sec/batch)
2017-05-10 02:39:08.317926: step 24400, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 02:40:14.173861: step 24420, loss = 0.0045, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-10 02:41:21.362165: step 24440, loss = 0.0043, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-10 02:42:28.231110: step 24460, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 02:43:35.029076: step 24480, loss = 0.0044, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 02:44:41.012992: step 24500, loss = 0.0060, acc = 0.9980 (19.7 examples/sec; 3.255 sec/batch)
2017-05-10 02:45:47.539860: step 24520, loss = 0.0053, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 02:46:55.404432: step 24540, loss = 0.0053, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-10 02:48:03.239896: step 24560, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 02:49:09.452576: step 24580, loss = 0.0047, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-10 02:50:15.526651: step 24600, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 02:51:22.598418: step 24620, loss = 0.0054, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-10 02:52:30.205816: step 24640, loss = 0.0068, acc = 0.9980 (19.3 examples/sec; 3.322 sec/batch)
2017-05-10 02:53:37.565792: step 24660, loss = 0.0049, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-10 02:54:44.055796: step 24680, loss = 0.0079, acc = 0.9980 (19.7 examples/sec; 3.254 sec/batch)
2017-05-10 02:55:52.569945: step 24700, loss = 0.0054, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-10 02:57:00.613395: step 24720, loss = 0.0049, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-10 02:58:07.850640: step 24740, loss = 0.0049, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 02:59:15.092627: step 24760, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 03:00:21.469158: step 24780, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 03:01:27.632978: step 24800, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 03:02:33.784749: step 24820, loss = 0.0057, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 03:03:40.966179: step 24840, loss = 0.0050, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 03:04:47.463267: step 24860, loss = 0.0050, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 03:05:53.580699: step 24880, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 03:07:00.898247: step 24900, loss = 0.0053, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 03:08:08.160513: step 24920, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 03:09:14.499878: step 24940, loss = 0.0048, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-10 03:10:21.479535: step 24960, loss = 0.0059, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 03:11:27.827253: step 24980, loss = 0.0050, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-10 03:12:34.017613: step 25000, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-10 03:12:49.385696: step 25000, acc = 0.9530, f1 = 0.9516
[Test] 2017-05-10 03:12:59.991481: step 25000, acc = 0.9404, f1 = 0.9399
[Status] 2017-05-10 03:12:59.991569: step 25000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 03:14:06.103375: step 25020, loss = 0.0061, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 03:15:12.879702: step 25040, loss = 0.0046, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-10 03:16:20.451399: step 25060, loss = 0.0048, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-10 03:17:27.982645: step 25080, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 03:18:34.922318: step 25100, loss = 0.0070, acc = 0.9980 (19.1 examples/sec; 3.346 sec/batch)
2017-05-10 03:19:42.115409: step 25120, loss = 0.0062, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-10 03:20:48.415806: step 25140, loss = 0.0066, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 03:21:54.701346: step 25160, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 03:23:01.953986: step 25180, loss = 0.0058, acc = 1.0000 (17.9 examples/sec; 3.568 sec/batch)
2017-05-10 03:24:09.637428: step 25200, loss = 0.0051, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-10 03:25:16.611948: step 25220, loss = 0.0065, acc = 0.9980 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 03:26:23.376543: step 25240, loss = 0.0049, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-10 03:27:30.627360: step 25260, loss = 0.0059, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 03:28:37.593180: step 25280, loss = 0.0045, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-10 03:29:44.930105: step 25300, loss = 0.0046, acc = 1.0000 (19.8 examples/sec; 3.234 sec/batch)
2017-05-10 03:30:51.786119: step 25320, loss = 0.0046, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-10 03:31:57.927257: step 25340, loss = 0.0051, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-10 03:33:04.725225: step 25360, loss = 0.0060, acc = 0.9980 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 03:34:10.934959: step 25380, loss = 0.0046, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 03:35:17.896318: step 25400, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 03:36:25.254323: step 25420, loss = 0.0042, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-10 03:37:31.229373: step 25440, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 03:38:38.262602: step 25460, loss = 0.0058, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 03:39:44.919577: step 25480, loss = 0.0052, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-10 03:40:51.986655: step 25500, loss = 0.0048, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-10 03:41:58.868040: step 25520, loss = 0.0067, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-10 03:43:05.078374: step 25540, loss = 0.0067, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 03:44:13.717928: step 25560, loss = 0.0048, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-10 03:45:19.945128: step 25580, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 03:46:25.880020: step 25600, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 03:47:34.049871: step 25620, loss = 0.0047, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-10 03:48:42.092022: step 25640, loss = 0.0044, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-10 03:49:49.204865: step 25660, loss = 0.0071, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 03:50:55.874969: step 25680, loss = 0.0044, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-10 03:52:02.649855: step 25700, loss = 0.0063, acc = 0.9980 (19.2 examples/sec; 3.336 sec/batch)
2017-05-10 03:53:09.095078: step 25720, loss = 0.0052, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 03:54:16.495056: step 25740, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 03:55:22.729964: step 25760, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 03:56:29.912446: step 25780, loss = 0.0047, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-10 03:57:36.638506: step 25800, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 03:58:42.499857: step 25820, loss = 0.0063, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 03:59:48.785617: step 25840, loss = 0.0045, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-10 04:00:56.019588: step 25860, loss = 0.0053, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 04:02:02.780836: step 25880, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-10 04:03:09.248240: step 25900, loss = 0.0055, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 04:04:15.545031: step 25920, loss = 0.0066, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 04:05:21.923134: step 25940, loss = 0.0193, acc = 0.9980 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 04:06:28.538459: step 25960, loss = 0.0307, acc = 0.9920 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 04:07:36.206192: step 25980, loss = 2.1206, acc = 0.7700 (18.2 examples/sec; 3.519 sec/batch)
2017-05-10 04:08:43.653357: step 26000, loss = 0.0533, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
[Eval] 2017-05-10 04:08:59.005902: step 26000, acc = 0.9431, f1 = 0.9414
[Test] 2017-05-10 04:09:09.489269: step 26000, acc = 0.9309, f1 = 0.9303
[Status] 2017-05-10 04:09:09.489348: step 26000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 04:10:16.958497: step 26020, loss = 0.2196, acc = 0.9820 (18.7 examples/sec; 3.417 sec/batch)
2017-05-10 04:11:24.500379: step 26040, loss = 0.2938, acc = 0.9720 (18.5 examples/sec; 3.460 sec/batch)
2017-05-10 04:12:30.936095: step 26060, loss = 0.1486, acc = 0.9880 (19.1 examples/sec; 3.343 sec/batch)
2017-05-10 04:13:37.593942: step 26080, loss = 0.1424, acc = 0.9900 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 04:14:43.671849: step 26100, loss = 0.0881, acc = 0.9940 (19.6 examples/sec; 3.268 sec/batch)
2017-05-10 04:15:50.607370: step 26120, loss = 0.1026, acc = 0.9900 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 04:16:57.708878: step 26140, loss = 0.1161, acc = 0.9920 (18.5 examples/sec; 3.459 sec/batch)
2017-05-10 04:18:04.704609: step 26160, loss = 0.0826, acc = 0.9920 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 04:19:10.622415: step 26180, loss = 0.0776, acc = 0.9920 (19.6 examples/sec; 3.262 sec/batch)
2017-05-10 04:20:16.609322: step 26200, loss = 0.0564, acc = 0.9980 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 04:21:23.469692: step 26220, loss = 0.0585, acc = 0.9960 (19.6 examples/sec; 3.266 sec/batch)
2017-05-10 04:22:29.616790: step 26240, loss = 0.0606, acc = 0.9940 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 04:23:35.826740: step 26260, loss = 0.0514, acc = 0.9960 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 04:24:42.188817: step 26280, loss = 0.0503, acc = 0.9960 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 04:25:49.715314: step 26300, loss = 0.0384, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 04:26:56.428978: step 26320, loss = 0.0430, acc = 0.9940 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 04:28:03.054458: step 26340, loss = 0.0336, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 04:29:10.810494: step 26360, loss = 0.0542, acc = 0.9940 (19.2 examples/sec; 3.336 sec/batch)
2017-05-10 04:30:17.015772: step 26380, loss = 0.0328, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 04:31:23.442233: step 26400, loss = 0.0600, acc = 0.9940 (18.9 examples/sec; 3.379 sec/batch)
2017-05-10 04:32:29.792073: step 26420, loss = 0.0277, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 04:33:36.723049: step 26440, loss = 0.0255, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 04:34:43.711052: step 26460, loss = 0.0255, acc = 0.9980 (18.7 examples/sec; 3.422 sec/batch)
2017-05-10 04:35:50.734163: step 26480, loss = 0.0262, acc = 0.9980 (19.0 examples/sec; 3.363 sec/batch)
2017-05-10 04:36:58.547330: step 26500, loss = 0.0203, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-10 04:38:04.381822: step 26520, loss = 0.0462, acc = 0.9920 (19.3 examples/sec; 3.316 sec/batch)
2017-05-10 04:39:11.003844: step 26540, loss = 0.0184, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 04:40:17.690478: step 26560, loss = 0.0187, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 04:41:24.418644: step 26580, loss = 0.0238, acc = 0.9980 (18.5 examples/sec; 3.455 sec/batch)
2017-05-10 04:42:31.141016: step 26600, loss = 0.0188, acc = 0.9980 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 04:43:37.311148: step 26620, loss = 0.0153, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 04:44:43.266908: step 26640, loss = 0.0153, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 04:45:49.331746: step 26660, loss = 0.0180, acc = 0.9980 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 04:46:55.734449: step 26680, loss = 0.0146, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-10 04:48:02.338543: step 26700, loss = 0.0156, acc = 0.9980 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 04:49:10.040137: step 26720, loss = 0.0120, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-10 04:50:16.801297: step 26740, loss = 0.0187, acc = 0.9960 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 04:51:22.956824: step 26760, loss = 0.0148, acc = 0.9980 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 04:52:29.144830: step 26780, loss = 0.0152, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 04:53:35.723713: step 26800, loss = 0.0156, acc = 0.9980 (19.1 examples/sec; 3.356 sec/batch)
2017-05-10 04:54:42.654673: step 26820, loss = 0.0126, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 04:55:49.651190: step 26840, loss = 0.0124, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 04:56:56.043592: step 26860, loss = 0.0134, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 04:58:02.829320: step 26880, loss = 0.0184, acc = 0.9960 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 04:59:10.459801: step 26900, loss = 0.0136, acc = 0.9960 (18.8 examples/sec; 3.409 sec/batch)
2017-05-10 05:00:16.776944: step 26920, loss = 0.0092, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-10 05:01:23.403620: step 26940, loss = 0.0093, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-10 05:02:30.320000: step 26960, loss = 0.0188, acc = 0.9940 (19.0 examples/sec; 3.363 sec/batch)
2017-05-10 05:03:37.455065: step 26980, loss = 0.0088, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-10 05:04:44.821713: step 27000, loss = 0.0112, acc = 0.9980 (17.6 examples/sec; 3.638 sec/batch)
[Eval] 2017-05-10 05:05:00.327445: step 27000, acc = 0.9502, f1 = 0.9485
[Test] 2017-05-10 05:05:10.911653: step 27000, acc = 0.9384, f1 = 0.9379
[Status] 2017-05-10 05:05:10.911765: step 27000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 05:06:18.140051: step 27020, loss = 0.0104, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 05:07:24.699028: step 27040, loss = 0.0134, acc = 0.9960 (18.8 examples/sec; 3.402 sec/batch)
2017-05-10 05:08:30.543776: step 27060, loss = 0.0076, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 05:09:36.913929: step 27080, loss = 0.0184, acc = 0.9940 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 05:10:44.075293: step 27100, loss = 0.0123, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-10 05:11:50.569699: step 27120, loss = 0.0136, acc = 0.9960 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 05:12:58.237727: step 27140, loss = 0.0118, acc = 0.9980 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 05:14:06.297688: step 27160, loss = 0.0106, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 05:15:12.078510: step 27180, loss = 0.0092, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-10 05:16:18.896247: step 27200, loss = 0.0084, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 05:17:26.067322: step 27220, loss = 0.0079, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 05:18:32.881647: step 27240, loss = 0.0075, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 05:19:40.222858: step 27260, loss = 0.0060, acc = 1.0000 (18.0 examples/sec; 3.565 sec/batch)
2017-05-10 05:20:47.243691: step 27280, loss = 0.0067, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 05:21:54.780956: step 27300, loss = 0.0064, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-10 05:23:01.828075: step 27320, loss = 0.0067, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-10 05:24:07.829194: step 27340, loss = 0.0069, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 05:25:14.664499: step 27360, loss = 0.0063, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 05:26:20.907451: step 27380, loss = 0.0067, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 05:27:27.417109: step 27400, loss = 0.0080, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 05:28:34.083232: step 27420, loss = 0.0082, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-10 05:29:41.256500: step 27440, loss = 0.0068, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-10 05:30:48.726770: step 27460, loss = 0.0063, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 05:31:55.333061: step 27480, loss = 0.0059, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 05:33:03.009686: step 27500, loss = 0.0068, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-10 05:34:09.785530: step 27520, loss = 0.0081, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 05:35:16.292378: step 27540, loss = 0.0062, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-10 05:36:22.652324: step 27560, loss = 0.0066, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-10 05:37:30.295978: step 27580, loss = 0.0167, acc = 0.9980 (18.3 examples/sec; 3.501 sec/batch)
2017-05-10 05:38:37.579697: step 27600, loss = 0.0064, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-10 05:39:44.827342: step 27620, loss = 0.0077, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-10 05:40:51.000143: step 27640, loss = 0.0076, acc = 0.9980 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 05:41:57.846560: step 27660, loss = 0.0074, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 05:43:04.756632: step 27680, loss = 0.0082, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 05:44:11.567450: step 27700, loss = 0.0068, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-10 05:45:18.534878: step 27720, loss = 0.0081, acc = 0.9980 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 05:46:24.716666: step 27740, loss = 0.0064, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 05:47:30.800191: step 27760, loss = 0.0081, acc = 0.9980 (19.3 examples/sec; 3.317 sec/batch)
2017-05-10 05:48:37.660366: step 27780, loss = 0.0066, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 05:49:44.358621: step 27800, loss = 0.0064, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 05:50:51.050006: step 27820, loss = 0.0059, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-10 05:51:56.857614: step 27840, loss = 0.0065, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 05:53:03.882118: step 27860, loss = 0.0124, acc = 0.9960 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 05:54:10.965062: step 27880, loss = 0.0078, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 05:55:18.001870: step 27900, loss = 0.0054, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-10 05:56:23.995456: step 27920, loss = 0.0065, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 05:57:31.274029: step 27940, loss = 0.0063, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-10 05:58:38.853100: step 27960, loss = 0.0071, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-10 05:59:44.758458: step 27980, loss = 0.0078, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 06:00:54.018243: step 28000, loss = 0.0070, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
[Eval] 2017-05-10 06:01:09.157157: step 28000, acc = 0.9516, f1 = 0.9500
[Test] 2017-05-10 06:01:19.695814: step 28000, acc = 0.9399, f1 = 0.9394
[Status] 2017-05-10 06:01:19.695911: step 28000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 06:02:26.299017: step 28020, loss = 0.0056, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-10 06:03:33.229050: step 28040, loss = 0.0067, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 06:04:39.966158: step 28060, loss = 0.0057, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-10 06:05:46.011349: step 28080, loss = 0.0088, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 06:06:52.217842: step 28100, loss = 0.0066, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 06:07:59.035442: step 28120, loss = 0.0069, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 06:09:05.108932: step 28140, loss = 0.0090, acc = 0.9980 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 06:10:11.687369: step 28160, loss = 0.0068, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-10 06:11:18.038461: step 28180, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 06:12:26.140735: step 28200, loss = 0.0083, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-10 06:13:33.019470: step 28220, loss = 0.0088, acc = 0.9980 (18.4 examples/sec; 3.487 sec/batch)
2017-05-10 06:14:39.587190: step 28240, loss = 0.0058, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 06:15:46.901294: step 28260, loss = 0.0090, acc = 0.9980 (18.3 examples/sec; 3.501 sec/batch)
2017-05-10 06:16:53.358398: step 28280, loss = 0.0056, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 06:18:00.585211: step 28300, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 06:19:07.927792: step 28320, loss = 0.0069, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 06:20:15.017318: step 28340, loss = 0.0053, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-10 06:21:22.113182: step 28360, loss = 0.0051, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-10 06:22:28.577810: step 28380, loss = 0.0055, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 06:23:35.620240: step 28400, loss = 0.0074, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 06:24:43.781748: step 28420, loss = 0.0061, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-10 06:25:50.862248: step 28440, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-10 06:26:56.822977: step 28460, loss = 0.0062, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-10 06:28:03.789267: step 28480, loss = 0.0061, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-10 06:29:10.772139: step 28500, loss = 0.0054, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 06:30:18.094653: step 28520, loss = 0.0075, acc = 0.9980 (18.9 examples/sec; 3.391 sec/batch)
2017-05-10 06:31:24.779628: step 28540, loss = 0.0061, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-10 06:32:31.323360: step 28560, loss = 0.0061, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-10 06:33:37.393131: step 28580, loss = 0.0060, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-10 06:34:43.865012: step 28600, loss = 0.0061, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 06:35:49.924950: step 28620, loss = 0.0058, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 06:36:56.843612: step 28640, loss = 0.0055, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-10 06:38:02.945467: step 28660, loss = 0.0055, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 06:39:08.954467: step 28680, loss = 0.0049, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-10 06:40:15.741839: step 28700, loss = 0.0055, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-10 06:41:23.044688: step 28720, loss = 0.0059, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-10 06:42:30.236846: step 28740, loss = 0.0052, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-10 06:43:36.173752: step 28760, loss = 0.0058, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-10 06:44:43.283593: step 28780, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 06:45:50.926142: step 28800, loss = 0.0053, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-10 06:46:57.544313: step 28820, loss = 0.0050, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-10 06:48:03.812762: step 28840, loss = 0.0055, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-10 06:49:10.308433: step 28860, loss = 0.0057, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-10 06:50:16.753645: step 28880, loss = 0.0051, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-10 06:51:23.276300: step 28900, loss = 0.0059, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-10 06:52:29.472888: step 28920, loss = 0.0049, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 06:53:37.282769: step 28940, loss = 0.0054, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-10 06:54:44.000548: step 28960, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 06:55:50.491501: step 28980, loss = 0.0058, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 06:56:56.739003: step 29000, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
[Eval] 2017-05-10 06:57:12.127988: step 29000, acc = 0.9523, f1 = 0.9507
[Test] 2017-05-10 06:57:22.694293: step 29000, acc = 0.9405, f1 = 0.9400
[Status] 2017-05-10 06:57:22.694377: step 29000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 06:58:29.197133: step 29020, loss = 0.0057, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-10 06:59:36.047251: step 29040, loss = 0.0053, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-10 07:00:43.036004: step 29060, loss = 0.0051, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-10 07:01:50.706292: step 29080, loss = 0.0050, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 07:02:57.689689: step 29100, loss = 0.0054, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-10 07:04:05.001846: step 29120, loss = 0.0053, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-10 07:05:11.680811: step 29140, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 07:06:18.994008: step 29160, loss = 0.0050, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 07:07:25.936753: step 29180, loss = 0.0048, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 07:08:31.894050: step 29200, loss = 0.0075, acc = 0.9980 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 07:09:38.581870: step 29220, loss = 0.0052, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-10 07:10:46.243434: step 29240, loss = 0.0059, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 07:11:54.131706: step 29260, loss = 0.0042, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-10 07:13:01.520025: step 29280, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 07:14:08.260948: step 29300, loss = 0.0057, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 07:15:14.620591: step 29320, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 07:16:22.879381: step 29340, loss = 0.0049, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-10 07:17:29.806317: step 29360, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 07:18:35.851501: step 29380, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 07:19:42.350482: step 29400, loss = 0.0046, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 07:20:50.164749: step 29420, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 07:21:56.642040: step 29440, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 07:23:04.534906: step 29460, loss = 0.0050, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-10 07:24:11.248470: step 29480, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 07:25:18.870750: step 29500, loss = 0.0049, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-10 07:26:25.119230: step 29520, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 07:27:31.161702: step 29540, loss = 0.0071, acc = 0.9980 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 07:28:37.060882: step 29560, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 07:29:44.059152: step 29580, loss = 0.0142, acc = 0.9980 (19.1 examples/sec; 3.343 sec/batch)
2017-05-10 07:30:49.844081: step 29600, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 07:31:57.867741: step 29620, loss = 0.0047, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-10 07:33:04.720949: step 29640, loss = 0.0052, acc = 1.0000 (19.7 examples/sec; 3.249 sec/batch)
2017-05-10 07:34:11.407844: step 29660, loss = 0.0079, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 07:35:18.682830: step 29680, loss = 0.0048, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-10 07:36:24.539640: step 29700, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 07:37:32.356586: step 29720, loss = 0.0053, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 07:38:39.462430: step 29740, loss = 0.0053, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 07:39:46.349450: step 29760, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 07:40:52.968100: step 29780, loss = 0.0053, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 07:42:00.399038: step 29800, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 07:43:06.551978: step 29820, loss = 0.0049, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 07:44:13.156934: step 29840, loss = 0.0072, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 07:45:19.981815: step 29860, loss = 0.0054, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 07:46:25.989603: step 29880, loss = 0.0081, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 07:47:33.128358: step 29900, loss = 0.0048, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 07:48:39.983724: step 29920, loss = 0.0058, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-10 07:49:46.903926: step 29940, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 07:50:54.016676: step 29960, loss = 0.0050, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-10 07:52:00.696188: step 29980, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 07:53:07.750330: step 30000, loss = 0.0056, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
[Eval] 2017-05-10 07:53:21.952262: step 30000, acc = 0.9518, f1 = 0.9502
[Test] 2017-05-10 07:53:32.574336: step 30000, acc = 0.9403, f1 = 0.9398
[Status] 2017-05-10 07:53:32.574488: step 30000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 07:54:38.750770: step 30020, loss = 0.0060, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-10 07:55:45.646844: step 30040, loss = 0.0058, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 07:56:52.645289: step 30060, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 07:58:00.562961: step 30080, loss = 0.0053, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 07:59:07.895947: step 30100, loss = 0.0044, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-10 08:00:15.003291: step 30120, loss = 0.0050, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 08:01:22.072832: step 30140, loss = 0.0072, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 08:02:27.989798: step 30160, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 08:03:34.069584: step 30180, loss = 0.0058, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 08:04:40.105820: step 30200, loss = 0.0054, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 08:05:47.458643: step 30220, loss = 0.0049, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-10 08:06:55.265359: step 30240, loss = 0.0063, acc = 0.9980 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 08:08:01.508163: step 30260, loss = 0.0044, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 08:09:08.373423: step 30280, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 08:10:15.718799: step 30300, loss = 0.0060, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-10 08:11:22.567331: step 30320, loss = 0.0044, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-10 08:12:29.714960: step 30340, loss = 0.0043, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 08:13:36.289230: step 30360, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 08:14:44.014038: step 30380, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 08:15:50.270963: step 30400, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 08:16:57.631870: step 30420, loss = 0.0053, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-10 08:18:03.812057: step 30440, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 08:19:10.426954: step 30460, loss = 0.0046, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-10 08:20:17.557483: step 30480, loss = 0.0050, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-10 08:21:24.018390: step 30500, loss = 0.0064, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-10 08:22:31.874572: step 30520, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-10 08:23:39.007897: step 30540, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-10 08:24:45.725729: step 30560, loss = 0.0041, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 08:25:51.653033: step 30580, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 08:26:58.506763: step 30600, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 08:28:04.965590: step 30620, loss = 0.0050, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 08:29:11.213858: step 30640, loss = 0.0047, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-10 08:30:17.944080: step 30660, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 08:31:24.171689: step 30680, loss = 0.0048, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 08:32:30.020584: step 30700, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 08:33:36.404108: step 30720, loss = 0.0053, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 08:34:42.996853: step 30740, loss = 0.0054, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-10 08:35:49.643871: step 30760, loss = 0.0039, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-10 08:36:56.529260: step 30780, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 08:38:02.937154: step 30800, loss = 0.0052, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-10 08:39:09.768432: step 30820, loss = 0.0051, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-10 08:40:16.508645: step 30840, loss = 0.0051, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-10 08:41:23.222313: step 30860, loss = 0.0048, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 08:42:31.112209: step 30880, loss = 0.0053, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-10 08:43:37.803944: step 30900, loss = 0.0058, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-10 08:44:44.120298: step 30920, loss = 0.0084, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 08:45:51.674338: step 30940, loss = 0.0054, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-10 08:46:58.407912: step 30960, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 08:48:05.631241: step 30980, loss = 0.0047, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-10 08:49:12.259856: step 31000, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
[Eval] 2017-05-10 08:49:27.708058: step 31000, acc = 0.9519, f1 = 0.9504
[Test] 2017-05-10 08:49:38.321139: step 31000, acc = 0.9408, f1 = 0.9403
[Status] 2017-05-10 08:49:38.321232: step 31000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 08:50:45.046770: step 31020, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 08:51:51.321775: step 31040, loss = 0.0059, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 08:52:58.865198: step 31060, loss = 0.0048, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-10 08:54:07.145550: step 31080, loss = 0.0052, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-10 08:55:13.963965: step 31100, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 08:56:20.752492: step 31120, loss = 0.0047, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-10 08:57:26.695907: step 31140, loss = 0.0057, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 08:58:32.582978: step 31160, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 08:59:39.291367: step 31180, loss = 0.0044, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 09:00:45.663015: step 31200, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 09:01:52.174392: step 31220, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 09:02:58.002937: step 31240, loss = 0.0047, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 09:04:05.445030: step 31260, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 09:05:12.272672: step 31280, loss = 0.0085, acc = 0.9980 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 09:06:19.290962: step 31300, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 09:07:25.484356: step 31320, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 09:08:33.085495: step 31340, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 09:09:39.540451: step 31360, loss = 0.0053, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 09:10:45.516958: step 31380, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 09:11:51.732958: step 31400, loss = 0.0050, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-10 09:12:58.414287: step 31420, loss = 0.0044, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 09:14:04.209039: step 31440, loss = 0.0044, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 09:15:09.981258: step 31460, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 09:16:16.288781: step 31480, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 09:17:22.538363: step 31500, loss = 0.0044, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-10 09:18:28.471324: step 31520, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 09:19:35.082602: step 31540, loss = 0.0045, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-10 09:20:41.456758: step 31560, loss = 0.0044, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-10 09:21:48.019614: step 31580, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 09:22:53.605078: step 31600, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 09:23:59.260182: step 31620, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 09:25:06.571953: step 31640, loss = 0.0048, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-10 09:26:13.272252: step 31660, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 09:27:20.333475: step 31680, loss = 0.0043, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-10 09:28:26.919431: step 31700, loss = 0.0045, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-10 09:29:34.025690: step 31720, loss = 0.0048, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-10 09:30:40.746919: step 31740, loss = 0.0046, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-10 09:31:46.933174: step 31760, loss = 0.0042, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-10 09:32:54.837673: step 31780, loss = 0.0050, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-10 09:34:01.754080: step 31800, loss = 0.0042, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-10 09:35:08.747257: step 31820, loss = 0.0046, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-10 09:36:15.077009: step 31840, loss = 0.0060, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 09:37:22.050926: step 31860, loss = 0.0053, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-10 09:38:29.177411: step 31880, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 09:39:35.157968: step 31900, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 09:40:42.153802: step 31920, loss = 0.0044, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-10 09:41:49.448942: step 31940, loss = 0.0060, acc = 0.9980 (19.1 examples/sec; 3.350 sec/batch)
2017-05-10 09:42:55.343212: step 31960, loss = 0.0058, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 09:44:01.410692: step 31980, loss = 0.0042, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 09:45:07.889773: step 32000, loss = 0.0066, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
[Eval] 2017-05-10 09:45:22.364074: step 32000, acc = 0.9522, f1 = 0.9506
[Test] 2017-05-10 09:45:32.715148: step 32000, acc = 0.9405, f1 = 0.9400
[Status] 2017-05-10 09:45:32.715366: step 32000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 09:46:39.773669: step 32020, loss = 0.0057, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-10 09:47:46.672136: step 32040, loss = 0.0041, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-10 09:48:52.960974: step 32060, loss = 0.0047, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 09:50:00.620709: step 32080, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 09:51:07.604792: step 32100, loss = 0.0042, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 09:52:15.174173: step 32120, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 09:53:21.378569: step 32140, loss = 0.0064, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-10 09:54:27.537485: step 32160, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 09:55:33.754847: step 32180, loss = 0.0055, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-10 09:56:40.969699: step 32200, loss = 0.0044, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 09:57:47.614193: step 32220, loss = 0.0060, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-10 09:58:53.433930: step 32240, loss = 0.0048, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 10:00:01.639354: step 32260, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 10:01:08.107886: step 32280, loss = 0.0042, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-10 10:02:15.097199: step 32300, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 10:03:21.272047: step 32320, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 10:04:28.412101: step 32340, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 10:05:35.723979: step 32360, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 10:06:42.249156: step 32380, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 10:07:48.258451: step 32400, loss = 0.0068, acc = 0.9980 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 10:08:55.477515: step 32420, loss = 0.0056, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-10 10:10:01.940012: step 32440, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 10:11:08.113990: step 32460, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-10 10:12:14.725477: step 32480, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 10:13:22.144302: step 32500, loss = 0.0041, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 10:14:28.695062: step 32520, loss = 0.0058, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 10:15:34.942001: step 32540, loss = 0.0044, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 10:16:41.446154: step 32560, loss = 0.0039, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-10 10:17:47.911977: step 32580, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 10:18:54.863205: step 32600, loss = 0.0043, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 10:20:01.950728: step 32620, loss = 0.0041, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 10:21:09.320702: step 32640, loss = 0.0041, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-10 10:22:15.294492: step 32660, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 10:23:21.623919: step 32680, loss = 0.0049, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 10:24:27.706137: step 32700, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 10:25:34.126560: step 32720, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 10:26:40.336251: step 32740, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 10:27:46.882679: step 32760, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 10:28:53.062933: step 32780, loss = 0.0046, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-10 10:29:59.986651: step 32800, loss = 0.0045, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-10 10:31:07.132454: step 32820, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 10:32:13.359468: step 32840, loss = 0.0048, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 10:33:20.620341: step 32860, loss = 0.0048, acc = 1.0000 (17.9 examples/sec; 3.581 sec/batch)
2017-05-10 10:34:26.747107: step 32880, loss = 0.0056, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 10:35:34.750442: step 32900, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 10:36:41.217813: step 32920, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 10:37:48.420369: step 32940, loss = 0.0044, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-10 10:38:55.241620: step 32960, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 10:40:02.255121: step 32980, loss = 0.0040, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-10 10:41:09.550809: step 33000, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
[Eval] 2017-05-10 10:41:24.972563: step 33000, acc = 0.9516, f1 = 0.9500
[Test] 2017-05-10 10:41:35.685694: step 33000, acc = 0.9403, f1 = 0.9399
[Status] 2017-05-10 10:41:35.685828: step 33000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 10:42:43.345920: step 33020, loss = 0.0047, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-10 10:43:49.983358: step 33040, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 10:44:56.770614: step 33060, loss = 0.0053, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-10 10:46:03.178896: step 33080, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 10:47:10.984015: step 33100, loss = 0.0044, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-10 10:48:17.906676: step 33120, loss = 0.0040, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 10:49:24.581766: step 33140, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 10:50:31.379939: step 33160, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 10:51:38.017109: step 33180, loss = 0.0054, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 10:52:45.450938: step 33200, loss = 0.0049, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-10 10:53:52.191723: step 33220, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 10:54:58.145340: step 33240, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 10:56:06.483911: step 33260, loss = 0.0044, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-10 10:57:12.979139: step 33280, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 10:58:19.690278: step 33300, loss = 0.0040, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-10 10:59:25.494660: step 33320, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 11:00:32.304055: step 33340, loss = 0.0038, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 11:01:40.453759: step 33360, loss = 0.0062, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-10 11:02:47.321485: step 33380, loss = 0.0045, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-10 11:03:53.504608: step 33400, loss = 0.0039, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 11:04:59.693867: step 33420, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-10 11:06:06.172187: step 33440, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 11:07:12.929874: step 33460, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 11:08:19.711427: step 33480, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 11:09:25.918143: step 33500, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 11:10:32.574505: step 33520, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 11:11:39.509505: step 33540, loss = 0.0099, acc = 0.9980 (19.5 examples/sec; 3.276 sec/batch)
2017-05-10 11:12:46.445289: step 33560, loss = 0.0040, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 11:13:52.640749: step 33580, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 11:14:59.454650: step 33600, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 11:16:06.106875: step 33620, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 11:17:13.289050: step 33640, loss = 0.0041, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-10 11:18:21.203070: step 33660, loss = 0.0040, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-10 11:19:27.494276: step 33680, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 11:20:33.551168: step 33700, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 11:21:40.106172: step 33720, loss = 0.0044, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-10 11:22:46.742677: step 33740, loss = 0.0043, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-10 11:23:52.917834: step 33760, loss = 0.0046, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 11:24:58.995128: step 33780, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-10 11:26:05.536301: step 33800, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 11:27:12.091709: step 33820, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-10 11:28:20.692626: step 33840, loss = 0.0042, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-10 11:29:26.962284: step 33860, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 11:30:33.636016: step 33880, loss = 0.0040, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 11:31:40.274181: step 33900, loss = 0.0039, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-10 11:32:46.860719: step 33920, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 11:33:52.961052: step 33940, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 11:34:59.585330: step 33960, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-10 11:36:05.949398: step 33980, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 11:37:12.483627: step 34000, loss = 0.0050, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
[Eval] 2017-05-10 11:37:27.863102: step 34000, acc = 0.9512, f1 = 0.9497
[Test] 2017-05-10 11:37:38.429244: step 34000, acc = 0.9399, f1 = 0.9394
[Status] 2017-05-10 11:37:38.429347: step 34000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 11:38:45.219227: step 34020, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 11:39:51.208611: step 34040, loss = 0.0040, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 11:40:58.484536: step 34060, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 11:42:05.022204: step 34080, loss = 0.0043, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 11:43:12.503436: step 34100, loss = 0.0044, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-10 11:44:18.971225: step 34120, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-10 11:45:25.390491: step 34140, loss = 0.0061, acc = 0.9980 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 11:46:32.104725: step 34160, loss = 0.0051, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-10 11:47:39.394212: step 34180, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 11:48:46.118561: step 34200, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 11:49:52.946850: step 34220, loss = 0.0040, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-10 11:50:59.272947: step 34240, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 11:52:05.666299: step 34260, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 11:53:13.610712: step 34280, loss = 0.0036, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-10 11:54:20.158464: step 34300, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 11:55:26.313125: step 34320, loss = 0.0050, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-10 11:56:32.543094: step 34340, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 11:57:39.460145: step 34360, loss = 0.0039, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-10 11:58:46.537700: step 34380, loss = 0.0042, acc = 1.0000 (18.4 examples/sec; 3.488 sec/batch)
2017-05-10 11:59:52.707185: step 34400, loss = 0.0096, acc = 0.9980 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 12:00:59.176974: step 34420, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 12:02:05.872576: step 34440, loss = 0.0045, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-10 12:03:11.963269: step 34460, loss = 0.0060, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 12:04:17.859575: step 34480, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 12:05:25.010587: step 34500, loss = 0.0041, acc = 1.0000 (17.9 examples/sec; 3.571 sec/batch)
2017-05-10 12:06:31.826783: step 34520, loss = 0.0040, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 12:07:39.459468: step 34540, loss = 0.0038, acc = 1.0000 (17.6 examples/sec; 3.634 sec/batch)
2017-05-10 12:08:45.479537: step 34560, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 12:09:53.274042: step 34580, loss = 0.0039, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-10 12:10:59.771276: step 34600, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 12:12:06.476581: step 34620, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 12:13:14.270623: step 34640, loss = 0.0042, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-10 12:14:22.987351: step 34660, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 12:15:30.031701: step 34680, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 12:16:37.087101: step 34700, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 12:17:45.154639: step 34720, loss = 0.0040, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-10 12:18:51.800579: step 34740, loss = 0.0039, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 12:19:57.844310: step 34760, loss = 0.0050, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-10 12:21:04.699199: step 34780, loss = 0.0041, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-10 12:22:12.038340: step 34800, loss = 0.0118, acc = 0.9980 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 12:23:20.065743: step 34820, loss = 0.0037, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-10 12:24:26.699517: step 34840, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 12:25:33.096890: step 34860, loss = 0.0039, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 12:26:39.314702: step 34880, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 12:27:45.457470: step 34900, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 12:28:51.947387: step 34920, loss = 0.0040, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-10 12:29:58.012185: step 34940, loss = 0.0040, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-10 12:31:05.135488: step 34960, loss = 0.0040, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-10 12:32:11.871480: step 34980, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 12:33:18.238960: step 35000, loss = 0.0042, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
[Eval] 2017-05-10 12:33:33.761907: step 35000, acc = 0.9512, f1 = 0.9496
[Test] 2017-05-10 12:33:44.507507: step 35000, acc = 0.9399, f1 = 0.9394
[Status] 2017-05-10 12:33:44.507648: step 35000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 12:34:51.394433: step 35020, loss = 0.0044, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-10 12:35:58.096783: step 35040, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 12:37:04.582012: step 35060, loss = 0.0074, acc = 0.9980 (19.2 examples/sec; 3.333 sec/batch)
2017-05-10 12:38:11.136928: step 35080, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-10 12:39:17.250980: step 35100, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 12:40:23.434074: step 35120, loss = 0.0050, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-10 12:41:30.028572: step 35140, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 12:42:38.047283: step 35160, loss = 0.0043, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-10 12:43:44.587616: step 35180, loss = 0.0043, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-10 12:44:50.533647: step 35200, loss = 0.0042, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 12:45:56.454847: step 35220, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 12:47:02.831620: step 35240, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-10 12:48:09.112947: step 35260, loss = 0.0039, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-10 12:49:16.923922: step 35280, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 12:50:23.951676: step 35300, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 12:51:31.469976: step 35320, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 12:52:38.628688: step 35340, loss = 0.0042, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-10 12:53:46.559759: step 35360, loss = 0.0044, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-10 12:54:52.868956: step 35380, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 12:56:01.260243: step 35400, loss = 0.0037, acc = 1.0000 (18.1 examples/sec; 3.531 sec/batch)
2017-05-10 12:57:08.531497: step 35420, loss = 0.0036, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-10 12:58:16.357917: step 35440, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 12:59:23.058997: step 35460, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 13:00:30.333986: step 35480, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 13:01:36.803844: step 35500, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 13:02:43.309626: step 35520, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 13:03:49.760083: step 35540, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 13:04:56.755465: step 35560, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 13:06:02.840910: step 35580, loss = 0.0041, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-10 13:07:09.731466: step 35600, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 13:08:17.060292: step 35620, loss = 0.0044, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-10 13:09:23.418880: step 35640, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 13:10:29.782760: step 35660, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-10 13:11:37.352969: step 35680, loss = 0.0046, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-10 13:12:44.156202: step 35700, loss = 0.0075, acc = 0.9980 (18.9 examples/sec; 3.379 sec/batch)
2017-05-10 13:13:50.460226: step 35720, loss = 0.0038, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-10 13:14:57.317425: step 35740, loss = 0.0039, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 13:16:04.392457: step 35760, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 13:17:11.486905: step 35780, loss = 0.0044, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 13:18:17.913126: step 35800, loss = 0.0054, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 13:19:26.244288: step 35820, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 13:20:33.102652: step 35840, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 13:21:38.898936: step 35860, loss = 0.0054, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 13:22:45.160941: step 35880, loss = 0.0068, acc = 0.9980 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 13:23:51.625767: step 35900, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 13:24:57.973182: step 35920, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-10 13:26:04.418712: step 35940, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 13:27:11.845303: step 35960, loss = 0.0038, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-10 13:28:18.998583: step 35980, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 13:29:26.565040: step 36000, loss = 0.0041, acc = 1.0000 (17.6 examples/sec; 3.631 sec/batch)
[Eval] 2017-05-10 13:29:41.824259: step 36000, acc = 0.9512, f1 = 0.9496
[Test] 2017-05-10 13:29:51.560795: step 36000, acc = 0.9397, f1 = 0.9392
[Status] 2017-05-10 13:29:51.560867: step 36000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 13:30:57.874418: step 36020, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 13:32:03.628311: step 36040, loss = 0.0036, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-10 13:33:10.265304: step 36060, loss = 0.0037, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 13:34:17.840786: step 36080, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 13:35:24.376710: step 36100, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 13:36:32.181414: step 36120, loss = 0.0040, acc = 1.0000 (17.7 examples/sec; 3.616 sec/batch)
2017-05-10 13:37:40.279091: step 36140, loss = 0.0035, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-10 13:38:46.359115: step 36160, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 13:39:54.232195: step 36180, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-10 13:41:01.346725: step 36200, loss = 0.0042, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-10 13:42:07.756595: step 36220, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 13:43:15.051058: step 36240, loss = 0.0056, acc = 0.9980 (18.8 examples/sec; 3.400 sec/batch)
2017-05-10 13:44:21.344559: step 36260, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 13:45:28.223289: step 36280, loss = 0.0042, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-10 13:46:35.192780: step 36300, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 13:47:41.485498: step 36320, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 13:48:47.646366: step 36340, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 13:49:54.736467: step 36360, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-10 13:51:00.751210: step 36380, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 13:52:07.882207: step 36400, loss = 0.0041, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-10 13:53:14.841879: step 36420, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 13:54:21.036276: step 36440, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 13:55:28.051622: step 36460, loss = 0.0041, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 13:56:34.939493: step 36480, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 13:57:42.235510: step 36500, loss = 0.0039, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-10 13:58:48.362003: step 36520, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 13:59:55.178143: step 36540, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 14:01:02.408921: step 36560, loss = 0.0053, acc = 0.9980 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 14:02:09.929433: step 36580, loss = 0.0053, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-10 14:03:16.497714: step 36600, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 14:04:23.362228: step 36620, loss = 0.0047, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-10 14:05:29.322959: step 36640, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 14:06:35.656719: step 36660, loss = 0.0039, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-10 14:07:41.935508: step 36680, loss = 0.0038, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 14:08:48.335388: step 36700, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 14:09:55.088277: step 36720, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 14:11:02.995041: step 36740, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 14:12:10.104615: step 36760, loss = 0.0038, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-10 14:13:16.762272: step 36780, loss = 0.0041, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-10 14:14:24.492624: step 36800, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-10 14:15:31.899882: step 36820, loss = 0.0038, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-10 14:16:38.905673: step 36840, loss = 0.0045, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-10 14:17:45.032287: step 36860, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-10 14:18:52.001060: step 36880, loss = 0.0039, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 14:19:58.902799: step 36900, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-10 14:21:05.804727: step 36920, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 14:22:12.549863: step 36940, loss = 0.0039, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-10 14:23:18.582381: step 36960, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 14:24:24.760440: step 36980, loss = 0.0045, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-10 14:25:32.247858: step 37000, loss = 0.0038, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
[Eval] 2017-05-10 14:25:46.654136: step 37000, acc = 0.9513, f1 = 0.9497
[Test] 2017-05-10 14:25:57.237221: step 37000, acc = 0.9396, f1 = 0.9391
[Status] 2017-05-10 14:25:57.237368: step 37000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 14:27:04.194750: step 37020, loss = 0.0036, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-10 14:28:10.014472: step 37040, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 14:29:15.992668: step 37060, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-10 14:30:22.515320: step 37080, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 14:31:29.294887: step 37100, loss = 0.0039, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-10 14:32:36.001043: step 37120, loss = 0.0043, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 14:33:43.189474: step 37140, loss = 0.0035, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
2017-05-10 14:34:49.864317: step 37160, loss = 0.0047, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 14:35:56.590885: step 37180, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 14:37:02.987960: step 37200, loss = 0.0044, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-10 14:38:10.911614: step 37220, loss = 0.0044, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-10 14:39:16.889741: step 37240, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 14:40:24.849676: step 37260, loss = 0.0037, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 14:41:33.068074: step 37280, loss = 0.0039, acc = 1.0000 (17.6 examples/sec; 3.646 sec/batch)
2017-05-10 14:42:40.962544: step 37300, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 14:43:47.865663: step 37320, loss = 0.0035, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-10 14:44:55.085362: step 37340, loss = 0.0104, acc = 0.9980 (19.0 examples/sec; 3.376 sec/batch)
2017-05-10 14:46:01.882352: step 37360, loss = 0.0035, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-10 14:47:08.140826: step 37380, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 14:48:14.967396: step 37400, loss = 0.0034, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-10 14:49:21.506469: step 37420, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 14:50:28.673907: step 37440, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 14:51:35.293330: step 37460, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 14:52:42.976446: step 37480, loss = 0.0035, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 14:53:50.122567: step 37500, loss = 0.0033, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-10 14:54:58.448343: step 37520, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 14:56:04.695383: step 37540, loss = 0.0039, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-10 14:57:10.938545: step 37560, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 14:58:17.008227: step 37580, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 14:59:24.238407: step 37600, loss = 0.0036, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-10 15:00:30.867837: step 37620, loss = 0.0089, acc = 0.9980 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 15:01:37.549254: step 37640, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 15:02:43.790987: step 37660, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 15:03:52.142376: step 37680, loss = 0.0034, acc = 1.0000 (17.6 examples/sec; 3.644 sec/batch)
2017-05-10 15:04:59.306369: step 37700, loss = 0.0034, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-10 15:06:05.706602: step 37720, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 15:07:11.625879: step 37740, loss = 0.0041, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-10 15:08:18.179365: step 37760, loss = 0.0038, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-10 15:09:24.817838: step 37780, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 15:10:31.492561: step 37800, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 15:11:37.646652: step 37820, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-10 15:12:45.658766: step 37840, loss = 0.0038, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-10 15:13:51.739919: step 37860, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 15:14:58.401764: step 37880, loss = 0.0043, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-10 15:16:06.219869: step 37900, loss = 0.0034, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-10 15:17:12.622312: step 37920, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 15:18:19.173258: step 37940, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 15:19:25.293174: step 37960, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 15:20:33.032531: step 37980, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 15:21:40.346401: step 38000, loss = 0.0036, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
[Eval] 2017-05-10 15:21:55.935547: step 38000, acc = 0.9504, f1 = 0.9489
[Test] 2017-05-10 15:22:06.539036: step 38000, acc = 0.9394, f1 = 0.9389
[Status] 2017-05-10 15:22:06.539133: step 38000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 15:23:14.929378: step 38020, loss = 0.0034, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-10 15:24:22.037535: step 38040, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-10 15:25:29.606177: step 38060, loss = 0.0039, acc = 1.0000 (17.8 examples/sec; 3.596 sec/batch)
2017-05-10 15:26:37.417866: step 38080, loss = 0.0031, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-10 15:27:43.601031: step 38100, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 15:28:50.637095: step 38120, loss = 0.0037, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-10 15:29:56.771860: step 38140, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 15:31:03.045385: step 38160, loss = 0.0038, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 15:32:10.387539: step 38180, loss = 0.0038, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-10 15:33:18.062225: step 38200, loss = 0.0044, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-10 15:34:24.413970: step 38220, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 15:35:32.043188: step 38240, loss = 0.0035, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-10 15:36:38.891925: step 38260, loss = 0.0043, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-10 15:37:45.381891: step 38280, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 15:38:53.333244: step 38300, loss = 0.0036, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-10 15:40:01.210741: step 38320, loss = 0.0042, acc = 1.0000 (18.0 examples/sec; 3.558 sec/batch)
2017-05-10 15:41:08.271276: step 38340, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 15:42:15.086090: step 38360, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 15:43:21.713129: step 38380, loss = 0.0049, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 15:44:28.150394: step 38400, loss = 0.0035, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-10 15:45:35.045355: step 38420, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 15:46:42.359837: step 38440, loss = 0.0032, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-10 15:47:48.511881: step 38460, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-10 15:48:56.760085: step 38480, loss = 0.0031, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-10 15:50:02.470509: step 38500, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 15:51:08.973942: step 38520, loss = 0.0038, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 15:52:15.322746: step 38540, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-10 15:53:22.404432: step 38560, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 15:54:28.898450: step 38580, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 15:55:35.804249: step 38600, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-10 15:56:42.050439: step 38620, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-10 15:57:48.655527: step 38640, loss = 0.0051, acc = 0.9980 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 15:58:54.740697: step 38660, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 16:00:01.781633: step 38680, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-10 16:01:09.650054: step 38700, loss = 0.0034, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-10 16:02:16.931406: step 38720, loss = 0.0037, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-10 16:03:22.916468: step 38740, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 16:04:30.633603: step 38760, loss = 0.0035, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-10 16:05:37.899437: step 38780, loss = 0.0046, acc = 0.9980 (18.4 examples/sec; 3.479 sec/batch)
2017-05-10 16:06:44.325320: step 38800, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 16:07:51.381945: step 38820, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 16:08:59.021765: step 38840, loss = 0.0048, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-10 16:10:05.916402: step 38860, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-10 16:11:11.990816: step 38880, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 16:12:18.237070: step 38900, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 16:13:24.789867: step 38920, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 16:14:31.819693: step 38940, loss = 0.0037, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-10 16:15:39.077214: step 38960, loss = 0.0033, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-10 16:16:45.658193: step 38980, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 16:17:52.162111: step 39000, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
[Eval] 2017-05-10 16:18:07.672950: step 39000, acc = 0.9499, f1 = 0.9483
[Test] 2017-05-10 16:18:18.349076: step 39000, acc = 0.9393, f1 = 0.9388
[Status] 2017-05-10 16:18:18.349160: step 39000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 16:19:25.747424: step 39020, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 16:20:33.929632: step 39040, loss = 0.0037, acc = 1.0000 (17.8 examples/sec; 3.587 sec/batch)
2017-05-10 16:21:42.251113: step 39060, loss = 0.0035, acc = 1.0000 (18.7 examples/sec; 3.432 sec/batch)
2017-05-10 16:22:48.895084: step 39080, loss = 0.0035, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-10 16:23:55.326655: step 39100, loss = 0.0033, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-10 16:25:02.387088: step 39120, loss = 0.0032, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-10 16:26:09.756796: step 39140, loss = 0.0034, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-10 16:27:16.384019: step 39160, loss = 0.0038, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-10 16:28:22.663703: step 39180, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-10 16:29:30.242126: step 39200, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 16:30:35.947981: step 39220, loss = 0.0036, acc = 1.0000 (19.7 examples/sec; 3.247 sec/batch)
2017-05-10 16:31:42.982236: step 39240, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 16:32:50.737193: step 39260, loss = 0.0032, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-10 16:33:57.821890: step 39280, loss = 0.0033, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-10 16:35:04.499615: step 39300, loss = 0.0040, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 16:36:12.229298: step 39320, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-10 16:37:19.043416: step 39340, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 16:38:26.249308: step 39360, loss = 0.0033, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-10 16:39:32.774184: step 39380, loss = 0.0035, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-10 16:40:39.491002: step 39400, loss = 0.0031, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-10 16:41:46.117304: step 39420, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 16:42:52.787927: step 39440, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 16:43:59.082317: step 39460, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 16:45:06.752882: step 39480, loss = 0.0035, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-10 16:46:13.467075: step 39500, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 16:47:21.715220: step 39520, loss = 0.0035, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-10 16:48:28.532973: step 39540, loss = 0.0071, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 16:49:35.284684: step 39560, loss = 0.0044, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-10 16:50:41.370066: step 39580, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 16:51:48.977714: step 39600, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 16:52:55.578101: step 39620, loss = 0.0037, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-10 16:54:02.202389: step 39640, loss = 0.0042, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-10 16:55:10.487672: step 39660, loss = 0.0034, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-10 16:56:16.815027: step 39680, loss = 0.0055, acc = 0.9980 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 16:57:23.822131: step 39700, loss = 0.0035, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-10 16:58:31.529164: step 39720, loss = 0.0031, acc = 1.0000 (18.0 examples/sec; 3.553 sec/batch)
2017-05-10 16:59:38.533129: step 39740, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 17:00:45.383448: step 39760, loss = 0.0032, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-10 17:01:51.955416: step 39780, loss = 0.0037, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-10 17:02:59.397324: step 39800, loss = 0.0031, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-10 17:04:07.445978: step 39820, loss = 0.0040, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-10 17:05:14.318847: step 39840, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 17:06:20.477758: step 39860, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 17:07:28.640633: step 39880, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 17:08:35.177224: step 39900, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 17:09:42.291699: step 39920, loss = 0.0034, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-10 17:10:49.311797: step 39940, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-10 17:11:56.744170: step 39960, loss = 0.0035, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-10 17:13:02.439871: step 39980, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 17:14:09.377142: step 40000, loss = 0.0033, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
[Eval] 2017-05-10 17:14:25.089934: step 40000, acc = 0.9497, f1 = 0.9481
[Test] 2017-05-10 17:14:35.752681: step 40000, acc = 0.9384, f1 = 0.9379
[Status] 2017-05-10 17:14:35.752803: step 40000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 17:15:42.330672: step 40020, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 17:16:49.691687: step 40040, loss = 0.0036, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-10 17:17:57.056143: step 40060, loss = 0.0041, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-10 17:19:04.687724: step 40080, loss = 0.0036, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-10 17:20:10.962878: step 40100, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 17:21:19.342962: step 40120, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-10 17:22:25.278234: step 40140, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 17:23:31.606329: step 40160, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 17:24:37.859196: step 40180, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 17:25:44.386451: step 40200, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 17:26:51.768916: step 40220, loss = 0.0043, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-10 17:27:57.640704: step 40240, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 17:29:04.498348: step 40260, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-10 17:30:11.246517: step 40280, loss = 0.0033, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-10 17:31:18.648715: step 40300, loss = 0.0032, acc = 1.0000 (17.9 examples/sec; 3.569 sec/batch)
2017-05-10 17:32:26.205052: step 40320, loss = 0.0038, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-10 17:33:33.354761: step 40340, loss = 0.0034, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-10 17:34:39.756885: step 40360, loss = 0.0049, acc = 0.9980 (18.9 examples/sec; 3.394 sec/batch)
2017-05-10 17:35:47.118749: step 40380, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-10 17:36:54.275337: step 40400, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 17:38:01.156906: step 40420, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 17:39:08.264985: step 40440, loss = 0.0031, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-10 17:40:15.393453: step 40460, loss = 0.0032, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-10 17:41:23.334533: step 40480, loss = 0.0034, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-10 17:42:30.115740: step 40500, loss = 0.0039, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-10 17:43:36.724436: step 40520, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 17:44:43.229536: step 40540, loss = 0.0062, acc = 0.9980 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 17:45:51.922852: step 40560, loss = 0.0037, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 17:46:59.190814: step 40580, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 17:48:07.014689: step 40600, loss = 0.0035, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-10 17:49:13.749295: step 40620, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 17:50:20.894794: step 40640, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-10 17:51:29.722209: step 40660, loss = 0.0031, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-10 17:52:37.387985: step 40680, loss = 0.0035, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-10 17:53:44.055105: step 40700, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 17:54:50.465739: step 40720, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 17:55:57.284636: step 40740, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 17:57:03.985607: step 40760, loss = 0.0035, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-10 17:58:10.299288: step 40780, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 17:59:17.095794: step 40800, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-10 18:00:23.622928: step 40820, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-10 18:01:31.082712: step 40840, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-10 18:02:38.138032: step 40860, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 18:03:46.007831: step 40880, loss = 0.0044, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 18:04:52.100738: step 40900, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 18:05:58.503813: step 40920, loss = 0.0033, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-10 18:07:04.846678: step 40940, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 18:08:12.208644: step 40960, loss = 0.0132, acc = 0.9980 (18.4 examples/sec; 3.482 sec/batch)
2017-05-10 18:09:19.183643: step 40980, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 18:10:25.758259: step 41000, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
[Eval] 2017-05-10 18:10:39.947405: step 41000, acc = 0.9507, f1 = 0.9491
[Test] 2017-05-10 18:10:50.631950: step 41000, acc = 0.9385, f1 = 0.9380
[Status] 2017-05-10 18:10:50.632075: step 41000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 18:11:56.654202: step 41020, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 18:13:03.024219: step 41040, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 18:14:10.129652: step 41060, loss = 0.0034, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-10 18:15:16.798552: step 41080, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 18:16:23.741031: step 41100, loss = 0.0073, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 18:17:30.166457: step 41120, loss = 0.0038, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-10 18:18:38.180239: step 41140, loss = 0.0051, acc = 0.9980 (18.4 examples/sec; 3.478 sec/batch)
2017-05-10 18:19:45.090865: step 41160, loss = 0.0033, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-10 18:20:51.618758: step 41180, loss = 0.0047, acc = 0.9980 (18.9 examples/sec; 3.392 sec/batch)
2017-05-10 18:21:58.812127: step 41200, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 18:23:05.032449: step 41220, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 18:24:11.790564: step 41240, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-10 18:25:18.314028: step 41260, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 18:26:24.884132: step 41280, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 18:27:32.502873: step 41300, loss = 0.0037, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 18:28:39.621783: step 41320, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 18:29:47.780973: step 41340, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 18:30:55.095950: step 41360, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 18:32:03.404053: step 41380, loss = 0.0051, acc = 0.9980 (18.4 examples/sec; 3.479 sec/batch)
2017-05-10 18:33:09.662114: step 41400, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 18:34:15.567792: step 41420, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-10 18:35:23.252596: step 41440, loss = 0.0030, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-10 18:36:31.601395: step 41460, loss = 0.0030, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-10 18:37:41.155076: step 41480, loss = 0.0037, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-10 18:38:53.321796: step 41500, loss = 0.0040, acc = 1.0000 (17.7 examples/sec; 3.619 sec/batch)
2017-05-10 18:40:06.355496: step 41520, loss = 0.0031, acc = 1.0000 (16.3 examples/sec; 3.916 sec/batch)
2017-05-10 18:41:14.244402: step 41540, loss = 0.0039, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-10 18:42:20.872771: step 41560, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 18:43:27.914572: step 41580, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-10 18:44:35.792175: step 41600, loss = 0.0030, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-10 18:45:42.821099: step 41620, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 18:46:50.047907: step 41640, loss = 0.0033, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-10 18:47:57.016338: step 41660, loss = 0.0033, acc = 1.0000 (18.1 examples/sec; 3.537 sec/batch)
2017-05-10 18:49:04.118239: step 41680, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 18:50:10.487804: step 41700, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-10 18:51:17.591435: step 41720, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 18:52:24.567161: step 41740, loss = 0.0033, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-10 18:53:32.772772: step 41760, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 18:54:39.420292: step 41780, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 18:55:47.240849: step 41800, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-10 18:56:54.090086: step 41820, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-10 18:58:02.041413: step 41840, loss = 0.0042, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-10 18:59:08.900142: step 41860, loss = 0.0031, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-10 19:00:15.503031: step 41880, loss = 0.0051, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-10 19:01:21.710044: step 41900, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 19:02:30.202369: step 41920, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-10 19:03:37.371494: step 41940, loss = 0.0038, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-10 19:04:44.954110: step 41960, loss = 0.0036, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 19:05:51.818796: step 41980, loss = 0.0036, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-10 19:06:58.617248: step 42000, loss = 0.0028, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
[Eval] 2017-05-10 19:07:14.049202: step 42000, acc = 0.9502, f1 = 0.9486
[Test] 2017-05-10 19:07:24.719871: step 42000, acc = 0.9386, f1 = 0.9381
[Status] 2017-05-10 19:07:24.719993: step 42000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 19:08:31.540699: step 42020, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 19:09:38.410660: step 42040, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-10 19:10:45.399882: step 42060, loss = 0.0037, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-10 19:11:51.882920: step 42080, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-10 19:12:58.904424: step 42100, loss = 0.0032, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-10 19:14:06.227101: step 42120, loss = 0.0031, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-10 19:15:12.461579: step 42140, loss = 0.0060, acc = 0.9980 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 19:16:19.316048: step 42160, loss = 0.0031, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-10 19:17:25.843261: step 42180, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 19:18:32.132743: step 42200, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-10 19:19:38.863100: step 42220, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 19:20:45.687152: step 42240, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 19:21:52.338890: step 42260, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-10 19:22:58.949377: step 42280, loss = 0.0042, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-10 19:24:05.733978: step 42300, loss = 0.0031, acc = 1.0000 (19.7 examples/sec; 3.249 sec/batch)
2017-05-10 19:25:12.975335: step 42320, loss = 0.0034, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-10 19:26:20.821385: step 42340, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 19:27:27.393150: step 42360, loss = 0.0032, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-10 19:28:34.601184: step 42380, loss = 0.0029, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-10 19:29:41.360938: step 42400, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 19:30:47.744973: step 42420, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-10 19:31:55.513462: step 42440, loss = 0.0034, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-10 19:33:02.549782: step 42460, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 19:34:08.847191: step 42480, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 19:35:16.275446: step 42500, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 19:36:22.052217: step 42520, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 19:37:28.261392: step 42540, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 19:38:34.872087: step 42560, loss = 0.0029, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 19:39:42.124696: step 42580, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-10 19:40:48.550776: step 42600, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 19:41:55.178821: step 42620, loss = 0.0040, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-10 19:43:02.220449: step 42640, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-10 19:44:09.949061: step 42660, loss = 0.0029, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-10 19:45:16.631187: step 42680, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 19:46:23.591649: step 42700, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-10 19:47:30.612977: step 42720, loss = 0.0029, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-10 19:48:37.418677: step 42740, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 19:49:43.770436: step 42760, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-10 19:50:50.398453: step 42780, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-10 19:51:57.157422: step 42800, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 19:53:04.912086: step 42820, loss = 0.0031, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-10 19:54:11.800932: step 42840, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-10 19:55:18.060460: step 42860, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 19:56:25.372977: step 42880, loss = 0.0036, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-10 19:57:32.042919: step 42900, loss = 0.0039, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-10 19:58:39.467728: step 42920, loss = 0.0030, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-10 19:59:46.470158: step 42940, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-10 20:00:53.242801: step 42960, loss = 0.0033, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 20:02:00.950867: step 42980, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-10 20:03:07.317664: step 43000, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
[Eval] 2017-05-10 20:03:22.899681: step 43000, acc = 0.9497, f1 = 0.9481
[Test] 2017-05-10 20:03:33.621688: step 43000, acc = 0.9379, f1 = 0.9374
[Status] 2017-05-10 20:03:33.621805: step 43000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 20:04:39.727950: step 43020, loss = 0.0031, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-10 20:05:46.318373: step 43040, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 20:06:52.334414: step 43060, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 20:07:58.567677: step 43080, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 20:09:06.117084: step 43100, loss = 0.0030, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-10 20:10:12.687269: step 43120, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 20:11:19.597489: step 43140, loss = 0.0074, acc = 0.9980 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 20:12:26.304161: step 43160, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 20:13:32.955995: step 43180, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 20:14:39.148916: step 43200, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-10 20:15:45.999184: step 43220, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 20:16:53.436901: step 43240, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 20:17:59.448359: step 43260, loss = 0.0041, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-10 20:19:06.723866: step 43280, loss = 0.0034, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-10 20:20:13.677521: step 43300, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-10 20:21:21.196466: step 43320, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 20:22:28.748103: step 43340, loss = 0.0032, acc = 1.0000 (17.9 examples/sec; 3.566 sec/batch)
2017-05-10 20:23:35.849411: step 43360, loss = 0.0040, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-10 20:24:43.912334: step 43380, loss = 0.0027, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-10 20:25:50.408375: step 43400, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-10 20:26:56.903791: step 43420, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-10 20:28:05.020994: step 43440, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-10 20:29:11.245798: step 43460, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-10 20:30:19.344514: step 43480, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 20:31:26.400064: step 43500, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-10 20:32:33.393219: step 43520, loss = 0.0030, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-10 20:33:39.566000: step 43540, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-10 20:34:47.611208: step 43560, loss = 0.0029, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-10 20:35:54.595439: step 43580, loss = 0.0028, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-10 20:37:01.750201: step 43600, loss = 0.0033, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-10 20:38:09.587836: step 43620, loss = 0.0030, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-10 20:39:17.238774: step 43640, loss = 0.0033, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 20:40:24.240257: step 43660, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-10 20:41:30.176226: step 43680, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 20:42:36.417858: step 43700, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 20:43:43.093740: step 43720, loss = 0.0033, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-10 20:44:49.671897: step 43740, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 20:45:56.672104: step 43760, loss = 0.0036, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-10 20:47:03.879205: step 43780, loss = 0.0030, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-10 20:48:11.077009: step 43800, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-10 20:49:17.371595: step 43820, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 20:50:23.547530: step 43840, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-10 20:51:30.343044: step 43860, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-10 20:52:37.337275: step 43880, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 20:53:45.025034: step 43900, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-10 20:54:51.148327: step 43920, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 20:55:57.032065: step 43940, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 20:57:03.808964: step 43960, loss = 0.0029, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-10 20:58:12.898687: step 43980, loss = 0.0031, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-10 20:59:19.295279: step 44000, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-10 20:59:34.725925: step 44000, acc = 0.9483, f1 = 0.9467
[Test] 2017-05-10 20:59:45.468601: step 44000, acc = 0.9372, f1 = 0.9367
[Status] 2017-05-10 20:59:45.468687: step 44000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 21:00:52.515271: step 44020, loss = 0.0039, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-10 21:01:59.261854: step 44040, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-10 21:03:05.257204: step 44060, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 21:04:12.071863: step 44080, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-10 21:05:19.194960: step 44100, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 21:06:26.171203: step 44120, loss = 0.0028, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-10 21:07:33.520115: step 44140, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 21:08:40.467783: step 44160, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 21:09:48.193590: step 44180, loss = 0.0065, acc = 0.9980 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 21:10:55.809277: step 44200, loss = 0.0034, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-10 21:12:01.620515: step 44220, loss = 0.0042, acc = 0.9980 (19.7 examples/sec; 3.256 sec/batch)
2017-05-10 21:13:08.647357: step 44240, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 21:14:15.682142: step 44260, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 21:15:21.654545: step 44280, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 21:16:28.407840: step 44300, loss = 0.0036, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-10 21:17:35.197190: step 44320, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-10 21:18:42.024648: step 44340, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 21:19:49.043921: step 44360, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 21:20:55.241992: step 44380, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 21:22:01.259406: step 44400, loss = 0.0053, acc = 0.9980 (19.5 examples/sec; 3.276 sec/batch)
2017-05-10 21:23:07.514452: step 44420, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-10 21:24:14.874806: step 44440, loss = 0.0028, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-10 21:25:21.935622: step 44460, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 21:26:28.216297: step 44480, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 21:27:35.758099: step 44500, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-10 21:28:41.637451: step 44520, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-10 21:29:48.711925: step 44540, loss = 0.0029, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-10 21:30:56.050357: step 44560, loss = 0.0027, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-10 21:32:02.312570: step 44580, loss = 0.0042, acc = 0.9980 (19.3 examples/sec; 3.320 sec/batch)
2017-05-10 21:33:08.659196: step 44600, loss = 0.0025, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-10 21:34:15.332830: step 44620, loss = 0.0025, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-10 21:35:22.460641: step 44640, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-10 21:36:28.567650: step 44660, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 21:37:36.196631: step 44680, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-10 21:38:42.283513: step 44700, loss = 0.0032, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-10 21:39:49.143642: step 44720, loss = 0.0029, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-10 21:40:55.253704: step 44740, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-10 21:42:02.442527: step 44760, loss = 0.0030, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-10 21:43:08.683874: step 44780, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 21:44:14.739343: step 44800, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-10 21:45:21.716812: step 44820, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-10 21:46:28.874763: step 44840, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-10 21:47:35.891252: step 44860, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-10 21:48:42.216533: step 44880, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-10 21:49:50.158273: step 44900, loss = 0.0029, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-10 21:50:56.222872: step 44920, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-10 21:52:03.607908: step 44940, loss = 0.0030, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-10 21:53:10.030390: step 44960, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 21:54:16.002766: step 44980, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-10 21:55:22.802974: step 45000, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
[Eval] 2017-05-10 21:55:37.855162: step 45000, acc = 0.9498, f1 = 0.9482
[Test] 2017-05-10 21:55:48.525288: step 45000, acc = 0.9378, f1 = 0.9373
[Status] 2017-05-10 21:55:48.525379: step 45000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 21:56:54.975472: step 45020, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 21:58:01.590905: step 45040, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 21:59:07.865614: step 45060, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 22:00:15.395811: step 45080, loss = 0.0030, acc = 1.0000 (17.9 examples/sec; 3.574 sec/batch)
2017-05-10 22:01:21.420010: step 45100, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-10 22:02:28.456730: step 45120, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 22:03:35.342103: step 45140, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-10 22:04:42.766116: step 45160, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-10 22:05:48.851558: step 45180, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-10 22:06:56.019152: step 45200, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 22:08:02.511604: step 45220, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-10 22:09:08.995269: step 45240, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-10 22:10:15.440806: step 45260, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-10 22:11:21.402312: step 45280, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 22:12:28.249313: step 45300, loss = 0.0030, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-10 22:13:36.272023: step 45320, loss = 0.0032, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-10 22:14:42.946158: step 45340, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 22:15:50.117015: step 45360, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-10 22:16:58.016223: step 45380, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-10 22:18:04.455875: step 45400, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 22:19:11.744789: step 45420, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-10 22:20:18.056612: step 45440, loss = 0.0039, acc = 0.9980 (19.6 examples/sec; 3.263 sec/batch)
2017-05-10 22:21:25.120200: step 45460, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-10 22:22:32.767677: step 45480, loss = 0.0030, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-10 22:23:39.709980: step 45500, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 22:24:47.143937: step 45520, loss = 0.0028, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-10 22:25:54.919189: step 45540, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-10 22:27:01.514578: step 45560, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-10 22:28:08.316799: step 45580, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 22:29:15.347200: step 45600, loss = 0.0027, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-10 22:30:21.978408: step 45620, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-10 22:31:29.322542: step 45640, loss = 0.0044, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-10 22:32:35.603758: step 45660, loss = 0.0032, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-10 22:33:42.426566: step 45680, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 22:34:49.832922: step 45700, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-10 22:35:57.118947: step 45720, loss = 0.0025, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-10 22:37:04.412816: step 45740, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-10 22:38:12.369479: step 45760, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 22:39:19.327857: step 45780, loss = 0.0043, acc = 0.9980 (18.8 examples/sec; 3.405 sec/batch)
2017-05-10 22:40:27.303480: step 45800, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-10 22:41:34.701366: step 45820, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 22:42:40.826788: step 45840, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-10 22:43:48.798812: step 45860, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-10 22:44:56.578818: step 45880, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-10 22:46:03.686261: step 45900, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 22:47:11.234359: step 45920, loss = 0.0040, acc = 0.9980 (18.6 examples/sec; 3.450 sec/batch)
2017-05-10 22:48:18.353014: step 45940, loss = 0.0025, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-10 22:49:25.288224: step 45960, loss = 0.0027, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-10 22:50:31.726088: step 45980, loss = 0.0030, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-10 22:51:38.903742: step 46000, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
[Eval] 2017-05-10 22:51:54.464133: step 46000, acc = 0.9480, f1 = 0.9464
[Test] 2017-05-10 22:52:05.153321: step 46000, acc = 0.9367, f1 = 0.9362
[Status] 2017-05-10 22:52:05.153428: step 46000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 22:53:12.441358: step 46020, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-10 22:54:19.115268: step 46040, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 22:55:25.982882: step 46060, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 22:56:31.936083: step 46080, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-10 22:57:38.014056: step 46100, loss = 0.0046, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-10 22:58:44.729340: step 46120, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-10 22:59:51.398029: step 46140, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 23:00:58.258212: step 46160, loss = 0.0031, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-10 23:02:06.275259: step 46180, loss = 0.0031, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-10 23:03:12.783966: step 46200, loss = 0.0049, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-10 23:04:20.491449: step 46220, loss = 0.0027, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-10 23:05:27.872626: step 46240, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-10 23:06:34.364874: step 46260, loss = 0.0029, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-10 23:07:41.678907: step 46280, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-10 23:08:48.855267: step 46300, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 23:09:56.498141: step 46320, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 23:11:04.326403: step 46340, loss = 0.0028, acc = 1.0000 (17.7 examples/sec; 3.615 sec/batch)
2017-05-10 23:12:12.010323: step 46360, loss = 0.0028, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-10 23:13:20.083822: step 46380, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-10 23:14:27.744305: step 46400, loss = 0.0024, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-10 23:15:35.555805: step 46420, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-10 23:16:42.988798: step 46440, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-10 23:17:50.181625: step 46460, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 23:18:56.166233: step 46480, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-10 23:20:02.487361: step 46500, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-10 23:21:09.157084: step 46520, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-10 23:22:15.826295: step 46540, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-10 23:23:22.326155: step 46560, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-10 23:24:28.826121: step 46580, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-10 23:25:35.211224: step 46600, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-10 23:26:42.231741: step 46620, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-10 23:27:48.638047: step 46640, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-10 23:28:54.909800: step 46660, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-10 23:30:01.440173: step 46680, loss = 0.0023, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-10 23:31:07.876701: step 46700, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-10 23:32:15.683798: step 46720, loss = 0.0026, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-10 23:33:21.775323: step 46740, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-10 23:34:29.513695: step 46760, loss = 0.0027, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-10 23:35:36.374237: step 46780, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-10 23:36:43.195147: step 46800, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-10 23:37:50.349074: step 46820, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-10 23:38:57.255927: step 46840, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-10 23:40:04.487692: step 46860, loss = 0.0043, acc = 0.9980 (18.7 examples/sec; 3.429 sec/batch)
2017-05-10 23:41:10.661189: step 46880, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-10 23:42:18.312132: step 46900, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-10 23:43:24.937623: step 46920, loss = 0.0030, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-10 23:44:31.603313: step 46940, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-10 23:45:38.316222: step 46960, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-10 23:46:44.554005: step 46980, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-10 23:47:50.746258: step 47000, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
[Eval] 2017-05-10 23:48:06.142043: step 47000, acc = 0.9495, f1 = 0.9479
[Test] 2017-05-10 23:48:16.802050: step 47000, acc = 0.9366, f1 = 0.9360
[Status] 2017-05-10 23:48:16.802170: step 47000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-10 23:49:23.907582: step 47020, loss = 0.0029, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-10 23:50:30.646948: step 47040, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-10 23:51:36.827669: step 47060, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-10 23:52:42.626606: step 47080, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-10 23:53:50.332630: step 47100, loss = 0.0024, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-10 23:54:56.911336: step 47120, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-10 23:56:03.566309: step 47140, loss = 0.0030, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-10 23:57:11.643458: step 47160, loss = 0.0028, acc = 1.0000 (18.2 examples/sec; 3.520 sec/batch)
2017-05-10 23:58:18.387716: step 47180, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-10 23:59:24.821463: step 47200, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 00:00:31.467727: step 47220, loss = 0.0148, acc = 0.9960 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 00:01:38.978024: step 47240, loss = 2.6723, acc = 0.7400 (19.6 examples/sec; 3.274 sec/batch)
2017-05-11 00:02:46.183688: step 47260, loss = 0.7475, acc = 0.9340 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 00:03:52.049787: step 47280, loss = 0.1231, acc = 0.9960 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 00:04:59.307629: step 47300, loss = 0.2664, acc = 0.9860 (19.6 examples/sec; 3.259 sec/batch)
2017-05-11 00:06:06.279334: step 47320, loss = 0.1696, acc = 0.9960 (18.8 examples/sec; 3.401 sec/batch)
2017-05-11 00:07:12.222716: step 47340, loss = 0.1457, acc = 0.9920 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 00:08:19.602634: step 47360, loss = 0.2639, acc = 0.9940 (18.9 examples/sec; 3.380 sec/batch)
2017-05-11 00:09:27.158224: step 47380, loss = 0.1257, acc = 0.9960 (19.1 examples/sec; 3.352 sec/batch)
2017-05-11 00:10:34.552207: step 47400, loss = 0.1508, acc = 0.9960 (18.3 examples/sec; 3.501 sec/batch)
2017-05-11 00:11:41.336237: step 47420, loss = 0.1288, acc = 0.9960 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 00:12:49.058218: step 47440, loss = 0.1177, acc = 0.9960 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 00:13:55.662088: step 47460, loss = 0.0992, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 00:15:02.170223: step 47480, loss = 0.1169, acc = 0.9920 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 00:16:08.713984: step 47500, loss = 0.1061, acc = 0.9960 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 00:17:15.286284: step 47520, loss = 0.0819, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 00:18:21.604681: step 47540, loss = 0.0826, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 00:19:28.185159: step 47560, loss = 0.0844, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 00:20:35.698745: step 47580, loss = 0.0702, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 00:21:43.217052: step 47600, loss = 0.0666, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 00:22:49.317357: step 47620, loss = 0.0641, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-11 00:23:55.596127: step 47640, loss = 0.0713, acc = 0.9980 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 00:25:02.602737: step 47660, loss = 0.0594, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 00:26:09.207991: step 47680, loss = 0.0571, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-11 00:27:15.709600: step 47700, loss = 0.0551, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 00:28:22.614954: step 47720, loss = 0.0528, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 00:29:29.525437: step 47740, loss = 0.0523, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 00:30:35.777901: step 47760, loss = 0.0709, acc = 0.9960 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 00:31:42.182050: step 47780, loss = 0.0485, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 00:32:49.976447: step 47800, loss = 0.0574, acc = 0.9980 (18.9 examples/sec; 3.387 sec/batch)
2017-05-11 00:33:56.413696: step 47820, loss = 0.0641, acc = 0.9960 (18.8 examples/sec; 3.403 sec/batch)
2017-05-11 00:35:02.984796: step 47840, loss = 0.0529, acc = 0.9980 (19.6 examples/sec; 3.273 sec/batch)
2017-05-11 00:36:09.822731: step 47860, loss = 0.0562, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 00:37:17.713231: step 47880, loss = 0.0433, acc = 0.9980 (18.4 examples/sec; 3.470 sec/batch)
2017-05-11 00:38:23.763116: step 47900, loss = 0.0788, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 00:39:30.912344: step 47920, loss = 0.0376, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-11 00:40:38.835194: step 47940, loss = 0.0454, acc = 0.9980 (17.9 examples/sec; 3.576 sec/batch)
2017-05-11 00:41:44.955550: step 47960, loss = 0.0344, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 00:42:51.310439: step 47980, loss = 0.0406, acc = 0.9960 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 00:43:58.656014: step 48000, loss = 0.0322, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
[Eval] 2017-05-11 00:44:13.792201: step 48000, acc = 0.9438, f1 = 0.9419
[Test] 2017-05-11 00:44:23.650111: step 48000, acc = 0.9323, f1 = 0.9318
[Status] 2017-05-11 00:44:23.650222: step 48000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 00:45:29.961440: step 48020, loss = 0.0367, acc = 0.9980 (19.6 examples/sec; 3.265 sec/batch)
2017-05-11 00:46:37.840645: step 48040, loss = 0.0613, acc = 0.9960 (18.1 examples/sec; 3.528 sec/batch)
2017-05-11 00:47:44.310138: step 48060, loss = 0.0295, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 00:48:50.890852: step 48080, loss = 0.0291, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-11 00:49:57.307775: step 48100, loss = 0.0288, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 00:51:04.138841: step 48120, loss = 0.0269, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-11 00:52:10.961328: step 48140, loss = 0.0261, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 00:53:18.671987: step 48160, loss = 0.0252, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-11 00:54:26.826837: step 48180, loss = 0.0254, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-11 00:55:34.913923: step 48200, loss = 0.0264, acc = 0.9980 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 00:56:41.792736: step 48220, loss = 0.0238, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 00:57:48.844487: step 48240, loss = 0.0345, acc = 0.9960 (19.1 examples/sec; 3.356 sec/batch)
2017-05-11 00:58:55.984654: step 48260, loss = 0.0307, acc = 0.9980 (18.4 examples/sec; 3.486 sec/batch)
2017-05-11 01:00:02.338751: step 48280, loss = 0.0226, acc = 0.9980 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 01:01:09.063888: step 48300, loss = 0.0243, acc = 0.9980 (18.5 examples/sec; 3.465 sec/batch)
2017-05-11 01:02:15.302707: step 48320, loss = 0.0197, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-11 01:03:21.228083: step 48340, loss = 0.0191, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 01:04:27.450374: step 48360, loss = 0.0187, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 01:05:36.435608: step 48380, loss = 0.0181, acc = 1.0000 (17.2 examples/sec; 3.716 sec/batch)
2017-05-11 01:06:43.764398: step 48400, loss = 0.0173, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-11 01:07:49.931117: step 48420, loss = 0.0167, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-11 01:08:57.262729: step 48440, loss = 0.0170, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 01:10:03.778097: step 48460, loss = 0.0160, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-11 01:11:11.257363: step 48480, loss = 0.0154, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 01:12:18.383817: step 48500, loss = 0.0150, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 01:13:25.596219: step 48520, loss = 0.0146, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 01:14:32.708683: step 48540, loss = 0.0144, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 01:15:39.689426: step 48560, loss = 0.0140, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 01:16:47.292750: step 48580, loss = 0.0133, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 01:17:53.711527: step 48600, loss = 0.0134, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-11 01:19:00.060441: step 48620, loss = 0.0127, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 01:20:06.444564: step 48640, loss = 0.0128, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-11 01:21:13.566079: step 48660, loss = 0.0120, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 01:22:20.527961: step 48680, loss = 0.0118, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-11 01:23:27.690714: step 48700, loss = 0.0120, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 01:24:34.883801: step 48720, loss = 0.0117, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 01:25:41.998702: step 48740, loss = 0.0116, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 01:26:49.268941: step 48760, loss = 0.0106, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 01:27:55.643453: step 48780, loss = 0.0105, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-11 01:29:01.594958: step 48800, loss = 0.0103, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 01:30:08.634267: step 48820, loss = 0.0096, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-11 01:31:15.366355: step 48840, loss = 0.0102, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 01:32:21.139928: step 48860, loss = 0.0098, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 01:33:27.257117: step 48880, loss = 0.0090, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 01:34:34.045859: step 48900, loss = 0.0091, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-11 01:35:40.805478: step 48920, loss = 0.0100, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 01:36:46.842668: step 48940, loss = 0.0084, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 01:37:53.889651: step 48960, loss = 0.0086, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 01:39:01.602534: step 48980, loss = 0.0207, acc = 0.9980 (18.7 examples/sec; 3.425 sec/batch)
2017-05-11 01:40:08.699960: step 49000, loss = 0.0081, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-11 01:40:22.932980: step 49000, acc = 0.9449, f1 = 0.9431
[Test] 2017-05-11 01:40:33.577446: step 49000, acc = 0.9333, f1 = 0.9328
[Status] 2017-05-11 01:40:33.577539: step 49000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 01:41:39.829719: step 49020, loss = 0.0080, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 01:42:47.113046: step 49040, loss = 0.0087, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 01:43:54.263345: step 49060, loss = 0.0078, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 01:45:01.335232: step 49080, loss = 0.0083, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-11 01:46:07.802814: step 49100, loss = 0.0080, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 01:47:14.904959: step 49120, loss = 0.0074, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 01:48:22.581320: step 49140, loss = 0.0070, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 01:49:29.152947: step 49160, loss = 0.0077, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-11 01:50:35.256353: step 49180, loss = 0.0068, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 01:51:41.695380: step 49200, loss = 0.0063, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 01:52:48.119379: step 49220, loss = 0.0061, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-11 01:53:54.638349: step 49240, loss = 0.0074, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-11 01:55:01.960956: step 49260, loss = 0.0063, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 01:56:08.617597: step 49280, loss = 0.0062, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 01:57:15.562494: step 49300, loss = 0.0075, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-11 01:58:22.522398: step 49320, loss = 0.0058, acc = 1.0000 (18.1 examples/sec; 3.544 sec/batch)
2017-05-11 01:59:28.934359: step 49340, loss = 0.0060, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-11 02:00:36.594362: step 49360, loss = 0.0062, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-11 02:01:42.796480: step 49380, loss = 0.0061, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-11 02:02:50.128004: step 49400, loss = 0.0068, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 02:03:58.145191: step 49420, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 02:05:05.911088: step 49440, loss = 0.0049, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-11 02:06:11.813526: step 49460, loss = 0.0054, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 02:07:19.055791: step 49480, loss = 0.0053, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 02:08:25.535187: step 49500, loss = 0.0055, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 02:09:31.422943: step 49520, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 02:10:39.071159: step 49540, loss = 0.0050, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-11 02:11:46.162468: step 49560, loss = 0.0044, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-11 02:12:52.347878: step 49580, loss = 0.0046, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 02:13:58.406375: step 49600, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 02:15:04.875854: step 49620, loss = 0.0048, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-11 02:16:12.174736: step 49640, loss = 0.0049, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-11 02:17:18.782329: step 49660, loss = 0.0047, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-11 02:18:24.721040: step 49680, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 02:19:31.189681: step 49700, loss = 0.0042, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-11 02:20:37.883786: step 49720, loss = 0.0047, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 02:21:44.006489: step 49740, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 02:22:50.590826: step 49760, loss = 0.0052, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 02:23:59.145773: step 49780, loss = 0.0044, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-11 02:25:06.012268: step 49800, loss = 0.0050, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-11 02:26:12.203642: step 49820, loss = 0.0040, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 02:27:19.706402: step 49840, loss = 0.0039, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-11 02:28:26.603018: step 49860, loss = 0.0051, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-11 02:29:33.688133: step 49880, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 02:30:39.958468: step 49900, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 02:31:46.468403: step 49920, loss = 0.0044, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 02:32:53.414802: step 49940, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 02:33:59.563215: step 49960, loss = 0.0047, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 02:35:07.609876: step 49980, loss = 0.0039, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 02:36:14.485503: step 50000, loss = 0.0037, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
[Eval] 2017-05-11 02:36:28.691273: step 50000, acc = 0.9461, f1 = 0.9444
[Test] 2017-05-11 02:36:38.284211: step 50000, acc = 0.9348, f1 = 0.9343
[Status] 2017-05-11 02:36:38.284288: step 50000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 02:37:44.856556: step 50020, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 02:38:51.471604: step 50040, loss = 0.0045, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-11 02:39:58.408021: step 50060, loss = 0.0053, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-11 02:41:04.413128: step 50080, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 02:42:10.571654: step 50100, loss = 0.0047, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-11 02:43:17.256238: step 50120, loss = 0.0050, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 02:44:23.944081: step 50140, loss = 0.0057, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-11 02:45:32.830969: step 50160, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 02:46:39.529324: step 50180, loss = 0.0038, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-11 02:47:45.768671: step 50200, loss = 0.0056, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 02:48:52.280606: step 50220, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 02:49:59.151138: step 50240, loss = 0.0043, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 02:51:06.801855: step 50260, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 02:52:13.552466: step 50280, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 02:53:20.735726: step 50300, loss = 0.0053, acc = 0.9980 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 02:54:27.330073: step 50320, loss = 0.0057, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-11 02:55:34.108342: step 50340, loss = 0.0072, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-11 02:56:40.547747: step 50360, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 02:57:47.484246: step 50380, loss = 0.0052, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 02:58:55.023049: step 50400, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 03:00:01.765975: step 50420, loss = 0.0044, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 03:01:08.911086: step 50440, loss = 0.0037, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 03:02:15.324594: step 50460, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 03:03:22.484725: step 50480, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 03:04:29.482856: step 50500, loss = 0.0047, acc = 0.9980 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 03:05:36.154536: step 50520, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-11 03:06:43.205820: step 50540, loss = 0.0049, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-11 03:07:49.705698: step 50560, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 03:08:58.772966: step 50580, loss = 0.0036, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-11 03:10:04.815533: step 50600, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 03:11:11.139914: step 50620, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 03:12:17.363229: step 50640, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 03:13:25.378237: step 50660, loss = 0.0038, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-11 03:14:32.326697: step 50680, loss = 0.0039, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-11 03:15:39.511167: step 50700, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 03:16:45.840747: step 50720, loss = 0.0041, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-11 03:17:53.414685: step 50740, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-11 03:19:01.266893: step 50760, loss = 0.0040, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-11 03:20:07.809065: step 50780, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 03:21:14.582950: step 50800, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 03:22:21.799197: step 50820, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 03:23:29.027891: step 50840, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-11 03:24:36.267540: step 50860, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 03:25:43.900278: step 50880, loss = 0.0046, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 03:26:50.067970: step 50900, loss = 0.0038, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-11 03:27:56.453356: step 50920, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 03:29:02.764927: step 50940, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 03:30:09.746167: step 50960, loss = 0.0039, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-11 03:31:16.289454: step 50980, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 03:32:22.403274: step 51000, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
[Eval] 2017-05-11 03:32:37.775840: step 51000, acc = 0.9471, f1 = 0.9454
[Test] 2017-05-11 03:32:47.592062: step 51000, acc = 0.9358, f1 = 0.9353
[Status] 2017-05-11 03:32:47.592138: step 51000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 03:33:54.570107: step 51020, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 03:35:02.241757: step 51040, loss = 0.0063, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-11 03:36:09.887206: step 51060, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 03:37:17.547006: step 51080, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 03:38:23.812602: step 51100, loss = 0.0050, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 03:39:31.363359: step 51120, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 03:40:37.591047: step 51140, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 03:41:44.377441: step 51160, loss = 0.0042, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 03:42:51.543904: step 51180, loss = 0.0036, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-11 03:43:59.219837: step 51200, loss = 0.0042, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-11 03:45:06.713050: step 51220, loss = 0.0046, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-11 03:46:13.340335: step 51240, loss = 0.0038, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-11 03:47:19.460592: step 51260, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 03:48:26.691118: step 51280, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 03:49:33.532723: step 51300, loss = 0.0036, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-11 03:50:40.845577: step 51320, loss = 0.0033, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-11 03:51:47.640547: step 51340, loss = 0.0037, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 03:52:53.773092: step 51360, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 03:54:01.736649: step 51380, loss = 0.0049, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-11 03:55:08.869753: step 51400, loss = 0.0040, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 03:56:16.938584: step 51420, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 03:57:23.109456: step 51440, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 03:58:29.541022: step 51460, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 03:59:36.003416: step 51480, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 04:00:43.379465: step 51500, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 04:01:50.278936: step 51520, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 04:02:57.002468: step 51540, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 04:04:04.107060: step 51560, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 04:05:10.441336: step 51580, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 04:06:16.913087: step 51600, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 04:07:23.722713: step 51620, loss = 0.0047, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-11 04:08:30.292508: step 51640, loss = 0.0039, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 04:09:36.665068: step 51660, loss = 0.0032, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-11 04:10:42.433329: step 51680, loss = 0.0035, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 04:11:49.845690: step 51700, loss = 0.0033, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-11 04:12:56.546799: step 51720, loss = 0.0034, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-11 04:14:03.355858: step 51740, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 04:15:09.977380: step 51760, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-11 04:16:16.427298: step 51780, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 04:17:23.732310: step 51800, loss = 0.0032, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-11 04:18:31.105309: step 51820, loss = 0.0059, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 04:19:38.212652: step 51840, loss = 0.0033, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-11 04:20:45.352990: step 51860, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-11 04:21:52.209755: step 51880, loss = 0.0038, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-11 04:22:58.921046: step 51900, loss = 0.0038, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-11 04:24:06.624972: step 51920, loss = 0.0036, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-11 04:25:12.632193: step 51940, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 04:26:19.530713: step 51960, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 04:27:26.768441: step 51980, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 04:28:34.133763: step 52000, loss = 0.0032, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
[Eval] 2017-05-11 04:28:49.588004: step 52000, acc = 0.9473, f1 = 0.9456
[Test] 2017-05-11 04:29:00.193497: step 52000, acc = 0.9359, f1 = 0.9354
[Status] 2017-05-11 04:29:00.193596: step 52000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 04:30:07.330917: step 52020, loss = 0.0049, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-11 04:31:13.862653: step 52040, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 04:32:20.901672: step 52060, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 04:33:27.200030: step 52080, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 04:34:33.548433: step 52100, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 04:35:41.363321: step 52120, loss = 0.0035, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 04:36:48.147986: step 52140, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 04:37:55.801217: step 52160, loss = 0.0035, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-11 04:39:03.337221: step 52180, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-11 04:40:10.555149: step 52200, loss = 0.0053, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-11 04:41:16.578011: step 52220, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 04:42:23.004308: step 52240, loss = 0.0038, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-11 04:43:29.376516: step 52260, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 04:44:35.531578: step 52280, loss = 0.0038, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 04:45:41.804357: step 52300, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-11 04:46:48.467879: step 52320, loss = 0.0034, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-11 04:47:55.241022: step 52340, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 04:49:01.783723: step 52360, loss = 0.0039, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-11 04:50:08.376207: step 52380, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 04:51:15.023970: step 52400, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 04:52:23.377967: step 52420, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 04:53:29.835161: step 52440, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 04:54:36.808459: step 52460, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-11 04:55:44.063700: step 52480, loss = 0.0052, acc = 0.9980 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 04:56:50.554510: step 52500, loss = 0.0032, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-11 04:57:57.922827: step 52520, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 04:59:05.238458: step 52540, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 05:00:11.212503: step 52560, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 05:01:17.558513: step 52580, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-11 05:02:24.184993: step 52600, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 05:03:31.385175: step 52620, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 05:04:38.801178: step 52640, loss = 0.0046, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-11 05:05:45.255115: step 52660, loss = 0.0049, acc = 0.9980 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 05:06:51.827551: step 52680, loss = 0.0048, acc = 0.9980 (18.8 examples/sec; 3.404 sec/batch)
2017-05-11 05:07:58.367316: step 52700, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 05:09:04.489381: step 52720, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 05:10:11.056808: step 52740, loss = 0.0034, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-11 05:11:17.787878: step 52760, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 05:12:24.982861: step 52780, loss = 0.0031, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-11 05:13:32.997609: step 52800, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 05:14:38.954107: step 52820, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 05:15:45.276141: step 52840, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 05:16:52.527287: step 52860, loss = 0.0033, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-11 05:17:59.814177: step 52880, loss = 0.0028, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-11 05:19:07.177660: step 52900, loss = 0.0029, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-11 05:20:13.917031: step 52920, loss = 0.0034, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-11 05:21:21.757800: step 52940, loss = 0.0033, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-11 05:22:28.137232: step 52960, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 05:23:34.598933: step 52980, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 05:24:41.575591: step 53000, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-11 05:24:57.061046: step 53000, acc = 0.9478, f1 = 0.9462
[Test] 2017-05-11 05:25:07.667764: step 53000, acc = 0.9363, f1 = 0.9358
[Status] 2017-05-11 05:25:07.667858: step 53000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 05:26:14.535013: step 53020, loss = 0.0035, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-11 05:27:20.690408: step 53040, loss = 0.0035, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-11 05:28:27.360485: step 53060, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 05:29:33.763578: step 53080, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 05:30:40.387011: step 53100, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 05:31:46.893971: step 53120, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 05:32:54.158692: step 53140, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 05:34:01.122232: step 53160, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-11 05:35:07.403102: step 53180, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-11 05:36:14.262396: step 53200, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 05:37:20.656757: step 53220, loss = 0.0035, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 05:38:26.478451: step 53240, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 05:39:32.931379: step 53260, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 05:40:39.712117: step 53280, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 05:41:46.445177: step 53300, loss = 0.0040, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-11 05:42:54.250275: step 53320, loss = 0.0030, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-11 05:44:00.704928: step 53340, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 05:45:09.038434: step 53360, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-11 05:46:16.284358: step 53380, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 05:47:22.527608: step 53400, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 05:48:29.608017: step 53420, loss = 0.0032, acc = 1.0000 (18.0 examples/sec; 3.561 sec/batch)
2017-05-11 05:49:35.853456: step 53440, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-11 05:50:41.706908: step 53460, loss = 0.0030, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-11 05:51:48.358964: step 53480, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 05:52:55.498930: step 53500, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 05:54:01.455944: step 53520, loss = 0.0047, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-11 05:55:07.865193: step 53540, loss = 0.0036, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-11 05:56:14.557092: step 53560, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 05:57:22.653370: step 53580, loss = 0.0029, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-11 05:58:29.072473: step 53600, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-11 05:59:35.516630: step 53620, loss = 0.0044, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 06:00:42.908293: step 53640, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 06:01:50.539595: step 53660, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-11 06:02:56.932365: step 53680, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 06:04:04.374949: step 53700, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 06:05:10.559752: step 53720, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 06:06:16.944937: step 53740, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 06:07:23.377021: step 53760, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 06:08:30.091494: step 53780, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 06:09:37.906837: step 53800, loss = 0.0033, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-11 06:10:45.208662: step 53820, loss = 0.0029, acc = 1.0000 (18.0 examples/sec; 3.561 sec/batch)
2017-05-11 06:11:52.307392: step 53840, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 06:12:59.615507: step 53860, loss = 0.0030, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-11 06:14:05.706158: step 53880, loss = 0.0038, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 06:15:11.578519: step 53900, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 06:16:17.561966: step 53920, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 06:17:24.341366: step 53940, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 06:18:31.762967: step 53960, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-11 06:19:38.651890: step 53980, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-11 06:20:45.221381: step 54000, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
[Eval] 2017-05-11 06:21:00.639184: step 54000, acc = 0.9477, f1 = 0.9461
[Test] 2017-05-11 06:21:11.266981: step 54000, acc = 0.9362, f1 = 0.9357
[Status] 2017-05-11 06:21:11.267083: step 54000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 06:22:19.164177: step 54020, loss = 0.0039, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-11 06:23:25.300933: step 54040, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 06:24:32.061736: step 54060, loss = 0.0030, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-11 06:25:39.271749: step 54080, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 06:26:45.662614: step 54100, loss = 0.0036, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-11 06:27:52.580421: step 54120, loss = 0.0104, acc = 0.9980 (18.6 examples/sec; 3.432 sec/batch)
2017-05-11 06:28:58.667403: step 54140, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 06:30:05.351816: step 54160, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-11 06:31:11.693773: step 54180, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 06:32:19.523622: step 54200, loss = 0.0031, acc = 1.0000 (18.1 examples/sec; 3.534 sec/batch)
2017-05-11 06:33:26.065631: step 54220, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 06:34:32.999736: step 54240, loss = 0.0030, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-11 06:35:40.294094: step 54260, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 06:36:47.672675: step 54280, loss = 0.0043, acc = 0.9980 (18.6 examples/sec; 3.433 sec/batch)
2017-05-11 06:37:55.074415: step 54300, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 06:39:01.451403: step 54320, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-11 06:40:08.183805: step 54340, loss = 0.0033, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-11 06:41:14.359990: step 54360, loss = 0.0042, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 06:42:21.312614: step 54380, loss = 0.0037, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-11 06:43:30.101661: step 54400, loss = 0.0124, acc = 0.9980 (18.4 examples/sec; 3.481 sec/batch)
2017-05-11 06:44:36.594362: step 54420, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 06:45:45.217936: step 54440, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 06:46:52.550801: step 54460, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-11 06:47:58.668351: step 54480, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 06:49:05.385145: step 54500, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-11 06:50:12.741892: step 54520, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 06:51:20.062021: step 54540, loss = 0.0026, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-11 06:52:27.387560: step 54560, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 06:53:34.352339: step 54580, loss = 0.0032, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-11 06:54:40.518657: step 54600, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-11 06:55:47.381091: step 54620, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 06:56:54.867897: step 54640, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 06:58:01.738437: step 54660, loss = 0.0029, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-11 06:59:08.585952: step 54680, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 07:00:15.431649: step 54700, loss = 0.0043, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 07:01:22.529410: step 54720, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 07:02:29.120333: step 54740, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 07:03:35.434288: step 54760, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 07:04:41.997545: step 54780, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-11 07:05:49.392487: step 54800, loss = 0.0029, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-11 07:06:57.786833: step 54820, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 07:08:04.637477: step 54840, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 07:09:10.855852: step 54860, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-11 07:10:17.362111: step 54880, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 07:11:23.723334: step 54900, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 07:12:30.152111: step 54920, loss = 0.0030, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-11 07:13:37.433547: step 54940, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 07:14:44.228395: step 54960, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 07:15:51.188597: step 54980, loss = 0.0030, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-11 07:16:57.112707: step 55000, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-11 07:17:11.910494: step 55000, acc = 0.9482, f1 = 0.9465
[Test] 2017-05-11 07:17:22.031215: step 55000, acc = 0.9364, f1 = 0.9359
[Status] 2017-05-11 07:17:22.031324: step 55000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 07:18:29.059568: step 55020, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-11 07:19:35.195459: step 55040, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 07:20:42.783192: step 55060, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-11 07:21:49.854500: step 55080, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 07:22:56.750793: step 55100, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-11 07:24:03.185070: step 55120, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-11 07:25:10.158229: step 55140, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 07:26:17.314489: step 55160, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 07:27:24.153291: step 55180, loss = 0.0035, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 07:28:30.548403: step 55200, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-11 07:29:37.455299: step 55220, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-11 07:30:43.979665: step 55240, loss = 0.0028, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-11 07:31:50.671206: step 55260, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 07:32:56.854838: step 55280, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 07:34:05.179036: step 55300, loss = 0.0026, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-11 07:35:11.956925: step 55320, loss = 0.0025, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-11 07:36:18.350684: step 55340, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 07:37:25.751228: step 55360, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 07:38:32.410717: step 55380, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 07:39:39.025598: step 55400, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 07:40:47.652457: step 55420, loss = 0.0032, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-11 07:41:57.034279: step 55440, loss = 0.0025, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-11 07:43:03.692009: step 55460, loss = 0.0034, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-11 07:44:09.797908: step 55480, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 07:45:16.766803: step 55500, loss = 0.0040, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-11 07:46:22.640063: step 55520, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 07:47:29.700916: step 55540, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 07:48:35.552453: step 55560, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 07:49:43.240096: step 55580, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 07:50:49.844610: step 55600, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-11 07:51:57.377128: step 55620, loss = 0.0029, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-11 07:53:04.095073: step 55640, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 07:54:11.372071: step 55660, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 07:55:17.901887: step 55680, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 07:56:24.252740: step 55700, loss = 0.0038, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 07:57:30.669043: step 55720, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 07:58:38.353762: step 55740, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 07:59:45.572244: step 55760, loss = 0.0029, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-11 08:00:52.475635: step 55780, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 08:01:59.971937: step 55800, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 08:03:06.560929: step 55820, loss = 0.0033, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-11 08:04:12.936284: step 55840, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 08:05:19.733595: step 55860, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 08:06:26.262195: step 55880, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 08:07:33.317584: step 55900, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-11 08:08:39.454480: step 55920, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 08:09:47.179986: step 55940, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-11 08:10:54.162132: step 55960, loss = 0.0026, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-11 08:12:01.286425: step 55980, loss = 0.0030, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-11 08:13:08.002950: step 56000, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
[Eval] 2017-05-11 08:13:22.944362: step 56000, acc = 0.9482, f1 = 0.9466
[Test] 2017-05-11 08:13:33.571370: step 56000, acc = 0.9364, f1 = 0.9359
[Status] 2017-05-11 08:13:33.571464: step 56000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 08:14:40.235935: step 56020, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 08:15:46.204366: step 56040, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-11 08:16:53.131441: step 56060, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 08:17:59.939530: step 56080, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 08:19:06.663875: step 56100, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 08:20:13.704057: step 56120, loss = 0.0029, acc = 1.0000 (18.0 examples/sec; 3.564 sec/batch)
2017-05-11 08:21:20.479555: step 56140, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 08:22:27.003315: step 56160, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 08:23:33.963688: step 56180, loss = 0.0028, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-11 08:24:40.274263: step 56200, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 08:25:47.069162: step 56220, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-11 08:26:54.885266: step 56240, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-11 08:28:01.653814: step 56260, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 08:29:08.349527: step 56280, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 08:30:15.098848: step 56300, loss = 0.0028, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-11 08:31:22.268204: step 56320, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 08:32:28.848566: step 56340, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 08:33:36.565000: step 56360, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 08:34:44.162686: step 56380, loss = 0.0029, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-11 08:35:51.820090: step 56400, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 08:36:57.816523: step 56420, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 08:38:04.415601: step 56440, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 08:39:12.450256: step 56460, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-11 08:40:18.719947: step 56480, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 08:41:25.315488: step 56500, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-11 08:42:31.102114: step 56520, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 08:43:38.774217: step 56540, loss = 0.0028, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-11 08:44:45.029243: step 56560, loss = 0.0038, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-11 08:45:52.016151: step 56580, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 08:46:58.820289: step 56600, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 08:48:05.488943: step 56620, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 08:49:12.225389: step 56640, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 08:50:20.225518: step 56660, loss = 0.0030, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 08:51:28.103761: step 56680, loss = 0.0026, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-11 08:52:34.798301: step 56700, loss = 0.0044, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 08:53:41.189588: step 56720, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-11 08:54:48.602067: step 56740, loss = 0.0026, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 08:55:55.614382: step 56760, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 08:57:02.154208: step 56780, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 08:58:08.295190: step 56800, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 08:59:16.060209: step 56820, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 09:00:22.627910: step 56840, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 09:01:29.887566: step 56860, loss = 0.0024, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-11 09:02:36.379794: step 56880, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-11 09:03:42.936353: step 56900, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 09:04:49.653023: step 56920, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 09:05:56.511720: step 56940, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-11 09:07:04.771893: step 56960, loss = 0.0030, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-11 09:08:12.819868: step 56980, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 09:09:19.525077: step 57000, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
[Eval] 2017-05-11 09:09:33.708993: step 57000, acc = 0.9482, f1 = 0.9465
[Test] 2017-05-11 09:09:44.176072: step 57000, acc = 0.9362, f1 = 0.9356
[Status] 2017-05-11 09:09:44.176196: step 57000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 09:10:50.793096: step 57020, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 09:11:57.397296: step 57040, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 09:13:04.588695: step 57060, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 09:14:10.539309: step 57080, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 09:15:16.353380: step 57100, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-11 09:16:23.127520: step 57120, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 09:17:29.744923: step 57140, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-11 09:18:35.771282: step 57160, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 09:19:43.043206: step 57180, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 09:20:49.669082: step 57200, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 09:21:55.802078: step 57220, loss = 0.0028, acc = 1.0000 (19.7 examples/sec; 3.251 sec/batch)
2017-05-11 09:23:03.084195: step 57240, loss = 0.0027, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-11 09:24:09.472357: step 57260, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 09:25:16.593061: step 57280, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 09:26:23.658691: step 57300, loss = 0.0030, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-11 09:27:30.394824: step 57320, loss = 0.0029, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-11 09:28:36.946759: step 57340, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 09:29:43.821856: step 57360, loss = 0.0024, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-11 09:30:49.762731: step 57380, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 09:31:56.719308: step 57400, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 09:33:03.140555: step 57420, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 09:34:11.092006: step 57440, loss = 0.0024, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-11 09:35:19.366257: step 57460, loss = 0.0027, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 09:36:25.395587: step 57480, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 09:37:32.275826: step 57500, loss = 0.0134, acc = 0.9980 (18.8 examples/sec; 3.398 sec/batch)
2017-05-11 09:38:38.564219: step 57520, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 09:39:44.326467: step 57540, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 09:40:51.779575: step 57560, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 09:41:59.159202: step 57580, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 09:43:07.133718: step 57600, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-11 09:44:14.137320: step 57620, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-11 09:45:21.054974: step 57640, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 09:46:27.990151: step 57660, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-11 09:47:34.658984: step 57680, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 09:48:40.711481: step 57700, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 09:49:46.505257: step 57720, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 09:50:53.767201: step 57740, loss = 0.0026, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-11 09:52:00.214752: step 57760, loss = 0.0025, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-11 09:53:06.405641: step 57780, loss = 0.0040, acc = 0.9980 (19.5 examples/sec; 3.274 sec/batch)
2017-05-11 09:54:14.521045: step 57800, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 09:55:20.409231: step 57820, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 09:56:27.533611: step 57840, loss = 0.0024, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-11 09:57:33.911634: step 57860, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 09:58:40.981605: step 57880, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 09:59:48.072383: step 57900, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 10:00:54.410324: step 57920, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 10:02:01.637293: step 57940, loss = 0.0027, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 10:03:08.417377: step 57960, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-11 10:04:14.587973: step 57980, loss = 0.0044, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 10:05:21.374445: step 58000, loss = 0.0045, acc = 0.9980 (19.5 examples/sec; 3.275 sec/batch)
[Eval] 2017-05-11 10:05:36.828573: step 58000, acc = 0.9486, f1 = 0.9470
[Test] 2017-05-11 10:05:47.578817: step 58000, acc = 0.9366, f1 = 0.9361
[Status] 2017-05-11 10:05:47.578889: step 58000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 10:06:54.630142: step 58020, loss = 0.0038, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-11 10:08:02.126341: step 58040, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-11 10:09:09.774955: step 58060, loss = 0.0029, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-11 10:10:18.648551: step 58080, loss = 0.0027, acc = 1.0000 (17.3 examples/sec; 3.703 sec/batch)
2017-05-11 10:11:25.312256: step 58100, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-11 10:12:31.753886: step 58120, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 10:13:39.156836: step 58140, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 10:14:45.559740: step 58160, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 10:15:53.650665: step 58180, loss = 0.0025, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-11 10:17:00.225985: step 58200, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 10:18:07.434245: step 58220, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 10:19:14.646022: step 58240, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 10:20:22.038227: step 58260, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 10:21:29.299583: step 58280, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 10:22:36.363969: step 58300, loss = 0.0023, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 10:23:42.949230: step 58320, loss = 0.0075, acc = 0.9980 (19.1 examples/sec; 3.344 sec/batch)
2017-05-11 10:24:48.883777: step 58340, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 10:25:55.539561: step 58360, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 10:27:01.490242: step 58380, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 10:28:08.747031: step 58400, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 10:29:16.348855: step 58420, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-11 10:30:23.168485: step 58440, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 10:31:32.103765: step 58460, loss = 0.0028, acc = 1.0000 (18.1 examples/sec; 3.530 sec/batch)
2017-05-11 10:32:39.834929: step 58480, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 10:33:46.778062: step 58500, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 10:34:53.506390: step 58520, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 10:35:59.919071: step 58540, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 10:37:07.491319: step 58560, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-11 10:38:13.897140: step 58580, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 10:39:19.857218: step 58600, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 10:40:26.925896: step 58620, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 10:41:33.400372: step 58640, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 10:42:40.015660: step 58660, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 10:43:47.542998: step 58680, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 10:44:53.750950: step 58700, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 10:46:00.498159: step 58720, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 10:47:07.108300: step 58740, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 10:48:14.697596: step 58760, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-11 10:49:21.211271: step 58780, loss = 0.0071, acc = 0.9980 (18.8 examples/sec; 3.411 sec/batch)
2017-05-11 10:50:27.794482: step 58800, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 10:51:34.908437: step 58820, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-11 10:52:41.852006: step 58840, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 10:53:48.576073: step 58860, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-11 10:54:55.303765: step 58880, loss = 0.0025, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-11 10:56:01.450418: step 58900, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 10:57:07.918516: step 58920, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-11 10:58:13.868142: step 58940, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 10:59:21.492892: step 58960, loss = 0.0026, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-11 11:00:27.848919: step 58980, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 11:01:34.572987: step 59000, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
[Eval] 2017-05-11 11:01:50.187353: step 59000, acc = 0.9486, f1 = 0.9470
[Test] 2017-05-11 11:02:00.850853: step 59000, acc = 0.9365, f1 = 0.9360
[Status] 2017-05-11 11:02:00.850966: step 59000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 11:03:07.751350: step 59020, loss = 0.0024, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-11 11:04:14.472161: step 59040, loss = 0.0025, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-11 11:05:21.956619: step 59060, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 11:06:28.970100: step 59080, loss = 0.0028, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-11 11:07:35.251580: step 59100, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 11:08:41.842710: step 59120, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 11:09:48.249328: step 59140, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 11:10:55.795844: step 59160, loss = 0.0038, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 11:12:03.887364: step 59180, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 11:13:09.932518: step 59200, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 11:14:16.819764: step 59220, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 11:15:25.125654: step 59240, loss = 0.0031, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-11 11:16:33.090582: step 59260, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 11:17:40.147384: step 59280, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-11 11:18:48.189756: step 59300, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-11 11:19:55.317972: step 59320, loss = 0.0027, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-11 11:21:02.404718: step 59340, loss = 0.0042, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 11:22:09.785510: step 59360, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 11:23:15.571729: step 59380, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 11:24:22.826509: step 59400, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 11:25:30.169553: step 59420, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 11:26:37.386970: step 59440, loss = 0.0039, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-11 11:27:43.611292: step 59460, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 11:28:52.144299: step 59480, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 11:30:00.564501: step 59500, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-11 11:31:07.048488: step 59520, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 11:32:13.537658: step 59540, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 11:33:20.100344: step 59560, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 11:34:26.579007: step 59580, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 11:35:32.799781: step 59600, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 11:36:39.232382: step 59620, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 11:37:45.543530: step 59640, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 11:38:51.689976: step 59660, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 11:39:58.479745: step 59680, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 11:41:06.856565: step 59700, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 11:42:13.364424: step 59720, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 11:43:20.363686: step 59740, loss = 0.0031, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-11 11:44:26.954128: step 59760, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 11:45:34.616099: step 59780, loss = 0.0024, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-11 11:46:42.185019: step 59800, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 11:47:48.216073: step 59820, loss = 0.0125, acc = 0.9980 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 11:48:54.883751: step 59840, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 11:50:01.556251: step 59860, loss = 0.0025, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-11 11:51:08.162282: step 59880, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 11:52:16.006817: step 59900, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 11:53:22.810599: step 59920, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 11:54:29.951662: step 59940, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 11:55:35.942454: step 59960, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-11 11:56:43.855459: step 59980, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 11:57:51.223599: step 60000, loss = 0.0025, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
[Eval] 2017-05-11 11:58:06.743785: step 60000, acc = 0.9484, f1 = 0.9468
[Test] 2017-05-11 11:58:17.446769: step 60000, acc = 0.9367, f1 = 0.9362
[Status] 2017-05-11 11:58:17.446877: step 60000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 11:59:24.671114: step 60020, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 12:00:31.520891: step 60040, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 12:01:38.355370: step 60060, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-11 12:02:44.282006: step 60080, loss = 0.0025, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 12:03:51.557435: step 60100, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 12:04:58.930626: step 60120, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 12:06:05.016194: step 60140, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 12:07:11.757261: step 60160, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 12:08:17.922747: step 60180, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 12:09:24.083112: step 60200, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-11 12:10:30.024995: step 60220, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-11 12:11:37.982735: step 60240, loss = 0.0026, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-11 12:12:44.817895: step 60260, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 12:13:51.774040: step 60280, loss = 0.0033, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-11 12:14:58.736351: step 60300, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-11 12:16:05.706866: step 60320, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 12:17:12.561213: step 60340, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-11 12:18:19.216224: step 60360, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 12:19:25.361618: step 60380, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 12:20:33.097320: step 60400, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 12:21:40.403461: step 60420, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 12:22:47.264046: step 60440, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 12:23:54.500544: step 60460, loss = 0.0025, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-11 12:25:02.435313: step 60480, loss = 0.0038, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-11 12:26:08.855227: step 60500, loss = 0.0025, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 12:27:15.474569: step 60520, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 12:28:22.718629: step 60540, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 12:29:29.187853: step 60560, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 12:30:36.260697: step 60580, loss = 0.0030, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-11 12:31:43.686767: step 60600, loss = 0.0025, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-11 12:32:50.329681: step 60620, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 12:33:58.551508: step 60640, loss = 0.0024, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-11 12:35:05.080423: step 60660, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 12:36:12.703669: step 60680, loss = 0.0024, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-11 12:37:19.277441: step 60700, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 12:38:26.183996: step 60720, loss = 0.0024, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-11 12:39:32.316487: step 60740, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 12:40:38.216166: step 60760, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-11 12:41:46.447671: step 60780, loss = 0.0052, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 12:42:53.800175: step 60800, loss = 0.0042, acc = 0.9980 (18.9 examples/sec; 3.382 sec/batch)
2017-05-11 12:44:00.202084: step 60820, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 12:45:07.339579: step 60840, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 12:46:14.046886: step 60860, loss = 0.0037, acc = 0.9980 (19.1 examples/sec; 3.353 sec/batch)
2017-05-11 12:47:21.421328: step 60880, loss = 0.0025, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-11 12:48:29.622449: step 60900, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 12:49:36.819534: step 60920, loss = 0.0036, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-11 12:50:43.323476: step 60940, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-11 12:51:49.531403: step 60960, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 12:52:55.985934: step 60980, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 12:54:03.164732: step 61000, loss = 0.0023, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
[Eval] 2017-05-11 12:54:18.506584: step 61000, acc = 0.9482, f1 = 0.9466
[Test] 2017-05-11 12:54:29.071867: step 61000, acc = 0.9362, f1 = 0.9357
[Status] 2017-05-11 12:54:29.071964: step 61000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 12:55:36.197761: step 61020, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 12:56:42.881422: step 61040, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-11 12:57:50.779217: step 61060, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 12:58:57.468421: step 61080, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-11 13:00:04.359013: step 61100, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 13:01:11.041634: step 61120, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 13:02:18.188876: step 61140, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 13:03:25.746016: step 61160, loss = 0.0025, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-11 13:04:32.579733: step 61180, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 13:05:38.956205: step 61200, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 13:06:45.733922: step 61220, loss = 0.0024, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-11 13:07:53.138262: step 61240, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 13:09:00.058680: step 61260, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 13:10:08.699763: step 61280, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 13:11:14.724827: step 61300, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 13:12:21.891205: step 61320, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 13:13:29.004938: step 61340, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 13:14:35.185312: step 61360, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 13:15:42.063713: step 61380, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 13:16:48.969400: step 61400, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 13:17:55.970266: step 61420, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 13:19:02.429183: step 61440, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 13:20:10.688532: step 61460, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 13:21:17.138390: step 61480, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 13:22:25.551347: step 61500, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 13:23:34.387288: step 61520, loss = 0.0022, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-11 13:24:41.012863: step 61540, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-11 13:25:47.983917: step 61560, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-11 13:26:54.907545: step 61580, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-11 13:28:02.148943: step 61600, loss = 0.0036, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-11 13:29:09.427473: step 61620, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 13:30:16.725423: step 61640, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 13:31:23.904397: step 61660, loss = 0.0023, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-11 13:32:30.790266: step 61680, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 13:33:39.643288: step 61700, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 13:34:47.309617: step 61720, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 13:35:55.189687: step 61740, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-11 13:37:03.453772: step 61760, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 13:38:11.197026: step 61780, loss = 0.0022, acc = 1.0000 (18.0 examples/sec; 3.556 sec/batch)
2017-05-11 13:39:17.727734: step 61800, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 13:40:24.790046: step 61820, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-11 13:41:31.972272: step 61840, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 13:42:40.064108: step 61860, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 13:43:46.622259: step 61880, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 13:44:53.216823: step 61900, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 13:46:00.167617: step 61920, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-11 13:47:06.161430: step 61940, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-11 13:48:12.581503: step 61960, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 13:49:20.672593: step 61980, loss = 0.0022, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-11 13:50:29.104621: step 62000, loss = 0.0023, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
[Eval] 2017-05-11 13:50:44.559198: step 62000, acc = 0.9483, f1 = 0.9466
[Test] 2017-05-11 13:50:55.289174: step 62000, acc = 0.9366, f1 = 0.9361
[Status] 2017-05-11 13:50:55.289278: step 62000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 13:52:02.388171: step 62020, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-11 13:53:10.729112: step 62040, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 13:54:17.808918: step 62060, loss = 0.0036, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-11 13:55:24.420907: step 62080, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 13:56:31.269251: step 62100, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 13:57:38.921967: step 62120, loss = 0.0028, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-11 13:58:46.608850: step 62140, loss = 0.0024, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-11 13:59:53.368112: step 62160, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-11 14:00:59.832818: step 62180, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 14:02:06.442020: step 62200, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 14:03:13.262277: step 62220, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 14:04:19.944744: step 62240, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-11 14:05:26.542313: step 62260, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 14:06:33.836155: step 62280, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 14:07:40.160804: step 62300, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 14:08:47.315697: step 62320, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-11 14:09:53.941678: step 62340, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 14:11:01.019959: step 62360, loss = 0.0023, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-11 14:12:07.181190: step 62380, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-11 14:13:14.509369: step 62400, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 14:14:22.442032: step 62420, loss = 0.0024, acc = 1.0000 (18.1 examples/sec; 3.535 sec/batch)
2017-05-11 14:15:29.459970: step 62440, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 14:16:35.945421: step 62460, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 14:17:42.336778: step 62480, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-11 14:18:49.758640: step 62500, loss = 0.0027, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-11 14:19:57.223651: step 62520, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-11 14:21:04.135997: step 62540, loss = 0.0026, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-11 14:22:10.934225: step 62560, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 14:23:19.313406: step 62580, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 14:24:27.685109: step 62600, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 14:25:34.803605: step 62620, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-11 14:26:41.400154: step 62640, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 14:27:48.817373: step 62660, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 14:28:55.826544: step 62680, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 14:30:02.328246: step 62700, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 14:31:09.211721: step 62720, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-11 14:32:15.636253: step 62740, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 14:33:21.914700: step 62760, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 14:34:28.169713: step 62780, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 14:35:35.363289: step 62800, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 14:36:42.682080: step 62820, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-11 14:37:50.460609: step 62840, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 14:38:57.027832: step 62860, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-11 14:40:03.359006: step 62880, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 14:41:10.186415: step 62900, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 14:42:16.630253: step 62920, loss = 0.0056, acc = 0.9980 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 14:43:23.502854: step 62940, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 14:44:30.938096: step 62960, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-11 14:45:37.494295: step 62980, loss = 0.0024, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-11 14:46:44.258456: step 63000, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
[Eval] 2017-05-11 14:46:59.611493: step 63000, acc = 0.9479, f1 = 0.9464
[Test] 2017-05-11 14:47:10.097917: step 63000, acc = 0.9361, f1 = 0.9356
[Status] 2017-05-11 14:47:10.098098: step 63000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 14:48:16.347927: step 63020, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-11 14:49:24.161652: step 63040, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 14:50:30.778141: step 63060, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 14:51:36.897726: step 63080, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 14:52:42.976017: step 63100, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 14:53:49.005596: step 63120, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-11 14:54:54.944367: step 63140, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 14:56:01.600865: step 63160, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-11 14:57:07.765064: step 63180, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 14:58:13.653091: step 63200, loss = 0.0098, acc = 0.9980 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 14:59:20.601600: step 63220, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-11 15:00:27.323276: step 63240, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 15:01:34.607206: step 63260, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-11 15:02:41.576728: step 63280, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-11 15:03:47.561132: step 63300, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 15:04:55.929213: step 63320, loss = 0.0024, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 15:06:03.088212: step 63340, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 15:07:09.860160: step 63360, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 15:08:17.754736: step 63380, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 15:09:24.468307: step 63400, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-11 15:10:31.636096: step 63420, loss = 0.0021, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-11 15:11:39.102006: step 63440, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 15:12:46.093776: step 63460, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-11 15:13:53.329579: step 63480, loss = 0.0023, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-11 15:15:01.994814: step 63500, loss = 0.0026, acc = 1.0000 (17.0 examples/sec; 3.761 sec/batch)
2017-05-11 15:16:08.353080: step 63520, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 15:17:15.814547: step 63540, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 15:18:21.970426: step 63560, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-11 15:19:28.847012: step 63580, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 15:20:35.742321: step 63600, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-11 15:21:42.157582: step 63620, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 15:22:49.125101: step 63640, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-11 15:23:56.239099: step 63660, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-11 15:25:03.197110: step 63680, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-11 15:26:09.773378: step 63700, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 15:27:16.807701: step 63720, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-11 15:28:23.070489: step 63740, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 15:29:29.147133: step 63760, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 15:30:35.501304: step 63780, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 15:31:42.969223: step 63800, loss = 0.0045, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-11 15:32:50.323040: step 63820, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-11 15:33:56.318781: step 63840, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 15:35:03.096560: step 63860, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-11 15:36:10.108938: step 63880, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 15:37:16.359778: step 63900, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 15:38:23.122707: step 63920, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 15:39:30.785253: step 63940, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-11 15:40:38.928081: step 63960, loss = 0.0021, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-11 15:41:46.802508: step 63980, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-11 15:42:53.217288: step 64000, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
[Eval] 2017-05-11 15:43:08.389799: step 64000, acc = 0.9485, f1 = 0.9469
[Test] 2017-05-11 15:43:18.312214: step 64000, acc = 0.9357, f1 = 0.9351
[Status] 2017-05-11 15:43:18.312320: step 64000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 15:44:26.314641: step 64020, loss = 0.0022, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-11 15:45:32.500430: step 64040, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 15:46:38.775933: step 64060, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 15:47:47.373268: step 64080, loss = 0.0030, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 15:48:54.064973: step 64100, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-11 15:50:01.082762: step 64120, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 15:51:08.639797: step 64140, loss = 0.0021, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-11 15:52:15.392198: step 64160, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 15:53:21.905304: step 64180, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 15:54:29.408597: step 64200, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-11 15:55:35.234839: step 64220, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 15:56:42.355711: step 64240, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 15:57:50.371001: step 64260, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 15:58:56.855683: step 64280, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 16:00:02.713906: step 64300, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 16:01:10.693207: step 64320, loss = 0.0025, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-11 16:02:18.200351: step 64340, loss = 0.0022, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-11 16:03:25.001748: step 64360, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 16:04:32.264386: step 64380, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 16:05:39.238049: step 64400, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-11 16:06:46.032342: step 64420, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 16:07:52.603043: step 64440, loss = 0.0030, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-11 16:08:59.179595: step 64460, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 16:10:05.878709: step 64480, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-11 16:11:12.925025: step 64500, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 16:12:22.204317: step 64520, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-11 16:13:29.704992: step 64540, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-11 16:14:37.088500: step 64560, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 16:15:44.232009: step 64580, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-11 16:16:51.219018: step 64600, loss = 0.0021, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-11 16:17:57.554775: step 64620, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 16:19:04.854582: step 64640, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-11 16:20:10.654529: step 64660, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 16:21:16.408869: step 64680, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-11 16:22:22.969864: step 64700, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 16:23:29.849485: step 64720, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 16:24:36.980060: step 64740, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-11 16:25:44.265336: step 64760, loss = 0.0023, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-11 16:26:50.722596: step 64780, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 16:27:57.563335: step 64800, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 16:29:03.801897: step 64820, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 16:30:10.213143: step 64840, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 16:31:16.301636: step 64860, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-11 16:32:23.852539: step 64880, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 16:33:31.127192: step 64900, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 16:34:38.140645: step 64920, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 16:35:44.010509: step 64940, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 16:36:50.341543: step 64960, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-11 16:37:57.853989: step 64980, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-11 16:39:04.595303: step 65000, loss = 0.0025, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
[Eval] 2017-05-11 16:39:19.880123: step 65000, acc = 0.9480, f1 = 0.9464
[Test] 2017-05-11 16:39:29.919446: step 65000, acc = 0.9360, f1 = 0.9355
[Status] 2017-05-11 16:39:29.919505: step 65000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 16:40:36.298651: step 65020, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 16:41:43.396363: step 65040, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-11 16:42:49.924367: step 65060, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 16:43:56.464596: step 65080, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-11 16:45:03.413468: step 65100, loss = 0.0021, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-11 16:46:11.990290: step 65120, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 16:47:19.584153: step 65140, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-11 16:48:25.842036: step 65160, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 16:49:32.073414: step 65180, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 16:50:40.308734: step 65200, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 16:51:46.685966: step 65220, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 16:52:53.570038: step 65240, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 16:53:59.863378: step 65260, loss = 0.0036, acc = 0.9980 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 16:55:06.964786: step 65280, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 16:56:13.823626: step 65300, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 16:57:21.766512: step 65320, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 16:58:28.537103: step 65340, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 16:59:35.629494: step 65360, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 17:00:43.279739: step 65380, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 17:01:50.610698: step 65400, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 17:02:58.053300: step 65420, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 17:04:06.454640: step 65440, loss = 0.0024, acc = 1.0000 (17.8 examples/sec; 3.591 sec/batch)
2017-05-11 17:05:14.618359: step 65460, loss = 0.0025, acc = 1.0000 (18.2 examples/sec; 3.526 sec/batch)
2017-05-11 17:06:21.079425: step 65480, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-11 17:07:29.046004: step 65500, loss = 0.0022, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-11 17:08:37.473711: step 65520, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 17:09:44.601825: step 65540, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 17:10:51.933378: step 65560, loss = 0.0023, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-11 17:11:58.786663: step 65580, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 17:13:07.234673: step 65600, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-11 17:14:14.977291: step 65620, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-11 17:15:21.922708: step 65640, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 17:16:28.399210: step 65660, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 17:17:35.933974: step 65680, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 17:18:42.361787: step 65700, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-11 17:19:49.333482: step 65720, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 17:20:55.226151: step 65740, loss = 0.0035, acc = 0.9980 (19.6 examples/sec; 3.272 sec/batch)
2017-05-11 17:22:02.290063: step 65760, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-11 17:23:08.859297: step 65780, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 17:24:15.796306: step 65800, loss = 0.0031, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 17:25:21.781636: step 65820, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-11 17:26:28.588767: step 65840, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 17:27:35.539837: step 65860, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 17:28:42.220274: step 65880, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 17:29:48.256897: step 65900, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 17:30:54.324258: step 65920, loss = 0.0059, acc = 0.9980 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 17:32:01.339487: step 65940, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-11 17:33:08.164412: step 65960, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-11 17:34:15.507199: step 65980, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 17:35:22.310043: step 66000, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
[Eval] 2017-05-11 17:35:37.605259: step 66000, acc = 0.9482, f1 = 0.9465
[Test] 2017-05-11 17:35:47.561706: step 66000, acc = 0.9360, f1 = 0.9354
[Status] 2017-05-11 17:35:47.561795: step 66000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 17:36:53.566227: step 66020, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-11 17:37:59.585178: step 66040, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 17:39:05.926636: step 66060, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 17:40:12.363287: step 66080, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 17:41:18.854461: step 66100, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-11 17:42:25.175815: step 66120, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 17:43:31.111528: step 66140, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 17:44:38.090359: step 66160, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 17:45:44.731676: step 66180, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 17:46:52.124039: step 66200, loss = 0.0022, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 17:47:59.055744: step 66220, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-11 17:49:05.093544: step 66240, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 17:50:13.514695: step 66260, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 17:51:21.003066: step 66280, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 17:52:27.691863: step 66300, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 17:53:35.402374: step 66320, loss = 0.0025, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-11 17:54:41.667183: step 66340, loss = 0.0041, acc = 0.9980 (18.6 examples/sec; 3.438 sec/batch)
2017-05-11 17:55:47.862139: step 66360, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 17:56:55.193527: step 66380, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 17:58:01.506946: step 66400, loss = 0.0045, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 17:59:08.229305: step 66420, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 18:00:14.951187: step 66440, loss = 0.0020, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-11 18:01:21.241111: step 66460, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 18:02:27.470568: step 66480, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 18:03:33.655399: step 66500, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-11 18:04:39.910042: step 66520, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 18:05:46.773968: step 66540, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 18:06:54.156805: step 66560, loss = 0.0022, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-11 18:08:00.522940: step 66580, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-11 18:09:07.503067: step 66600, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-11 18:10:15.181206: step 66620, loss = 0.0019, acc = 1.0000 (17.7 examples/sec; 3.620 sec/batch)
2017-05-11 18:11:22.764209: step 66640, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-11 18:12:29.186695: step 66660, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-11 18:13:34.893728: step 66680, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 18:14:42.116961: step 66700, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-11 18:15:48.324939: step 66720, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 18:16:55.079994: step 66740, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-11 18:18:03.399252: step 66760, loss = 0.0037, acc = 0.9980 (18.2 examples/sec; 3.511 sec/batch)
2017-05-11 18:19:11.054063: step 66780, loss = 0.0020, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-11 18:20:17.426833: step 66800, loss = 0.0029, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-11 18:21:24.480700: step 66820, loss = 0.0043, acc = 0.9980 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 18:22:31.605678: step 66840, loss = 0.0022, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-11 18:23:38.782738: step 66860, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 18:24:47.374258: step 66880, loss = 0.0021, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-11 18:25:54.014813: step 66900, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-11 18:27:00.491232: step 66920, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 18:28:07.144239: step 66940, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-11 18:29:13.413366: step 66960, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-11 18:30:19.633580: step 66980, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 18:31:27.682020: step 67000, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
[Eval] 2017-05-11 18:31:43.103163: step 67000, acc = 0.9476, f1 = 0.9460
[Test] 2017-05-11 18:31:53.304703: step 67000, acc = 0.9357, f1 = 0.9352
[Status] 2017-05-11 18:31:53.304797: step 67000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 18:32:59.563237: step 67020, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 18:34:06.583748: step 67040, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 18:35:12.821409: step 67060, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 18:36:20.743626: step 67080, loss = 0.0021, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-11 18:37:28.404242: step 67100, loss = 0.0022, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-11 18:38:35.356528: step 67120, loss = 0.0022, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-11 18:39:42.587889: step 67140, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 18:40:48.992460: step 67160, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 18:41:55.719780: step 67180, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 18:43:04.035090: step 67200, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 18:44:11.234869: step 67220, loss = 0.0027, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-11 18:45:17.231230: step 67240, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-11 18:46:23.796564: step 67260, loss = 0.0024, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-11 18:47:30.607252: step 67280, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 18:48:38.998955: step 67300, loss = 0.0023, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-11 18:49:44.896623: step 67320, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-11 18:50:51.360930: step 67340, loss = 0.0021, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-11 18:51:58.851975: step 67360, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 18:53:06.653980: step 67380, loss = 0.0028, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-11 18:54:12.858985: step 67400, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-11 18:55:18.825978: step 67420, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 18:56:25.094297: step 67440, loss = 0.0036, acc = 0.9980 (18.9 examples/sec; 3.380 sec/batch)
2017-05-11 18:57:32.295474: step 67460, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 18:58:39.017640: step 67480, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-11 18:59:46.975108: step 67500, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-11 19:00:53.973730: step 67520, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-11 19:02:01.712857: step 67540, loss = 0.0020, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-11 19:03:09.403303: step 67560, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 19:04:16.444046: step 67580, loss = 0.0043, acc = 0.9980 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 19:05:24.347372: step 67600, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 19:06:30.786435: step 67620, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 19:07:38.657485: step 67640, loss = 0.0031, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-11 19:08:45.466181: step 67660, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-11 19:09:52.412392: step 67680, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 19:10:58.681618: step 67700, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-11 19:12:05.090342: step 67720, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 19:13:11.827695: step 67740, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 19:14:18.003953: step 67760, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-11 19:15:24.750808: step 67780, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-11 19:16:32.278345: step 67800, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-11 19:17:39.758797: step 67820, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-11 19:18:46.458196: step 67840, loss = 0.0045, acc = 0.9980 (18.6 examples/sec; 3.438 sec/batch)
2017-05-11 19:19:53.232283: step 67860, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-11 19:20:59.165189: step 67880, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 19:22:05.402478: step 67900, loss = 0.0038, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 19:23:11.941502: step 67920, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-11 19:24:18.327043: step 67940, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 19:25:25.240129: step 67960, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 19:26:32.112337: step 67980, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-11 19:27:38.785785: step 68000, loss = 0.0028, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
[Eval] 2017-05-11 19:27:54.130293: step 68000, acc = 0.9478, f1 = 0.9461
[Test] 2017-05-11 19:28:04.632850: step 68000, acc = 0.9359, f1 = 0.9354
[Status] 2017-05-11 19:28:04.632954: step 68000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 19:29:11.596644: step 68020, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 19:30:18.117202: step 68040, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 19:31:26.036068: step 68060, loss = 0.0019, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-11 19:32:33.581951: step 68080, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-11 19:33:39.901603: step 68100, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-11 19:34:46.708369: step 68120, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 19:35:53.532305: step 68140, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-11 19:37:00.902613: step 68160, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 19:38:08.393676: step 68180, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-11 19:39:15.204229: step 68200, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 19:40:22.853595: step 68220, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-11 19:41:29.573323: step 68240, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 19:42:35.775597: step 68260, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 19:43:41.894571: step 68280, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 19:44:48.742186: step 68300, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 19:45:55.991275: step 68320, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 19:47:03.245028: step 68340, loss = 0.0025, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-11 19:48:09.373365: step 68360, loss = 0.0075, acc = 0.9960 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 19:49:16.365649: step 68380, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 19:50:22.423196: step 68400, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 19:51:29.187128: step 68420, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-11 19:52:36.721389: step 68440, loss = 0.0019, acc = 1.0000 (17.8 examples/sec; 3.595 sec/batch)
2017-05-11 19:53:44.760452: step 68460, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-11 19:54:51.010395: step 68480, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 19:55:59.010745: step 68500, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-11 19:57:05.559828: step 68520, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 19:58:14.401603: step 68540, loss = 0.0021, acc = 1.0000 (17.8 examples/sec; 3.594 sec/batch)
2017-05-11 19:59:21.154925: step 68560, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-11 20:00:28.849752: step 68580, loss = 0.0020, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-11 20:01:36.662457: step 68600, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-11 20:02:44.271442: step 68620, loss = 0.0022, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-11 20:03:51.615047: step 68640, loss = 0.0023, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-11 20:04:59.032645: step 68660, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 20:06:05.561203: step 68680, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 20:07:12.648618: step 68700, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-11 20:08:18.528000: step 68720, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-11 20:09:25.935095: step 68740, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-11 20:10:32.282824: step 68760, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-11 20:11:38.379300: step 68780, loss = 0.0020, acc = 1.0000 (19.7 examples/sec; 3.254 sec/batch)
2017-05-11 20:12:44.777800: step 68800, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 20:13:51.946125: step 68820, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 20:14:59.745024: step 68840, loss = 0.0022, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-11 20:16:06.265938: step 68860, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 20:17:12.647889: step 68880, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-11 20:18:19.393128: step 68900, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-11 20:19:26.023352: step 68920, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 20:20:33.383567: step 68940, loss = 0.0021, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-11 20:21:39.616152: step 68960, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 20:22:47.081288: step 68980, loss = 0.0046, acc = 0.9980 (19.0 examples/sec; 3.373 sec/batch)
2017-05-11 20:23:54.157086: step 69000, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
[Eval] 2017-05-11 20:24:09.487690: step 69000, acc = 0.9477, f1 = 0.9460
[Test] 2017-05-11 20:24:20.116757: step 69000, acc = 0.9362, f1 = 0.9356
[Status] 2017-05-11 20:24:20.116838: step 69000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 20:25:27.276061: step 69020, loss = 0.0020, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-11 20:26:33.618028: step 69040, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-11 20:27:40.393784: step 69060, loss = 0.0020, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-11 20:28:47.095775: step 69080, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 20:29:53.415005: step 69100, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 20:31:01.353554: step 69120, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-11 20:32:08.816855: step 69140, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-11 20:33:16.962098: step 69160, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 20:34:23.207905: step 69180, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 20:35:29.880136: step 69200, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 20:36:36.457412: step 69220, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 20:37:42.707839: step 69240, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-11 20:38:49.482341: step 69260, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-11 20:39:56.741801: step 69280, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-11 20:41:03.288772: step 69300, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-11 20:42:11.774152: step 69320, loss = 0.0027, acc = 1.0000 (18.2 examples/sec; 3.522 sec/batch)
2017-05-11 20:43:19.084501: step 69340, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-11 20:44:26.446730: step 69360, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-11 20:45:32.674187: step 69380, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 20:46:39.905988: step 69400, loss = 0.0020, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-11 20:47:47.438520: step 69420, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 20:48:53.573747: step 69440, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 20:49:59.897366: step 69460, loss = 0.0076, acc = 0.9980 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 20:51:06.494636: step 69480, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 20:52:15.100651: step 69500, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-11 20:53:22.096094: step 69520, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 20:54:29.189956: step 69540, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-11 20:55:36.483008: step 69560, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-11 20:56:43.315728: step 69580, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-11 20:57:50.260083: step 69600, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 20:58:56.137039: step 69620, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 21:00:01.988953: step 69640, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-11 21:01:09.242404: step 69660, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 21:02:15.345164: step 69680, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-11 21:03:22.659658: step 69700, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-11 21:04:29.226164: step 69720, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 21:05:35.728858: step 69740, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-11 21:06:42.084513: step 69760, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 21:07:48.484818: step 69780, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-11 21:08:55.105777: step 69800, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 21:10:02.216254: step 69820, loss = 0.0023, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-11 21:11:08.564839: step 69840, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 21:12:15.797235: step 69860, loss = 0.0018, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 21:13:23.675816: step 69880, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-11 21:14:30.020090: step 69900, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-11 21:15:37.300907: step 69920, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-11 21:16:44.239887: step 69940, loss = 0.0022, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-11 21:17:51.129589: step 69960, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-11 21:18:58.107291: step 69980, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-11 21:20:04.432641: step 70000, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
[Eval] 2017-05-11 21:20:19.003768: step 70000, acc = 0.9470, f1 = 0.9454
[Test] 2017-05-11 21:20:28.943807: step 70000, acc = 0.9348, f1 = 0.9342
[Status] 2017-05-11 21:20:28.943902: step 70000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 21:21:36.462541: step 70020, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-11 21:22:43.388286: step 70040, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 21:23:49.958680: step 70060, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-11 21:24:56.620782: step 70080, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-11 21:26:03.265777: step 70100, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 21:27:09.941101: step 70120, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-11 21:28:16.539105: step 70140, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-11 21:29:22.804815: step 70160, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 21:30:29.870607: step 70180, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-11 21:31:37.281804: step 70200, loss = 0.0021, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-11 21:32:43.703753: step 70220, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 21:33:52.532895: step 70240, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 21:34:58.718311: step 70260, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-11 21:36:05.715834: step 70280, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-11 21:37:12.686285: step 70300, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 21:38:19.772100: step 70320, loss = 0.0019, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-11 21:39:27.444572: step 70340, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-11 21:40:33.510404: step 70360, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 21:41:40.525509: step 70380, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-11 21:42:46.745341: step 70400, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 21:43:53.838998: step 70420, loss = 0.0041, acc = 0.9980 (19.5 examples/sec; 3.289 sec/batch)
2017-05-11 21:45:00.476202: step 70440, loss = 0.0033, acc = 0.9980 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 21:46:07.049878: step 70460, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-11 21:47:15.407006: step 70480, loss = 0.0024, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-11 21:48:22.149402: step 70500, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-11 21:49:28.627455: step 70520, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-11 21:50:34.955819: step 70540, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-11 21:51:42.589722: step 70560, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 21:52:49.705061: step 70580, loss = 0.0036, acc = 0.9980 (19.3 examples/sec; 3.318 sec/batch)
2017-05-11 21:53:56.440703: step 70600, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-11 21:55:04.086652: step 70620, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-11 21:56:12.260335: step 70640, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-11 21:57:19.952396: step 70660, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-11 21:58:26.937327: step 70680, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-11 21:59:34.828176: step 70700, loss = 0.0035, acc = 0.9980 (19.3 examples/sec; 3.323 sec/batch)
2017-05-11 22:00:42.522037: step 70720, loss = 0.0022, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-11 22:01:50.356626: step 70740, loss = 0.0026, acc = 1.0000 (17.8 examples/sec; 3.591 sec/batch)
2017-05-11 22:02:56.408492: step 70760, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 22:04:02.776296: step 70780, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-11 22:05:10.560369: step 70800, loss = 0.0021, acc = 1.0000 (17.9 examples/sec; 3.577 sec/batch)
2017-05-11 22:06:16.595992: step 70820, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 22:07:23.911251: step 70840, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-11 22:08:32.940551: step 70860, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-11 22:09:39.349147: step 70880, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 22:10:46.493167: step 70900, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-11 22:11:53.514422: step 70920, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 22:13:01.825715: step 70940, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-11 22:14:10.164601: step 70960, loss = 0.0021, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-11 22:15:16.121445: step 70980, loss = 0.0037, acc = 0.9980 (19.5 examples/sec; 3.274 sec/batch)
2017-05-11 22:16:23.690333: step 71000, loss = 0.0019, acc = 1.0000 (17.8 examples/sec; 3.600 sec/batch)
[Eval] 2017-05-11 22:16:38.996666: step 71000, acc = 0.9472, f1 = 0.9456
[Test] 2017-05-11 22:16:49.696654: step 71000, acc = 0.9350, f1 = 0.9345
[Status] 2017-05-11 22:16:49.696734: step 71000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 22:17:57.119204: step 71020, loss = 0.0035, acc = 0.9980 (18.8 examples/sec; 3.402 sec/batch)
2017-05-11 22:19:03.697127: step 71040, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 22:20:12.143692: step 71060, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 22:21:18.391049: step 71080, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-11 22:22:25.054364: step 71100, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-11 22:23:31.206369: step 71120, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-11 22:24:38.301818: step 71140, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-11 22:25:44.417734: step 71160, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 22:26:52.626332: step 71180, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-11 22:28:00.481790: step 71200, loss = 0.0020, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-11 22:29:06.467239: step 71220, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-11 22:30:14.034979: step 71240, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 22:31:20.679678: step 71260, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-11 22:32:28.166368: step 71280, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 22:33:36.246255: step 71300, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-11 22:34:42.667883: step 71320, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-11 22:35:50.274968: step 71340, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 22:36:57.350188: step 71360, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-11 22:38:05.017870: step 71380, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-11 22:39:12.143280: step 71400, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-11 22:40:19.805692: step 71420, loss = 0.0029, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-11 22:41:27.997603: step 71440, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-11 22:42:34.745872: step 71460, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-11 22:43:42.268353: step 71480, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-11 22:44:49.017655: step 71500, loss = 0.0082, acc = 0.9980 (19.1 examples/sec; 3.348 sec/batch)
2017-05-11 22:45:55.302066: step 71520, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-11 22:47:01.603455: step 71540, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-11 22:48:08.073854: step 71560, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-11 22:49:15.782471: step 71580, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-11 22:50:22.256945: step 71600, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-11 22:51:28.775250: step 71620, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 22:52:35.373357: step 71640, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 22:53:42.833945: step 71660, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-11 22:54:49.976412: step 71680, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-11 22:55:56.714051: step 71700, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-11 22:57:02.877180: step 71720, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-11 22:58:08.704669: step 71740, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-11 22:59:15.751914: step 71760, loss = 0.0035, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 23:00:23.104685: step 71780, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-11 23:01:30.032110: step 71800, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-11 23:02:36.548769: step 71820, loss = 0.0024, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-11 23:03:43.849297: step 71840, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-11 23:04:50.392926: step 71860, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 23:05:57.492998: step 71880, loss = 0.0020, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-11 23:07:04.646037: step 71900, loss = 0.0019, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-11 23:08:11.157779: step 71920, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-11 23:09:17.806346: step 71940, loss = 0.0036, acc = 0.9980 (18.7 examples/sec; 3.428 sec/batch)
2017-05-11 23:10:24.630365: step 71960, loss = 0.0020, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-11 23:11:31.760332: step 71980, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-11 23:12:39.265371: step 72000, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
[Eval] 2017-05-11 23:12:54.648613: step 72000, acc = 0.9477, f1 = 0.9460
[Test] 2017-05-11 23:13:05.282984: step 72000, acc = 0.9349, f1 = 0.9344
[Status] 2017-05-11 23:13:05.283195: step 72000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-11 23:14:12.866772: step 72020, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-11 23:15:19.560407: step 72040, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-11 23:16:26.265938: step 72060, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-11 23:17:33.541757: step 72080, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-11 23:18:40.272765: step 72100, loss = 0.0036, acc = 0.9980 (19.1 examples/sec; 3.354 sec/batch)
2017-05-11 23:19:47.573686: step 72120, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-11 23:20:53.824924: step 72140, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-11 23:22:00.599620: step 72160, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-11 23:23:07.304641: step 72180, loss = 0.0022, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-11 23:24:15.728667: step 72200, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 23:25:21.890164: step 72220, loss = 0.0030, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-11 23:26:28.277784: step 72240, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 23:27:34.581837: step 72260, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-11 23:28:42.536968: step 72280, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-11 23:29:49.412566: step 72300, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-11 23:30:57.115287: step 72320, loss = 0.0019, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-11 23:32:03.536986: step 72340, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 23:33:10.810952: step 72360, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-11 23:34:18.397522: step 72380, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-11 23:35:26.199620: step 72400, loss = 0.0022, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-11 23:36:33.041471: step 72420, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-11 23:37:40.050482: step 72440, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-11 23:38:47.337317: step 72460, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-11 23:39:54.300098: step 72480, loss = 0.0019, acc = 1.0000 (19.7 examples/sec; 3.257 sec/batch)
2017-05-11 23:41:01.616960: step 72500, loss = 0.0022, acc = 1.0000 (18.1 examples/sec; 3.533 sec/batch)
2017-05-11 23:42:09.129697: step 72520, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-11 23:43:15.811945: step 72540, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-11 23:44:23.411006: step 72560, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-11 23:45:31.644869: step 72580, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-11 23:46:38.514023: step 72600, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-11 23:47:44.601929: step 72620, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-11 23:48:52.781421: step 72640, loss = 0.0020, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-11 23:49:59.649524: step 72660, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-11 23:51:06.351305: step 72680, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-11 23:52:14.169896: step 72700, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-11 23:53:20.255798: step 72720, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-11 23:54:26.560709: step 72740, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-11 23:55:33.016005: step 72760, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-11 23:56:39.465733: step 72780, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-11 23:57:46.247472: step 72800, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-11 23:58:52.758348: step 72820, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-11 23:59:59.888901: step 72840, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 00:01:08.101923: step 72860, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-12 00:02:15.288784: step 72880, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-12 00:03:23.012254: step 72900, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 00:04:30.424423: step 72920, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 00:05:38.157948: step 72940, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-12 00:06:45.283217: step 72960, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 00:07:52.018398: step 72980, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 00:08:58.130965: step 73000, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
[Eval] 2017-05-12 00:09:13.514044: step 73000, acc = 0.9471, f1 = 0.9454
[Test] 2017-05-12 00:09:24.162722: step 73000, acc = 0.9344, f1 = 0.9339
[Status] 2017-05-12 00:09:24.162809: step 73000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 00:10:31.704005: step 73020, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-12 00:11:40.608999: step 73040, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 00:12:48.146637: step 73060, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 00:13:54.977102: step 73080, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-12 00:15:01.193929: step 73100, loss = 0.0052, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-12 00:16:07.866517: step 73120, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 00:17:15.151399: step 73140, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 00:18:21.398935: step 73160, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 00:19:28.337471: step 73180, loss = 0.0022, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-12 00:20:36.007464: step 73200, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-12 00:21:42.413208: step 73220, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 00:22:48.615322: step 73240, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 00:23:55.741947: step 73260, loss = 0.0020, acc = 1.0000 (18.1 examples/sec; 3.544 sec/batch)
2017-05-12 00:25:01.871745: step 73280, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 00:26:08.471227: step 73300, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 00:27:16.049386: step 73320, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-12 00:28:23.021022: step 73340, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-12 00:29:29.571638: step 73360, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-12 00:30:35.909953: step 73380, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 00:31:42.854669: step 73400, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-12 00:32:49.042199: step 73420, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 00:33:55.686222: step 73440, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 00:35:02.425830: step 73460, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 00:36:10.183706: step 73480, loss = 0.0020, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-12 00:37:16.552791: step 73500, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 00:38:22.633084: step 73520, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 00:39:29.880064: step 73540, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 00:40:37.525401: step 73560, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 00:41:45.322659: step 73580, loss = 0.0018, acc = 1.0000 (17.8 examples/sec; 3.596 sec/batch)
2017-05-12 00:42:53.802079: step 73600, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-12 00:44:01.182745: step 73620, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 00:45:07.161001: step 73640, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 00:46:15.753259: step 73660, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 00:47:23.110016: step 73680, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-12 00:48:29.050344: step 73700, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 00:49:35.500981: step 73720, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 00:50:42.351428: step 73740, loss = 0.0018, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-12 00:51:50.085772: step 73760, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 00:52:58.443593: step 73780, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-12 00:54:06.086220: step 73800, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 00:55:12.457640: step 73820, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 00:56:18.571150: step 73840, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 00:57:25.425710: step 73860, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-12 00:58:32.096092: step 73880, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 00:59:38.427567: step 73900, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 01:00:44.761175: step 73920, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.290 sec/batch)
2017-05-12 01:01:51.098746: step 73940, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 01:02:57.268322: step 73960, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 01:04:04.221501: step 73980, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 01:05:11.986029: step 74000, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-12 01:05:27.471189: step 74000, acc = 0.9455, f1 = 0.9440
[Test] 2017-05-12 01:05:38.140574: step 74000, acc = 0.9334, f1 = 0.9329
[Status] 2017-05-12 01:05:38.140667: step 74000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 01:06:45.863601: step 74020, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 01:07:53.892573: step 74040, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-12 01:09:00.626259: step 74060, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-12 01:10:06.997634: step 74080, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 01:11:14.254682: step 74100, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-12 01:12:20.711277: step 74120, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 01:13:29.162196: step 74140, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 01:14:36.437362: step 74160, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 01:15:43.509341: step 74180, loss = 0.0022, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-12 01:16:50.735174: step 74200, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 01:17:58.542490: step 74220, loss = 0.0035, acc = 0.9980 (18.4 examples/sec; 3.478 sec/batch)
2017-05-12 01:19:06.789012: step 74240, loss = 0.0020, acc = 1.0000 (17.5 examples/sec; 3.647 sec/batch)
2017-05-12 01:20:13.510177: step 74260, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-12 01:21:20.233493: step 74280, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 01:22:27.062427: step 74300, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 01:23:34.459459: step 74320, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-12 01:24:40.885905: step 74340, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 01:25:46.860640: step 74360, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 01:26:54.768508: step 74380, loss = 0.0021, acc = 1.0000 (17.6 examples/sec; 3.628 sec/batch)
2017-05-12 01:28:02.186975: step 74400, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-12 01:29:08.824624: step 74420, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-12 01:30:16.248753: step 74440, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 01:31:23.060407: step 74460, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 01:32:29.305058: step 74480, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-12 01:33:36.138310: step 74500, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 01:34:42.904271: step 74520, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-12 01:35:50.938683: step 74540, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-12 01:36:57.440347: step 74560, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 01:38:04.192307: step 74580, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-12 01:39:12.866364: step 74600, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-12 01:40:20.703215: step 74620, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 01:41:28.655041: step 74640, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 01:42:35.674015: step 74660, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-12 01:43:42.137982: step 74680, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 01:44:48.734032: step 74700, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-12 01:45:55.478580: step 74720, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-12 01:47:01.453606: step 74740, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 01:48:08.584077: step 74760, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 01:49:14.624970: step 74780, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-12 01:50:21.803882: step 74800, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 01:51:28.493419: step 74820, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-12 01:52:36.367539: step 74840, loss = 0.0018, acc = 1.0000 (17.5 examples/sec; 3.652 sec/batch)
2017-05-12 01:53:43.745635: step 74860, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 01:54:50.627082: step 74880, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-12 01:55:58.201000: step 74900, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 01:57:04.538941: step 74920, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-12 01:58:12.198656: step 74940, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 01:59:19.760025: step 74960, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-12 02:00:26.613615: step 74980, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 02:01:32.471627: step 75000, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
[Eval] 2017-05-12 02:01:47.811893: step 75000, acc = 0.9464, f1 = 0.9447
[Test] 2017-05-12 02:01:58.626027: step 75000, acc = 0.9350, f1 = 0.9344
[Status] 2017-05-12 02:01:58.626124: step 75000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 02:03:05.389298: step 75020, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-12 02:04:11.617052: step 75040, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 02:05:19.388605: step 75060, loss = 0.0020, acc = 1.0000 (18.0 examples/sec; 3.564 sec/batch)
2017-05-12 02:06:25.809413: step 75080, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 02:07:32.598059: step 75100, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 02:08:39.393887: step 75120, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 02:09:45.386522: step 75140, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 02:10:52.433004: step 75160, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-12 02:11:58.370855: step 75180, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 02:13:05.991976: step 75200, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 02:14:12.778614: step 75220, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-12 02:15:19.708459: step 75240, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 02:16:26.486927: step 75260, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 02:17:32.796850: step 75280, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 02:18:40.275148: step 75300, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-12 02:19:46.448506: step 75320, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 02:20:54.579454: step 75340, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-12 02:22:00.832748: step 75360, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 02:23:08.433267: step 75380, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 02:24:15.664551: step 75400, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-12 02:25:22.287225: step 75420, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 02:26:29.432038: step 75440, loss = 0.0028, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-12 02:27:36.633965: step 75460, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 02:28:42.929744: step 75480, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 02:29:49.968255: step 75500, loss = 0.0072, acc = 0.9980 (19.0 examples/sec; 3.363 sec/batch)
2017-05-12 02:30:55.905347: step 75520, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 02:32:03.903129: step 75540, loss = 0.0018, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-12 02:33:11.266555: step 75560, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-12 02:34:17.788674: step 75580, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 02:35:25.542109: step 75600, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 02:36:32.215040: step 75620, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 02:37:39.036605: step 75640, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 02:38:46.088949: step 75660, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 02:39:52.661167: step 75680, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-12 02:40:59.452178: step 75700, loss = 0.0016, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-12 02:42:05.926663: step 75720, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 02:43:12.873660: step 75740, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-12 02:44:18.899769: step 75760, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 02:45:26.026011: step 75780, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-12 02:46:33.897981: step 75800, loss = 0.0023, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-12 02:47:40.703801: step 75820, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 02:48:46.938211: step 75840, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 02:49:52.930009: step 75860, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 02:51:00.613052: step 75880, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 02:52:09.176823: step 75900, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 02:53:17.095091: step 75920, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 02:54:24.353384: step 75940, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-12 02:55:31.643974: step 75960, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 02:56:37.606213: step 75980, loss = 0.0035, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 02:57:48.291028: step 76000, loss = 0.0021, acc = 1.0000 (18.1 examples/sec; 3.526 sec/batch)
[Eval] 2017-05-12 02:58:03.656623: step 76000, acc = 0.9463, f1 = 0.9445
[Test] 2017-05-12 02:58:14.427940: step 76000, acc = 0.9346, f1 = 0.9341
[Status] 2017-05-12 02:58:14.428039: step 76000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 02:59:20.535455: step 76020, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 03:00:26.577836: step 76040, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 03:01:33.298516: step 76060, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 03:02:39.916284: step 76080, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 03:03:46.233500: step 76100, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-12 03:04:54.084296: step 76120, loss = 0.0018, acc = 1.0000 (18.1 examples/sec; 3.534 sec/batch)
2017-05-12 03:06:02.180412: step 76140, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-12 03:07:08.497220: step 76160, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 03:08:16.532827: step 76180, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 03:09:23.501006: step 76200, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-12 03:10:30.304627: step 76220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-12 03:11:36.849856: step 76240, loss = 0.0025, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-12 03:12:43.017426: step 76260, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-12 03:13:49.867637: step 76280, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-12 03:14:56.918180: step 76300, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-12 03:16:03.449087: step 76320, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 03:17:10.687630: step 76340, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-12 03:18:18.135293: step 76360, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-12 03:19:25.006308: step 76380, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 03:20:31.480045: step 76400, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 03:21:38.319691: step 76420, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-12 03:22:47.718301: step 76440, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 03:23:54.428796: step 76460, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-12 03:25:02.571449: step 76480, loss = 0.0019, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-12 03:26:09.059426: step 76500, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 03:27:16.542067: step 76520, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 03:28:22.437910: step 76540, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 03:29:29.632929: step 76560, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-12 03:30:36.283403: step 76580, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-12 03:31:43.263327: step 76600, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 03:32:50.134710: step 76620, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 03:33:56.854886: step 76640, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 03:35:03.770112: step 76660, loss = 0.0019, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-12 03:36:11.516410: step 76680, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 03:37:18.970522: step 76700, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 03:38:25.345717: step 76720, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 03:39:33.771228: step 76740, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-12 03:40:40.842977: step 76760, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 03:41:48.531069: step 76780, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 03:42:54.829142: step 76800, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 03:44:00.959694: step 76820, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 03:45:07.360590: step 76840, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-12 03:46:14.805051: step 76860, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 03:47:22.857357: step 76880, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-12 03:48:29.963290: step 76900, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-12 03:49:36.366800: step 76920, loss = 0.0033, acc = 0.9980 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 03:50:42.839586: step 76940, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 03:51:50.027850: step 76960, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-12 03:52:57.313933: step 76980, loss = 0.0018, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-12 03:54:04.199119: step 77000, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
[Eval] 2017-05-12 03:54:19.743873: step 77000, acc = 0.9468, f1 = 0.9452
[Test] 2017-05-12 03:54:30.492209: step 77000, acc = 0.9350, f1 = 0.9344
[Status] 2017-05-12 03:54:30.492311: step 77000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 03:55:36.371942: step 77020, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 03:56:43.962142: step 77040, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.488 sec/batch)
2017-05-12 03:57:50.584092: step 77060, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 03:58:57.336017: step 77080, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 04:00:04.490115: step 77100, loss = 0.0018, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-12 04:01:12.249466: step 77120, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 04:02:18.205241: step 77140, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 04:03:25.898755: step 77160, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-12 04:04:33.050916: step 77180, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 04:05:39.608451: step 77200, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 04:06:46.278389: step 77220, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 04:07:52.768289: step 77240, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 04:09:00.494785: step 77260, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 04:10:07.450732: step 77280, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-12 04:11:14.063187: step 77300, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 04:12:22.111901: step 77320, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-12 04:13:29.270199: step 77340, loss = 0.0033, acc = 0.9980 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 04:14:37.626926: step 77360, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 04:15:44.265961: step 77380, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 04:16:50.615895: step 77400, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 04:17:57.369730: step 77420, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 04:19:03.968165: step 77440, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-12 04:20:09.850512: step 77460, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-12 04:21:17.320676: step 77480, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-12 04:22:23.661510: step 77500, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 04:23:29.476456: step 77520, loss = 12.3368, acc = 0.5320 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 04:24:36.358661: step 77540, loss = 0.0479, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-12 04:25:44.710147: step 77560, loss = 0.6807, acc = 0.9580 (18.4 examples/sec; 3.483 sec/batch)
2017-05-12 04:26:51.766212: step 77580, loss = 0.1695, acc = 0.9960 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 04:27:58.431549: step 77600, loss = 0.1822, acc = 0.9960 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 04:29:05.546486: step 77620, loss = 0.2216, acc = 0.9920 (19.2 examples/sec; 3.338 sec/batch)
2017-05-12 04:30:13.389259: step 77640, loss = 0.1950, acc = 0.9940 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 04:31:20.028733: step 77660, loss = 0.2381, acc = 0.9900 (18.6 examples/sec; 3.434 sec/batch)
2017-05-12 04:32:27.396522: step 77680, loss = 0.2129, acc = 0.9940 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 04:33:34.588221: step 77700, loss = 0.1728, acc = 0.9980 (19.6 examples/sec; 3.263 sec/batch)
2017-05-12 04:34:41.120120: step 77720, loss = 0.1777, acc = 0.9940 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 04:35:47.335197: step 77740, loss = 0.1419, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 04:36:54.007563: step 77760, loss = 0.1418, acc = 0.9980 (19.1 examples/sec; 3.359 sec/batch)
2017-05-12 04:38:01.341451: step 77780, loss = 0.1605, acc = 0.9940 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 04:39:08.941233: step 77800, loss = 0.1278, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-12 04:40:15.269176: step 77820, loss = 0.1537, acc = 0.9940 (18.7 examples/sec; 3.419 sec/batch)
2017-05-12 04:41:21.478670: step 77840, loss = 0.1197, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 04:42:28.302091: step 77860, loss = 0.1157, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 04:43:35.485583: step 77880, loss = 0.1368, acc = 0.9960 (18.9 examples/sec; 3.379 sec/batch)
2017-05-12 04:44:42.326068: step 77900, loss = 0.1682, acc = 0.9940 (19.0 examples/sec; 3.361 sec/batch)
2017-05-12 04:45:49.257661: step 77920, loss = 0.1056, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 04:46:54.692463: step 77940, loss = 0.1021, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 04:48:02.352266: step 77960, loss = 0.1146, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 04:49:09.049825: step 77980, loss = 0.1520, acc = 0.9920 (19.3 examples/sec; 3.323 sec/batch)
2017-05-12 04:50:16.003919: step 78000, loss = 0.0941, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
[Eval] 2017-05-12 04:50:31.522381: step 78000, acc = 0.9373, f1 = 0.9353
[Test] 2017-05-12 04:50:42.275872: step 78000, acc = 0.9246, f1 = 0.9240
[Status] 2017-05-12 04:50:42.275955: step 78000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 04:51:48.909122: step 78020, loss = 0.0914, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 04:52:55.997244: step 78040, loss = 0.0892, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-12 04:54:03.031773: step 78060, loss = 0.0878, acc = 0.9980 (19.0 examples/sec; 3.377 sec/batch)
2017-05-12 04:55:09.229135: step 78080, loss = 0.0848, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 04:56:15.386578: step 78100, loss = 0.0822, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 04:57:22.311631: step 78120, loss = 0.0961, acc = 0.9960 (19.1 examples/sec; 3.348 sec/batch)
2017-05-12 04:58:30.627823: step 78140, loss = 0.0879, acc = 0.9980 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 04:59:36.845150: step 78160, loss = 0.0757, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 05:00:43.933114: step 78180, loss = 0.0742, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 05:01:49.799189: step 78200, loss = 0.0746, acc = 0.9980 (19.7 examples/sec; 3.249 sec/batch)
2017-05-12 05:02:57.362329: step 78220, loss = 0.0856, acc = 0.9980 (18.2 examples/sec; 3.526 sec/batch)
2017-05-12 05:04:04.033504: step 78240, loss = 0.0731, acc = 0.9980 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 05:05:11.430011: step 78260, loss = 0.0768, acc = 0.9980 (19.3 examples/sec; 3.324 sec/batch)
2017-05-12 05:06:18.652735: step 78280, loss = 0.0650, acc = 1.0000 (18.0 examples/sec; 3.557 sec/batch)
2017-05-12 05:07:24.869698: step 78300, loss = 0.0635, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-12 05:08:31.445204: step 78320, loss = 0.0626, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-12 05:09:38.398680: step 78340, loss = 0.0767, acc = 0.9980 (19.1 examples/sec; 3.359 sec/batch)
2017-05-12 05:10:46.327281: step 78360, loss = 0.0590, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-12 05:11:52.725320: step 78380, loss = 0.0575, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 05:12:58.912197: step 78400, loss = 0.0561, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 05:14:06.359126: step 78420, loss = 0.0548, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-12 05:15:13.597472: step 78440, loss = 0.0535, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 05:16:20.351494: step 78460, loss = 0.0523, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-12 05:17:27.221897: step 78480, loss = 0.0511, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 05:18:35.048267: step 78500, loss = 0.0574, acc = 0.9980 (18.2 examples/sec; 3.521 sec/batch)
2017-05-12 05:19:41.878192: step 78520, loss = 0.0493, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-12 05:20:48.200433: step 78540, loss = 0.0475, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 05:21:54.533465: step 78560, loss = 0.0464, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-12 05:23:00.972140: step 78580, loss = 0.0502, acc = 0.9960 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 05:24:07.407225: step 78600, loss = 0.0447, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-12 05:25:14.700552: step 78620, loss = 0.0454, acc = 0.9980 (18.3 examples/sec; 3.505 sec/batch)
2017-05-12 05:26:21.388011: step 78640, loss = 0.0424, acc = 1.0000 (18.2 examples/sec; 3.507 sec/batch)
2017-05-12 05:27:28.254817: step 78660, loss = 0.0417, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-12 05:28:35.068394: step 78680, loss = 0.0404, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 05:29:42.273743: step 78700, loss = 0.0397, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-12 05:30:49.189332: step 78720, loss = 0.0429, acc = 0.9980 (18.7 examples/sec; 3.414 sec/batch)
2017-05-12 05:31:55.502098: step 78740, loss = 0.0394, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 05:33:03.398831: step 78760, loss = 0.0370, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 05:34:10.202551: step 78780, loss = 0.0361, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 05:35:16.217732: step 78800, loss = 0.0652, acc = 0.9980 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 05:36:23.392080: step 78820, loss = 0.0350, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-12 05:37:29.520248: step 78840, loss = 0.0395, acc = 0.9980 (19.2 examples/sec; 3.334 sec/batch)
2017-05-12 05:38:36.834818: step 78860, loss = 0.0330, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-12 05:39:43.314324: step 78880, loss = 0.0339, acc = 0.9980 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 05:40:50.061827: step 78900, loss = 0.0317, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-12 05:41:56.642012: step 78920, loss = 0.0309, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 05:43:02.273657: step 78940, loss = 0.0302, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 05:44:08.071219: step 78960, loss = 0.0296, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 05:45:14.536980: step 78980, loss = 0.0289, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 05:46:21.589510: step 79000, loss = 0.0284, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
[Eval] 2017-05-12 05:46:36.632947: step 79000, acc = 0.9388, f1 = 0.9368
[Test] 2017-05-12 05:46:47.332473: step 79000, acc = 0.9270, f1 = 0.9264
[Status] 2017-05-12 05:46:47.332573: step 79000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 05:47:56.411878: step 79020, loss = 0.0281, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 05:49:02.342188: step 79040, loss = 0.0274, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 05:50:07.926767: step 79060, loss = 0.0270, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 05:51:14.622649: step 79080, loss = 0.0260, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-12 05:52:21.741468: step 79100, loss = 0.0254, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-12 05:53:28.479739: step 79120, loss = 0.0253, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 05:54:34.727961: step 79140, loss = 0.0244, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 05:55:40.999209: step 79160, loss = 0.0237, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 05:56:46.875937: step 79180, loss = 0.0232, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 05:57:54.247014: step 79200, loss = 0.0227, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 05:59:02.235180: step 79220, loss = 0.0229, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-12 06:00:09.242274: step 79240, loss = 0.0218, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 06:01:15.875726: step 79260, loss = 0.0219, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-12 06:02:22.816123: step 79280, loss = 0.0209, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 06:03:29.959067: step 79300, loss = 0.0205, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-12 06:04:36.444301: step 79320, loss = 0.0200, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 06:05:43.723021: step 79340, loss = 0.0195, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 06:06:50.453678: step 79360, loss = 0.0197, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 06:07:57.290535: step 79380, loss = 0.0197, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-12 06:09:04.097165: step 79400, loss = 0.0185, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-12 06:10:11.258386: step 79420, loss = 0.0179, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-12 06:11:18.075038: step 79440, loss = 0.0176, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-12 06:12:24.292954: step 79460, loss = 0.0184, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 06:13:31.649417: step 79480, loss = 0.0168, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 06:14:38.661140: step 79500, loss = 0.0164, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-12 06:15:45.232516: step 79520, loss = 0.0185, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 06:16:51.404858: step 79540, loss = 0.0176, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-12 06:17:59.112656: step 79560, loss = 0.0154, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 06:19:05.286483: step 79580, loss = 0.0153, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 06:20:12.602251: step 79600, loss = 0.0147, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 06:21:18.579614: step 79620, loss = 0.0166, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 06:22:25.525990: step 79640, loss = 0.0141, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 06:23:31.707686: step 79660, loss = 0.0139, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-12 06:24:38.746704: step 79680, loss = 0.0136, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-12 06:25:45.613963: step 79700, loss = 0.0133, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 06:26:51.992839: step 79720, loss = 0.0130, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 06:27:59.655631: step 79740, loss = 0.0126, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-12 06:29:06.789572: step 79760, loss = 0.0124, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 06:30:13.167962: step 79780, loss = 0.0122, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-12 06:31:19.148886: step 79800, loss = 0.0121, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 06:32:26.108295: step 79820, loss = 0.0119, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 06:33:32.838500: step 79840, loss = 0.0114, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-12 06:34:39.434686: step 79860, loss = 0.0112, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 06:35:46.952504: step 79880, loss = 0.0109, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 06:36:53.259619: step 79900, loss = 0.0107, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 06:38:00.331946: step 79920, loss = 0.0104, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 06:39:07.215595: step 79940, loss = 0.0116, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-12 06:40:13.824671: step 79960, loss = 0.0100, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 06:41:20.327758: step 79980, loss = 0.0098, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 06:42:27.989517: step 80000, loss = 0.0095, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
[Eval] 2017-05-12 06:42:43.637464: step 80000, acc = 0.9387, f1 = 0.9367
[Test] 2017-05-12 06:42:54.301382: step 80000, acc = 0.9265, f1 = 0.9259
[Status] 2017-05-12 06:42:54.301465: step 80000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 06:44:02.476603: step 80020, loss = 0.0094, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-12 06:45:08.905864: step 80040, loss = 0.0094, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 06:46:15.371853: step 80060, loss = 0.0094, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 06:47:22.194096: step 80080, loss = 0.0088, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-12 06:48:29.248457: step 80100, loss = 0.0093, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-12 06:49:36.390368: step 80120, loss = 0.0085, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 06:50:42.314348: step 80140, loss = 0.0085, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-12 06:51:49.391547: step 80160, loss = 0.0082, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 06:52:55.424801: step 80180, loss = 0.0079, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 06:54:01.895396: step 80200, loss = 0.0081, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-12 06:55:08.650538: step 80220, loss = 0.0080, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 06:56:16.252449: step 80240, loss = 0.0079, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-12 06:57:22.437516: step 80260, loss = 0.0073, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 06:58:29.124370: step 80280, loss = 0.0071, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-12 06:59:35.585522: step 80300, loss = 0.0073, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 07:00:43.116931: step 80320, loss = 0.0075, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 07:01:50.258501: step 80340, loss = 0.0068, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 07:02:56.740580: step 80360, loss = 0.0071, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-12 07:04:02.916618: step 80380, loss = 0.0065, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 07:05:09.323014: step 80400, loss = 0.0068, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 07:06:15.344942: step 80420, loss = 0.0071, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 07:07:22.503240: step 80440, loss = 0.0063, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 07:08:29.758986: step 80460, loss = 0.0061, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-12 07:09:36.031198: step 80480, loss = 0.0059, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 07:10:42.302910: step 80500, loss = 0.0063, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 07:11:48.686006: step 80520, loss = 0.0059, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 07:12:55.533358: step 80540, loss = 0.0055, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 07:14:01.869390: step 80560, loss = 0.0055, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-12 07:15:08.912993: step 80580, loss = 0.0056, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-12 07:16:15.032730: step 80600, loss = 0.0052, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 07:17:21.771518: step 80620, loss = 0.0053, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-12 07:18:28.999729: step 80640, loss = 0.0050, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-12 07:19:35.356218: step 80660, loss = 0.0050, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-12 07:20:41.376511: step 80680, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 07:21:47.415343: step 80700, loss = 0.0048, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 07:22:55.429721: step 80720, loss = 0.0058, acc = 1.0000 (18.0 examples/sec; 3.558 sec/batch)
2017-05-12 07:24:01.717371: step 80740, loss = 0.0048, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 07:25:07.811029: step 80760, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 07:26:15.425398: step 80780, loss = 0.0058, acc = 0.9980 (18.6 examples/sec; 3.438 sec/batch)
2017-05-12 07:27:21.814457: step 80800, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 07:28:27.948244: step 80820, loss = 0.0046, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-12 07:29:35.259248: step 80840, loss = 0.0043, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-12 07:30:43.006965: step 80860, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 07:31:49.714737: step 80880, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-12 07:32:56.249220: step 80900, loss = 0.0040, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-12 07:34:02.491250: step 80920, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 07:35:09.769098: step 80940, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 07:36:17.073831: step 80960, loss = 0.0046, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 07:37:23.492907: step 80980, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 07:38:30.019718: step 81000, loss = 0.0039, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
[Eval] 2017-05-12 07:38:44.171944: step 81000, acc = 0.9405, f1 = 0.9385
[Test] 2017-05-12 07:38:54.776981: step 81000, acc = 0.9286, f1 = 0.9280
[Status] 2017-05-12 07:38:54.777057: step 81000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 07:40:00.971919: step 81020, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 07:41:08.150889: step 81040, loss = 0.0051, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-12 07:42:15.637843: step 81060, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-12 07:43:22.378217: step 81080, loss = 0.0039, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-12 07:44:28.680745: step 81100, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 07:45:35.491733: step 81120, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 07:46:43.012416: step 81140, loss = 0.0036, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-12 07:47:51.244922: step 81160, loss = 0.0044, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-12 07:48:57.855408: step 81180, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 07:50:05.182752: step 81200, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 07:51:12.072896: step 81220, loss = 0.0036, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-12 07:52:19.661839: step 81240, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 07:53:26.063536: step 81260, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 07:54:32.723767: step 81280, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 07:55:39.273622: step 81300, loss = 0.0037, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 07:56:46.646177: step 81320, loss = 0.0031, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 07:57:53.649028: step 81340, loss = 0.0034, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-12 07:58:59.730853: step 81360, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 08:00:05.928831: step 81380, loss = 0.0049, acc = 0.9980 (18.9 examples/sec; 3.384 sec/batch)
2017-05-12 08:01:12.528835: step 81400, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 08:02:19.931332: step 81420, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 08:03:27.387838: step 81440, loss = 0.0029, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-12 08:04:34.765288: step 81460, loss = 0.0030, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-12 08:05:41.867575: step 81480, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-12 08:06:48.880746: step 81500, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 08:07:55.094395: step 81520, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 08:09:03.044125: step 81540, loss = 0.0029, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-12 08:10:10.984052: step 81560, loss = 0.0030, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-12 08:11:16.938280: step 81580, loss = 0.0053, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 08:12:23.056064: step 81600, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 08:13:30.002874: step 81620, loss = 0.0027, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-12 08:14:37.026252: step 81640, loss = 0.0033, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-12 08:15:44.720823: step 81660, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 08:16:52.475752: step 81680, loss = 0.0033, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 08:17:59.794340: step 81700, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-12 08:19:07.524328: step 81720, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-12 08:20:15.512665: step 81740, loss = 0.0030, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 08:21:21.660042: step 81760, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 08:22:28.741515: step 81780, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 08:23:35.482056: step 81800, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 08:24:42.260102: step 81820, loss = 0.0033, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 08:25:50.357199: step 81840, loss = 0.0030, acc = 1.0000 (17.7 examples/sec; 3.617 sec/batch)
2017-05-12 08:26:57.044927: step 81860, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 08:28:03.703581: step 81880, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-12 08:29:10.056742: step 81900, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 08:30:16.651540: step 81920, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 08:31:23.365754: step 81940, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 08:32:29.670783: step 81960, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 08:33:36.693755: step 81980, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 08:34:43.558358: step 82000, loss = 0.0027, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
[Eval] 2017-05-12 08:34:59.120926: step 82000, acc = 0.9428, f1 = 0.9410
[Test] 2017-05-12 08:35:09.232503: step 82000, acc = 0.9309, f1 = 0.9304
[Status] 2017-05-12 08:35:09.232563: step 82000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 08:36:15.410597: step 82020, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 08:37:22.131958: step 82040, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-12 08:38:28.975872: step 82060, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-12 08:39:35.177692: step 82080, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 08:40:41.833402: step 82100, loss = 0.0031, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-12 08:41:49.705244: step 82120, loss = 0.0032, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-12 08:42:56.615263: step 82140, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 08:44:03.356114: step 82160, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 08:45:10.214214: step 82180, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 08:46:17.531263: step 82200, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 08:47:23.616554: step 82220, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 08:48:30.291540: step 82240, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 08:49:36.994745: step 82260, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 08:50:44.015213: step 82280, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-12 08:51:50.663492: step 82300, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-12 08:52:57.286377: step 82320, loss = 0.0029, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 08:54:03.390598: step 82340, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 08:55:11.507053: step 82360, loss = 0.0035, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-12 08:56:19.034033: step 82380, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 08:57:25.133917: step 82400, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 08:58:32.058321: step 82420, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 08:59:38.698405: step 82440, loss = 0.0035, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-12 09:00:46.070523: step 82460, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 09:01:53.274131: step 82480, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 09:03:00.471974: step 82500, loss = 0.0046, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 09:04:07.471663: step 82520, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-12 09:05:14.026379: step 82540, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 09:06:21.284003: step 82560, loss = 0.0031, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-12 09:07:28.867948: step 82580, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-12 09:08:35.685624: step 82600, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 09:09:44.248313: step 82620, loss = 0.0022, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-12 09:10:51.600195: step 82640, loss = 0.0025, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-12 09:11:58.295379: step 82660, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 09:13:05.301883: step 82680, loss = 0.0030, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-12 09:14:13.219628: step 82700, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-12 09:15:20.015562: step 82720, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 09:16:26.967686: step 82740, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 09:17:35.650012: step 82760, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 09:18:42.832382: step 82780, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 09:19:49.478861: step 82800, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-12 09:20:55.845193: step 82820, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 09:22:03.054437: step 82840, loss = 0.0033, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 09:23:09.392657: step 82860, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 09:24:16.954219: step 82880, loss = 0.0022, acc = 1.0000 (18.2 examples/sec; 3.523 sec/batch)
2017-05-12 09:25:23.903212: step 82900, loss = 0.0026, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-12 09:26:31.353476: step 82920, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 09:27:37.676964: step 82940, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 09:28:43.846284: step 82960, loss = 0.0027, acc = 1.0000 (19.7 examples/sec; 3.252 sec/batch)
2017-05-12 09:29:50.043097: step 82980, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 09:30:56.431380: step 83000, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
[Eval] 2017-05-12 09:31:11.083078: step 83000, acc = 0.9432, f1 = 0.9414
[Test] 2017-05-12 09:31:21.021887: step 83000, acc = 0.9314, f1 = 0.9309
[Status] 2017-05-12 09:31:21.021983: step 83000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 09:32:27.038504: step 83020, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 09:33:33.558557: step 83040, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 09:34:40.894537: step 83060, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-12 09:35:47.180945: step 83080, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 09:36:53.301918: step 83100, loss = 0.0029, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 09:38:00.515017: step 83120, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 09:39:07.057431: step 83140, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 09:40:14.663284: step 83160, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-12 09:41:21.144856: step 83180, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 09:42:28.016679: step 83200, loss = 0.0021, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-12 09:43:35.776007: step 83220, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 09:44:42.927206: step 83240, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-12 09:45:49.968735: step 83260, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-12 09:46:57.252824: step 83280, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 09:48:04.866702: step 83300, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 09:49:11.533611: step 83320, loss = 0.0041, acc = 0.9980 (19.1 examples/sec; 3.343 sec/batch)
2017-05-12 09:50:18.817868: step 83340, loss = 0.0030, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-12 09:51:26.958902: step 83360, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 09:52:34.199685: step 83380, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 09:53:40.960050: step 83400, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 09:54:47.680057: step 83420, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 09:55:54.545103: step 83440, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-12 09:57:00.957865: step 83460, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 09:58:07.554196: step 83480, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 09:59:14.193759: step 83500, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 10:00:21.259595: step 83520, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 10:01:27.962865: step 83540, loss = 0.0037, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 10:02:34.501513: step 83560, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 10:03:41.355796: step 83580, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 10:04:48.500928: step 83600, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 10:05:54.685715: step 83620, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 10:07:01.588581: step 83640, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-12 10:08:08.977647: step 83660, loss = 0.0022, acc = 1.0000 (17.9 examples/sec; 3.566 sec/batch)
2017-05-12 10:09:15.522542: step 83680, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 10:10:21.519892: step 83700, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 10:11:27.753213: step 83720, loss = 0.0055, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 10:12:34.139037: step 83740, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 10:13:41.210272: step 83760, loss = 0.0029, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-12 10:14:48.600832: step 83780, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-12 10:15:55.968539: step 83800, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-12 10:17:02.491816: step 83820, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 10:18:09.339701: step 83840, loss = 0.0021, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-12 10:19:15.391981: step 83860, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 10:20:21.695185: step 83880, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 10:21:28.556186: step 83900, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-12 10:22:35.750938: step 83920, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 10:23:41.855670: step 83940, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 10:24:50.880582: step 83960, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-12 10:25:58.197523: step 83980, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 10:27:06.226531: step 84000, loss = 0.0024, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
[Eval] 2017-05-12 10:27:21.578051: step 84000, acc = 0.9437, f1 = 0.9420
[Test] 2017-05-12 10:27:32.255296: step 84000, acc = 0.9321, f1 = 0.9315
[Status] 2017-05-12 10:27:32.255380: step 84000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 10:28:38.877579: step 84020, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-12 10:29:46.071392: step 84040, loss = 0.0025, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-12 10:30:53.704581: step 84060, loss = 0.0023, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-12 10:32:00.429895: step 84080, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 10:33:07.735198: step 84100, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 10:34:14.997741: step 84120, loss = 0.0024, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-12 10:35:21.102668: step 84140, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 10:36:27.112674: step 84160, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 10:37:33.796885: step 84180, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 10:38:41.053083: step 84200, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 10:39:47.710965: step 84220, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-12 10:40:55.309519: step 84240, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 10:42:01.925343: step 84260, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 10:43:08.522525: step 84280, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 10:44:15.358691: step 84300, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 10:45:23.433853: step 84320, loss = 0.0026, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-12 10:46:29.982257: step 84340, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 10:47:36.358796: step 84360, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-12 10:48:42.705486: step 84380, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 10:49:49.274311: step 84400, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 10:50:55.591311: step 84420, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-12 10:52:01.836551: step 84440, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 10:53:08.916812: step 84460, loss = 0.0044, acc = 0.9980 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 10:54:15.431457: step 84480, loss = 0.0026, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-12 10:55:22.681323: step 84500, loss = 0.0023, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-12 10:56:29.952954: step 84520, loss = 0.0021, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-12 10:57:37.430564: step 84540, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-12 10:58:44.274482: step 84560, loss = 0.0027, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-12 10:59:51.249602: step 84580, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 11:00:58.330705: step 84600, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 11:02:05.888349: step 84620, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 11:03:12.371682: step 84640, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 11:04:18.913737: step 84660, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-12 11:05:25.636663: step 84680, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-12 11:06:32.181973: step 84700, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 11:07:39.399364: step 84720, loss = 0.0025, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-12 11:08:46.383446: step 84740, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 11:09:53.607591: step 84760, loss = 0.0056, acc = 0.9980 (18.7 examples/sec; 3.413 sec/batch)
2017-05-12 11:11:00.929588: step 84780, loss = 0.0021, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-12 11:12:07.408716: step 84800, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 11:13:15.428170: step 84820, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 11:14:22.688032: step 84840, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 11:15:29.284487: step 84860, loss = 0.0022, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-12 11:16:36.537816: step 84880, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 11:17:42.979777: step 84900, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 11:18:50.045415: step 84920, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 11:19:57.157404: step 84940, loss = 0.0021, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-12 11:21:05.324966: step 84960, loss = 0.0022, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-12 11:22:12.647107: step 84980, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 11:23:20.462641: step 85000, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
[Eval] 2017-05-12 11:23:35.803141: step 85000, acc = 0.9440, f1 = 0.9423
[Test] 2017-05-12 11:23:45.979586: step 85000, acc = 0.9321, f1 = 0.9315
[Status] 2017-05-12 11:23:45.979753: step 85000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 11:24:52.581844: step 85020, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-12 11:25:59.566223: step 85040, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 11:27:06.501634: step 85060, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 11:28:12.909729: step 85080, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-12 11:29:19.190503: step 85100, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 11:30:26.034834: step 85120, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 11:31:33.040891: step 85140, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 11:32:39.791395: step 85160, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 11:33:47.441360: step 85180, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 11:34:53.994015: step 85200, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-12 11:36:00.356026: step 85220, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 11:37:07.371943: step 85240, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-12 11:38:14.099955: step 85260, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 11:39:20.987109: step 85280, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 11:40:28.281531: step 85300, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 11:41:34.697765: step 85320, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 11:42:42.208985: step 85340, loss = 0.0020, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-12 11:43:49.150810: step 85360, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 11:44:56.702202: step 85380, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-12 11:46:03.656044: step 85400, loss = 0.0021, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-12 11:47:10.368708: step 85420, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 11:48:16.344455: step 85440, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 11:49:23.825053: step 85460, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 11:50:31.140428: step 85480, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 11:51:37.689535: step 85500, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-12 11:52:43.783471: step 85520, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-12 11:53:49.634552: step 85540, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 11:54:56.681875: step 85560, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 11:56:04.149806: step 85580, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 11:57:10.963738: step 85600, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 11:58:18.673258: step 85620, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-12 11:59:24.984877: step 85640, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 12:00:33.223745: step 85660, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 12:01:40.305268: step 85680, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 12:02:46.303876: step 85700, loss = 0.0019, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-12 12:03:52.803354: step 85720, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 12:04:58.717378: step 85740, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 12:06:05.070320: step 85760, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 12:07:12.295200: step 85780, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 12:08:19.597474: step 85800, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-12 12:09:26.138226: step 85820, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-12 12:10:33.765361: step 85840, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-12 12:11:40.489356: step 85860, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 12:12:47.307135: step 85880, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-12 12:13:53.964160: step 85900, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 12:15:01.499768: step 85920, loss = 0.0021, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-12 12:16:07.543324: step 85940, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 12:17:14.398504: step 85960, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-12 12:18:21.427970: step 85980, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 12:19:27.710204: step 86000, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
[Eval] 2017-05-12 12:19:41.781281: step 86000, acc = 0.9442, f1 = 0.9424
[Test] 2017-05-12 12:19:52.206736: step 86000, acc = 0.9320, f1 = 0.9315
[Status] 2017-05-12 12:19:52.206820: step 86000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 12:20:59.057369: step 86020, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-12 12:22:05.955182: step 86040, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-12 12:23:12.985964: step 86060, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 12:24:19.810177: step 86080, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-12 12:25:26.538538: step 86100, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 12:26:34.708549: step 86120, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-12 12:27:41.623421: step 86140, loss = 0.0021, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-12 12:28:48.100577: step 86160, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-12 12:29:55.165007: step 86180, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-12 12:31:02.587758: step 86200, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 12:32:09.295204: step 86220, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 12:33:15.604465: step 86240, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 12:34:21.707546: step 86260, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-12 12:35:27.928905: step 86280, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 12:36:34.805031: step 86300, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-12 12:37:41.025885: step 86320, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-12 12:38:47.584659: step 86340, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 12:39:54.747933: step 86360, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 12:41:00.997690: step 86380, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-12 12:42:07.522671: step 86400, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-12 12:43:15.205913: step 86420, loss = 0.0022, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-12 12:44:22.125704: step 86440, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-12 12:45:29.286103: step 86460, loss = 0.0034, acc = 0.9980 (18.9 examples/sec; 3.394 sec/batch)
2017-05-12 12:46:35.900923: step 86480, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 12:47:42.890536: step 86500, loss = 0.0022, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-12 12:48:49.378472: step 86520, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 12:49:56.265749: step 86540, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 12:51:02.620537: step 86560, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-12 12:52:09.796682: step 86580, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-12 12:53:16.436144: step 86600, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-12 12:54:23.056033: step 86620, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 12:55:30.585792: step 86640, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-12 12:56:36.841166: step 86660, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 12:57:43.526522: step 86680, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 12:58:50.817002: step 86700, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 12:59:58.514553: step 86720, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-12 13:01:05.267418: step 86740, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-12 13:02:11.492253: step 86760, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-12 13:03:17.529816: step 86780, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 13:04:24.134525: step 86800, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-12 13:05:30.406805: step 86820, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-12 13:06:38.886955: step 86840, loss = 0.0021, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-12 13:07:45.295813: step 86860, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-12 13:08:52.067617: step 86880, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-12 13:09:58.364024: step 86900, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 13:11:05.722935: step 86920, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 13:12:12.391679: step 86940, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-12 13:13:19.821374: step 86960, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 13:14:27.072388: step 86980, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-12 13:15:33.684743: step 87000, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
[Eval] 2017-05-12 13:15:49.071565: step 87000, acc = 0.9445, f1 = 0.9428
[Test] 2017-05-12 13:15:59.631769: step 87000, acc = 0.9324, f1 = 0.9318
[Status] 2017-05-12 13:15:59.631876: step 87000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 13:17:07.152360: step 87020, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-12 13:18:14.065578: step 87040, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 13:19:20.552918: step 87060, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-12 13:20:27.262171: step 87080, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 13:21:35.086810: step 87100, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 13:22:42.370499: step 87120, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-12 13:23:48.620072: step 87140, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-12 13:24:56.300575: step 87160, loss = 0.0025, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-12 13:26:02.755499: step 87180, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 13:27:09.396772: step 87200, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 13:28:16.403053: step 87220, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 13:29:22.949040: step 87240, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 13:30:29.901102: step 87260, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 13:31:36.746190: step 87280, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 13:32:45.762097: step 87300, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 13:33:53.287816: step 87320, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 13:35:01.168282: step 87340, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 13:36:07.666194: step 87360, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-12 13:37:14.127590: step 87380, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-12 13:38:20.153085: step 87400, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-12 13:39:28.232182: step 87420, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-12 13:40:42.102956: step 87440, loss = 0.0021, acc = 1.0000 (15.5 examples/sec; 4.124 sec/batch)
2017-05-12 13:41:59.346518: step 87460, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.785 sec/batch)
2017-05-12 13:43:14.971654: step 87480, loss = 0.0021, acc = 1.0000 (16.9 examples/sec; 3.797 sec/batch)
2017-05-12 13:44:30.605101: step 87500, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.787 sec/batch)
2017-05-12 13:45:46.456897: step 87520, loss = 0.0018, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-12 13:47:02.270175: step 87540, loss = 0.0020, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-12 13:48:17.981593: step 87560, loss = 0.0022, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-12 13:49:33.558514: step 87580, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.783 sec/batch)
2017-05-12 13:50:49.366110: step 87600, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.781 sec/batch)
2017-05-12 13:52:05.041757: step 87620, loss = 0.0020, acc = 1.0000 (16.9 examples/sec; 3.783 sec/batch)
2017-05-12 13:53:20.997602: step 87640, loss = 0.0020, acc = 1.0000 (16.9 examples/sec; 3.777 sec/batch)
2017-05-12 13:54:36.701786: step 87660, loss = 0.0020, acc = 1.0000 (17.0 examples/sec; 3.770 sec/batch)
2017-05-12 13:55:52.606258: step 87680, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.780 sec/batch)
2017-05-12 13:57:09.414857: step 87700, loss = 0.0027, acc = 1.0000 (16.8 examples/sec; 3.805 sec/batch)
2017-05-12 13:58:25.498812: step 87720, loss = 0.0020, acc = 1.0000 (17.0 examples/sec; 3.765 sec/batch)
2017-05-12 13:59:41.270214: step 87740, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.782 sec/batch)
2017-05-12 14:00:57.259840: step 87760, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.785 sec/batch)
2017-05-12 14:02:13.673164: step 87780, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.787 sec/batch)
2017-05-12 14:03:29.679361: step 87800, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.796 sec/batch)
2017-05-12 14:04:45.391568: step 87820, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.778 sec/batch)
2017-05-12 14:06:01.327202: step 87840, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.779 sec/batch)
2017-05-12 14:07:16.828436: step 87860, loss = 0.0029, acc = 1.0000 (17.0 examples/sec; 3.764 sec/batch)
2017-05-12 14:08:32.516600: step 87880, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.796 sec/batch)
2017-05-12 14:09:48.099357: step 87900, loss = 0.0017, acc = 1.0000 (17.0 examples/sec; 3.765 sec/batch)
2017-05-12 14:11:03.615997: step 87920, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.776 sec/batch)
2017-05-12 14:12:19.237318: step 87940, loss = 0.0020, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-12 14:13:34.786518: step 87960, loss = 0.0025, acc = 1.0000 (17.0 examples/sec; 3.770 sec/batch)
2017-05-12 14:14:50.322023: step 87980, loss = 0.0038, acc = 1.0000 (16.9 examples/sec; 3.786 sec/batch)
2017-05-12 14:16:05.898348: step 88000, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.787 sec/batch)
[Eval] 2017-05-12 14:16:20.875430: step 88000, acc = 0.9446, f1 = 0.9429
[Test] 2017-05-12 14:16:31.381604: step 88000, acc = 0.9329, f1 = 0.9324
[Status] 2017-05-12 14:16:31.381699: step 88000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 14:17:38.257435: step 88020, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-12 14:18:45.447395: step 88040, loss = 0.0023, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-12 14:19:51.281238: step 88060, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 14:20:57.597331: step 88080, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-12 14:22:04.918658: step 88100, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-12 14:23:11.265864: step 88120, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 14:24:17.439156: step 88140, loss = 0.0095, acc = 0.9980 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 14:25:23.845138: step 88160, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-12 14:26:30.649554: step 88180, loss = 0.0019, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-12 14:27:37.716634: step 88200, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-12 14:28:44.107218: step 88220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 14:29:50.590592: step 88240, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 14:30:56.814587: step 88260, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-12 14:32:04.804065: step 88280, loss = 0.0018, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-12 14:33:11.509798: step 88300, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 14:34:18.022667: step 88320, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 14:35:24.736778: step 88340, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-12 14:36:31.767616: step 88360, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 14:37:37.718810: step 88380, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 14:38:45.621653: step 88400, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 14:39:53.500635: step 88420, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-12 14:41:00.241331: step 88440, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 14:42:07.035489: step 88460, loss = 0.0051, acc = 0.9980 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 14:43:14.465117: step 88480, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 14:44:21.361715: step 88500, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-12 14:45:27.907738: step 88520, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-12 14:46:34.622773: step 88540, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-12 14:47:41.048153: step 88560, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 14:48:46.916816: step 88580, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 14:49:53.078784: step 88600, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-12 14:51:04.810439: step 88620, loss = 0.0018, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-12 14:52:12.351589: step 88640, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 14:53:18.768272: step 88660, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 14:54:25.498286: step 88680, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-12 14:55:33.343123: step 88700, loss = 0.0055, acc = 0.9980 (18.2 examples/sec; 3.512 sec/batch)
2017-05-12 14:56:39.501075: step 88720, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-12 14:57:46.369933: step 88740, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-12 14:58:53.886686: step 88760, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-12 15:00:02.162985: step 88780, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 15:01:09.695825: step 88800, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-12 15:02:15.905048: step 88820, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-12 15:03:22.590921: step 88840, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 15:04:29.409850: step 88860, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 15:05:35.486737: step 88880, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 15:06:41.902920: step 88900, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-12 15:07:48.602644: step 88920, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 15:08:55.321120: step 88940, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-12 15:10:01.667206: step 88960, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 15:11:09.383352: step 88980, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 15:12:17.150225: step 89000, loss = 0.0022, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
[Eval] 2017-05-12 15:12:32.559739: step 89000, acc = 0.9449, f1 = 0.9432
[Test] 2017-05-12 15:12:42.837629: step 89000, acc = 0.9330, f1 = 0.9324
[Status] 2017-05-12 15:12:42.837747: step 89000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 15:13:51.010017: step 89020, loss = 0.0019, acc = 1.0000 (17.7 examples/sec; 3.611 sec/batch)
2017-05-12 15:14:59.496983: step 89040, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-12 15:16:05.670100: step 89060, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 15:17:12.143858: step 89080, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-12 15:18:19.622171: step 89100, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 15:19:26.300067: step 89120, loss = 0.0033, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-12 15:20:32.448732: step 89140, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 15:21:39.189880: step 89160, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 15:22:45.948847: step 89180, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 15:23:53.585308: step 89200, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-12 15:25:02.139781: step 89220, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-12 15:26:08.859953: step 89240, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 15:27:16.163407: step 89260, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-12 15:28:22.742916: step 89280, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 15:29:29.651778: step 89300, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 15:30:40.172612: step 89320, loss = 0.0019, acc = 1.0000 (18.0 examples/sec; 3.558 sec/batch)
2017-05-12 15:31:58.265347: step 89340, loss = 0.0018, acc = 1.0000 (16.8 examples/sec; 3.798 sec/batch)
2017-05-12 15:33:13.891625: step 89360, loss = 0.0018, acc = 1.0000 (16.8 examples/sec; 3.803 sec/batch)
2017-05-12 15:34:29.544136: step 89380, loss = 0.0018, acc = 1.0000 (16.8 examples/sec; 3.802 sec/batch)
2017-05-12 15:35:45.257444: step 89400, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.793 sec/batch)
2017-05-12 15:37:00.911218: step 89420, loss = 0.0018, acc = 1.0000 (16.8 examples/sec; 3.807 sec/batch)
2017-05-12 15:38:16.423140: step 89440, loss = 0.0025, acc = 1.0000 (16.9 examples/sec; 3.792 sec/batch)
2017-05-12 15:39:32.220185: step 89460, loss = 0.0017, acc = 1.0000 (16.8 examples/sec; 3.804 sec/batch)
2017-05-12 15:40:47.865591: step 89480, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.784 sec/batch)
2017-05-12 15:42:03.693103: step 89500, loss = 0.0020, acc = 1.0000 (16.9 examples/sec; 3.784 sec/batch)
2017-05-12 15:43:19.362874: step 89520, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.777 sec/batch)
2017-05-12 15:44:34.868793: step 89540, loss = 0.0020, acc = 1.0000 (16.9 examples/sec; 3.780 sec/batch)
2017-05-12 15:45:50.589732: step 89560, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.790 sec/batch)
2017-05-12 15:47:06.227746: step 89580, loss = 0.0021, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-12 15:48:21.940830: step 89600, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.777 sec/batch)
2017-05-12 15:49:37.467832: step 89620, loss = 0.0018, acc = 1.0000 (16.9 examples/sec; 3.785 sec/batch)
2017-05-12 15:50:53.197712: step 89640, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.779 sec/batch)
2017-05-12 15:52:08.627305: step 89660, loss = 0.0016, acc = 1.0000 (17.0 examples/sec; 3.773 sec/batch)
2017-05-12 15:53:24.518988: step 89680, loss = 0.0017, acc = 1.0000 (16.9 examples/sec; 3.790 sec/batch)
2017-05-12 15:54:40.278916: step 89700, loss = 0.0018, acc = 1.0000 (17.0 examples/sec; 3.773 sec/batch)
2017-05-12 15:55:56.918280: step 89720, loss = 0.0018, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-12 15:57:12.713475: step 89740, loss = 0.0021, acc = 1.0000 (16.7 examples/sec; 3.836 sec/batch)
2017-05-12 15:58:28.194499: step 89760, loss = 0.0019, acc = 1.0000 (17.0 examples/sec; 3.775 sec/batch)
2017-05-12 15:59:43.924515: step 89780, loss = 0.0019, acc = 1.0000 (16.8 examples/sec; 3.814 sec/batch)
2017-05-12 16:00:59.567043: step 89800, loss = 0.0015, acc = 1.0000 (17.0 examples/sec; 3.774 sec/batch)
2017-05-12 16:02:15.174274: step 89820, loss = 0.0026, acc = 1.0000 (17.0 examples/sec; 3.773 sec/batch)
2017-05-12 16:03:31.302061: step 89840, loss = 0.0016, acc = 1.0000 (16.9 examples/sec; 3.790 sec/batch)
2017-05-12 16:04:47.010894: step 89860, loss = 0.0018, acc = 1.0000 (16.6 examples/sec; 3.848 sec/batch)
2017-05-12 16:06:02.660830: step 89880, loss = 0.0019, acc = 1.0000 (16.9 examples/sec; 3.788 sec/batch)
2017-05-12 16:07:13.672265: step 89900, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-12 16:08:22.775315: step 89920, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 16:09:30.728414: step 89940, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-12 16:10:38.106457: step 89960, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 16:11:45.264157: step 89980, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-12 16:12:52.241229: step 90000, loss = 0.0042, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
[Eval] 2017-05-12 16:13:07.797434: step 90000, acc = 0.9449, f1 = 0.9431
[Test] 2017-05-12 16:13:18.428420: step 90000, acc = 0.9334, f1 = 0.9329
[Status] 2017-05-12 16:13:18.428551: step 90000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 16:14:24.510666: step 90020, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-12 16:15:33.424912: step 90040, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-12 16:16:42.973721: step 90060, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 16:17:50.659584: step 90080, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-12 16:18:57.332706: step 90100, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 16:20:04.775241: step 90120, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-12 16:21:11.292048: step 90140, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-12 16:22:18.688300: step 90160, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 16:23:24.965646: step 90180, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 16:24:31.581637: step 90200, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-12 16:25:38.130914: step 90220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 16:26:44.471374: step 90240, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 16:27:51.538037: step 90260, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 16:28:57.936400: step 90280, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 16:30:05.956145: step 90300, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-12 16:31:12.338683: step 90320, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 16:32:19.637476: step 90340, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-12 16:33:26.051354: step 90360, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-12 16:34:32.969457: step 90380, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 16:35:39.052073: step 90400, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 16:36:47.796961: step 90420, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-12 16:37:53.895310: step 90440, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 16:39:00.587448: step 90460, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-12 16:40:06.719374: step 90480, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 16:41:13.475197: step 90500, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 16:42:20.630277: step 90520, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 16:43:28.484122: step 90540, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 16:44:35.000286: step 90560, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 16:45:41.147394: step 90580, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 16:46:47.549898: step 90600, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 16:47:55.739712: step 90620, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 16:49:01.998764: step 90640, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 16:50:08.516641: step 90660, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-12 16:51:15.928948: step 90680, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-12 16:52:23.608728: step 90700, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 16:53:31.077802: step 90720, loss = 0.0031, acc = 0.9980 (18.9 examples/sec; 3.387 sec/batch)
2017-05-12 16:54:36.886773: step 90740, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 16:55:44.738499: step 90760, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 16:56:51.732294: step 90780, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 16:57:58.271499: step 90800, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 16:59:06.467777: step 90820, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-12 17:00:14.165036: step 90840, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 17:01:20.495157: step 90860, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 17:02:26.511231: step 90880, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 17:03:33.795319: step 90900, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-12 17:04:40.101842: step 90920, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 17:05:46.932774: step 90940, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 17:06:54.595108: step 90960, loss = 0.0018, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-12 17:08:00.547987: step 90980, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 17:09:07.810024: step 91000, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
[Eval] 2017-05-12 17:09:23.223210: step 91000, acc = 0.9447, f1 = 0.9429
[Test] 2017-05-12 17:09:33.987740: step 91000, acc = 0.9334, f1 = 0.9328
[Status] 2017-05-12 17:09:33.987831: step 91000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 17:10:41.336166: step 91020, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-12 17:11:48.531094: step 91040, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 17:12:55.191350: step 91060, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-12 17:14:03.281214: step 91080, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-12 17:15:09.039019: step 91100, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-12 17:16:16.424128: step 91120, loss = 0.0019, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-12 17:17:22.994446: step 91140, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-12 17:18:30.246897: step 91160, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 17:19:37.948838: step 91180, loss = 0.0018, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-12 17:20:44.245890: step 91200, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-12 17:21:51.449214: step 91220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 17:22:57.995548: step 91240, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-12 17:24:04.781177: step 91260, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 17:25:11.457410: step 91280, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 17:26:17.781313: step 91300, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 17:27:24.559952: step 91320, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 17:28:32.289263: step 91340, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-12 17:29:38.602281: step 91360, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-12 17:30:45.680001: step 91380, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 17:31:53.381771: step 91400, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.290 sec/batch)
2017-05-12 17:32:59.717606: step 91420, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-12 17:34:07.073000: step 91440, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-12 17:35:16.603609: step 91460, loss = 0.0021, acc = 1.0000 (16.9 examples/sec; 3.798 sec/batch)
2017-05-12 17:36:25.400025: step 91480, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 17:37:32.092148: step 91500, loss = 0.0026, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-12 17:38:41.007464: step 91520, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-12 17:39:49.233190: step 91540, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 17:41:01.421782: step 91560, loss = 0.0018, acc = 1.0000 (17.9 examples/sec; 3.572 sec/batch)
2017-05-12 17:42:11.600435: step 91580, loss = 0.0026, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-12 17:43:17.947434: step 91600, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 17:44:24.793356: step 91620, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-12 17:45:34.536469: step 91640, loss = 0.0029, acc = 1.0000 (18.0 examples/sec; 3.553 sec/batch)
2017-05-12 17:46:44.340321: step 91660, loss = 0.0032, acc = 0.9980 (19.3 examples/sec; 3.323 sec/batch)
2017-05-12 17:47:52.068433: step 91680, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-12 17:49:02.802883: step 91700, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-12 17:50:09.876150: step 91720, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-12 17:51:16.774321: step 91740, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 17:52:24.180194: step 91760, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-12 17:53:30.723236: step 91780, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 17:54:38.550825: step 91800, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-12 17:55:48.779760: step 91820, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 17:56:56.671110: step 91840, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-12 17:58:07.575399: step 91860, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-12 17:59:14.686132: step 91880, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-12 18:00:21.117286: step 91900, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-12 18:01:28.289146: step 91920, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 18:02:35.418848: step 91940, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-12 18:03:42.307381: step 91960, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 18:04:48.328636: step 91980, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-12 18:05:56.000596: step 92000, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
[Eval] 2017-05-12 18:06:11.417514: step 92000, acc = 0.9448, f1 = 0.9430
[Test] 2017-05-12 18:06:21.895468: step 92000, acc = 0.9335, f1 = 0.9329
[Status] 2017-05-12 18:06:21.895581: step 92000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 18:07:28.597292: step 92020, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-12 18:08:35.814937: step 92040, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 18:09:42.284197: step 92060, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 18:10:49.040626: step 92080, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-12 18:11:55.219984: step 92100, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 18:13:03.806156: step 92120, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-12 18:14:16.721595: step 92140, loss = 0.0015, acc = 1.0000 (17.8 examples/sec; 3.592 sec/batch)
2017-05-12 18:15:23.216145: step 92160, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 18:16:30.739744: step 92180, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-12 18:17:39.849991: step 92200, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 18:18:45.965000: step 92220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 18:19:52.859457: step 92240, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 18:20:59.891879: step 92260, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 18:22:05.727412: step 92280, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-12 18:23:12.591111: step 92300, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-12 18:24:19.778637: step 92320, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-12 18:25:26.375019: step 92340, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 18:26:32.537145: step 92360, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-12 18:27:39.026282: step 92380, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-12 18:28:46.357695: step 92400, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 18:29:53.731054: step 92420, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 18:31:01.453855: step 92440, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 18:32:09.277154: step 92460, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-12 18:33:15.706154: step 92480, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-12 18:34:22.763611: step 92500, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 18:35:29.938858: step 92520, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 18:36:36.585143: step 92540, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-12 18:37:43.961401: step 92560, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-12 18:38:50.268872: step 92580, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 18:39:56.866877: step 92600, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 18:41:02.958798: step 92620, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 18:42:10.006098: step 92640, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-12 18:43:16.552098: step 92660, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 18:44:23.233912: step 92680, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 18:45:29.825100: step 92700, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 18:46:35.745393: step 92720, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-12 18:47:43.595788: step 92740, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-12 18:48:51.162772: step 92760, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 18:49:58.305344: step 92780, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-12 18:51:05.143907: step 92800, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 18:52:11.061220: step 92820, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-12 18:53:18.061380: step 92840, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 18:54:24.479297: step 92860, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 18:55:31.111038: step 92880, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 18:56:37.887792: step 92900, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-12 18:57:46.021982: step 92920, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-12 18:58:52.740530: step 92940, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 18:59:59.313903: step 92960, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 19:01:06.073683: step 92980, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-12 19:02:13.620187: step 93000, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
[Eval] 2017-05-12 19:02:28.935240: step 93000, acc = 0.9451, f1 = 0.9433
[Test] 2017-05-12 19:02:39.575751: step 93000, acc = 0.9338, f1 = 0.9332
[Status] 2017-05-12 19:02:39.575829: step 93000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 19:03:46.851725: step 93020, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 19:04:52.979252: step 93040, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-12 19:05:59.409630: step 93060, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-12 19:07:06.364225: step 93080, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-12 19:08:13.011215: step 93100, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-12 19:09:19.361658: step 93120, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 19:10:25.788574: step 93140, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 19:11:33.105236: step 93160, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 19:12:39.869652: step 93180, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 19:13:46.653436: step 93200, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-12 19:14:53.442536: step 93220, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-12 19:16:01.597356: step 93240, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 19:17:08.253953: step 93260, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-12 19:18:15.466453: step 93280, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-12 19:19:22.218062: step 93300, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-12 19:20:29.045072: step 93320, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 19:21:36.839218: step 93340, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-12 19:22:43.811202: step 93360, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-12 19:23:50.745501: step 93380, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 19:24:56.863695: step 93400, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 19:26:04.410645: step 93420, loss = 0.0028, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-12 19:27:10.615839: step 93440, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-12 19:28:17.492014: step 93460, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-12 19:29:24.153124: step 93480, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-12 19:30:31.181794: step 93500, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-12 19:31:37.640030: step 93520, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 19:32:44.653459: step 93540, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 19:33:52.762862: step 93560, loss = 0.0018, acc = 1.0000 (17.6 examples/sec; 3.640 sec/batch)
2017-05-12 19:34:59.448766: step 93580, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 19:36:05.628752: step 93600, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 19:37:11.732001: step 93620, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-12 19:38:17.877144: step 93640, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 19:39:24.130256: step 93660, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 19:40:30.753204: step 93680, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 19:41:36.848360: step 93700, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 19:42:43.080506: step 93720, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-12 19:43:50.976629: step 93740, loss = 0.0019, acc = 1.0000 (17.6 examples/sec; 3.628 sec/batch)
2017-05-12 19:44:57.845452: step 93760, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 19:46:04.829339: step 93780, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 19:47:11.130727: step 93800, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 19:48:17.385527: step 93820, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-12 19:49:23.780543: step 93840, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-12 19:50:29.894306: step 93860, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 19:51:35.942758: step 93880, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 19:52:42.430281: step 93900, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-12 19:53:49.361045: step 93920, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 19:54:56.523212: step 93940, loss = 0.0026, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-12 19:56:02.833674: step 93960, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 19:57:09.141301: step 93980, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 19:58:15.361767: step 94000, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
[Eval] 2017-05-12 19:58:30.941600: step 94000, acc = 0.9451, f1 = 0.9433
[Test] 2017-05-12 19:58:41.681364: step 94000, acc = 0.9339, f1 = 0.9334
[Status] 2017-05-12 19:58:41.681454: step 94000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 19:59:49.044104: step 94020, loss = 0.0017, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-12 20:00:55.261466: step 94040, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 20:02:03.024573: step 94060, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 20:03:10.100854: step 94080, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 20:04:16.909382: step 94100, loss = 0.0045, acc = 0.9980 (19.1 examples/sec; 3.347 sec/batch)
2017-05-12 20:05:24.387093: step 94120, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-12 20:06:31.224536: step 94140, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-12 20:07:38.112439: step 94160, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-12 20:08:44.561810: step 94180, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-12 20:09:52.152181: step 94200, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 20:10:58.422666: step 94220, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-12 20:12:04.474152: step 94240, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 20:13:11.140115: step 94260, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-12 20:14:17.433187: step 94280, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 20:15:23.983912: step 94300, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 20:16:31.584486: step 94320, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-12 20:17:38.199569: step 94340, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 20:18:44.742620: step 94360, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-12 20:19:51.648810: step 94380, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 20:20:58.868164: step 94400, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-12 20:22:05.187555: step 94420, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 20:23:11.513917: step 94440, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 20:24:18.504029: step 94460, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-12 20:25:25.314757: step 94480, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-12 20:26:31.346319: step 94500, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 20:27:39.628373: step 94520, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-12 20:28:46.876214: step 94540, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 20:29:53.720647: step 94560, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-12 20:31:00.299177: step 94580, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 20:32:07.351718: step 94600, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-12 20:33:13.447479: step 94620, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-12 20:34:19.595333: step 94640, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 20:35:26.737981: step 94660, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-12 20:36:33.921937: step 94680, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 20:37:40.978594: step 94700, loss = 0.0031, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 20:38:47.599850: step 94720, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-12 20:39:54.444114: step 94740, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 20:41:03.430212: step 94760, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 20:42:10.282130: step 94780, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-12 20:43:17.254192: step 94800, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-12 20:44:23.269041: step 94820, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 20:45:30.089940: step 94840, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 20:46:37.191615: step 94860, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-12 20:47:43.931785: step 94880, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 20:48:51.099828: step 94900, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-12 20:49:59.397373: step 94920, loss = 0.0016, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-12 20:51:06.676842: step 94940, loss = 0.0016, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-12 20:52:13.469147: step 94960, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 20:53:19.520323: step 94980, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 20:54:25.517842: step 95000, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
[Eval] 2017-05-12 20:54:39.768273: step 95000, acc = 0.9452, f1 = 0.9435
[Test] 2017-05-12 20:54:50.530923: step 95000, acc = 0.9339, f1 = 0.9333
[Status] 2017-05-12 20:54:50.531021: step 95000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 20:55:56.958757: step 95020, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-12 20:57:04.459204: step 95040, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-12 20:58:10.639211: step 95060, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 20:59:19.180787: step 95080, loss = 0.0016, acc = 1.0000 (17.6 examples/sec; 3.640 sec/batch)
2017-05-12 21:00:25.146140: step 95100, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 21:01:31.866282: step 95120, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 21:02:38.108926: step 95140, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-12 21:03:44.538879: step 95160, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-12 21:04:51.736883: step 95180, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 21:05:59.030838: step 95200, loss = 0.0015, acc = 1.0000 (18.2 examples/sec; 3.520 sec/batch)
2017-05-12 21:07:04.918544: step 95220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 21:08:11.653266: step 95240, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 21:09:18.408035: step 95260, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-12 21:10:25.479192: step 95280, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 21:11:32.754481: step 95300, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-12 21:12:40.754442: step 95320, loss = 0.0015, acc = 1.0000 (17.9 examples/sec; 3.570 sec/batch)
2017-05-12 21:13:47.941033: step 95340, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 21:14:54.813598: step 95360, loss = 0.0015, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-12 21:16:01.336801: step 95380, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-12 21:17:08.023656: step 95400, loss = 0.0016, acc = 1.0000 (18.1 examples/sec; 3.542 sec/batch)
2017-05-12 21:18:15.235431: step 95420, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-12 21:19:22.755995: step 95440, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-12 21:20:30.775198: step 95460, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-12 21:21:38.316433: step 95480, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-12 21:22:44.911250: step 95500, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-12 21:23:51.159900: step 95520, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-12 21:24:58.172250: step 95540, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-12 21:26:04.813256: step 95560, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-12 21:27:11.002481: step 95580, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 21:28:17.170736: step 95600, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 21:29:23.755578: step 95620, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 21:30:29.948687: step 95640, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-12 21:31:37.256019: step 95660, loss = 0.0016, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-12 21:32:44.268594: step 95680, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-12 21:33:51.472618: step 95700, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 21:34:57.670104: step 95720, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 21:36:04.268631: step 95740, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 21:37:11.871960: step 95760, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 21:38:19.859693: step 95780, loss = 0.0027, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-12 21:39:27.057924: step 95800, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-12 21:40:35.263795: step 95820, loss = 0.0024, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-12 21:41:41.532690: step 95840, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-12 21:42:47.981720: step 95860, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 21:43:54.414526: step 95880, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-12 21:45:01.035313: step 95900, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-12 21:46:08.314913: step 95920, loss = 0.0015, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-12 21:47:15.237264: step 95940, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 21:48:22.743768: step 95960, loss = 0.0018, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-12 21:49:29.855170: step 95980, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-12 21:50:37.804402: step 96000, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
[Eval] 2017-05-12 21:50:53.215487: step 96000, acc = 0.9456, f1 = 0.9438
[Test] 2017-05-12 21:51:03.961210: step 96000, acc = 0.9339, f1 = 0.9333
[Status] 2017-05-12 21:51:03.961298: step 96000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 21:52:10.827960: step 96020, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-12 21:53:17.478178: step 96040, loss = 0.0016, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-12 21:54:24.713981: step 96060, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 21:55:31.881869: step 96080, loss = 0.0067, acc = 0.9980 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 21:56:37.888953: step 96100, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 21:57:44.618142: step 96120, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-12 21:58:51.575173: step 96140, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-12 21:59:59.775084: step 96160, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 22:01:07.087081: step 96180, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-12 22:02:12.868242: step 96200, loss = 0.0016, acc = 1.0000 (19.7 examples/sec; 3.251 sec/batch)
2017-05-12 22:03:20.130662: step 96220, loss = 0.0032, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-12 22:04:27.393964: step 96240, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 22:05:33.449733: step 96260, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 22:06:40.127110: step 96280, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-12 22:07:46.962821: step 96300, loss = 0.0030, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 22:08:53.045989: step 96320, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 22:10:00.195700: step 96340, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-12 22:11:06.980128: step 96360, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-12 22:12:14.057742: step 96380, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-12 22:13:20.220957: step 96400, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-12 22:14:28.290394: step 96420, loss = 0.0031, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-12 22:15:35.231505: step 96440, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 22:16:42.407940: step 96460, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-12 22:17:49.059119: step 96480, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-12 22:18:56.599106: step 96500, loss = 0.0016, acc = 1.0000 (18.1 examples/sec; 3.534 sec/batch)
2017-05-12 22:20:02.593929: step 96520, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-12 22:21:09.095285: step 96540, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-12 22:22:15.753166: step 96560, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-12 22:23:23.889718: step 96580, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-12 22:24:31.137228: step 96600, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-12 22:25:36.636543: step 96620, loss = 0.0028, acc = 1.0000 (19.7 examples/sec; 3.252 sec/batch)
2017-05-12 22:26:43.928304: step 96640, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 22:27:50.700131: step 96660, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-12 22:28:57.634235: step 96680, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-12 22:30:04.414574: step 96700, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-12 22:31:12.115141: step 96720, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-12 22:32:18.621401: step 96740, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-12 22:33:25.526982: step 96760, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-12 22:34:32.199802: step 96780, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 22:35:38.850036: step 96800, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-12 22:36:45.803787: step 96820, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-12 22:37:52.466632: step 96840, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-12 22:38:59.222353: step 96860, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-12 22:40:05.552086: step 96880, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-12 22:41:13.123917: step 96900, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 22:42:19.285619: step 96920, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-12 22:43:25.419403: step 96940, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-12 22:44:32.342397: step 96960, loss = 0.0032, acc = 0.9980 (19.0 examples/sec; 3.370 sec/batch)
2017-05-12 22:45:39.303432: step 96980, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 22:46:46.263296: step 97000, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
[Eval] 2017-05-12 22:47:01.531311: step 97000, acc = 0.9454, f1 = 0.9436
[Test] 2017-05-12 22:47:11.911534: step 97000, acc = 0.9339, f1 = 0.9333
[Status] 2017-05-12 22:47:11.911622: step 97000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 22:48:18.503303: step 97020, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-12 22:49:25.722606: step 97040, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-12 22:50:32.037196: step 97060, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-12 22:51:38.282763: step 97080, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 22:52:45.388489: step 97100, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-12 22:53:52.539943: step 97120, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-12 22:54:58.757761: step 97140, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-12 22:56:05.230174: step 97160, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-12 22:57:12.568552: step 97180, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-12 22:58:18.901105: step 97200, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-12 22:59:26.618127: step 97220, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-12 23:00:33.318832: step 97240, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-12 23:01:39.787834: step 97260, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-12 23:02:46.063030: step 97280, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-12 23:03:52.132033: step 97300, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-12 23:04:58.677037: step 97320, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 23:06:04.971658: step 97340, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 23:07:12.030268: step 97360, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-12 23:08:18.603896: step 97380, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-12 23:09:24.847701: step 97400, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 23:10:31.369107: step 97420, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-12 23:11:38.113399: step 97440, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-12 23:12:44.173414: step 97460, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-12 23:13:50.540709: step 97480, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-12 23:14:57.508519: step 97500, loss = 0.0015, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-12 23:16:04.228994: step 97520, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-12 23:17:10.088395: step 97540, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-12 23:18:17.845135: step 97560, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-12 23:19:25.129516: step 97580, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-12 23:20:31.438263: step 97600, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-12 23:21:38.081956: step 97620, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-12 23:22:44.512155: step 97640, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-12 23:23:51.373444: step 97660, loss = 0.0060, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
2017-05-12 23:24:57.489918: step 97680, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-12 23:26:04.822885: step 97700, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-12 23:27:11.835452: step 97720, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-12 23:28:18.204722: step 97740, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-12 23:29:24.613964: step 97760, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-12 23:30:32.446324: step 97780, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-12 23:31:39.989109: step 97800, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-12 23:32:46.260968: step 97820, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-12 23:33:52.663759: step 97840, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-12 23:34:58.756607: step 97860, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-12 23:36:06.424226: step 97880, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-12 23:37:12.789301: step 97900, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-12 23:38:19.705523: step 97920, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-12 23:39:26.385082: step 97940, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-12 23:40:33.902678: step 97960, loss = 0.0042, acc = 0.9980 (19.4 examples/sec; 3.291 sec/batch)
2017-05-12 23:41:41.677871: step 97980, loss = 0.0027, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-12 23:42:48.740069: step 98000, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
[Eval] 2017-05-12 23:43:04.120133: step 98000, acc = 0.9452, f1 = 0.9435
[Test] 2017-05-12 23:43:14.758106: step 98000, acc = 0.9336, f1 = 0.9331
[Status] 2017-05-12 23:43:14.758336: step 98000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-12 23:44:21.955009: step 98020, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-12 23:45:29.579878: step 98040, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-12 23:46:35.961348: step 98060, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-12 23:47:42.992588: step 98080, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-12 23:48:49.392305: step 98100, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-12 23:49:55.785756: step 98120, loss = 0.0029, acc = 0.9980 (19.6 examples/sec; 3.258 sec/batch)
2017-05-12 23:51:03.224249: step 98140, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-12 23:52:09.853225: step 98160, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-12 23:53:16.418183: step 98180, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-12 23:54:23.727536: step 98200, loss = 0.0029, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-12 23:55:31.193244: step 98220, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-12 23:56:38.382448: step 98240, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-12 23:57:46.480040: step 98260, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-12 23:58:53.562404: step 98280, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-13 00:00:00.396129: step 98300, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-13 00:01:06.495808: step 98320, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-13 00:02:13.828311: step 98340, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-13 00:03:20.765000: step 98360, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 00:04:26.906588: step 98380, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 00:05:33.314011: step 98400, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 00:06:40.650275: step 98420, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-13 00:07:46.427042: step 98440, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 00:08:54.437023: step 98460, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 00:10:00.778037: step 98480, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 00:11:07.034331: step 98500, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 00:12:13.013512: step 98520, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 00:13:20.749277: step 98540, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-13 00:14:26.890270: step 98560, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 00:15:34.231859: step 98580, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-13 00:16:40.260628: step 98600, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-13 00:17:47.869894: step 98620, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 00:18:55.569979: step 98640, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 00:20:02.387696: step 98660, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 00:21:08.603918: step 98680, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 00:22:14.856216: step 98700, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 00:23:23.772808: step 98720, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 00:24:30.401653: step 98740, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 00:25:36.342943: step 98760, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 00:26:43.191611: step 98780, loss = 0.0015, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-13 00:27:50.509707: step 98800, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-13 00:28:58.310734: step 98820, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 00:30:05.339277: step 98840, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-13 00:31:11.372235: step 98860, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 00:32:17.985023: step 98880, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 00:33:25.612579: step 98900, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 00:34:32.278834: step 98920, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-13 00:35:39.378742: step 98940, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 00:36:45.408977: step 98960, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-13 00:37:52.670128: step 98980, loss = 0.0028, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-13 00:38:59.275996: step 99000, loss = 0.0027, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
[Eval] 2017-05-13 00:39:14.806042: step 99000, acc = 0.9454, f1 = 0.9436
[Test] 2017-05-13 00:39:25.522746: step 99000, acc = 0.9336, f1 = 0.9331
[Status] 2017-05-13 00:39:25.522846: step 99000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 00:40:31.537251: step 99020, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-13 00:41:38.150946: step 99040, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-13 00:42:45.470319: step 99060, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 00:43:52.111316: step 99080, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 00:45:00.302448: step 99100, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 00:46:06.465785: step 99120, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 00:47:13.090669: step 99140, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 00:48:19.151215: step 99160, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-13 00:49:25.293063: step 99180, loss = 0.0030, acc = 0.9980 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 00:50:32.295014: step 99200, loss = 0.0014, acc = 1.0000 (17.8 examples/sec; 3.586 sec/batch)
2017-05-13 00:51:40.051642: step 99220, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-13 00:52:47.203195: step 99240, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 00:53:55.039757: step 99260, loss = 0.0041, acc = 0.9980 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 00:55:01.758459: step 99280, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 00:56:08.562560: step 99300, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 00:57:15.319378: step 99320, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-13 00:58:23.007501: step 99340, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-13 00:59:30.167744: step 99360, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 01:00:37.192521: step 99380, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 01:01:44.052665: step 99400, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 01:02:51.058827: step 99420, loss = 0.0038, acc = 0.9980 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 01:03:57.854120: step 99440, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-13 01:05:03.849019: step 99460, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 01:06:09.848747: step 99480, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 01:07:16.200066: step 99500, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 01:08:22.970337: step 99520, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-13 01:09:29.816529: step 99540, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-13 01:10:36.579034: step 99560, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-13 01:11:43.433050: step 99580, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 01:12:50.150275: step 99600, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 01:13:56.207092: step 99620, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-13 01:15:02.532566: step 99640, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 01:16:09.714044: step 99660, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-13 01:17:17.052209: step 99680, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 01:18:22.865012: step 99700, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 01:19:29.542099: step 99720, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-13 01:20:36.658625: step 99740, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-13 01:21:43.395171: step 99760, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-13 01:22:52.440096: step 99780, loss = 0.0016, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-13 01:23:59.422730: step 99800, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 01:25:05.758168: step 99820, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 01:26:12.647641: step 99840, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-13 01:27:18.875240: step 99860, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-13 01:28:26.231176: step 99880, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-13 01:29:31.883347: step 99900, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 01:30:38.154438: step 99920, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-13 01:31:45.593923: step 99940, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-13 01:32:53.098893: step 99960, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-13 01:34:00.005429: step 99980, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 01:35:07.536255: step 100000, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
[Eval] 2017-05-13 01:35:22.918999: step 100000, acc = 0.9450, f1 = 0.9432
[Test] 2017-05-13 01:35:33.627361: step 100000, acc = 0.9339, f1 = 0.9333
[Status] 2017-05-13 01:35:33.627454: step 100000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 01:36:40.571005: step 100020, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-13 01:37:47.599794: step 100040, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 01:38:54.964381: step 100060, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-13 01:40:02.022509: step 100080, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-13 01:41:08.941590: step 100100, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 01:42:14.992980: step 100120, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 01:43:21.198896: step 100140, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 01:44:29.348759: step 100160, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 01:45:35.788340: step 100180, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 01:46:42.803626: step 100200, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 01:47:50.225368: step 100220, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-13 01:48:57.584952: step 100240, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 01:50:05.137972: step 100260, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 01:51:12.005514: step 100280, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 01:52:18.671174: step 100300, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 01:53:26.573057: step 100320, loss = 0.0014, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-13 01:54:33.415122: step 100340, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-13 01:55:41.240182: step 100360, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-13 01:56:48.648293: step 100380, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-13 01:57:56.704802: step 100400, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 01:59:03.183337: step 100420, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-13 02:00:09.519272: step 100440, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 02:01:15.874199: step 100460, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 02:02:22.626856: step 100480, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 02:03:28.914269: step 100500, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 02:04:35.099661: step 100520, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 02:05:43.578865: step 100540, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-13 02:06:51.144410: step 100560, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-13 02:07:58.379482: step 100580, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-13 02:09:05.281176: step 100600, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 02:10:12.513435: step 100620, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 02:11:20.197724: step 100640, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-13 02:12:26.535176: step 100660, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 02:13:32.230879: step 100680, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 02:14:40.053582: step 100700, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-13 02:15:46.354184: step 100720, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 02:16:54.276040: step 100740, loss = 0.0028, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-13 02:18:02.090692: step 100760, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 02:19:11.886717: step 100780, loss = 0.0015, acc = 1.0000 (15.5 examples/sec; 4.117 sec/batch)
2017-05-13 02:20:19.478857: step 100800, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 02:21:28.475684: step 100820, loss = 0.0015, acc = 1.0000 (17.1 examples/sec; 3.748 sec/batch)
2017-05-13 02:22:35.078105: step 100840, loss = 0.0079, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 02:23:43.430749: step 100860, loss = 0.0015, acc = 1.0000 (16.9 examples/sec; 3.782 sec/batch)
2017-05-13 02:24:49.706136: step 100880, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 02:25:56.512561: step 100900, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 02:27:03.624809: step 100920, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-13 02:28:10.046599: step 100940, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 02:29:16.569314: step 100960, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 02:30:23.258354: step 100980, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 02:31:30.039837: step 101000, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
[Eval] 2017-05-13 02:31:45.450783: step 101000, acc = 0.9445, f1 = 0.9428
[Test] 2017-05-13 02:31:56.114080: step 101000, acc = 0.9333, f1 = 0.9328
[Status] 2017-05-13 02:31:56.114198: step 101000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 02:33:02.666444: step 101020, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 02:34:09.242823: step 101040, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 02:35:16.952132: step 101060, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-13 02:36:23.639757: step 101080, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 02:37:30.357941: step 101100, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 02:38:36.431230: step 101120, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-13 02:39:43.386436: step 101140, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-13 02:40:49.275444: step 101160, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 02:41:56.185937: step 101180, loss = 0.0032, acc = 0.9980 (19.0 examples/sec; 3.374 sec/batch)
2017-05-13 02:43:02.742848: step 101200, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 02:44:08.938042: step 101220, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 02:45:16.308433: step 101240, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 02:46:24.365697: step 101260, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-13 02:47:30.724727: step 101280, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 02:48:36.934193: step 101300, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 02:49:43.464466: step 101320, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 02:50:50.991070: step 101340, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-13 02:51:57.285434: step 101360, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 02:53:03.805421: step 101380, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-13 02:54:11.003719: step 101400, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-13 02:55:18.084990: step 101420, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 02:56:25.228413: step 101440, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-13 02:57:31.119846: step 101460, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 02:58:38.868545: step 101480, loss = 0.0029, acc = 0.9980 (18.4 examples/sec; 3.481 sec/batch)
2017-05-13 02:59:46.038284: step 101500, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-13 03:00:52.197377: step 101520, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 03:01:58.558339: step 101540, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-13 03:03:05.567774: step 101560, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-13 03:04:13.755188: step 101580, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-13 03:05:21.888477: step 101600, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-13 03:06:28.502645: step 101620, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 03:07:36.590667: step 101640, loss = 0.0026, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-13 03:08:42.502858: step 101660, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 03:09:48.741953: step 101680, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 03:10:56.728220: step 101700, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-13 03:12:03.989675: step 101720, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-13 03:13:12.347437: step 101740, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-13 03:14:19.920902: step 101760, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-13 03:15:26.377073: step 101780, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 03:16:32.615793: step 101800, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 03:17:40.485720: step 101820, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-13 03:18:46.632639: step 101840, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-13 03:19:53.746897: step 101860, loss = 0.0029, acc = 0.9980 (19.2 examples/sec; 3.336 sec/batch)
2017-05-13 03:21:00.342880: step 101880, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-13 03:22:07.467568: step 101900, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-13 03:23:14.197263: step 101920, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-13 03:24:21.772935: step 101940, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-13 03:25:30.707244: step 101960, loss = 0.0015, acc = 1.0000 (17.7 examples/sec; 3.611 sec/batch)
2017-05-13 03:26:37.877797: step 101980, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-13 03:27:44.833710: step 102000, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
[Eval] 2017-05-13 03:28:00.122778: step 102000, acc = 0.9454, f1 = 0.9436
[Test] 2017-05-13 03:28:10.630334: step 102000, acc = 0.9339, f1 = 0.9333
[Status] 2017-05-13 03:28:10.630429: step 102000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 03:29:17.808274: step 102020, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 03:30:24.748521: step 102040, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-13 03:31:31.673710: step 102060, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-13 03:32:39.718720: step 102080, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-13 03:33:45.969678: step 102100, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 03:34:53.721050: step 102120, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 03:35:59.973626: step 102140, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-13 03:37:06.446137: step 102160, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 03:38:13.743423: step 102180, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 03:39:21.146939: step 102200, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 03:40:27.858057: step 102220, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 03:41:34.460625: step 102240, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 03:42:42.907085: step 102260, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 03:43:49.633054: step 102280, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 03:44:56.750111: step 102300, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 03:46:04.031459: step 102320, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-13 03:47:10.794766: step 102340, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 03:48:18.081495: step 102360, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 03:49:25.521961: step 102380, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-13 03:50:32.899421: step 102400, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-13 03:51:40.405675: step 102420, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 03:52:49.019341: step 102440, loss = 0.0014, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-13 03:53:55.592123: step 102460, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 03:55:02.216292: step 102480, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-13 03:56:09.150384: step 102500, loss = 0.0103, acc = 0.9960 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 03:57:15.921970: step 102520, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 03:58:22.232645: step 102540, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-13 03:59:30.757020: step 102560, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-13 04:00:37.637472: step 102580, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 04:01:44.314865: step 102600, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 04:02:52.287355: step 102620, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 04:03:59.629413: step 102640, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 04:05:05.877303: step 102660, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 04:06:13.754023: step 102680, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-13 04:07:20.269141: step 102700, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 04:08:28.389126: step 102720, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 04:09:35.801341: step 102740, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 04:10:41.866888: step 102760, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 04:11:47.764156: step 102780, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 04:12:54.259485: step 102800, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-13 04:14:02.751686: step 102820, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-13 04:15:10.043265: step 102840, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 04:16:16.419300: step 102860, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 04:17:23.678809: step 102880, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-13 04:18:29.926552: step 102900, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 04:19:36.519966: step 102920, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 04:20:43.316582: step 102940, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-13 04:21:49.815033: step 102960, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-13 04:22:56.228753: step 102980, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 04:24:02.236882: step 103000, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
[Eval] 2017-05-13 04:24:17.645035: step 103000, acc = 0.9451, f1 = 0.9433
[Test] 2017-05-13 04:24:27.978739: step 103000, acc = 0.9341, f1 = 0.9335
[Status] 2017-05-13 04:24:27.978825: step 103000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 04:25:34.904174: step 103020, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 04:26:41.266582: step 103040, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 04:27:48.515272: step 103060, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 04:28:55.216290: step 103080, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-13 04:30:02.130492: step 103100, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 04:31:09.900107: step 103120, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 04:32:16.709776: step 103140, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 04:33:23.867148: step 103160, loss = 0.0030, acc = 0.9980 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 04:34:31.659215: step 103180, loss = 0.0019, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-13 04:35:38.486754: step 103200, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-13 04:36:45.429371: step 103220, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-13 04:37:53.519794: step 103240, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-13 04:39:01.122337: step 103260, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 04:40:07.524301: step 103280, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 04:41:14.366276: step 103300, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 04:42:20.308174: step 103320, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 04:43:26.764005: step 103340, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 04:44:33.514801: step 103360, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 04:45:39.744991: step 103380, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 04:46:45.976103: step 103400, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 04:47:52.825029: step 103420, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-13 04:48:59.855152: step 103440, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 04:50:06.965701: step 103460, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-13 04:51:13.116125: step 103480, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 04:52:18.943425: step 103500, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-13 04:53:25.436398: step 103520, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 04:54:31.858223: step 103540, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-13 04:55:39.469999: step 103560, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-13 04:56:46.360852: step 103580, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 04:57:54.085491: step 103600, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-13 04:59:00.705581: step 103620, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-13 05:00:07.100296: step 103640, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-13 05:01:15.145788: step 103660, loss = 0.0049, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 05:02:22.526334: step 103680, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-13 05:03:29.857869: step 103700, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 05:04:36.225490: step 103720, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 05:05:42.730244: step 103740, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 05:06:49.074789: step 103760, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 05:07:55.398388: step 103780, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 05:09:02.035558: step 103800, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-13 05:10:09.863210: step 103820, loss = 0.0014, acc = 1.0000 (18.0 examples/sec; 3.559 sec/batch)
2017-05-13 05:11:17.632150: step 103840, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-13 05:12:25.137698: step 103860, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-13 05:13:32.015625: step 103880, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 05:14:38.776568: step 103900, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 05:15:45.433963: step 103920, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-13 05:16:51.831490: step 103940, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 05:17:59.391034: step 103960, loss = 0.0022, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-13 05:19:06.786935: step 103980, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 05:20:13.543685: step 104000, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
[Eval] 2017-05-13 05:20:28.990462: step 104000, acc = 0.9455, f1 = 0.9437
[Test] 2017-05-13 05:20:39.619402: step 104000, acc = 0.9340, f1 = 0.9335
[Status] 2017-05-13 05:20:39.619487: step 104000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 05:21:47.183915: step 104020, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-13 05:22:54.248820: step 104040, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 05:24:00.296200: step 104060, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 05:25:07.428144: step 104080, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 05:26:14.228700: step 104100, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-13 05:27:21.052308: step 104120, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 05:28:27.195509: step 104140, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 05:29:35.032264: step 104160, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 05:30:41.236499: step 104180, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 05:31:48.174200: step 104200, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-13 05:32:54.702982: step 104220, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-13 05:34:02.128063: step 104240, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-13 05:35:08.147043: step 104260, loss = 0.0034, acc = 0.9980 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 05:36:14.468382: step 104280, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-13 05:37:20.937820: step 104300, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-13 05:38:27.949529: step 104320, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 05:39:34.809781: step 104340, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-13 05:40:41.319654: step 104360, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 05:41:49.659000: step 104380, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 05:42:56.844487: step 104400, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-13 05:44:04.068010: step 104420, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 05:45:11.760802: step 104440, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-13 05:46:19.216384: step 104460, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-13 05:47:26.193129: step 104480, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 05:48:32.396896: step 104500, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 05:49:39.408862: step 104520, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 05:50:46.348387: step 104540, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-13 05:51:52.906296: step 104560, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-13 05:52:59.601135: step 104580, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-13 05:54:05.475993: step 104600, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 05:55:12.414759: step 104620, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-13 05:56:18.958009: step 104640, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 05:57:25.708969: step 104660, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-13 05:58:33.600200: step 104680, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 05:59:40.644559: step 104700, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-13 06:00:48.036056: step 104720, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 06:01:55.753148: step 104740, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 06:03:02.626739: step 104760, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 06:04:10.953976: step 104780, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-13 06:05:18.144170: step 104800, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 06:06:24.985044: step 104820, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 06:07:32.302742: step 104840, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 06:08:39.370043: step 104860, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-13 06:09:46.156840: step 104880, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 06:10:53.436896: step 104900, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 06:12:00.273323: step 104920, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-13 06:13:07.012126: step 104940, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-13 06:14:14.043497: step 104960, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 06:15:21.227709: step 104980, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-13 06:16:29.441733: step 105000, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
[Eval] 2017-05-13 06:16:44.971539: step 105000, acc = 0.9453, f1 = 0.9436
[Test] 2017-05-13 06:16:55.066579: step 105000, acc = 0.9341, f1 = 0.9335
[Status] 2017-05-13 06:16:55.066677: step 105000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 06:18:01.470472: step 105020, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 06:19:08.040615: step 105040, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 06:20:14.130811: step 105060, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 06:21:21.965849: step 105080, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-13 06:22:28.282658: step 105100, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 06:23:34.880583: step 105120, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 06:24:41.312015: step 105140, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-13 06:25:48.372230: step 105160, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-13 06:26:55.125969: step 105180, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-13 06:28:01.379640: step 105200, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 06:29:07.918999: step 105220, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-13 06:30:14.682648: step 105240, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-13 06:31:20.950336: step 105260, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-13 06:32:27.831609: step 105280, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-13 06:33:34.487479: step 105300, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-13 06:34:41.767895: step 105320, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-13 06:35:48.081689: step 105340, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-13 06:36:54.550856: step 105360, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 06:38:01.145105: step 105380, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 06:39:07.237229: step 105400, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 06:40:14.646693: step 105420, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-13 06:41:22.258861: step 105440, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 06:42:28.942003: step 105460, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-13 06:43:35.395444: step 105480, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-13 06:44:41.903320: step 105500, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-13 06:45:48.269279: step 105520, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 06:46:55.473824: step 105540, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 06:48:02.539029: step 105560, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-13 06:49:10.375784: step 105580, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 06:50:17.435808: step 105600, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-13 06:51:24.962184: step 105620, loss = 0.0026, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-13 06:52:30.875151: step 105640, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-13 06:53:37.190868: step 105660, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-13 06:54:44.655080: step 105680, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 06:55:51.696021: step 105700, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 06:56:59.437427: step 105720, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-13 06:58:06.249783: step 105740, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-13 06:59:13.320177: step 105760, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 07:00:20.161270: step 105780, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 07:01:26.239059: step 105800, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 07:02:33.693192: step 105820, loss = 0.0016, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-13 07:03:41.165350: step 105840, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 07:04:47.438256: step 105860, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-13 07:05:53.959273: step 105880, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 07:07:01.989860: step 105900, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-13 07:08:09.321685: step 105920, loss = 0.0025, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-13 07:09:16.108436: step 105940, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-13 07:10:22.333100: step 105960, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 07:11:29.571637: step 105980, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 07:12:36.824142: step 106000, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
[Eval] 2017-05-13 07:12:52.103345: step 106000, acc = 0.9449, f1 = 0.9433
[Test] 2017-05-13 07:13:02.742239: step 106000, acc = 0.9334, f1 = 0.9328
[Status] 2017-05-13 07:13:02.742359: step 106000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 07:14:09.901407: step 106020, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 07:15:16.571885: step 106040, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 07:16:23.446482: step 106060, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 07:17:29.847064: step 106080, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 07:18:37.445475: step 106100, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-13 07:19:43.174021: step 106120, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 07:20:50.130105: step 106140, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-13 07:21:57.354529: step 106160, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 07:23:05.501044: step 106180, loss = 0.0014, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-13 07:24:12.338932: step 106200, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 07:25:18.757084: step 106220, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 07:26:25.484106: step 106240, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-13 07:27:32.118480: step 106260, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 07:28:38.722447: step 106280, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-13 07:29:45.236467: step 106300, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 07:30:51.959718: step 106320, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 07:31:58.840504: step 106340, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-13 07:33:05.455443: step 106360, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 07:34:12.069915: step 106380, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 07:35:18.623211: step 106400, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 07:36:26.208698: step 106420, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-13 07:37:33.803179: step 106440, loss = 0.0016, acc = 1.0000 (18.0 examples/sec; 3.551 sec/batch)
2017-05-13 07:38:40.311680: step 106460, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-13 07:39:47.220255: step 106480, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-13 07:40:54.749074: step 106500, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 07:42:00.377257: step 106520, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-13 07:43:08.053341: step 106540, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-13 07:44:15.393874: step 106560, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-13 07:45:22.033927: step 106580, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 07:46:30.277196: step 106600, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-13 07:47:36.433759: step 106620, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 07:48:43.297330: step 106640, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 07:49:51.255377: step 106660, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-13 07:50:57.459817: step 106680, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 07:52:03.835241: step 106700, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 07:53:10.270474: step 106720, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 07:54:18.026209: step 106740, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-13 07:55:25.672876: step 106760, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-13 07:56:32.683122: step 106780, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 07:57:40.507857: step 106800, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-13 07:58:47.098947: step 106820, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-13 07:59:53.808652: step 106840, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 08:01:01.252975: step 106860, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-13 08:02:09.809306: step 106880, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-13 08:03:16.595441: step 106900, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-13 08:04:23.445944: step 106920, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 08:05:30.680084: step 106940, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 08:06:37.441981: step 106960, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 08:07:43.975378: step 106980, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 08:08:50.767874: step 107000, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
[Eval] 2017-05-13 08:09:06.065721: step 107000, acc = 0.9451, f1 = 0.9433
[Test] 2017-05-13 08:09:16.851929: step 107000, acc = 0.9335, f1 = 0.9330
[Status] 2017-05-13 08:09:16.852040: step 107000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 08:10:23.131359: step 107020, loss = 0.0014, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-13 08:11:30.163772: step 107040, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 08:12:37.129172: step 107060, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-13 08:13:44.557965: step 107080, loss = 0.0015, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-13 08:14:50.683978: step 107100, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 08:15:57.219776: step 107120, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 08:17:04.784952: step 107140, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 08:18:11.082678: step 107160, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-13 08:19:17.533225: step 107180, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 08:20:24.777283: step 107200, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-13 08:21:32.046692: step 107220, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 08:22:38.810962: step 107240, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 08:23:46.784186: step 107260, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 08:24:53.573148: step 107280, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 08:25:59.606289: step 107300, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 08:27:05.618889: step 107320, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 08:28:12.763975: step 107340, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-13 08:29:19.107497: step 107360, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 08:30:25.765942: step 107380, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-13 08:31:32.491150: step 107400, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-13 08:32:39.474175: step 107420, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-13 08:33:45.520729: step 107440, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 08:34:51.927903: step 107460, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-13 08:35:59.691029: step 107480, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 08:37:06.675118: step 107500, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-13 08:38:12.538651: step 107520, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 08:39:19.793699: step 107540, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 08:40:26.136873: step 107560, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 08:41:32.789405: step 107580, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 08:42:39.123098: step 107600, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 08:43:46.619994: step 107620, loss = 2.0502, acc = 0.7740 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 08:44:52.686954: step 107640, loss = 0.4884, acc = 0.9480 (19.0 examples/sec; 3.360 sec/batch)
2017-05-13 08:45:59.061437: step 107660, loss = 0.1377, acc = 0.9980 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 08:47:05.681688: step 107680, loss = 0.1495, acc = 0.9980 (18.3 examples/sec; 3.501 sec/batch)
2017-05-13 08:48:13.190076: step 107700, loss = 0.2182, acc = 0.9960 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 08:49:19.785550: step 107720, loss = 0.1704, acc = 0.9960 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 08:50:27.122023: step 107740, loss = 0.1561, acc = 0.9980 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 08:51:33.445808: step 107760, loss = 0.1897, acc = 0.9980 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 08:52:40.075048: step 107780, loss = 0.1431, acc = 0.9960 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 08:53:46.149390: step 107800, loss = 0.1438, acc = 0.9960 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 08:54:52.465828: step 107820, loss = 0.1268, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 08:55:59.317768: step 107840, loss = 0.1230, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-13 08:57:07.989499: step 107860, loss = 0.1184, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-13 08:58:14.915457: step 107880, loss = 0.1143, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 08:59:21.548139: step 107900, loss = 0.1131, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 09:00:27.573152: step 107920, loss = 0.1073, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 09:01:33.835130: step 107940, loss = 0.1268, acc = 0.9980 (19.5 examples/sec; 3.278 sec/batch)
2017-05-13 09:02:40.375551: step 107960, loss = 0.1013, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 09:03:46.931827: step 107980, loss = 0.0987, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-13 09:04:53.435450: step 108000, loss = 0.0980, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-13 09:05:08.724388: step 108000, acc = 0.9370, f1 = 0.9350
[Test] 2017-05-13 09:05:19.043436: step 108000, acc = 0.9230, f1 = 0.9223
[Status] 2017-05-13 09:05:19.043518: step 108000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 09:06:25.701993: step 108020, loss = 0.0942, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-13 09:07:33.828622: step 108040, loss = 0.0908, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 09:08:40.795532: step 108060, loss = 0.0884, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-13 09:09:48.329045: step 108080, loss = 0.0860, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-13 09:10:54.729799: step 108100, loss = 0.0957, acc = 0.9960 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 09:12:01.647860: step 108120, loss = 0.0816, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 09:13:08.352191: step 108140, loss = 0.0814, acc = 0.9980 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 09:14:14.808430: step 108160, loss = 0.0778, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 09:15:21.977471: step 108180, loss = 0.0756, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-13 09:16:29.144983: step 108200, loss = 0.0737, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-13 09:17:35.351082: step 108220, loss = 0.0719, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 09:18:43.572822: step 108240, loss = 0.0702, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 09:19:49.792188: step 108260, loss = 0.0686, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 09:20:56.722999: step 108280, loss = 0.0672, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-13 09:22:03.805700: step 108300, loss = 0.0653, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-13 09:23:10.373251: step 108320, loss = 0.0788, acc = 0.9980 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 09:24:17.906802: step 108340, loss = 0.0884, acc = 0.9960 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 09:25:24.787536: step 108360, loss = 0.0611, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 09:26:31.795418: step 108380, loss = 0.0600, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 09:27:38.710771: step 108400, loss = 0.0580, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 09:28:45.760579: step 108420, loss = 0.0567, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-13 09:29:52.335778: step 108440, loss = 0.0554, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-13 09:31:00.175213: step 108460, loss = 0.0543, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-13 09:32:07.270312: step 108480, loss = 0.0532, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-13 09:33:13.634804: step 108500, loss = 0.0521, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 09:34:20.493861: step 108520, loss = 0.0506, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 09:35:26.381649: step 108540, loss = 0.0496, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 09:36:33.490441: step 108560, loss = 0.0483, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-13 09:37:39.721058: step 108580, loss = 0.0474, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 09:38:47.279949: step 108600, loss = 0.0462, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-13 09:39:53.016519: step 108620, loss = 0.0451, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 09:40:59.427605: step 108640, loss = 0.0441, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 09:42:07.346963: step 108660, loss = 0.0432, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 09:43:14.278879: step 108680, loss = 0.0424, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-13 09:44:21.231806: step 108700, loss = 0.0413, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 09:45:27.681423: step 108720, loss = 0.0404, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-13 09:46:34.275462: step 108740, loss = 0.0420, acc = 0.9980 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 09:47:41.161161: step 108760, loss = 0.0387, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 09:48:49.154555: step 108780, loss = 0.0378, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-13 09:49:56.483238: step 108800, loss = 0.0370, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 09:51:03.516535: step 108820, loss = 0.0362, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 09:52:09.626466: step 108840, loss = 0.0354, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-13 09:53:16.766251: step 108860, loss = 0.0346, acc = 1.0000 (17.9 examples/sec; 3.569 sec/batch)
2017-05-13 09:54:23.066943: step 108880, loss = 0.0339, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-13 09:55:30.687389: step 108900, loss = 0.0331, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 09:56:36.980118: step 108920, loss = 0.0328, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 09:57:43.250047: step 108940, loss = 0.0317, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 09:58:50.144704: step 108960, loss = 0.0323, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 09:59:56.582561: step 108980, loss = 0.0304, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 10:01:03.524115: step 109000, loss = 0.0298, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
[Eval] 2017-05-13 10:01:17.651963: step 109000, acc = 0.9379, f1 = 0.9358
[Test] 2017-05-13 10:01:27.759806: step 109000, acc = 0.9254, f1 = 0.9248
[Status] 2017-05-13 10:01:27.759915: step 109000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 10:02:35.009769: step 109020, loss = 0.0304, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-13 10:03:41.593429: step 109040, loss = 0.0285, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 10:04:48.273412: step 109060, loss = 0.0279, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 10:05:56.718906: step 109080, loss = 0.0277, acc = 1.0000 (17.8 examples/sec; 3.596 sec/batch)
2017-05-13 10:07:04.145999: step 109100, loss = 0.0267, acc = 1.0000 (17.8 examples/sec; 3.589 sec/batch)
2017-05-13 10:08:11.669849: step 109120, loss = 0.0267, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-13 10:09:18.063957: step 109140, loss = 0.0256, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 10:10:24.473027: step 109160, loss = 0.0250, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 10:11:30.886840: step 109180, loss = 0.0245, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 10:12:37.587245: step 109200, loss = 0.0240, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 10:13:44.797768: step 109220, loss = 0.0235, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 10:14:51.002106: step 109240, loss = 0.0230, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 10:15:57.198746: step 109260, loss = 0.0225, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 10:17:04.087067: step 109280, loss = 0.0220, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 10:18:10.468985: step 109300, loss = 0.0216, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 10:19:17.955572: step 109320, loss = 0.0211, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-13 10:20:24.878898: step 109340, loss = 0.0215, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-13 10:21:33.283879: step 109360, loss = 0.0203, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 10:22:40.261962: step 109380, loss = 0.0198, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-13 10:23:47.497304: step 109400, loss = 0.0194, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-13 10:24:54.631168: step 109420, loss = 0.0190, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-13 10:26:02.502470: step 109440, loss = 0.0186, acc = 1.0000 (18.1 examples/sec; 3.537 sec/batch)
2017-05-13 10:27:09.062579: step 109460, loss = 0.0260, acc = 0.9980 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 10:28:15.968168: step 109480, loss = 0.0178, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 10:29:21.663534: step 109500, loss = 0.0175, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-13 10:30:28.196954: step 109520, loss = 0.0171, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 10:31:34.281260: step 109540, loss = 0.0167, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-13 10:32:42.487453: step 109560, loss = 0.0163, acc = 1.0000 (17.3 examples/sec; 3.692 sec/batch)
2017-05-13 10:33:49.795981: step 109580, loss = 0.0160, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-13 10:34:56.216973: step 109600, loss = 0.0163, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 10:36:04.190082: step 109620, loss = 0.0153, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-13 10:37:11.777144: step 109640, loss = 0.0150, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-13 10:38:18.581542: step 109660, loss = 0.0161, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 10:39:24.888315: step 109680, loss = 0.0173, acc = 0.9980 (18.8 examples/sec; 3.413 sec/batch)
2017-05-13 10:40:31.462825: step 109700, loss = 0.0142, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 10:41:38.378929: step 109720, loss = 0.0139, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-13 10:42:45.502546: step 109740, loss = 0.0136, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 10:43:53.695855: step 109760, loss = 0.0132, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-13 10:45:01.257289: step 109780, loss = 0.0131, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 10:46:08.461009: step 109800, loss = 0.0127, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 10:47:15.068297: step 109820, loss = 0.0129, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-13 10:48:22.064439: step 109840, loss = 0.0124, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-13 10:49:30.513878: step 109860, loss = 0.0119, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-13 10:50:38.059438: step 109880, loss = 0.0117, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 10:51:44.071293: step 109900, loss = 0.0115, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-13 10:52:51.248409: step 109920, loss = 0.0112, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-13 10:53:57.814161: step 109940, loss = 0.0110, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 10:55:04.288612: step 109960, loss = 0.0107, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-13 10:56:10.910407: step 109980, loss = 0.0104, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 10:57:18.303133: step 110000, loss = 0.0103, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
[Eval] 2017-05-13 10:57:33.922417: step 110000, acc = 0.9386, f1 = 0.9366
[Test] 2017-05-13 10:57:44.486866: step 110000, acc = 0.9268, f1 = 0.9262
[Status] 2017-05-13 10:57:44.486963: step 110000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 10:58:51.374790: step 110020, loss = 0.0101, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 10:59:58.613298: step 110040, loss = 0.0098, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-13 11:01:06.118539: step 110060, loss = 0.0096, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-13 11:02:12.800321: step 110080, loss = 0.0094, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 11:03:19.922058: step 110100, loss = 0.0092, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 11:04:25.873732: step 110120, loss = 0.0090, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 11:05:33.832218: step 110140, loss = 0.0089, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 11:06:39.711383: step 110160, loss = 0.0087, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 11:07:47.569726: step 110180, loss = 0.0086, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 11:08:54.819921: step 110200, loss = 0.0085, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-13 11:10:00.664363: step 110220, loss = 0.0081, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 11:11:08.149308: step 110240, loss = 0.0081, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-13 11:12:15.121366: step 110260, loss = 0.0079, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-13 11:13:21.312646: step 110280, loss = 0.0077, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 11:14:27.491084: step 110300, loss = 0.0074, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 11:15:33.927149: step 110320, loss = 0.0073, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 11:16:40.934569: step 110340, loss = 0.0078, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-13 11:17:47.867568: step 110360, loss = 0.0071, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-13 11:18:54.371113: step 110380, loss = 0.0068, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 11:20:00.559185: step 110400, loss = 0.0069, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 11:21:07.003144: step 110420, loss = 0.0066, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 11:22:13.944615: step 110440, loss = 0.0065, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 11:23:20.581923: step 110460, loss = 0.0062, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 11:24:27.476718: step 110480, loss = 0.0072, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 11:25:33.648381: step 110500, loss = 0.0060, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 11:26:42.659571: step 110520, loss = 0.0063, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-13 11:27:49.997244: step 110540, loss = 0.0059, acc = 1.0000 (18.1 examples/sec; 3.535 sec/batch)
2017-05-13 11:28:56.222541: step 110560, loss = 0.0057, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 11:30:02.531626: step 110580, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 11:31:11.070756: step 110600, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 11:32:17.138939: step 110620, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 11:33:24.380803: step 110640, loss = 0.0054, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 11:34:30.510275: step 110660, loss = 0.0051, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 11:35:37.816738: step 110680, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-13 11:36:44.199104: step 110700, loss = 0.0051, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-13 11:37:51.817123: step 110720, loss = 0.0049, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 11:38:58.904271: step 110740, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-13 11:40:05.578757: step 110760, loss = 0.0047, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 11:41:12.146090: step 110780, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 11:42:19.847946: step 110800, loss = 0.0047, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-13 11:43:26.111801: step 110820, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 11:44:33.535926: step 110840, loss = 0.0052, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 11:45:39.550850: step 110860, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 11:46:46.702490: step 110880, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 11:47:53.734113: step 110900, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 11:49:00.484164: step 110920, loss = 0.0039, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-13 11:50:07.751611: step 110940, loss = 0.0040, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 11:51:14.848929: step 110960, loss = 0.0039, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-13 11:52:21.784967: step 110980, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 11:53:28.610159: step 111000, loss = 0.0036, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
[Eval] 2017-05-13 11:53:44.128798: step 111000, acc = 0.9393, f1 = 0.9373
[Test] 2017-05-13 11:53:54.854225: step 111000, acc = 0.9276, f1 = 0.9270
[Status] 2017-05-13 11:53:54.854352: step 111000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 11:55:00.913647: step 111020, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 11:56:07.482192: step 111040, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 11:57:14.190274: step 111060, loss = 0.0051, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 11:58:20.258766: step 111080, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 11:59:26.680855: step 111100, loss = 0.0035, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-13 12:00:33.220977: step 111120, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 12:01:39.883336: step 111140, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 12:02:46.951675: step 111160, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-13 12:03:53.288921: step 111180, loss = 0.0045, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-13 12:05:00.527381: step 111200, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 12:06:07.520518: step 111220, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 12:07:14.226593: step 111240, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 12:08:20.944520: step 111260, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 12:09:26.986438: step 111280, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 12:10:34.318760: step 111300, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 12:11:40.818716: step 111320, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 12:12:48.090478: step 111340, loss = 0.0031, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-13 12:13:55.210766: step 111360, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 12:15:01.654816: step 111380, loss = 0.0033, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-13 12:16:08.160746: step 111400, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 12:17:15.195823: step 111420, loss = 0.0025, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-13 12:18:21.744532: step 111440, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 12:19:28.161546: step 111460, loss = 0.0032, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-13 12:20:34.348167: step 111480, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 12:21:41.489685: step 111500, loss = 0.0037, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-13 12:22:49.316596: step 111520, loss = 0.0026, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-13 12:23:56.032469: step 111540, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-13 12:25:02.052964: step 111560, loss = 0.0026, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 12:26:08.967618: step 111580, loss = 0.0023, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-13 12:27:16.960254: step 111600, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-13 12:28:23.845682: step 111620, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 12:29:29.958338: step 111640, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 12:30:38.230785: step 111660, loss = 0.0023, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-13 12:31:45.554238: step 111680, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 12:32:52.742638: step 111700, loss = 0.0027, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 12:33:59.558783: step 111720, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-13 12:35:07.222258: step 111740, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 12:36:14.223248: step 111760, loss = 0.0029, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-13 12:37:23.209546: step 111780, loss = 0.0024, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-13 12:38:30.557526: step 111800, loss = 0.0024, acc = 1.0000 (17.9 examples/sec; 3.575 sec/batch)
2017-05-13 12:39:36.195100: step 111820, loss = 0.0027, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-13 12:40:42.214352: step 111840, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-13 12:41:48.522937: step 111860, loss = 0.0024, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-13 12:42:55.491731: step 111880, loss = 0.0025, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-13 12:44:04.548883: step 111900, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-13 12:45:11.756993: step 111920, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-13 12:46:18.823077: step 111940, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 12:47:26.538381: step 111960, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-13 12:48:33.229529: step 111980, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 12:49:39.764750: step 112000, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
[Eval] 2017-05-13 12:49:55.147182: step 112000, acc = 0.9410, f1 = 0.9390
[Test] 2017-05-13 12:50:05.794131: step 112000, acc = 0.9292, f1 = 0.9286
[Status] 2017-05-13 12:50:05.794254: step 112000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 12:51:12.313807: step 112020, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-13 12:52:19.899152: step 112040, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-13 12:53:26.267921: step 112060, loss = 0.0023, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-13 12:54:32.866226: step 112080, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-13 12:55:40.888418: step 112100, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 12:56:47.476797: step 112120, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 12:57:54.982976: step 112140, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-13 12:59:02.350550: step 112160, loss = 0.0027, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-13 13:00:10.487249: step 112180, loss = 0.0020, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-13 13:01:17.051188: step 112200, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-13 13:02:23.740195: step 112220, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 13:03:31.087241: step 112240, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 13:04:38.097257: step 112260, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-13 13:05:45.375835: step 112280, loss = 0.0024, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-13 13:06:53.536340: step 112300, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-13 13:08:00.198050: step 112320, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-13 13:09:06.303996: step 112340, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-13 13:10:13.513656: step 112360, loss = 0.0023, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-13 13:11:19.884432: step 112380, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 13:12:25.931295: step 112400, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 13:13:32.136190: step 112420, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 13:14:40.009160: step 112440, loss = 0.0018, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-13 13:15:47.063873: step 112460, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 13:16:53.073981: step 112480, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-13 13:18:00.172419: step 112500, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 13:19:07.598455: step 112520, loss = 0.0022, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-13 13:20:14.090267: step 112540, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 13:21:20.538612: step 112560, loss = 0.0020, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-13 13:22:26.516452: step 112580, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 13:23:33.386280: step 112600, loss = 0.0026, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-13 13:24:39.644732: step 112620, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-13 13:25:46.207007: step 112640, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-13 13:26:52.039129: step 112660, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 13:27:58.820668: step 112680, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-13 13:29:06.232392: step 112700, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 13:30:12.793404: step 112720, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 13:31:19.374919: step 112740, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 13:32:25.859995: step 112760, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 13:33:32.142724: step 112780, loss = 0.0031, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 13:34:39.510710: step 112800, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-13 13:35:46.985821: step 112820, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 13:36:53.764499: step 112840, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 13:38:00.963122: step 112860, loss = 0.0020, acc = 1.0000 (18.3 examples/sec; 3.507 sec/batch)
2017-05-13 13:39:07.427853: step 112880, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 13:40:16.029099: step 112900, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-13 13:41:22.251213: step 112920, loss = 0.0023, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-13 13:42:29.872765: step 112940, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 13:43:35.895909: step 112960, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 13:44:43.155506: step 112980, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-13 13:45:52.580357: step 113000, loss = 0.0019, acc = 1.0000 (17.8 examples/sec; 3.589 sec/batch)
[Eval] 2017-05-13 13:46:07.814431: step 113000, acc = 0.9423, f1 = 0.9404
[Test] 2017-05-13 13:46:18.376783: step 113000, acc = 0.9301, f1 = 0.9295
[Status] 2017-05-13 13:46:18.376872: step 113000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 13:47:25.495325: step 113020, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 13:48:33.472725: step 113040, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 13:49:40.576731: step 113060, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 13:50:47.795784: step 113080, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-13 13:51:54.530983: step 113100, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 13:53:01.426737: step 113120, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-13 13:54:07.651759: step 113140, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 13:55:14.375666: step 113160, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 13:56:21.928968: step 113180, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 13:57:27.858509: step 113200, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 13:58:33.899408: step 113220, loss = 0.0037, acc = 0.9980 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 13:59:40.776839: step 113240, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-13 14:00:46.746780: step 113260, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 14:01:52.770886: step 113280, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 14:02:59.081303: step 113300, loss = 0.0048, acc = 0.9980 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 14:04:06.125203: step 113320, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 14:05:12.789041: step 113340, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 14:06:19.776682: step 113360, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 14:07:26.176909: step 113380, loss = 0.0033, acc = 0.9980 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 14:08:33.407894: step 113400, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-13 14:09:39.988170: step 113420, loss = 0.0023, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-13 14:10:47.086326: step 113440, loss = 0.0034, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-13 14:11:53.993446: step 113460, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 14:13:01.939444: step 113480, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 14:14:10.099430: step 113500, loss = 0.0020, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 14:15:16.417023: step 113520, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 14:16:22.978790: step 113540, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-13 14:17:29.587801: step 113560, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 14:18:36.466437: step 113580, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 14:19:43.980661: step 113600, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 14:20:50.469455: step 113620, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-13 14:21:56.872831: step 113640, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 14:23:04.855941: step 113660, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 14:24:12.203573: step 113680, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-13 14:25:18.401729: step 113700, loss = 0.0015, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-13 14:26:25.470130: step 113720, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 14:27:32.730729: step 113740, loss = 0.0024, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-13 14:28:38.674568: step 113760, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 14:29:47.806143: step 113780, loss = 0.0018, acc = 1.0000 (18.0 examples/sec; 3.565 sec/batch)
2017-05-13 14:30:54.564187: step 113800, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-13 14:32:01.276850: step 113820, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-13 14:33:08.144209: step 113840, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 14:34:14.247634: step 113860, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-13 14:35:21.103069: step 113880, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-13 14:36:29.767726: step 113900, loss = 0.0018, acc = 1.0000 (17.4 examples/sec; 3.680 sec/batch)
2017-05-13 14:37:36.556878: step 113920, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 14:38:43.411056: step 113940, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 14:39:50.059177: step 113960, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-13 14:40:55.872661: step 113980, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 14:42:02.527315: step 114000, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
[Eval] 2017-05-13 14:42:17.234925: step 114000, acc = 0.9426, f1 = 0.9408
[Test] 2017-05-13 14:42:27.029384: step 114000, acc = 0.9311, f1 = 0.9305
[Status] 2017-05-13 14:42:27.029471: step 114000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 14:43:33.798376: step 114020, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 14:44:39.755429: step 114040, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 14:45:46.159648: step 114060, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-13 14:46:53.495110: step 114080, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-13 14:48:01.212749: step 114100, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-13 14:49:07.909333: step 114120, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-13 14:50:13.795929: step 114140, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 14:51:21.040977: step 114160, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-13 14:52:28.965624: step 114180, loss = 0.0016, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-13 14:53:38.404515: step 114200, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-13 14:54:45.023327: step 114220, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 14:55:52.115185: step 114240, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-13 14:56:58.688964: step 114260, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 14:58:05.896235: step 114280, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 14:59:12.070075: step 114300, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-13 15:00:19.431471: step 114320, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-13 15:01:26.506692: step 114340, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 15:02:33.501240: step 114360, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-13 15:03:39.899974: step 114380, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 15:04:47.674894: step 114400, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-13 15:05:56.125350: step 114420, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 15:07:04.266274: step 114440, loss = 0.0022, acc = 1.0000 (18.1 examples/sec; 3.536 sec/batch)
2017-05-13 15:08:10.693982: step 114460, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-13 15:09:17.495769: step 114480, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 15:10:23.911639: step 114500, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 15:11:30.267127: step 114520, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 15:12:36.590250: step 114540, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 15:13:43.611998: step 114560, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 15:14:50.967705: step 114580, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-13 15:15:58.731479: step 114600, loss = 0.0018, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-13 15:17:05.602744: step 114620, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 15:18:12.940143: step 114640, loss = 0.0028, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-13 15:19:19.385536: step 114660, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 15:20:26.555482: step 114680, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-13 15:21:32.614555: step 114700, loss = 0.0021, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-13 15:22:39.439923: step 114720, loss = 0.0030, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 15:23:46.224019: step 114740, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 15:24:53.391815: step 114760, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 15:26:00.280132: step 114780, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-13 15:27:06.495634: step 114800, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 15:28:14.847801: step 114820, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 15:29:22.340872: step 114840, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-13 15:30:28.371394: step 114860, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 15:31:34.581620: step 114880, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 15:32:41.613458: step 114900, loss = 0.0021, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-13 15:33:49.157257: step 114920, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-13 15:34:55.800743: step 114940, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-13 15:36:02.023821: step 114960, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 15:37:09.074767: step 114980, loss = 0.0019, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-13 15:38:17.224977: step 115000, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
[Eval] 2017-05-13 15:38:32.503841: step 115000, acc = 0.9427, f1 = 0.9409
[Test] 2017-05-13 15:38:42.768510: step 115000, acc = 0.9310, f1 = 0.9305
[Status] 2017-05-13 15:38:42.768598: step 115000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 15:39:48.940210: step 115020, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 15:40:55.728376: step 115040, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 15:42:02.531228: step 115060, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-13 15:43:08.788606: step 115080, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 15:44:15.948364: step 115100, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 15:45:23.505042: step 115120, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 15:46:30.410438: step 115140, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 15:47:36.679615: step 115160, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 15:48:43.833122: step 115180, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 15:49:50.917433: step 115200, loss = 0.0024, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-13 15:50:57.423016: step 115220, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-13 15:52:04.093416: step 115240, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 15:53:10.786909: step 115260, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 15:54:16.878607: step 115280, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-13 15:55:23.221341: step 115300, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 15:56:29.655830: step 115320, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 15:57:36.221280: step 115340, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 15:58:42.227128: step 115360, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-13 15:59:48.757055: step 115380, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 16:00:55.066555: step 115400, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 16:02:02.592268: step 115420, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 16:03:09.200567: step 115440, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-13 16:04:15.393246: step 115460, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 16:05:23.680681: step 115480, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 16:06:30.101439: step 115500, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 16:07:36.657171: step 115520, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-13 16:08:43.526133: step 115540, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-13 16:09:49.562716: step 115560, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 16:10:57.212536: step 115580, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-13 16:12:06.348884: step 115600, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 16:13:12.756741: step 115620, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 16:14:18.952564: step 115640, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 16:15:26.005447: step 115660, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 16:16:33.110250: step 115680, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-13 16:17:40.610428: step 115700, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-13 16:18:47.637821: step 115720, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-13 16:19:54.460520: step 115740, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-13 16:21:02.214905: step 115760, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 16:22:08.603642: step 115780, loss = 0.0021, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-13 16:23:16.514725: step 115800, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-13 16:24:23.128159: step 115820, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-13 16:25:29.417553: step 115840, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-13 16:26:36.563628: step 115860, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-13 16:27:42.785931: step 115880, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 16:28:48.935227: step 115900, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 16:29:56.355975: step 115920, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-13 16:31:03.428459: step 115940, loss = 0.0025, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-13 16:32:10.546756: step 115960, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 16:33:16.208935: step 115980, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 16:34:23.294148: step 116000, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
[Eval] 2017-05-13 16:34:38.736082: step 116000, acc = 0.9428, f1 = 0.9410
[Test] 2017-05-13 16:34:49.311967: step 116000, acc = 0.9310, f1 = 0.9304
[Status] 2017-05-13 16:34:49.312055: step 116000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 16:35:57.985984: step 116020, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-13 16:37:06.533980: step 116040, loss = 0.0016, acc = 1.0000 (17.5 examples/sec; 3.650 sec/batch)
2017-05-13 16:38:12.814210: step 116060, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 16:39:19.640401: step 116080, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-13 16:40:26.428727: step 116100, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 16:41:32.807897: step 116120, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-13 16:42:39.337931: step 116140, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-13 16:43:46.412398: step 116160, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-13 16:44:52.815070: step 116180, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-13 16:46:00.223092: step 116200, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 16:47:06.945320: step 116220, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 16:48:14.014470: step 116240, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 16:49:20.771155: step 116260, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-13 16:50:27.697966: step 116280, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-13 16:51:34.458679: step 116300, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 16:52:40.610704: step 116320, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 16:53:47.213673: step 116340, loss = 0.0033, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 16:54:53.606530: step 116360, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 16:56:00.367948: step 116380, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 16:57:07.415245: step 116400, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-13 16:58:13.723904: step 116420, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 16:59:20.321083: step 116440, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-13 17:00:26.273473: step 116460, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 17:01:32.962703: step 116480, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 17:02:40.031210: step 116500, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-13 17:03:47.040022: step 116520, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-13 17:04:54.007285: step 116540, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 17:06:00.358262: step 116560, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 17:07:07.089152: step 116580, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 17:08:13.272929: step 116600, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 17:09:20.938566: step 116620, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-13 17:10:27.536138: step 116640, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 17:11:34.936173: step 116660, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 17:12:41.332563: step 116680, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 17:13:48.297210: step 116700, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-13 17:14:54.742329: step 116720, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 17:16:00.915328: step 116740, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 17:17:06.814493: step 116760, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 17:18:13.357932: step 116780, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 17:19:19.938876: step 116800, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 17:20:26.320467: step 116820, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 17:21:32.829414: step 116840, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 17:22:39.264262: step 116860, loss = 0.0032, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 17:23:46.142714: step 116880, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 17:24:53.919258: step 116900, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-13 17:25:59.839269: step 116920, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 17:27:07.482354: step 116940, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 17:28:14.244539: step 116960, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-13 17:29:21.572911: step 116980, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-13 17:30:27.830357: step 117000, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
[Eval] 2017-05-13 17:30:43.317259: step 117000, acc = 0.9431, f1 = 0.9412
[Test] 2017-05-13 17:30:53.876613: step 117000, acc = 0.9312, f1 = 0.9306
[Status] 2017-05-13 17:30:53.876745: step 117000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 17:32:01.242900: step 117020, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-13 17:33:08.040585: step 117040, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-13 17:34:15.225254: step 117060, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 17:35:23.473428: step 117080, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 17:36:30.268543: step 117100, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-13 17:37:37.470497: step 117120, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 17:38:43.595827: step 117140, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 17:39:50.718963: step 117160, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-13 17:40:57.313900: step 117180, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 17:42:03.866447: step 117200, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 17:43:10.195139: step 117220, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 17:44:16.311923: step 117240, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 17:45:22.584463: step 117260, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-13 17:46:28.895021: step 117280, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 17:47:36.865905: step 117300, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-13 17:48:43.109683: step 117320, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-13 17:49:50.186573: step 117340, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-13 17:50:57.790606: step 117360, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-13 17:52:03.471327: step 117380, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 17:53:11.517447: step 117400, loss = 0.0017, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-13 17:54:17.689478: step 117420, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 17:55:24.032910: step 117440, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 17:56:30.909697: step 117460, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 17:57:37.380295: step 117480, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-13 17:58:43.542433: step 117500, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 17:59:50.003200: step 117520, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-13 18:00:56.843126: step 117540, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 18:02:03.072784: step 117560, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 18:03:10.520127: step 117580, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-13 18:04:17.473956: step 117600, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 18:05:23.986879: step 117620, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-13 18:06:30.250197: step 117640, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 18:07:36.759783: step 117660, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-13 18:08:44.609916: step 117680, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 18:09:51.281022: step 117700, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 18:10:57.575566: step 117720, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 18:12:04.511848: step 117740, loss = 0.0015, acc = 1.0000 (18.2 examples/sec; 3.520 sec/batch)
2017-05-13 18:13:12.399028: step 117760, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 18:14:18.384213: step 117780, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 18:15:25.145867: step 117800, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 18:16:32.025466: step 117820, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 18:17:39.724589: step 117840, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 18:18:47.044723: step 117860, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-13 18:19:53.808729: step 117880, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 18:21:00.715593: step 117900, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 18:22:06.573623: step 117920, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 18:23:13.658985: step 117940, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 18:24:20.096766: step 117960, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 18:25:27.024375: step 117980, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-13 18:26:35.203563: step 118000, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
[Eval] 2017-05-13 18:26:50.726355: step 118000, acc = 0.9432, f1 = 0.9414
[Test] 2017-05-13 18:27:01.515290: step 118000, acc = 0.9316, f1 = 0.9310
[Status] 2017-05-13 18:27:01.515393: step 118000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 18:28:07.471117: step 118020, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 18:29:13.610953: step 118040, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 18:30:19.743745: step 118060, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 18:31:25.891920: step 118080, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 18:32:31.961512: step 118100, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 18:33:39.016011: step 118120, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 18:34:45.158060: step 118140, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-13 18:35:51.618714: step 118160, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 18:36:58.148262: step 118180, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 18:38:05.020125: step 118200, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 18:39:11.577312: step 118220, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 18:40:18.462491: step 118240, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 18:41:24.886913: step 118260, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 18:42:31.162915: step 118280, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 18:43:37.527562: step 118300, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 18:44:44.022877: step 118320, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 18:45:51.395087: step 118340, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 18:46:58.091007: step 118360, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-13 18:48:04.204412: step 118380, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 18:49:11.276665: step 118400, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-13 18:50:18.173723: step 118420, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 18:51:24.394849: step 118440, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 18:52:31.931042: step 118460, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-13 18:53:38.690282: step 118480, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 18:54:45.012579: step 118500, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 18:55:52.018887: step 118520, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-13 18:56:58.918430: step 118540, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-13 18:58:05.592132: step 118560, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 18:59:12.735843: step 118580, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 19:00:19.264263: step 118600, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 19:01:26.291779: step 118620, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 19:02:32.911118: step 118640, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 19:03:39.803236: step 118660, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-13 19:04:46.461406: step 118680, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-13 19:05:54.329782: step 118700, loss = 0.0027, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-13 19:07:00.725294: step 118720, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 19:08:07.450003: step 118740, loss = 0.0026, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-13 19:09:14.818594: step 118760, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 19:10:21.095159: step 118780, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 19:11:30.275485: step 118800, loss = 0.0015, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-13 19:12:36.945059: step 118820, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 19:13:44.928013: step 118840, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-13 19:14:51.733728: step 118860, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-13 19:15:58.416858: step 118880, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 19:17:05.501755: step 118900, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 19:18:12.711154: step 118920, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-13 19:19:19.537459: step 118940, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-13 19:20:26.880836: step 118960, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 19:21:33.966855: step 118980, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 19:22:40.330245: step 119000, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
[Eval] 2017-05-13 19:22:55.593170: step 119000, acc = 0.9434, f1 = 0.9416
[Test] 2017-05-13 19:23:06.406290: step 119000, acc = 0.9318, f1 = 0.9312
[Status] 2017-05-13 19:23:06.406382: step 119000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 19:24:14.180692: step 119020, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 19:25:21.097871: step 119040, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-13 19:26:29.217803: step 119060, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-13 19:27:36.077328: step 119080, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-13 19:28:43.733801: step 119100, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-13 19:29:50.005543: step 119120, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 19:30:57.745945: step 119140, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 19:32:04.096706: step 119160, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 19:33:10.323058: step 119180, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 19:34:18.369260: step 119200, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 19:35:25.908958: step 119220, loss = 0.0029, acc = 0.9980 (18.3 examples/sec; 3.505 sec/batch)
2017-05-13 19:36:32.561636: step 119240, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 19:37:39.984199: step 119260, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-13 19:38:46.409499: step 119280, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-13 19:39:53.441152: step 119300, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-13 19:40:59.853396: step 119320, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 19:42:05.787883: step 119340, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-13 19:43:12.459573: step 119360, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 19:44:18.899334: step 119380, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 19:45:26.206905: step 119400, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 19:46:32.261947: step 119420, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 19:47:39.020651: step 119440, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-13 19:48:44.623764: step 119460, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 19:49:50.915683: step 119480, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 19:50:59.471004: step 119500, loss = 0.0015, acc = 1.0000 (17.6 examples/sec; 3.629 sec/batch)
2017-05-13 19:52:07.147764: step 119520, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 19:53:14.336759: step 119540, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-13 19:54:21.931114: step 119560, loss = 0.0016, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-13 19:55:28.329064: step 119580, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 19:56:35.426046: step 119600, loss = 0.0019, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-13 19:57:41.281810: step 119620, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 19:58:47.872134: step 119640, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 19:59:54.972029: step 119660, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 20:01:02.151613: step 119680, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 20:02:09.422152: step 119700, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 20:03:16.164567: step 119720, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 20:04:22.649412: step 119740, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-13 20:05:29.858669: step 119760, loss = 0.0046, acc = 0.9980 (18.5 examples/sec; 3.467 sec/batch)
2017-05-13 20:06:36.431019: step 119780, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 20:07:43.015904: step 119800, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-13 20:08:50.093216: step 119820, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 20:09:57.006242: step 119840, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-13 20:11:03.152524: step 119860, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 20:12:10.187484: step 119880, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-13 20:13:16.862392: step 119900, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 20:14:25.600179: step 119920, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-13 20:15:33.208216: step 119940, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 20:16:41.620236: step 119960, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-13 20:17:47.337472: step 119980, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 20:18:53.919912: step 120000, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
[Eval] 2017-05-13 20:19:07.995375: step 120000, acc = 0.9436, f1 = 0.9418
[Test] 2017-05-13 20:19:17.963450: step 120000, acc = 0.9319, f1 = 0.9314
[Status] 2017-05-13 20:19:17.963555: step 120000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 20:20:24.966870: step 120020, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-13 20:21:32.530798: step 120040, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-13 20:22:38.799521: step 120060, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-13 20:23:44.670695: step 120080, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 20:24:52.005144: step 120100, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-13 20:25:58.542634: step 120120, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-13 20:27:04.505287: step 120140, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 20:28:10.737781: step 120160, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 20:29:17.726546: step 120180, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 20:30:24.163353: step 120200, loss = 0.0031, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 20:31:31.506513: step 120220, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-13 20:32:37.942284: step 120240, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-13 20:33:45.512918: step 120260, loss = 0.0012, acc = 1.0000 (17.9 examples/sec; 3.569 sec/batch)
2017-05-13 20:34:52.817935: step 120280, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-13 20:35:59.583429: step 120300, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-13 20:37:05.839972: step 120320, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 20:38:13.786266: step 120340, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-13 20:39:20.142221: step 120360, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 20:40:26.047225: step 120380, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 20:41:33.637204: step 120400, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-13 20:42:40.870114: step 120420, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-13 20:43:48.027730: step 120440, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-13 20:44:53.785057: step 120460, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 20:46:00.549677: step 120480, loss = 0.0013, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-13 20:47:08.574901: step 120500, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 20:48:17.204077: step 120520, loss = 0.0027, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-13 20:49:24.029839: step 120540, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 20:50:30.839617: step 120560, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 20:51:38.175866: step 120580, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 20:52:45.567844: step 120600, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-13 20:53:51.891306: step 120620, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-13 20:54:59.805115: step 120640, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-13 20:56:06.898140: step 120660, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-13 20:57:14.427111: step 120680, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 20:58:21.252067: step 120700, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 20:59:28.032366: step 120720, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-13 21:00:34.307182: step 120740, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 21:01:41.505320: step 120760, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 21:02:47.987743: step 120780, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-13 21:03:54.588394: step 120800, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-13 21:05:02.289452: step 120820, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-13 21:06:10.638118: step 120840, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-13 21:07:17.838067: step 120860, loss = 0.0029, acc = 0.9980 (19.1 examples/sec; 3.353 sec/batch)
2017-05-13 21:08:23.932840: step 120880, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-13 21:09:31.400579: step 120900, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-13 21:10:38.578598: step 120920, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 21:11:45.629698: step 120940, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 21:12:53.135568: step 120960, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-13 21:14:00.629274: step 120980, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-13 21:15:08.938351: step 121000, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
[Eval] 2017-05-13 21:15:24.415762: step 121000, acc = 0.9438, f1 = 0.9420
[Test] 2017-05-13 21:15:34.936573: step 121000, acc = 0.9322, f1 = 0.9316
[Status] 2017-05-13 21:15:34.936650: step 121000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 21:16:41.886271: step 121020, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-13 21:17:49.165111: step 121040, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 21:18:56.565427: step 121060, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 21:20:02.885433: step 121080, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-13 21:21:09.155962: step 121100, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 21:22:16.463879: step 121120, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-13 21:23:23.280326: step 121140, loss = 0.0013, acc = 1.0000 (18.1 examples/sec; 3.537 sec/batch)
2017-05-13 21:24:29.156097: step 121160, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 21:25:35.495972: step 121180, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 21:26:41.967339: step 121200, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-13 21:27:48.187701: step 121220, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-13 21:28:54.469006: step 121240, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 21:30:00.917742: step 121260, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 21:31:07.147197: step 121280, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 21:32:13.240641: step 121300, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-13 21:33:20.077902: step 121320, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 21:34:26.593994: step 121340, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-13 21:35:32.764936: step 121360, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 21:36:41.338497: step 121380, loss = 0.0013, acc = 1.0000 (17.5 examples/sec; 3.654 sec/batch)
2017-05-13 21:37:48.100800: step 121400, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 21:38:55.428728: step 121420, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-13 21:40:01.986408: step 121440, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 21:41:08.354256: step 121460, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-13 21:42:15.028643: step 121480, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-13 21:43:21.971829: step 121500, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 21:44:28.885264: step 121520, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-13 21:45:35.568186: step 121540, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 21:46:42.560794: step 121560, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-13 21:47:49.522156: step 121580, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-13 21:48:57.028953: step 121600, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-13 21:50:03.512071: step 121620, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 21:51:10.109824: step 121640, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-13 21:52:16.584428: step 121660, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-13 21:53:23.272327: step 121680, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 21:54:29.491802: step 121700, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-13 21:55:35.459538: step 121720, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-13 21:56:43.870194: step 121740, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 21:57:50.809229: step 121760, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-13 21:58:56.817118: step 121780, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-13 22:00:03.753710: step 121800, loss = 0.0028, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-13 22:01:10.100927: step 121820, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-13 22:02:16.887713: step 121840, loss = 0.0028, acc = 0.9980 (19.3 examples/sec; 3.318 sec/batch)
2017-05-13 22:03:24.892523: step 121860, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-13 22:04:31.521964: step 121880, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-13 22:05:40.937910: step 121900, loss = 0.0013, acc = 1.0000 (16.8 examples/sec; 3.808 sec/batch)
2017-05-13 22:06:47.362962: step 121920, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 22:07:54.183968: step 121940, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-13 22:09:00.482693: step 121960, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 22:10:07.754732: step 121980, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-13 22:11:14.040074: step 122000, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
[Eval] 2017-05-13 22:11:29.541610: step 122000, acc = 0.9438, f1 = 0.9420
[Test] 2017-05-13 22:11:40.113471: step 122000, acc = 0.9322, f1 = 0.9317
[Status] 2017-05-13 22:11:40.113554: step 122000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 22:12:48.113013: step 122020, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 22:13:54.769201: step 122040, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 22:15:00.461305: step 122060, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 22:16:07.500492: step 122080, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 22:17:14.721950: step 122100, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 22:18:20.998162: step 122120, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 22:19:27.444790: step 122140, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-13 22:20:33.356977: step 122160, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 22:21:40.186025: step 122180, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-13 22:22:47.046081: step 122200, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-13 22:23:53.220046: step 122220, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 22:25:00.404995: step 122240, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 22:26:07.389889: step 122260, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-13 22:27:13.668422: step 122280, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-13 22:28:20.746886: step 122300, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 22:29:26.633665: step 122320, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 22:30:33.749078: step 122340, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-13 22:31:41.577251: step 122360, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-13 22:32:47.891588: step 122380, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-13 22:33:54.137944: step 122400, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-13 22:35:01.922342: step 122420, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-13 22:36:08.942400: step 122440, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 22:37:15.206778: step 122460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 22:38:22.073084: step 122480, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 22:39:30.655511: step 122500, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-13 22:40:37.890715: step 122520, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-13 22:41:43.955847: step 122540, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-13 22:42:50.794364: step 122560, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 22:43:56.949690: step 122580, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 22:45:04.321311: step 122600, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-13 22:46:11.121186: step 122620, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-13 22:47:17.650280: step 122640, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-13 22:48:26.284341: step 122660, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-13 22:49:33.321588: step 122680, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-13 22:50:41.296893: step 122700, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 22:51:50.150755: step 122720, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-13 22:52:56.625466: step 122740, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-13 22:54:02.541119: step 122760, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-13 22:55:09.260543: step 122780, loss = 0.0018, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-13 22:56:16.081653: step 122800, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-13 22:57:22.795713: step 122820, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-13 22:58:30.788931: step 122840, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 22:59:36.641396: step 122860, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-13 23:00:45.247193: step 122880, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-13 23:01:51.628914: step 122900, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-13 23:02:58.234957: step 122920, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-13 23:04:05.179127: step 122940, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-13 23:05:11.534830: step 122960, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-13 23:06:19.104109: step 122980, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-13 23:07:25.439122: step 123000, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
[Eval] 2017-05-13 23:07:40.898489: step 123000, acc = 0.9435, f1 = 0.9417
[Test] 2017-05-13 23:07:51.519259: step 123000, acc = 0.9321, f1 = 0.9315
[Status] 2017-05-13 23:07:51.519386: step 123000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-13 23:08:58.640700: step 123020, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-13 23:10:05.617360: step 123040, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-13 23:11:13.181397: step 123060, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-13 23:12:19.271062: step 123080, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 23:13:26.860038: step 123100, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-13 23:14:33.880283: step 123120, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-13 23:15:40.295693: step 123140, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 23:16:47.459934: step 123160, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-13 23:17:53.544426: step 123180, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-13 23:18:59.973023: step 123200, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-13 23:20:06.652339: step 123220, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-13 23:21:12.709924: step 123240, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-13 23:22:19.378453: step 123260, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-13 23:23:26.052166: step 123280, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-13 23:24:32.344615: step 123300, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-13 23:25:39.489585: step 123320, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-13 23:26:46.620485: step 123340, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-13 23:27:53.242551: step 123360, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 23:28:59.105193: step 123380, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-13 23:30:05.773377: step 123400, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-13 23:31:12.734887: step 123420, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-13 23:32:19.413688: step 123440, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-13 23:33:26.365433: step 123460, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-13 23:34:32.754972: step 123480, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-13 23:35:39.610821: step 123500, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-13 23:36:45.483551: step 123520, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-13 23:37:53.031863: step 123540, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-13 23:38:59.588808: step 123560, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-13 23:40:06.236924: step 123580, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 23:41:12.849165: step 123600, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-13 23:42:19.366241: step 123620, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 23:43:26.263582: step 123640, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-13 23:44:33.270909: step 123660, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-13 23:45:39.705029: step 123680, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-13 23:46:46.449323: step 123700, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-13 23:47:52.980980: step 123720, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-13 23:48:59.813141: step 123740, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-13 23:50:07.878545: step 123760, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-13 23:51:15.243954: step 123780, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-13 23:52:21.212278: step 123800, loss = 0.0031, acc = 0.9980 (19.1 examples/sec; 3.346 sec/batch)
2017-05-13 23:53:28.243320: step 123820, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-13 23:54:36.107915: step 123840, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-13 23:55:43.205687: step 123860, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-13 23:56:49.871539: step 123880, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-13 23:57:56.324939: step 123900, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-13 23:59:02.658928: step 123920, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 00:00:09.396918: step 123940, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 00:01:15.867271: step 123960, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 00:02:24.881455: step 123980, loss = 0.0013, acc = 1.0000 (17.6 examples/sec; 3.645 sec/batch)
2017-05-14 00:03:32.465145: step 124000, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
[Eval] 2017-05-14 00:03:48.049792: step 124000, acc = 0.9441, f1 = 0.9423
[Test] 2017-05-14 00:03:58.624817: step 124000, acc = 0.9329, f1 = 0.9323
[Status] 2017-05-14 00:03:58.624915: step 124000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 00:05:04.531397: step 124020, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-14 00:06:11.354961: step 124040, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-14 00:07:17.688865: step 124060, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 00:08:24.659684: step 124080, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-14 00:09:30.711663: step 124100, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-14 00:10:36.652522: step 124120, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 00:11:43.596560: step 124140, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 00:12:49.791048: step 124160, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 00:13:57.274948: step 124180, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-14 00:15:04.593733: step 124200, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 00:16:12.659340: step 124220, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 00:17:19.010683: step 124240, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 00:18:25.664819: step 124260, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-14 00:19:31.827677: step 124280, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 00:20:38.286185: step 124300, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-14 00:21:44.464044: step 124320, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 00:22:51.408558: step 124340, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 00:23:58.190992: step 124360, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 00:25:04.384810: step 124380, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 00:26:11.606141: step 124400, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 00:27:17.965077: step 124420, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 00:28:25.675432: step 124440, loss = 0.0025, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-14 00:29:32.029987: step 124460, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 00:30:38.074706: step 124480, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-14 00:31:46.537404: step 124500, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-14 00:32:53.349084: step 124520, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-14 00:34:00.513606: step 124540, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-14 00:35:08.005162: step 124560, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-14 00:36:14.880327: step 124580, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-14 00:37:22.654463: step 124600, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-14 00:38:29.181806: step 124620, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 00:39:36.531176: step 124640, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-14 00:40:42.917541: step 124660, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 00:41:50.277323: step 124680, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 00:42:57.285340: step 124700, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 00:44:03.765290: step 124720, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 00:45:10.015535: step 124740, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 00:46:16.906318: step 124760, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 00:47:23.401403: step 124780, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 00:48:31.631201: step 124800, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 00:49:38.107349: step 124820, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 00:50:44.122643: step 124840, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 00:51:50.849860: step 124860, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 00:52:57.641859: step 124880, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-14 00:54:04.214793: step 124900, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 00:55:10.351303: step 124920, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 00:56:17.094203: step 124940, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-14 00:57:23.254250: step 124960, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-14 00:58:29.215956: step 124980, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-14 00:59:37.028154: step 125000, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
[Eval] 2017-05-14 00:59:52.371559: step 125000, acc = 0.9438, f1 = 0.9420
[Test] 2017-05-14 01:00:02.965955: step 125000, acc = 0.9327, f1 = 0.9322
[Status] 2017-05-14 01:00:02.966170: step 125000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 01:01:10.185071: step 125020, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 01:02:17.163788: step 125040, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-14 01:03:24.503180: step 125060, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 01:04:30.525543: step 125080, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 01:05:36.908197: step 125100, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 01:06:44.660269: step 125120, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 01:07:52.397621: step 125140, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-14 01:08:59.843131: step 125160, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 01:10:06.776558: step 125180, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 01:11:13.387125: step 125200, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 01:12:19.710087: step 125220, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-14 01:13:26.994969: step 125240, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 01:14:33.760244: step 125260, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 01:15:40.143686: step 125280, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-14 01:16:47.230041: step 125300, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-14 01:17:54.000643: step 125320, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 01:19:00.255194: step 125340, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 01:20:06.194618: step 125360, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 01:21:13.503293: step 125380, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 01:22:19.623134: step 125400, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-14 01:23:26.328662: step 125420, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 01:24:33.157060: step 125440, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 01:25:39.497098: step 125460, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-14 01:26:46.331738: step 125480, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 01:27:53.236890: step 125500, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-14 01:29:00.254897: step 125520, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 01:30:06.895767: step 125540, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-14 01:31:13.344252: step 125560, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-14 01:32:21.032606: step 125580, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-14 01:33:28.574136: step 125600, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-14 01:34:34.354642: step 125620, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 01:35:41.620736: step 125640, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-14 01:36:48.937911: step 125660, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 01:37:55.703439: step 125680, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 01:39:03.478576: step 125700, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.523 sec/batch)
2017-05-14 01:40:10.788651: step 125720, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 01:41:17.696700: step 125740, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 01:42:25.917749: step 125760, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-14 01:43:33.910921: step 125780, loss = 0.0036, acc = 0.9980 (18.3 examples/sec; 3.491 sec/batch)
2017-05-14 01:44:40.805283: step 125800, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 01:45:46.901428: step 125820, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 01:46:53.309251: step 125840, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 01:47:59.594897: step 125860, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-14 01:49:06.850947: step 125880, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 01:50:13.038053: step 125900, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 01:51:19.793091: step 125920, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 01:52:26.862007: step 125940, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 01:53:33.678437: step 125960, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 01:54:40.534575: step 125980, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 01:55:49.008080: step 126000, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
[Eval] 2017-05-14 01:56:04.399966: step 126000, acc = 0.9441, f1 = 0.9423
[Test] 2017-05-14 01:56:15.154528: step 126000, acc = 0.9329, f1 = 0.9324
[Status] 2017-05-14 01:56:15.154802: step 126000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 01:57:22.804630: step 126020, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-14 01:58:29.945702: step 126040, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 01:59:36.594773: step 126060, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 02:00:42.564524: step 126080, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 02:01:48.897768: step 126100, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 02:02:55.151853: step 126120, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 02:04:01.568647: step 126140, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-14 02:05:07.997273: step 126160, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 02:06:14.248698: step 126180, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-14 02:07:21.367865: step 126200, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-14 02:08:30.267562: step 126220, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 02:09:36.999974: step 126240, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-14 02:10:43.214054: step 126260, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 02:11:50.296213: step 126280, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 02:12:56.823645: step 126300, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 02:14:03.851220: step 126320, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-14 02:15:09.893092: step 126340, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-14 02:16:16.247288: step 126360, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 02:17:22.562331: step 126380, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 02:18:29.637398: step 126400, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-14 02:19:37.207933: step 126420, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 02:20:43.979391: step 126440, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 02:21:50.763592: step 126460, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-14 02:22:57.402619: step 126480, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-14 02:24:03.594255: step 126500, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-14 02:25:10.199412: step 126520, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 02:26:17.222154: step 126540, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 02:27:24.406507: step 126560, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 02:28:31.957960: step 126580, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 02:29:38.528161: step 126600, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 02:30:45.001901: step 126620, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 02:31:51.873587: step 126640, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 02:32:58.979078: step 126660, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 02:34:06.897509: step 126680, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-14 02:35:15.136297: step 126700, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-14 02:36:22.825366: step 126720, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-14 02:37:29.521703: step 126740, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 02:38:36.762254: step 126760, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-14 02:39:42.878080: step 126780, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 02:40:49.640517: step 126800, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-14 02:41:56.430991: step 126820, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 02:43:03.043567: step 126840, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 02:44:09.483937: step 126860, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 02:45:16.151606: step 126880, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 02:46:22.765017: step 126900, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 02:47:29.819007: step 126920, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-14 02:48:37.178454: step 126940, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-14 02:49:43.819647: step 126960, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-14 02:50:51.247917: step 126980, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-14 02:51:58.798724: step 127000, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
[Eval] 2017-05-14 02:52:14.087599: step 127000, acc = 0.9444, f1 = 0.9425
[Test] 2017-05-14 02:52:24.034355: step 127000, acc = 0.9324, f1 = 0.9318
[Status] 2017-05-14 02:52:24.034459: step 127000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 02:53:32.873128: step 127020, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-14 02:54:39.611665: step 127040, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 02:55:46.180884: step 127060, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 02:56:51.907048: step 127080, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 02:57:58.039039: step 127100, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 02:59:05.250859: step 127120, loss = 0.0012, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-14 03:00:12.683074: step 127140, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-14 03:01:18.586058: step 127160, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 03:02:25.668989: step 127180, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 03:03:32.860508: step 127200, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-14 03:04:39.265378: step 127220, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 03:05:46.407397: step 127240, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-14 03:06:53.807655: step 127260, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 03:08:00.721963: step 127280, loss = 0.0028, acc = 0.9980 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 03:09:07.690762: step 127300, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 03:10:14.975313: step 127320, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-14 03:11:21.543290: step 127340, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-14 03:12:28.859908: step 127360, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 03:13:35.236941: step 127380, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 03:14:41.683123: step 127400, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-14 03:15:48.703225: step 127420, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 03:16:55.383224: step 127440, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 03:18:02.094175: step 127460, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-14 03:19:08.331100: step 127480, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-14 03:20:15.748228: step 127500, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 03:21:23.388181: step 127520, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 03:22:29.827693: step 127540, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 03:23:37.726127: step 127560, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 03:24:45.357765: step 127580, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-14 03:25:52.372591: step 127600, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-14 03:27:00.410506: step 127620, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-14 03:28:06.788665: step 127640, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-14 03:29:14.466329: step 127660, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-14 03:30:20.621530: step 127680, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 03:31:26.873937: step 127700, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 03:32:34.097129: step 127720, loss = 0.0012, acc = 1.0000 (17.7 examples/sec; 3.622 sec/batch)
2017-05-14 03:33:40.710518: step 127740, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 03:34:47.183385: step 127760, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 03:35:54.174130: step 127780, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 03:37:01.729711: step 127800, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-14 03:38:08.074234: step 127820, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-14 03:39:16.063480: step 127840, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-14 03:40:22.612389: step 127860, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 03:41:29.542290: step 127880, loss = 0.0025, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 03:42:35.645789: step 127900, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-14 03:43:42.358809: step 127920, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-14 03:44:49.212280: step 127940, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 03:45:55.300115: step 127960, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 03:47:01.592574: step 127980, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 03:48:10.269714: step 128000, loss = 0.0016, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
[Eval] 2017-05-14 03:48:25.770779: step 128000, acc = 0.9443, f1 = 0.9425
[Test] 2017-05-14 03:48:36.371627: step 128000, acc = 0.9328, f1 = 0.9323
[Status] 2017-05-14 03:48:36.371701: step 128000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 03:49:43.898438: step 128020, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-14 03:50:51.611717: step 128040, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 03:51:59.146702: step 128060, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-14 03:53:05.827544: step 128080, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 03:54:12.925221: step 128100, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-14 03:55:19.429310: step 128120, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-14 03:56:25.234107: step 128140, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 03:57:31.769253: step 128160, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 03:58:38.684833: step 128180, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-14 03:59:45.208259: step 128200, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-14 04:00:52.460775: step 128220, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 04:01:59.276342: step 128240, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-14 04:03:06.957449: step 128260, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-14 04:04:14.160088: step 128280, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 04:05:20.813320: step 128300, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 04:06:27.023023: step 128320, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 04:07:33.106242: step 128340, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 04:08:39.714054: step 128360, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-14 04:09:46.918787: step 128380, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-14 04:10:53.558660: step 128400, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 04:12:00.131679: step 128420, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 04:13:06.664637: step 128440, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 04:14:12.769100: step 128460, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 04:15:18.777633: step 128480, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 04:16:25.937431: step 128500, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-14 04:17:32.733152: step 128520, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 04:18:38.788828: step 128540, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-14 04:19:46.663817: step 128560, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-14 04:20:52.561861: step 128580, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 04:21:59.798425: step 128600, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 04:23:07.213812: step 128620, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 04:24:13.657256: step 128640, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 04:25:20.543741: step 128660, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-14 04:26:26.724221: step 128680, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 04:27:34.166068: step 128700, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-14 04:28:41.488323: step 128720, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-14 04:29:49.307585: step 128740, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-14 04:30:56.288522: step 128760, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 04:32:03.086417: step 128780, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 04:33:10.739251: step 128800, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-14 04:34:17.674334: step 128820, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 04:35:24.842473: step 128840, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-14 04:36:31.749189: step 128860, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 04:37:38.536559: step 128880, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-14 04:38:45.940688: step 128900, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 04:39:54.907332: step 128920, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 04:41:02.450477: step 128940, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 04:42:09.235236: step 128960, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-14 04:43:15.619512: step 128980, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 04:44:22.341763: step 129000, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
[Eval] 2017-05-14 04:44:37.690654: step 129000, acc = 0.9439, f1 = 0.9421
[Test] 2017-05-14 04:44:48.398212: step 129000, acc = 0.9326, f1 = 0.9320
[Status] 2017-05-14 04:44:48.398310: step 129000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 04:45:56.134527: step 129020, loss = 0.0012, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-14 04:47:03.290444: step 129040, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-14 04:48:09.935945: step 129060, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 04:49:16.170724: step 129080, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 04:50:23.053298: step 129100, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 04:51:29.906606: step 129120, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-14 04:52:38.508661: step 129140, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-14 04:53:45.145082: step 129160, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 04:54:51.946304: step 129180, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-14 04:55:58.702490: step 129200, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-14 04:57:05.783946: step 129220, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 04:58:13.904733: step 129240, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-14 04:59:20.495338: step 129260, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-14 05:00:26.858836: step 129280, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-14 05:01:33.334293: step 129300, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-14 05:02:39.368033: step 129320, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 05:03:45.427860: step 129340, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 05:04:51.819381: step 129360, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 05:05:58.610666: step 129380, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 05:07:05.856407: step 129400, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 05:08:13.444473: step 129420, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 05:09:20.100604: step 129440, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-14 05:10:27.185854: step 129460, loss = 0.0029, acc = 0.9980 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 05:11:32.988312: step 129480, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 05:12:39.218422: step 129500, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 05:13:45.972072: step 129520, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-14 05:14:52.997381: step 129540, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 05:15:59.131216: step 129560, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-14 05:17:06.040301: step 129580, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 05:18:12.521427: step 129600, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-14 05:19:19.458577: step 129620, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 05:20:26.556204: step 129640, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-14 05:21:34.448702: step 129660, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-14 05:22:41.405199: step 129680, loss = 0.0039, acc = 0.9980 (18.5 examples/sec; 3.455 sec/batch)
2017-05-14 05:23:47.855378: step 129700, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 05:24:54.169503: step 129720, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 05:26:00.921103: step 129740, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-14 05:27:07.423749: step 129760, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-14 05:28:14.479622: step 129780, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-14 05:29:20.666463: step 129800, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-14 05:30:26.561671: step 129820, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-14 05:31:33.771000: step 129840, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 05:32:40.974385: step 129860, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 05:33:47.181481: step 129880, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 05:34:53.820296: step 129900, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-14 05:36:01.131253: step 129920, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 05:37:07.560495: step 129940, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 05:38:15.505754: step 129960, loss = 0.0012, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
2017-05-14 05:39:22.012106: step 129980, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 05:40:28.958889: step 130000, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
[Eval] 2017-05-14 05:40:44.410560: step 130000, acc = 0.9440, f1 = 0.9422
[Test] 2017-05-14 05:40:54.876656: step 130000, acc = 0.9323, f1 = 0.9318
[Status] 2017-05-14 05:40:54.876748: step 130000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 05:42:01.757998: step 130020, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 05:43:09.121472: step 130040, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 05:44:15.058204: step 130060, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 05:45:21.355740: step 130080, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 05:46:29.314623: step 130100, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 05:47:35.374333: step 130120, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 05:48:41.432554: step 130140, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 05:49:48.115113: step 130160, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 05:50:55.303016: step 130180, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 05:52:02.075242: step 130200, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-14 05:53:08.985028: step 130220, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-14 05:54:16.395311: step 130240, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 05:55:24.103440: step 130260, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-14 05:56:30.277686: step 130280, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 05:57:37.518225: step 130300, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 05:58:43.965653: step 130320, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 05:59:51.555091: step 130340, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-14 06:00:57.578829: step 130360, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 06:02:04.496584: step 130380, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-14 06:03:10.929016: step 130400, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 06:04:17.433876: step 130420, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 06:05:23.724190: step 130440, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-14 06:06:31.966759: step 130460, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-14 06:07:38.489684: step 130480, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 06:08:44.617742: step 130500, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 06:09:50.715510: step 130520, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 06:10:57.862863: step 130540, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-14 06:12:05.215943: step 130560, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-14 06:13:12.526696: step 130580, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-14 06:14:19.533817: step 130600, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-14 06:15:26.104173: step 130620, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-14 06:16:32.323948: step 130640, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-14 06:17:39.831422: step 130660, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-14 06:18:46.781631: step 130680, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 06:19:54.558698: step 130700, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-14 06:21:01.056749: step 130720, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 06:22:07.444914: step 130740, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-14 06:23:14.841527: step 130760, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 06:24:21.284284: step 130780, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-14 06:25:29.278954: step 130800, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 06:26:36.479678: step 130820, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-14 06:27:43.339360: step 130840, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-14 06:28:49.564696: step 130860, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 06:29:56.336626: step 130880, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 06:31:02.593669: step 130900, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 06:32:08.727018: step 130920, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 06:33:15.852846: step 130940, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 06:34:22.437725: step 130960, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 06:35:28.555348: step 130980, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-14 06:36:35.813740: step 131000, loss = 0.0026, acc = 1.0000 (18.0 examples/sec; 3.563 sec/batch)
[Eval] 2017-05-14 06:36:51.162723: step 131000, acc = 0.9441, f1 = 0.9423
[Test] 2017-05-14 06:37:01.891597: step 131000, acc = 0.9330, f1 = 0.9324
[Status] 2017-05-14 06:37:01.891691: step 131000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 06:38:09.352954: step 131020, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-14 06:39:16.281773: step 131040, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 06:40:24.102816: step 131060, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-14 06:41:31.007793: step 131080, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 06:42:38.697099: step 131100, loss = 0.0012, acc = 1.0000 (18.0 examples/sec; 3.560 sec/batch)
2017-05-14 06:43:45.397261: step 131120, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 06:44:52.176375: step 131140, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-14 06:45:58.676416: step 131160, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 06:47:04.990025: step 131180, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 06:48:11.660423: step 131200, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 06:49:19.377916: step 131220, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 06:50:26.539108: step 131240, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 06:51:32.979071: step 131260, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-14 06:52:39.833876: step 131280, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-14 06:53:45.851183: step 131300, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-14 06:54:52.240155: step 131320, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-14 06:55:58.864793: step 131340, loss = 0.0026, acc = 0.9980 (18.5 examples/sec; 3.453 sec/batch)
2017-05-14 06:57:05.228108: step 131360, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 06:58:11.712378: step 131380, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-14 06:59:17.782565: step 131400, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-14 07:00:24.839553: step 131420, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-14 07:01:32.045616: step 131440, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-14 07:02:38.189639: step 131460, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-14 07:03:45.091324: step 131480, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-14 07:04:52.038998: step 131500, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 07:05:58.089743: step 131520, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-14 07:07:06.286208: step 131540, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-14 07:08:12.091979: step 131560, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 07:09:18.847686: step 131580, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-14 07:10:25.081028: step 131600, loss = 0.0014, acc = 1.0000 (19.7 examples/sec; 3.247 sec/batch)
2017-05-14 07:11:32.534725: step 131620, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 07:12:40.067390: step 131640, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-14 07:13:46.990048: step 131660, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-14 07:14:54.525546: step 131680, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-14 07:16:03.104452: step 131700, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-14 07:17:09.988263: step 131720, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 07:18:16.262916: step 131740, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 07:19:23.213971: step 131760, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 07:20:29.514433: step 131780, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 07:21:38.328886: step 131800, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-14 07:22:44.586504: step 131820, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 07:23:51.680303: step 131840, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 07:24:58.251589: step 131860, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 07:26:06.031333: step 131880, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 07:27:13.094856: step 131900, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 07:28:19.367645: step 131920, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 07:29:26.642215: step 131940, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 07:30:32.632631: step 131960, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 07:31:39.370126: step 131980, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 07:32:47.702024: step 132000, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
[Eval] 2017-05-14 07:33:03.344628: step 132000, acc = 0.9443, f1 = 0.9425
[Test] 2017-05-14 07:33:13.999326: step 132000, acc = 0.9328, f1 = 0.9323
[Status] 2017-05-14 07:33:13.999432: step 132000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 07:34:21.445667: step 132020, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 07:35:29.860777: step 132040, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 07:36:37.701235: step 132060, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 07:37:44.224837: step 132080, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-14 07:38:51.923400: step 132100, loss = 0.0011, acc = 1.0000 (17.9 examples/sec; 3.572 sec/batch)
2017-05-14 07:39:58.220133: step 132120, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 07:41:05.054448: step 132140, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-14 07:42:11.656228: step 132160, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 07:43:18.973179: step 132180, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 07:44:25.850910: step 132200, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 07:45:32.268359: step 132220, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-14 07:46:39.211179: step 132240, loss = 0.0012, acc = 1.0000 (18.1 examples/sec; 3.539 sec/batch)
2017-05-14 07:47:45.433770: step 132260, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 07:48:52.022250: step 132280, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 07:49:58.685001: step 132300, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 07:51:05.663333: step 132320, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 07:52:12.675400: step 132340, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 07:53:19.105206: step 132360, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 07:54:25.849319: step 132380, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-14 07:55:32.597819: step 132400, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 07:56:38.678234: step 132420, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 07:57:46.139773: step 132440, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-14 07:58:52.504441: step 132460, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-14 07:59:59.524137: step 132480, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 08:01:06.010617: step 132500, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 08:02:13.991813: step 132520, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-14 08:03:20.426687: step 132540, loss = 0.0011, acc = 1.0000 (19.7 examples/sec; 3.254 sec/batch)
2017-05-14 08:04:26.784322: step 132560, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 08:05:33.351389: step 132580, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 08:06:39.313857: step 132600, loss = 0.0041, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 08:07:46.506668: step 132620, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-14 08:08:54.413688: step 132640, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-14 08:10:01.773028: step 132660, loss = 0.0028, acc = 0.9980 (19.6 examples/sec; 3.271 sec/batch)
2017-05-14 08:11:09.037115: step 132680, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 08:12:15.013831: step 132700, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 08:13:21.651537: step 132720, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-14 08:14:28.189849: step 132740, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 08:15:35.808638: step 132760, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-14 08:16:42.428329: step 132780, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 08:17:50.377264: step 132800, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-14 08:18:57.301478: step 132820, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-14 08:20:04.862793: step 132840, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-14 08:21:11.431909: step 132860, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-14 08:22:18.259837: step 132880, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-14 08:23:26.156175: step 132900, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-14 08:24:32.838445: step 132920, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 08:25:40.179274: step 132940, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-14 08:26:47.035182: step 132960, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 08:27:53.080583: step 132980, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 08:28:59.124817: step 133000, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.360 sec/batch)
[Eval] 2017-05-14 08:29:14.501408: step 133000, acc = 0.9444, f1 = 0.9427
[Test] 2017-05-14 08:29:25.189176: step 133000, acc = 0.9332, f1 = 0.9327
[Status] 2017-05-14 08:29:25.189255: step 133000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 08:30:31.333079: step 133020, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 08:31:38.396024: step 133040, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 08:32:46.826294: step 133060, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 08:33:52.875816: step 133080, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 08:34:59.664138: step 133100, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-14 08:36:06.036926: step 133120, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 08:37:12.437374: step 133140, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 08:38:18.639638: step 133160, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-14 08:39:25.730422: step 133180, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 08:40:32.377672: step 133200, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-14 08:41:39.361688: step 133220, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-14 08:42:46.326544: step 133240, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-14 08:43:52.338771: step 133260, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 08:44:59.086810: step 133280, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 08:46:05.314327: step 133300, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 08:47:12.025312: step 133320, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-14 08:48:18.712078: step 133340, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 08:49:25.069270: step 133360, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 08:50:31.208161: step 133380, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 08:51:38.363515: step 133400, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 08:52:45.658727: step 133420, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-14 08:53:51.539623: step 133440, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 08:54:58.128409: step 133460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 08:56:04.610477: step 133480, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 08:57:10.825904: step 133500, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 08:58:18.286290: step 133520, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 08:59:24.791175: step 133540, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 09:00:32.025659: step 133560, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-14 09:01:39.488649: step 133580, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-14 09:02:46.287168: step 133600, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 09:03:52.276476: step 133620, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-14 09:04:59.654218: step 133640, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 09:06:06.798124: step 133660, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 09:07:14.207263: step 133680, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-14 09:08:20.766686: step 133700, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 09:09:26.743501: step 133720, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-14 09:10:32.821723: step 133740, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 09:11:40.560237: step 133760, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-14 09:12:46.935268: step 133780, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 09:13:53.520005: step 133800, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 09:15:00.135170: step 133820, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-14 09:16:06.734905: step 133840, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 09:17:12.990756: step 133860, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-14 09:18:20.016587: step 133880, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 09:19:26.680435: step 133900, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 09:20:34.813195: step 133920, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 09:21:41.766524: step 133940, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-14 09:22:48.736524: step 133960, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-14 09:23:54.914430: step 133980, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 09:25:01.107910: step 134000, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
[Eval] 2017-05-14 09:25:15.610300: step 134000, acc = 0.9433, f1 = 0.9416
[Test] 2017-05-14 09:25:26.174343: step 134000, acc = 0.9316, f1 = 0.9310
[Status] 2017-05-14 09:25:26.174469: step 134000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 09:26:32.892063: step 134020, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 09:27:39.661298: step 134040, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-14 09:28:47.278722: step 134060, loss = 0.0013, acc = 1.0000 (17.5 examples/sec; 3.653 sec/batch)
2017-05-14 09:29:53.288683: step 134080, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 09:30:59.615972: step 134100, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 09:32:06.648999: step 134120, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 09:33:13.190209: step 134140, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 09:34:19.868813: step 134160, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 09:35:27.377857: step 134180, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 09:36:33.625622: step 134200, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 09:37:40.269792: step 134220, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-14 09:38:46.982479: step 134240, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 09:39:53.404386: step 134260, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 09:40:59.824149: step 134280, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-14 09:42:06.218973: step 134300, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 09:43:13.035810: step 134320, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-14 09:44:19.213432: step 134340, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 09:45:26.001855: step 134360, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 09:46:33.158717: step 134380, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-14 09:47:39.225674: step 134400, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 09:48:46.215411: step 134420, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 09:49:52.763630: step 134440, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 09:51:00.696886: step 134460, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 09:52:07.719793: step 134480, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-14 09:53:13.972231: step 134500, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 09:54:20.505283: step 134520, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-14 09:55:27.326154: step 134540, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 09:56:33.901319: step 134560, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 09:57:40.795094: step 134580, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-14 09:58:46.992121: step 134600, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 09:59:53.493582: step 134620, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 10:01:00.418259: step 134640, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-14 10:02:08.206873: step 134660, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-14 10:03:16.213134: step 134680, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-14 10:04:22.203690: step 134700, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-14 10:05:28.096799: step 134720, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 10:06:35.140204: step 134740, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 10:07:41.224708: step 134760, loss = 0.0026, acc = 0.9980 (18.9 examples/sec; 3.382 sec/batch)
2017-05-14 10:08:48.041344: step 134780, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 10:09:54.549244: step 134800, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-14 10:11:01.042718: step 134820, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-14 10:12:07.417100: step 134840, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 10:13:14.656072: step 134860, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-14 10:14:22.362519: step 134880, loss = 0.0033, acc = 0.9980 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 10:15:29.326845: step 134900, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 10:16:35.720413: step 134920, loss = 0.0025, acc = 0.9980 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 10:17:42.587148: step 134940, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 10:18:49.392044: step 134960, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-14 10:19:55.853699: step 134980, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-14 10:21:02.691836: step 135000, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-14 10:21:18.295251: step 135000, acc = 0.9446, f1 = 0.9428
[Test] 2017-05-14 10:21:28.749546: step 135000, acc = 0.9329, f1 = 0.9324
[Status] 2017-05-14 10:21:28.749636: step 135000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 10:22:34.713251: step 135020, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 10:23:43.006319: step 135040, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-14 10:24:49.880276: step 135060, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 10:25:56.991226: step 135080, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-14 10:27:03.374172: step 135100, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 10:28:09.545657: step 135120, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 10:29:16.143669: step 135140, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 10:30:22.715156: step 135160, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 10:31:28.730103: step 135180, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 10:32:35.306072: step 135200, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-14 10:33:41.874303: step 135220, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-14 10:34:47.984170: step 135240, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 10:35:55.942047: step 135260, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-14 10:37:02.296104: step 135280, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 10:38:09.421997: step 135300, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-14 10:39:16.222624: step 135320, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 10:40:22.980841: step 135340, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 10:41:29.648569: step 135360, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 10:42:36.907341: step 135380, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 10:43:42.990881: step 135400, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 10:44:50.213384: step 135420, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-14 10:45:57.236829: step 135440, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-14 10:47:04.085085: step 135460, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 10:48:10.341899: step 135480, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-14 10:49:17.679155: step 135500, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 10:50:24.098050: step 135520, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 10:51:30.730187: step 135540, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-14 10:52:37.199940: step 135560, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 10:53:44.124981: step 135580, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 10:54:50.831109: step 135600, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-14 10:55:58.641372: step 135620, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 10:57:06.257929: step 135640, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-14 10:58:13.684943: step 135660, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 10:59:20.146223: step 135680, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 11:00:27.026850: step 135700, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-14 11:01:33.663094: step 135720, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 11:02:40.859324: step 135740, loss = 0.0011, acc = 1.0000 (18.0 examples/sec; 3.557 sec/batch)
2017-05-14 11:03:48.243596: step 135760, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 11:04:54.466688: step 135780, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-14 11:06:01.715433: step 135800, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-14 11:07:08.977026: step 135820, loss = 0.0024, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-14 11:08:16.963184: step 135840, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-14 11:09:24.119048: step 135860, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 11:10:30.904837: step 135880, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 11:11:37.170671: step 135900, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-14 11:12:43.937299: step 135920, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 11:13:50.908101: step 135940, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 11:14:57.483413: step 135960, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 11:16:03.713020: step 135980, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 11:17:09.992359: step 136000, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
[Eval] 2017-05-14 11:17:25.560197: step 136000, acc = 0.9448, f1 = 0.9430
[Test] 2017-05-14 11:17:36.002867: step 136000, acc = 0.9328, f1 = 0.9323
[Status] 2017-05-14 11:17:36.002977: step 136000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 11:18:42.111734: step 136020, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 11:19:49.763946: step 136040, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 11:20:56.148802: step 136060, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 11:22:04.558466: step 136080, loss = 0.0023, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-14 11:23:10.965922: step 136100, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-14 11:24:17.153630: step 136120, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 11:25:24.294874: step 136140, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-14 11:26:30.463434: step 136160, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 11:27:36.804487: step 136180, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 11:28:43.506092: step 136200, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 11:29:50.467462: step 136220, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 11:30:56.856071: step 136240, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 11:32:03.291230: step 136260, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 11:33:10.662079: step 136280, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-14 11:34:17.854571: step 136300, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 11:35:24.719815: step 136320, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 11:36:31.254857: step 136340, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 11:37:38.861370: step 136360, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-14 11:38:45.628303: step 136380, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 11:39:51.778372: step 136400, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 11:40:59.588571: step 136420, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 11:42:06.606270: step 136440, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-14 11:43:13.539348: step 136460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 11:44:19.904067: step 136480, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 11:45:26.568845: step 136500, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 11:46:32.556761: step 136520, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 11:47:39.742038: step 136540, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 11:48:47.984382: step 136560, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 11:49:54.047590: step 136580, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 11:51:00.599935: step 136600, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 11:52:07.515431: step 136620, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 11:53:14.503761: step 136640, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-14 11:54:20.932540: step 136660, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 11:55:27.648992: step 136680, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-14 11:56:34.932367: step 136700, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-14 11:57:41.144369: step 136720, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 11:58:47.445120: step 136740, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-14 11:59:53.897241: step 136760, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 12:01:01.349514: step 136780, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-14 12:02:07.422403: step 136800, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 12:03:14.564997: step 136820, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 12:04:20.748353: step 136840, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-14 12:05:27.290998: step 136860, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 12:06:33.379324: step 136880, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-14 12:07:40.923114: step 136900, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-14 12:08:47.561849: step 136920, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 12:09:54.970435: step 136940, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-14 12:11:02.288416: step 136960, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 12:12:08.332193: step 136980, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 12:13:14.919465: step 137000, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
[Eval] 2017-05-14 12:13:30.253118: step 137000, acc = 0.9442, f1 = 0.9425
[Test] 2017-05-14 12:13:40.814627: step 137000, acc = 0.9326, f1 = 0.9320
[Status] 2017-05-14 12:13:40.814730: step 137000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 12:14:47.226048: step 137020, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 12:15:54.296156: step 137040, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 12:17:01.507670: step 137060, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 12:18:07.590956: step 137080, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 12:19:14.509501: step 137100, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 12:20:22.489528: step 137120, loss = 0.0012, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-14 12:21:30.412328: step 137140, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-14 12:22:37.268961: step 137160, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-14 12:23:44.983684: step 137180, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-14 12:24:52.378165: step 137200, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-14 12:26:00.310923: step 137220, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-14 12:27:07.119735: step 137240, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-14 12:28:14.983726: step 137260, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 12:29:23.328746: step 137280, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 12:30:30.397136: step 137300, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 12:31:37.467109: step 137320, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 12:32:44.504894: step 137340, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-14 12:33:51.315574: step 137360, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-14 12:34:58.646237: step 137380, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 12:36:05.587885: step 137400, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-14 12:37:13.025124: step 137420, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 12:38:19.372991: step 137440, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 12:39:25.642709: step 137460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-14 12:40:32.103722: step 137480, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-14 12:41:38.585415: step 137500, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-14 12:42:45.800652: step 137520, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-14 12:43:51.965862: step 137540, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 12:44:58.387336: step 137560, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 12:46:05.601365: step 137580, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-14 12:47:12.724765: step 137600, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-14 12:48:19.706770: step 137620, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 12:49:26.510168: step 137640, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 12:50:32.519901: step 137660, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 12:51:39.202954: step 137680, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 12:52:45.943886: step 137700, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 12:53:52.635962: step 137720, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-14 12:54:59.619530: step 137740, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 12:56:06.205105: step 137760, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 12:57:13.669502: step 137780, loss = 0.0019, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-14 12:58:21.203691: step 137800, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-14 12:59:27.817177: step 137820, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 13:00:35.217377: step 137840, loss = 0.0025, acc = 0.9980 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 13:01:41.248408: step 137860, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 13:02:49.177018: step 137880, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-14 13:03:56.491346: step 137900, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-14 13:05:03.921294: step 137920, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-14 13:06:11.430177: step 137940, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-14 13:07:17.641068: step 137960, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 13:08:24.166676: step 137980, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 13:09:31.473020: step 138000, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
[Eval] 2017-05-14 13:09:46.795728: step 138000, acc = 0.9442, f1 = 0.9424
[Test] 2017-05-14 13:09:57.514227: step 138000, acc = 0.9324, f1 = 0.9318
[Status] 2017-05-14 13:09:57.514321: step 138000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 13:11:04.779052: step 138020, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 13:12:11.374168: step 138040, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 13:13:17.926451: step 138060, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 13:14:24.417512: step 138080, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 13:15:32.023678: step 138100, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 13:16:39.160867: step 138120, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-14 13:17:45.470337: step 138140, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 13:18:52.219317: step 138160, loss = 0.0057, acc = 0.9980 (19.6 examples/sec; 3.264 sec/batch)
2017-05-14 13:19:58.091350: step 138180, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 13:21:05.955064: step 138200, loss = 0.0016, acc = 1.0000 (17.7 examples/sec; 3.625 sec/batch)
2017-05-14 13:22:13.027585: step 138220, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-14 13:23:19.689374: step 138240, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-14 13:24:27.075467: step 138260, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 13:25:33.953085: step 138280, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 13:26:40.879804: step 138300, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-14 13:27:47.699224: step 138320, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-14 13:28:54.190983: step 138340, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 13:30:00.761256: step 138360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-14 13:31:06.805201: step 138380, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 13:32:14.320481: step 138400, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-14 13:33:20.301972: step 138420, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 13:34:26.763079: step 138440, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 13:35:33.746492: step 138460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 13:36:41.161133: step 138480, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 13:37:48.791065: step 138500, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-14 13:38:56.006104: step 138520, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-14 13:40:04.027396: step 138540, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-14 13:41:11.334983: step 138560, loss = 0.0028, acc = 0.9980 (18.6 examples/sec; 3.447 sec/batch)
2017-05-14 13:42:19.341333: step 138580, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-14 13:43:25.954813: step 138600, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 13:44:32.156980: step 138620, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 13:45:38.819730: step 138640, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 13:46:46.186648: step 138660, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 13:47:53.484337: step 138680, loss = 0.0011, acc = 1.0000 (17.8 examples/sec; 3.603 sec/batch)
2017-05-14 13:49:01.044355: step 138700, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 13:50:07.677874: step 138720, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-14 13:51:14.913547: step 138740, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 13:52:21.567164: step 138760, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 13:53:28.008035: step 138780, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-14 13:54:34.726791: step 138800, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 13:55:42.354170: step 138820, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-14 13:56:49.170855: step 138840, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 13:57:58.268577: step 138860, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-14 13:59:05.337330: step 138880, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 14:00:11.953895: step 138900, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 14:01:19.510449: step 138920, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 14:02:26.322095: step 138940, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-14 14:03:32.438002: step 138960, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 14:04:38.687998: step 138980, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 14:05:45.691439: step 139000, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
[Eval] 2017-05-14 14:06:00.115577: step 139000, acc = 0.9438, f1 = 0.9420
[Test] 2017-05-14 14:06:09.736951: step 139000, acc = 0.9324, f1 = 0.9319
[Status] 2017-05-14 14:06:09.737025: step 139000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 14:07:15.782414: step 139020, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 14:08:22.848914: step 139040, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-14 14:09:28.992744: step 139060, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 14:10:35.217274: step 139080, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-14 14:11:42.724366: step 139100, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-14 14:12:50.326127: step 139120, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-14 14:13:58.575797: step 139140, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 14:15:04.872959: step 139160, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-14 14:16:12.126445: step 139180, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 14:17:19.963539: step 139200, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.526 sec/batch)
2017-05-14 14:18:27.326268: step 139220, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-14 14:19:34.685635: step 139240, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-14 14:20:42.582213: step 139260, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 14:21:49.576881: step 139280, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 14:22:55.644799: step 139300, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 14:24:02.074678: step 139320, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 14:25:10.033390: step 139340, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-14 14:26:17.557410: step 139360, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 14:27:24.453663: step 139380, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-14 14:28:31.208762: step 139400, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 14:29:38.133112: step 139420, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 14:30:45.426016: step 139440, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 14:31:51.616676: step 139460, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 14:32:58.284343: step 139480, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 14:34:05.072097: step 139500, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-14 14:35:11.994269: step 139520, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-14 14:36:18.148464: step 139540, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 14:37:25.047589: step 139560, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 14:38:31.625968: step 139580, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 14:39:38.663656: step 139600, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 14:40:45.426145: step 139620, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-14 14:41:52.195931: step 139640, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 14:42:58.651373: step 139660, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 14:44:06.173830: step 139680, loss = 0.0022, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-14 14:45:13.130858: step 139700, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 14:46:21.360322: step 139720, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-14 14:47:27.861977: step 139740, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-14 14:48:36.364768: step 139760, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 14:49:42.901687: step 139780, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 14:50:49.643120: step 139800, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 14:51:57.009924: step 139820, loss = 0.0026, acc = 0.9980 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 14:53:03.721139: step 139840, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 14:54:10.893192: step 139860, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 14:55:17.283468: step 139880, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 14:56:23.986173: step 139900, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-14 14:57:30.361508: step 139920, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 14:58:36.554499: step 139940, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 14:59:43.159491: step 139960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 15:00:49.775587: step 139980, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 15:01:56.132486: step 140000, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-14 15:02:10.410215: step 140000, acc = 0.9443, f1 = 0.9425
[Test] 2017-05-14 15:02:20.145075: step 140000, acc = 0.9322, f1 = 0.9316
[Status] 2017-05-14 15:02:20.145253: step 140000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 15:03:26.837082: step 140020, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 15:04:33.886148: step 140040, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-14 15:05:41.036946: step 140060, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 15:06:48.341368: step 140080, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-14 15:07:55.348158: step 140100, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 15:09:03.782384: step 140120, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 15:10:09.852814: step 140140, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 15:11:16.572056: step 140160, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 15:12:23.684435: step 140180, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-14 15:13:29.994038: step 140200, loss = 0.0017, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-14 15:14:36.897252: step 140220, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-14 15:15:43.712933: step 140240, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-14 15:16:51.148886: step 140260, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-14 15:17:57.058813: step 140280, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-14 15:19:03.871815: step 140300, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 15:20:11.320992: step 140320, loss = 0.0014, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-14 15:21:18.194939: step 140340, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 15:22:25.326814: step 140360, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-14 15:23:31.998933: step 140380, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 15:24:38.962297: step 140400, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-14 15:25:45.805665: step 140420, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-14 15:26:53.840367: step 140440, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 15:27:59.939759: step 140460, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 15:29:07.066594: step 140480, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-14 15:30:13.052043: step 140500, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-14 15:31:20.335906: step 140520, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-14 15:32:28.006941: step 140540, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 15:33:34.777524: step 140560, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-14 15:34:43.025602: step 140580, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-14 15:35:50.362333: step 140600, loss = 0.0013, acc = 1.0000 (17.6 examples/sec; 3.646 sec/batch)
2017-05-14 15:36:58.140462: step 140620, loss = 0.0011, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-14 15:38:05.025977: step 140640, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 15:39:12.240150: step 140660, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 15:40:18.653279: step 140680, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 15:41:25.054693: step 140700, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 15:42:31.724864: step 140720, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 15:43:39.008782: step 140740, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 15:44:45.692974: step 140760, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-14 15:45:51.749667: step 140780, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 15:46:57.949924: step 140800, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 15:48:04.479048: step 140820, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 15:49:11.436969: step 140840, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-14 15:50:18.097276: step 140860, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 15:51:24.834463: step 140880, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 15:52:31.351013: step 140900, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 15:53:38.757007: step 140920, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-14 15:54:45.204133: step 140940, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 15:55:52.772191: step 140960, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-14 15:56:58.932562: step 140980, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 15:58:05.497814: step 141000, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
[Eval] 2017-05-14 15:58:20.836860: step 141000, acc = 0.9443, f1 = 0.9426
[Test] 2017-05-14 15:58:31.357611: step 141000, acc = 0.9325, f1 = 0.9319
[Status] 2017-05-14 15:58:31.357710: step 141000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 15:59:37.651881: step 141020, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-14 16:00:45.742293: step 141040, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 16:01:52.617421: step 141060, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-14 16:03:00.088848: step 141080, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 16:04:07.742970: step 141100, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 16:05:14.825814: step 141120, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 16:06:21.975698: step 141140, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-14 16:07:29.654952: step 141160, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-14 16:08:36.137010: step 141180, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 16:09:43.126875: step 141200, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-14 16:10:50.343388: step 141220, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 16:11:56.793313: step 141240, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-14 16:13:04.291711: step 141260, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-14 16:14:11.854007: step 141280, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-14 16:15:18.727971: step 141300, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-14 16:16:24.858601: step 141320, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 16:17:33.172231: step 141340, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-14 16:18:40.132236: step 141360, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 16:19:47.207943: step 141380, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 16:20:53.792574: step 141400, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-14 16:22:01.359450: step 141420, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-14 16:23:07.917128: step 141440, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 16:24:15.385712: step 141460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 16:25:21.976977: step 141480, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 16:26:29.586983: step 141500, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 16:27:37.009324: step 141520, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-14 16:28:44.226497: step 141540, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-14 16:29:51.460839: step 141560, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-14 16:30:57.529337: step 141580, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 16:32:05.164246: step 141600, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-14 16:33:11.518345: step 141620, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-14 16:34:18.248477: step 141640, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-14 16:35:24.389677: step 141660, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 16:36:31.157118: step 141680, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-14 16:37:37.900140: step 141700, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 16:38:44.941227: step 141720, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 16:39:51.771759: step 141740, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-14 16:40:59.275079: step 141760, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 16:42:06.072030: step 141780, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-14 16:43:12.581246: step 141800, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 16:44:19.001286: step 141820, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 16:45:25.547224: step 141840, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-14 16:46:32.916570: step 141860, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 16:47:40.694858: step 141880, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-14 16:48:47.301723: step 141900, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 16:49:53.360952: step 141920, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 16:51:01.166410: step 141940, loss = 0.0024, acc = 0.9980 (18.6 examples/sec; 3.444 sec/batch)
2017-05-14 16:52:08.493892: step 141960, loss = 0.0042, acc = 0.9980 (17.9 examples/sec; 3.567 sec/batch)
2017-05-14 16:53:15.578800: step 141980, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 16:54:23.679471: step 142000, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
[Eval] 2017-05-14 16:54:39.133853: step 142000, acc = 0.9414, f1 = 0.9396
[Test] 2017-05-14 16:54:49.821646: step 142000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-14 16:54:49.821745: step 142000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 16:55:56.355389: step 142020, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 16:57:03.574333: step 142040, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-14 16:58:09.872752: step 142060, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 16:59:15.908915: step 142080, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 17:00:22.645823: step 142100, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 17:01:28.891246: step 142120, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 17:02:36.967882: step 142140, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-14 17:03:43.420352: step 142160, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-14 17:04:49.935601: step 142180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 17:05:57.034029: step 142200, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-14 17:07:04.894782: step 142220, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-14 17:08:12.458096: step 142240, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-14 17:09:19.361548: step 142260, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 17:10:26.483863: step 142280, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-14 17:11:33.569326: step 142300, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-14 17:12:41.291381: step 142320, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-14 17:13:48.978512: step 142340, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-14 17:14:55.981262: step 142360, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 17:16:02.474361: step 142380, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-14 17:17:09.581354: step 142400, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-14 17:18:16.224496: step 142420, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 17:19:22.811307: step 142440, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 17:20:29.959378: step 142460, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-14 17:21:36.430077: step 142480, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-14 17:22:43.373467: step 142500, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 17:23:50.797668: step 142520, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 17:24:57.489418: step 142540, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-14 17:26:04.023338: step 142560, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-14 17:27:11.381501: step 142580, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-14 17:28:17.715480: step 142600, loss = 13.9863, acc = 0.4640 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 17:29:24.261374: step 142620, loss = 0.0613, acc = 0.9960 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 17:30:31.214779: step 142640, loss = 0.4841, acc = 0.9740 (19.3 examples/sec; 3.322 sec/batch)
2017-05-14 17:31:37.918864: step 142660, loss = 0.1409, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 17:32:44.252220: step 142680, loss = 0.1511, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 17:33:51.203137: step 142700, loss = 0.1806, acc = 0.9980 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 17:34:57.659701: step 142720, loss = 0.1857, acc = 0.9980 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 17:36:04.059735: step 142740, loss = 0.1591, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 17:37:10.908970: step 142760, loss = 0.1693, acc = 0.9960 (19.0 examples/sec; 3.373 sec/batch)
2017-05-14 17:38:18.088095: step 142780, loss = 0.1445, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 17:39:24.422271: step 142800, loss = 0.1653, acc = 0.9960 (19.5 examples/sec; 3.283 sec/batch)
2017-05-14 17:40:32.072263: step 142820, loss = 0.1344, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 17:41:39.240539: step 142840, loss = 0.1646, acc = 0.9960 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 17:42:46.606731: step 142860, loss = 0.1294, acc = 0.9980 (19.1 examples/sec; 3.359 sec/batch)
2017-05-14 17:43:53.373922: step 142880, loss = 0.1336, acc = 0.9980 (18.7 examples/sec; 3.430 sec/batch)
2017-05-14 17:45:01.661603: step 142900, loss = 0.1423, acc = 0.9980 (17.8 examples/sec; 3.587 sec/batch)
2017-05-14 17:46:07.979586: step 142920, loss = 0.1156, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-14 17:47:15.342967: step 142940, loss = 0.1124, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 17:48:22.110965: step 142960, loss = 0.1094, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 17:49:29.123776: step 142980, loss = 0.1065, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 17:50:35.610104: step 143000, loss = 0.1055, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
[Eval] 2017-05-14 17:50:51.202868: step 143000, acc = 0.9351, f1 = 0.9331
[Test] 2017-05-14 17:51:01.901663: step 143000, acc = 0.9218, f1 = 0.9212
[Status] 2017-05-14 17:51:01.901757: step 143000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 17:52:08.934514: step 143020, loss = 0.1034, acc = 0.9980 (18.7 examples/sec; 3.418 sec/batch)
2017-05-14 17:53:14.982895: step 143040, loss = 0.0989, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 17:54:22.323427: step 143060, loss = 0.0963, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-14 17:55:28.445489: step 143080, loss = 0.0949, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 17:56:35.867761: step 143100, loss = 0.0917, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-14 17:57:42.831455: step 143120, loss = 0.0895, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 17:58:50.292424: step 143140, loss = 0.0874, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 17:59:58.036178: step 143160, loss = 0.0854, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 18:01:05.577904: step 143180, loss = 0.0834, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 18:02:12.597088: step 143200, loss = 0.0814, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 18:03:18.882004: step 143220, loss = 0.0796, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 18:04:25.496237: step 143240, loss = 0.0777, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 18:05:32.368984: step 143260, loss = 0.0760, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-14 18:06:39.018577: step 143280, loss = 0.0744, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 18:07:45.754265: step 143300, loss = 0.0726, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 18:08:52.624662: step 143320, loss = 0.0710, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-14 18:09:59.298497: step 143340, loss = 0.0701, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 18:11:05.866677: step 143360, loss = 0.0678, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 18:12:12.076264: step 143380, loss = 0.0664, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 18:13:19.783846: step 143400, loss = 0.0872, acc = 0.9980 (17.8 examples/sec; 3.593 sec/batch)
2017-05-14 18:14:26.540389: step 143420, loss = 0.0636, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 18:15:33.171686: step 143440, loss = 0.0926, acc = 0.9980 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 18:16:40.310318: step 143460, loss = 0.0607, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-14 18:17:46.150338: step 143480, loss = 0.0594, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-14 18:18:53.581989: step 143500, loss = 0.0585, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-14 18:20:00.203409: step 143520, loss = 0.0570, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 18:21:06.965262: step 143540, loss = 0.0559, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 18:22:13.883863: step 143560, loss = 0.0544, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-14 18:23:20.228906: step 143580, loss = 0.0532, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 18:24:26.969510: step 143600, loss = 0.0526, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 18:25:33.651696: step 143620, loss = 0.0511, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 18:26:41.309859: step 143640, loss = 0.0499, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 18:27:48.606769: step 143660, loss = 0.0488, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-14 18:28:55.822122: step 143680, loss = 0.0478, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-14 18:30:02.236618: step 143700, loss = 0.0570, acc = 0.9980 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 18:31:09.713539: step 143720, loss = 0.0458, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 18:32:16.469599: step 143740, loss = 0.0448, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 18:33:23.155479: step 143760, loss = 0.0438, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 18:34:29.450339: step 143780, loss = 0.0429, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-14 18:35:35.760310: step 143800, loss = 0.0460, acc = 0.9980 (19.5 examples/sec; 3.289 sec/batch)
2017-05-14 18:36:44.118518: step 143820, loss = 0.0411, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-14 18:37:51.035723: step 143840, loss = 0.0403, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-14 18:38:57.863975: step 143860, loss = 0.0398, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-14 18:40:05.366685: step 143880, loss = 0.0416, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 18:41:12.185548: step 143900, loss = 0.0377, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-14 18:42:19.558335: step 143920, loss = 0.0381, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-14 18:43:26.698819: step 143940, loss = 0.0366, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 18:44:33.921707: step 143960, loss = 0.0355, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-14 18:45:40.778502: step 143980, loss = 0.0347, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-14 18:46:50.727870: step 144000, loss = 0.0351, acc = 1.0000 (17.8 examples/sec; 3.593 sec/batch)
[Eval] 2017-05-14 18:47:06.406681: step 144000, acc = 0.9361, f1 = 0.9342
[Test] 2017-05-14 18:47:17.076387: step 144000, acc = 0.9232, f1 = 0.9226
[Status] 2017-05-14 18:47:17.076472: step 144000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 18:48:23.649817: step 144020, loss = 0.0334, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-14 18:49:30.313019: step 144040, loss = 0.0325, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 18:50:37.564719: step 144060, loss = 0.0321, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 18:51:44.744666: step 144080, loss = 0.0312, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 18:52:51.581064: step 144100, loss = 0.0305, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-14 18:53:58.883066: step 144120, loss = 0.0309, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 18:55:06.614328: step 144140, loss = 0.0293, acc = 1.0000 (17.4 examples/sec; 3.679 sec/batch)
2017-05-14 18:56:13.750080: step 144160, loss = 0.0288, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 18:57:21.198580: step 144180, loss = 0.0296, acc = 0.9980 (19.0 examples/sec; 3.360 sec/batch)
2017-05-14 18:58:28.922509: step 144200, loss = 0.0275, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-14 18:59:36.172623: step 144220, loss = 0.0269, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-14 19:00:42.438111: step 144240, loss = 0.0265, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 19:01:48.541660: step 144260, loss = 0.0261, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-14 19:02:54.617781: step 144280, loss = 0.0255, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 19:04:01.033147: step 144300, loss = 0.0249, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 19:05:08.090969: step 144320, loss = 0.0243, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 19:06:15.424912: step 144340, loss = 0.0238, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-14 19:07:22.016653: step 144360, loss = 0.0246, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 19:08:30.294650: step 144380, loss = 0.0236, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 19:09:36.250189: step 144400, loss = 0.0223, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 19:10:44.288421: step 144420, loss = 0.0219, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 19:11:51.404016: step 144440, loss = 0.0215, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-14 19:12:58.077434: step 144460, loss = 0.0210, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-14 19:14:05.826463: step 144480, loss = 0.0206, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 19:15:12.123242: step 144500, loss = 0.0202, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 19:16:18.914852: step 144520, loss = 0.0197, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-14 19:17:25.409875: step 144540, loss = 0.0195, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 19:18:32.590582: step 144560, loss = 0.0189, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-14 19:19:40.093147: step 144580, loss = 0.0185, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 19:20:46.778843: step 144600, loss = 0.0183, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-14 19:21:53.185115: step 144620, loss = 0.0178, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 19:23:00.570291: step 144640, loss = 0.0187, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-14 19:24:06.634228: step 144660, loss = 0.0171, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 19:25:12.707655: step 144680, loss = 0.0168, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 19:26:18.924100: step 144700, loss = 0.0164, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 19:27:25.277078: step 144720, loss = 0.0162, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-14 19:28:32.703325: step 144740, loss = 0.0365, acc = 0.9960 (18.8 examples/sec; 3.401 sec/batch)
2017-05-14 19:29:39.593646: step 144760, loss = 0.0155, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 19:30:46.970401: step 144780, loss = 0.0154, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-14 19:31:55.078675: step 144800, loss = 0.0148, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 19:33:01.934080: step 144820, loss = 0.0145, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-14 19:34:08.120959: step 144840, loss = 0.0142, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 19:35:15.858910: step 144860, loss = 0.0139, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-14 19:36:23.042033: step 144880, loss = 0.0137, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 19:37:30.376795: step 144900, loss = 0.0134, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 19:38:37.054464: step 144920, loss = 0.0132, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-14 19:39:44.695063: step 144940, loss = 0.0128, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 19:40:51.628226: step 144960, loss = 0.0125, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 19:41:59.402205: step 144980, loss = 0.0123, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-14 19:43:05.934733: step 145000, loss = 0.0121, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
[Eval] 2017-05-14 19:43:21.334628: step 145000, acc = 0.9356, f1 = 0.9337
[Test] 2017-05-14 19:43:32.047976: step 145000, acc = 0.9236, f1 = 0.9230
[Status] 2017-05-14 19:43:32.048064: step 145000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 19:44:38.934495: step 145020, loss = 0.0118, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 19:45:45.790923: step 145040, loss = 0.0116, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-14 19:46:52.766603: step 145060, loss = 0.0113, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 19:48:00.117038: step 145080, loss = 0.0111, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 19:49:07.462888: step 145100, loss = 0.0120, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-14 19:50:15.529956: step 145120, loss = 0.0107, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-14 19:51:22.079754: step 145140, loss = 0.0111, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 19:52:29.698556: step 145160, loss = 0.0102, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 19:53:36.490557: step 145180, loss = 0.0100, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 19:54:43.636668: step 145200, loss = 0.0102, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-14 19:55:50.137863: step 145220, loss = 0.0096, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 19:56:57.524478: step 145240, loss = 0.0094, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-14 19:58:04.235856: step 145260, loss = 0.0092, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 19:59:10.435364: step 145280, loss = 0.0090, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 20:00:18.663551: step 145300, loss = 0.0090, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 20:01:26.606104: step 145320, loss = 0.0099, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-14 20:02:33.489052: step 145340, loss = 0.0085, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-14 20:03:40.619578: step 145360, loss = 0.0083, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-14 20:04:47.846492: step 145380, loss = 0.0083, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-14 20:05:55.258702: step 145400, loss = 0.0095, acc = 0.9980 (19.2 examples/sec; 3.335 sec/batch)
2017-05-14 20:07:02.439245: step 145420, loss = 0.0078, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-14 20:08:09.100994: step 145440, loss = 0.0076, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 20:09:16.644987: step 145460, loss = 0.0075, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-14 20:10:24.409468: step 145480, loss = 0.0073, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-14 20:11:33.229019: step 145500, loss = 0.0071, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 20:12:41.348603: step 145520, loss = 0.0072, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-14 20:13:48.208368: step 145540, loss = 0.0069, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-14 20:14:56.106315: step 145560, loss = 0.0076, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-14 20:16:02.769094: step 145580, loss = 0.0067, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 20:17:09.746391: step 145600, loss = 0.0066, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-14 20:18:17.019398: step 145620, loss = 0.0064, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-14 20:19:23.702163: step 145640, loss = 0.0062, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 20:20:30.719275: step 145660, loss = 0.0061, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 20:21:38.078939: step 145680, loss = 0.0066, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 20:22:44.082632: step 145700, loss = 0.0059, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 20:23:50.514638: step 145720, loss = 0.0057, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 20:24:57.325281: step 145740, loss = 0.0056, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-14 20:26:03.458177: step 145760, loss = 0.0060, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 20:27:09.719599: step 145780, loss = 0.0054, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 20:28:17.063789: step 145800, loss = 0.0053, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-14 20:29:24.202294: step 145820, loss = 0.0053, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-14 20:30:31.193652: step 145840, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 20:31:37.497878: step 145860, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-14 20:32:44.550558: step 145880, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 20:33:51.648886: step 145900, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 20:34:57.912215: step 145920, loss = 0.0062, acc = 0.9980 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 20:36:04.284545: step 145940, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 20:37:10.913216: step 145960, loss = 0.0045, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 20:38:18.264065: step 145980, loss = 0.0046, acc = 1.0000 (18.0 examples/sec; 3.564 sec/batch)
2017-05-14 20:39:26.951216: step 146000, loss = 0.0048, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
[Eval] 2017-05-14 20:39:41.342369: step 146000, acc = 0.9359, f1 = 0.9341
[Test] 2017-05-14 20:39:51.848838: step 146000, acc = 0.9239, f1 = 0.9233
[Status] 2017-05-14 20:39:51.848943: step 146000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 20:40:58.835556: step 146020, loss = 0.0043, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-14 20:42:05.754207: step 146040, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 20:43:13.229669: step 146060, loss = 0.0043, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-14 20:44:19.579877: step 146080, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 20:45:27.181623: step 146100, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 20:46:33.991693: step 146120, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 20:47:40.323484: step 146140, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-14 20:48:49.016566: step 146160, loss = 0.0050, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-14 20:49:56.422079: step 146180, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 20:51:04.358402: step 146200, loss = 0.0036, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-14 20:52:11.613000: step 146220, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 20:53:17.883628: step 146240, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 20:54:24.757723: step 146260, loss = 0.0033, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-14 20:55:33.393813: step 146280, loss = 0.0033, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-14 20:56:40.306479: step 146300, loss = 0.0032, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 20:57:47.127040: step 146320, loss = 0.0031, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-14 20:58:53.437267: step 146340, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 21:00:00.918508: step 146360, loss = 0.0032, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-14 21:01:08.124970: step 146380, loss = 0.0031, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-14 21:02:14.645771: step 146400, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-14 21:03:23.650918: step 146420, loss = 0.0031, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-14 21:04:31.662397: step 146440, loss = 0.0032, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-14 21:05:38.195941: step 146460, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 21:06:46.198710: step 146480, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 21:07:52.908385: step 146500, loss = 0.0035, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-14 21:09:00.106152: step 146520, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 21:10:07.597938: step 146540, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 21:11:13.927620: step 146560, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 21:12:19.875181: step 146580, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 21:13:26.253303: step 146600, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-14 21:14:32.896752: step 146620, loss = 0.0029, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-14 21:15:39.190221: step 146640, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 21:16:47.057974: step 146660, loss = 0.0027, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-14 21:17:54.588094: step 146680, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-14 21:19:00.864236: step 146700, loss = 0.0040, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-14 21:20:07.987524: step 146720, loss = 0.0096, acc = 0.9960 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 21:21:15.941033: step 146740, loss = 0.0023, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-14 21:22:22.100376: step 146760, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-14 21:23:29.199685: step 146780, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-14 21:24:36.180700: step 146800, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-14 21:25:42.439491: step 146820, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-14 21:26:48.650085: step 146840, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 21:27:56.226508: step 146860, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-14 21:29:03.310316: step 146880, loss = 0.0023, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 21:30:10.153887: step 146900, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 21:31:16.307446: step 146920, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 21:32:23.592171: step 146940, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 21:33:31.537207: step 146960, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 21:34:37.958900: step 146980, loss = 0.0026, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 21:35:44.818914: step 147000, loss = 0.0033, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
[Eval] 2017-05-14 21:36:00.278306: step 147000, acc = 0.9384, f1 = 0.9366
[Test] 2017-05-14 21:36:10.217100: step 147000, acc = 0.9263, f1 = 0.9257
[Status] 2017-05-14 21:36:10.217188: step 147000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 21:37:17.663117: step 147020, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-14 21:38:24.614309: step 147040, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-14 21:39:31.873087: step 147060, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-14 21:40:39.247902: step 147080, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-14 21:41:45.436764: step 147100, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-14 21:42:52.057853: step 147120, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-14 21:43:59.424946: step 147140, loss = 0.0021, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-14 21:45:06.491920: step 147160, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-14 21:46:13.725389: step 147180, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-14 21:47:20.373591: step 147200, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-14 21:48:27.764969: step 147220, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-14 21:49:33.880733: step 147240, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-14 21:50:40.837297: step 147260, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-14 21:51:48.214487: step 147280, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-14 21:52:54.551891: step 147300, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 21:54:01.777895: step 147320, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-14 21:55:08.549442: step 147340, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-14 21:56:15.552109: step 147360, loss = 0.0056, acc = 0.9980 (19.1 examples/sec; 3.344 sec/batch)
2017-05-14 21:57:22.714312: step 147380, loss = 0.0036, acc = 0.9980 (19.1 examples/sec; 3.357 sec/batch)
2017-05-14 21:58:29.523190: step 147400, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 21:59:36.497432: step 147420, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-14 22:00:42.484139: step 147440, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-14 22:01:49.028978: step 147460, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-14 22:02:55.760258: step 147480, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-14 22:04:02.048490: step 147500, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 22:05:09.178083: step 147520, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 22:06:15.538905: step 147540, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-14 22:07:23.504368: step 147560, loss = 0.0016, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-14 22:08:30.689623: step 147580, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-14 22:09:36.715936: step 147600, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 22:10:43.766239: step 147620, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-14 22:11:50.632839: step 147640, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-14 22:12:57.912799: step 147660, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-14 22:14:04.807301: step 147680, loss = 0.0044, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 22:15:11.113087: step 147700, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 22:16:17.437433: step 147720, loss = 0.0020, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-14 22:17:24.013510: step 147740, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-14 22:18:31.421945: step 147760, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-14 22:19:37.795344: step 147780, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-14 22:20:44.336422: step 147800, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 22:21:52.112843: step 147820, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-14 22:22:58.770326: step 147840, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-14 22:24:06.437026: step 147860, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-14 22:25:13.581652: step 147880, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 22:26:19.590876: step 147900, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 22:27:26.933090: step 147920, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 22:28:33.712988: step 147940, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-14 22:29:40.694328: step 147960, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-14 22:30:46.923290: step 147980, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-14 22:31:53.550297: step 148000, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
[Eval] 2017-05-14 22:32:08.840834: step 148000, acc = 0.9404, f1 = 0.9386
[Test] 2017-05-14 22:32:18.889226: step 148000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-14 22:32:18.889294: step 148000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 22:33:25.116071: step 148020, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-14 22:34:31.578924: step 148040, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-14 22:35:39.157218: step 148060, loss = 0.0017, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-14 22:36:45.671551: step 148080, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 22:37:52.542964: step 148100, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-14 22:38:59.044889: step 148120, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-14 22:40:06.263341: step 148140, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-14 22:41:13.968722: step 148160, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-14 22:42:21.846952: step 148180, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-14 22:43:30.089760: step 148200, loss = 0.0030, acc = 0.9980 (18.6 examples/sec; 3.439 sec/batch)
2017-05-14 22:44:37.450727: step 148220, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-14 22:45:45.298697: step 148240, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 22:46:53.701230: step 148260, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-14 22:48:00.800310: step 148280, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 22:49:07.743063: step 148300, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-14 22:50:14.640331: step 148320, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 22:51:20.789054: step 148340, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-14 22:52:27.861245: step 148360, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-14 22:53:34.262231: step 148380, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 22:54:41.027504: step 148400, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-14 22:55:48.446594: step 148420, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-14 22:56:54.969798: step 148440, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-14 22:58:01.574763: step 148460, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 22:59:07.890192: step 148480, loss = 0.0040, acc = 0.9980 (19.3 examples/sec; 3.312 sec/batch)
2017-05-14 23:00:14.566784: step 148500, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-14 23:01:20.770013: step 148520, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-14 23:02:27.724653: step 148540, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-14 23:03:33.990443: step 148560, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 23:04:40.552387: step 148580, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-14 23:05:47.673216: step 148600, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-14 23:06:53.944213: step 148620, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-14 23:08:01.168520: step 148640, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-14 23:09:08.828136: step 148660, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-14 23:10:14.922034: step 148680, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-14 23:11:21.849167: step 148700, loss = 0.0061, acc = 0.9980 (18.7 examples/sec; 3.423 sec/batch)
2017-05-14 23:12:29.306648: step 148720, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-14 23:13:35.332042: step 148740, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-14 23:14:42.071046: step 148760, loss = 0.0031, acc = 0.9980 (19.0 examples/sec; 3.369 sec/batch)
2017-05-14 23:15:48.608899: step 148780, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-14 23:16:55.122904: step 148800, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-14 23:18:01.165401: step 148820, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-14 23:19:08.421618: step 148840, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-14 23:20:14.678148: step 148860, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-14 23:21:22.180213: step 148880, loss = 0.0018, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-14 23:22:28.176753: step 148900, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-14 23:23:34.508893: step 148920, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-14 23:24:40.942225: step 148940, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-14 23:25:47.065282: step 148960, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-14 23:26:53.326372: step 148980, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-14 23:28:00.214377: step 149000, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-14 23:28:15.660120: step 149000, acc = 0.9412, f1 = 0.9393
[Test] 2017-05-14 23:28:26.260082: step 149000, acc = 0.9287, f1 = 0.9281
[Status] 2017-05-14 23:28:26.260178: step 149000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-14 23:29:33.727747: step 149020, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-14 23:30:40.347521: step 149040, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-14 23:31:48.154065: step 149060, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.307 sec/batch)
2017-05-14 23:32:55.674593: step 149080, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-14 23:34:02.119042: step 149100, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-14 23:35:09.872279: step 149120, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-14 23:36:16.123960: step 149140, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-14 23:37:23.219047: step 149160, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-14 23:38:30.664670: step 149180, loss = 0.0016, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-14 23:39:37.515786: step 149200, loss = 0.0028, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-14 23:40:44.641782: step 149220, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-14 23:41:52.717448: step 149240, loss = 0.0015, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-14 23:43:00.469802: step 149260, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-14 23:44:08.362402: step 149280, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-14 23:45:15.842844: step 149300, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-14 23:46:23.423946: step 149320, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-14 23:47:30.345240: step 149340, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-14 23:48:37.375419: step 149360, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-14 23:49:45.813540: step 149380, loss = 0.0026, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-14 23:50:53.173524: step 149400, loss = 0.0029, acc = 0.9980 (18.3 examples/sec; 3.501 sec/batch)
2017-05-14 23:51:59.216051: step 149420, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-14 23:53:06.834437: step 149440, loss = 0.0020, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-14 23:54:14.071055: step 149460, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-14 23:55:21.832076: step 149480, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-14 23:56:29.818074: step 149500, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-14 23:57:36.170310: step 149520, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-14 23:58:44.020912: step 149540, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-14 23:59:51.293163: step 149560, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-15 00:00:57.622698: step 149580, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 00:02:04.776000: step 149600, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 00:03:12.079949: step 149620, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-15 00:04:18.652069: step 149640, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 00:05:25.373928: step 149660, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 00:06:33.073210: step 149680, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 00:07:40.193201: step 149700, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-15 00:08:46.841167: step 149720, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-15 00:09:53.712133: step 149740, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-15 00:10:59.483800: step 149760, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 00:12:06.446575: step 149780, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-15 00:13:16.341458: step 149800, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-15 00:14:24.430789: step 149820, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 00:15:31.736413: step 149840, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-15 00:16:38.247752: step 149860, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 00:17:45.378927: step 149880, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-15 00:18:51.751518: step 149900, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-15 00:19:58.188537: step 149920, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 00:21:05.740598: step 149940, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-15 00:22:11.977330: step 149960, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 00:23:19.503074: step 149980, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-15 00:24:27.147211: step 150000, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
[Eval] 2017-05-15 00:24:42.729523: step 150000, acc = 0.9418, f1 = 0.9399
[Test] 2017-05-15 00:24:53.402354: step 150000, acc = 0.9295, f1 = 0.9289
[Status] 2017-05-15 00:24:53.402448: step 150000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 00:26:00.365055: step 150020, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 00:27:08.559911: step 150040, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-15 00:28:15.763979: step 150060, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 00:29:22.564943: step 150080, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 00:30:28.741747: step 150100, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 00:31:36.047133: step 150120, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 00:32:42.873266: step 150140, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-15 00:33:49.541713: step 150160, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-15 00:34:56.424128: step 150180, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 00:36:04.190547: step 150200, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 00:37:10.577285: step 150220, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-15 00:38:17.228199: step 150240, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 00:39:23.508828: step 150260, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-15 00:40:31.193386: step 150280, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-15 00:41:37.613613: step 150300, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-15 00:42:44.578002: step 150320, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 00:43:51.103858: step 150340, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 00:44:57.840206: step 150360, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-15 00:46:04.193500: step 150380, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-15 00:47:11.036479: step 150400, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-15 00:48:17.635850: step 150420, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 00:49:24.171693: step 150440, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 00:50:31.502189: step 150460, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-15 00:51:37.812524: step 150480, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 00:52:44.873714: step 150500, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 00:53:52.137830: step 150520, loss = 0.0030, acc = 0.9980 (18.9 examples/sec; 3.390 sec/batch)
2017-05-15 00:54:58.081018: step 150540, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 00:56:05.786469: step 150560, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 00:57:11.763605: step 150580, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 00:58:18.592561: step 150600, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-15 00:59:26.011306: step 150620, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-15 01:00:32.729788: step 150640, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 01:01:39.997846: step 150660, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 01:02:46.068545: step 150680, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 01:03:52.254995: step 150700, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 01:04:58.451634: step 150720, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-15 01:06:06.335223: step 150740, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-15 01:07:12.556453: step 150760, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 01:08:18.819432: step 150780, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 01:09:25.941875: step 150800, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 01:10:33.137532: step 150820, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 01:11:40.207031: step 150840, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 01:12:46.598102: step 150860, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 01:13:52.681241: step 150880, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-15 01:14:59.669815: step 150900, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-15 01:16:07.416772: step 150920, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 01:17:13.346676: step 150940, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 01:18:19.743161: step 150960, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 01:19:25.921599: step 150980, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 01:20:33.155577: step 151000, loss = 0.0049, acc = 0.9980 (18.3 examples/sec; 3.493 sec/batch)
[Eval] 2017-05-15 01:20:48.707892: step 151000, acc = 0.9419, f1 = 0.9401
[Test] 2017-05-15 01:20:59.277474: step 151000, acc = 0.9297, f1 = 0.9292
[Status] 2017-05-15 01:20:59.277626: step 151000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 01:22:06.522886: step 151020, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-15 01:23:13.117038: step 151040, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 01:24:20.606939: step 151060, loss = 0.0026, acc = 0.9980 (18.6 examples/sec; 3.449 sec/batch)
2017-05-15 01:25:28.761422: step 151080, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-15 01:26:36.108018: step 151100, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-15 01:27:43.003245: step 151120, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-15 01:28:49.947590: step 151140, loss = 0.0021, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-15 01:29:57.443542: step 151160, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-15 01:31:04.667163: step 151180, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 01:32:13.051312: step 151200, loss = 0.0013, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-15 01:33:20.040476: step 151220, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 01:34:26.401016: step 151240, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 01:35:34.117799: step 151260, loss = 0.0014, acc = 1.0000 (17.6 examples/sec; 3.646 sec/batch)
2017-05-15 01:36:40.035028: step 151280, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 01:37:46.961084: step 151300, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 01:38:53.172839: step 151320, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 01:39:59.478745: step 151340, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-15 01:41:06.986999: step 151360, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-15 01:42:14.177319: step 151380, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-15 01:43:22.114187: step 151400, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 01:44:29.776833: step 151420, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 01:45:36.346855: step 151440, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 01:46:44.059560: step 151460, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-15 01:47:51.886084: step 151480, loss = 0.0025, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-15 01:48:57.780523: step 151500, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 01:50:04.039802: step 151520, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 01:51:10.678853: step 151540, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 01:52:18.098708: step 151560, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 01:53:24.756268: step 151580, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 01:54:31.148778: step 151600, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 01:55:39.292281: step 151620, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 01:56:45.598881: step 151640, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 01:57:53.767177: step 151660, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 01:59:00.455416: step 151680, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 02:00:07.741612: step 151700, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-15 02:01:14.734608: step 151720, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-15 02:02:20.955246: step 151740, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 02:03:27.440628: step 151760, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-15 02:04:33.624975: step 151780, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 02:05:41.641364: step 151800, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 02:06:49.656945: step 151820, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-15 02:07:55.820755: step 151840, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 02:09:02.156579: step 151860, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 02:10:09.185067: step 151880, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 02:11:16.120270: step 151900, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 02:12:22.603224: step 151920, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 02:13:28.918116: step 151940, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 02:14:35.438220: step 151960, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 02:15:41.996150: step 151980, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 02:16:48.612623: step 152000, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
[Eval] 2017-05-15 02:17:03.748963: step 152000, acc = 0.9424, f1 = 0.9405
[Test] 2017-05-15 02:17:14.130132: step 152000, acc = 0.9301, f1 = 0.9295
[Status] 2017-05-15 02:17:14.130238: step 152000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 02:18:21.141025: step 152020, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-15 02:19:28.270056: step 152040, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 02:20:35.732361: step 152060, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-15 02:21:42.426891: step 152080, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 02:22:50.224060: step 152100, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-15 02:23:56.968534: step 152120, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 02:25:04.446724: step 152140, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 02:26:10.647870: step 152160, loss = 0.0059, acc = 0.9980 (19.6 examples/sec; 3.270 sec/batch)
2017-05-15 02:27:17.508788: step 152180, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 02:28:25.164100: step 152200, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 02:29:33.437246: step 152220, loss = 0.0025, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-15 02:30:39.501887: step 152240, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 02:31:45.853966: step 152260, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 02:32:53.632207: step 152280, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-15 02:34:00.193576: step 152300, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 02:35:06.902392: step 152320, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 02:36:14.460269: step 152340, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-15 02:37:21.905509: step 152360, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-15 02:38:28.753625: step 152380, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-15 02:39:34.956147: step 152400, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 02:40:41.465286: step 152420, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 02:41:48.767925: step 152440, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-15 02:42:55.821055: step 152460, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-15 02:44:02.849134: step 152480, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-15 02:45:10.781337: step 152500, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-15 02:46:17.880461: step 152520, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 02:47:24.724582: step 152540, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 02:48:31.614552: step 152560, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-15 02:49:40.286542: step 152580, loss = 0.0014, acc = 1.0000 (17.6 examples/sec; 3.635 sec/batch)
2017-05-15 02:50:46.507321: step 152600, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 02:51:53.324013: step 152620, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 02:52:59.826233: step 152640, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 02:54:06.997551: step 152660, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 02:55:14.206951: step 152680, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 02:56:20.769917: step 152700, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 02:57:27.902150: step 152720, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 02:58:34.790941: step 152740, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 02:59:41.507635: step 152760, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 03:00:49.089144: step 152780, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-15 03:01:56.598025: step 152800, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 03:03:04.046259: step 152820, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 03:04:10.986681: step 152840, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 03:05:17.997580: step 152860, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 03:06:25.113028: step 152880, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-15 03:07:32.447292: step 152900, loss = 0.0021, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-15 03:08:39.721824: step 152920, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-15 03:09:47.132317: step 152940, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-15 03:10:53.863415: step 152960, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 03:12:01.469599: step 152980, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-15 03:13:08.550280: step 153000, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
[Eval] 2017-05-15 03:13:24.162185: step 153000, acc = 0.9426, f1 = 0.9408
[Test] 2017-05-15 03:13:34.726449: step 153000, acc = 0.9299, f1 = 0.9293
[Status] 2017-05-15 03:13:34.726543: step 153000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 03:14:40.731479: step 153020, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 03:15:47.089532: step 153040, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 03:16:53.782298: step 153060, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 03:18:01.114255: step 153080, loss = 0.0013, acc = 1.0000 (18.1 examples/sec; 3.536 sec/batch)
2017-05-15 03:19:08.870470: step 153100, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 03:20:15.631126: step 153120, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-15 03:21:23.043412: step 153140, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 03:22:29.823976: step 153160, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 03:23:37.292753: step 153180, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-15 03:24:44.504681: step 153200, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-15 03:25:53.505896: step 153220, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-15 03:27:00.325635: step 153240, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-15 03:28:07.967745: step 153260, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 03:29:15.618198: step 153280, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-15 03:30:23.172428: step 153300, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-15 03:31:29.218588: step 153320, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-15 03:32:35.798767: step 153340, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-15 03:33:42.372749: step 153360, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-15 03:34:49.472528: step 153380, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 03:35:56.049128: step 153400, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 03:37:03.334839: step 153420, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 03:38:09.397266: step 153440, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 03:39:15.796693: step 153460, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 03:40:23.003969: step 153480, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 03:41:29.112551: step 153500, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 03:42:35.520623: step 153520, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-15 03:43:41.983973: step 153540, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 03:44:48.344457: step 153560, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-15 03:45:54.590251: step 153580, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 03:47:01.859772: step 153600, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-15 03:48:10.149485: step 153620, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-15 03:49:17.925938: step 153640, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 03:50:25.433485: step 153660, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-15 03:51:33.780776: step 153680, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-15 03:52:42.718238: step 153700, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 03:53:50.027730: step 153720, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-15 03:54:57.868094: step 153740, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-15 03:56:04.067506: step 153760, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 03:57:10.918890: step 153780, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 03:58:18.093863: step 153800, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 03:59:25.177659: step 153820, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 04:00:32.057694: step 153840, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-15 04:01:38.284616: step 153860, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 04:02:46.045117: step 153880, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 04:03:54.067173: step 153900, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-15 04:05:00.587987: step 153920, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-15 04:06:08.304822: step 153940, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 04:07:15.284452: step 153960, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-15 04:08:22.916667: step 153980, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 04:09:29.083608: step 154000, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-15 04:09:44.537929: step 154000, acc = 0.9427, f1 = 0.9409
[Test] 2017-05-15 04:09:55.178214: step 154000, acc = 0.9304, f1 = 0.9298
[Status] 2017-05-15 04:09:55.178310: step 154000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 04:11:01.578496: step 154020, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-15 04:12:08.935703: step 154040, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-15 04:13:16.008933: step 154060, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 04:14:22.489653: step 154080, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 04:15:30.784349: step 154100, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-15 04:16:37.306459: step 154120, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 04:17:44.633154: step 154140, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 04:18:51.377596: step 154160, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 04:19:58.177792: step 154180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 04:21:04.973821: step 154200, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 04:22:13.441875: step 154220, loss = 0.0011, acc = 1.0000 (17.9 examples/sec; 3.570 sec/batch)
2017-05-15 04:23:20.182132: step 154240, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 04:24:27.402498: step 154260, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 04:25:33.916934: step 154280, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-15 04:26:41.028917: step 154300, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-15 04:27:47.622410: step 154320, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-15 04:28:54.838801: step 154340, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-15 04:30:01.551666: step 154360, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 04:31:08.579186: step 154380, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-15 04:32:15.699773: step 154400, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-15 04:33:23.424070: step 154420, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 04:34:30.497291: step 154440, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-15 04:35:36.658649: step 154460, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 04:36:43.785416: step 154480, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-15 04:37:53.036278: step 154500, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-15 04:39:01.756023: step 154520, loss = 0.0011, acc = 1.0000 (17.9 examples/sec; 3.568 sec/batch)
2017-05-15 04:40:08.801078: step 154540, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-15 04:41:15.462583: step 154560, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-15 04:42:22.298742: step 154580, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-15 04:43:29.850408: step 154600, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 04:44:36.734355: step 154620, loss = 0.0012, acc = 1.0000 (18.1 examples/sec; 3.533 sec/batch)
2017-05-15 04:45:44.351419: step 154640, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 04:46:50.574297: step 154660, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 04:47:57.781229: step 154680, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-15 04:49:04.394358: step 154700, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 04:50:10.354050: step 154720, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 04:51:16.507647: step 154740, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 04:52:23.236208: step 154760, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-15 04:53:29.435266: step 154780, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-15 04:54:35.727109: step 154800, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 04:55:41.831050: step 154820, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 04:56:50.612191: step 154840, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-15 04:57:58.540263: step 154860, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 04:59:06.126078: step 154880, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-15 05:00:13.272988: step 154900, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-15 05:01:20.206081: step 154920, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 05:02:26.269455: step 154940, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 05:03:32.381650: step 154960, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-15 05:04:39.085621: step 154980, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-15 05:05:45.332455: step 155000, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
[Eval] 2017-05-15 05:06:00.827729: step 155000, acc = 0.9427, f1 = 0.9408
[Test] 2017-05-15 05:06:11.371347: step 155000, acc = 0.9300, f1 = 0.9294
[Status] 2017-05-15 05:06:11.371443: step 155000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 05:07:18.976715: step 155020, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-15 05:08:25.025348: step 155040, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 05:09:32.241271: step 155060, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-15 05:10:38.328886: step 155080, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 05:11:44.509924: step 155100, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 05:12:51.020988: step 155120, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 05:13:57.969395: step 155140, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 05:15:05.245040: step 155160, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-15 05:16:12.643360: step 155180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-15 05:17:19.068178: step 155200, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-15 05:18:26.586866: step 155220, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-15 05:19:33.751012: step 155240, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 05:20:40.620943: step 155260, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 05:21:48.020906: step 155280, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-15 05:22:55.751256: step 155300, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-15 05:24:02.595768: step 155320, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-15 05:25:09.511036: step 155340, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 05:26:16.051956: step 155360, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 05:27:22.692496: step 155380, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 05:28:28.969350: step 155400, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 05:29:35.407073: step 155420, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-15 05:30:41.501237: step 155440, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 05:31:47.923259: step 155460, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 05:32:54.251748: step 155480, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-15 05:34:00.796265: step 155500, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-15 05:35:07.495721: step 155520, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-15 05:36:13.859958: step 155540, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 05:37:20.604775: step 155560, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 05:38:27.556944: step 155580, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 05:39:34.473051: step 155600, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 05:40:41.912385: step 155620, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 05:41:48.193524: step 155640, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-15 05:42:55.375904: step 155660, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-15 05:44:02.459882: step 155680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 05:45:09.774823: step 155700, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 05:46:16.729861: step 155720, loss = 0.0028, acc = 0.9980 (18.9 examples/sec; 3.392 sec/batch)
2017-05-15 05:47:23.129243: step 155740, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-15 05:48:30.304794: step 155760, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 05:49:37.987113: step 155780, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-15 05:50:44.921575: step 155800, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 05:51:51.057085: step 155820, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 05:52:57.394221: step 155840, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 05:54:04.057365: step 155860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 05:55:11.461709: step 155880, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-15 05:56:18.655801: step 155900, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 05:57:25.582222: step 155920, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 05:58:32.198401: step 155940, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 05:59:40.068862: step 155960, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-15 06:00:48.044896: step 155980, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-15 06:01:54.604044: step 156000, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
[Eval] 2017-05-15 06:02:09.773705: step 156000, acc = 0.9428, f1 = 0.9410
[Test] 2017-05-15 06:02:20.343886: step 156000, acc = 0.9306, f1 = 0.9300
[Status] 2017-05-15 06:02:20.343988: step 156000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 06:03:28.157811: step 156020, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 06:04:35.128474: step 156040, loss = 0.0021, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-15 06:05:42.114961: step 156060, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-15 06:06:48.843070: step 156080, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 06:07:55.409320: step 156100, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-15 06:09:01.915986: step 156120, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 06:10:08.687270: step 156140, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-15 06:11:15.020480: step 156160, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-15 06:12:23.125338: step 156180, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-15 06:13:29.360432: step 156200, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 06:14:35.631691: step 156220, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-15 06:15:43.179886: step 156240, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-15 06:16:49.722477: step 156260, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-15 06:17:56.101889: step 156280, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-15 06:19:03.230312: step 156300, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 06:20:10.876434: step 156320, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 06:21:16.989786: step 156340, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 06:22:23.838990: step 156360, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 06:23:30.621299: step 156380, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-15 06:24:37.712680: step 156400, loss = 0.0030, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 06:25:45.019183: step 156420, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 06:26:51.424706: step 156440, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 06:27:58.922039: step 156460, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 06:29:06.873704: step 156480, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-15 06:30:13.439272: step 156500, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-15 06:31:19.775974: step 156520, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 06:32:26.227549: step 156540, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 06:33:34.241591: step 156560, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-15 06:34:41.173376: step 156580, loss = 0.0026, acc = 0.9980 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 06:35:49.015207: step 156600, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-15 06:36:55.985127: step 156620, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-15 06:38:03.272943: step 156640, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-15 06:39:10.274391: step 156660, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 06:40:17.083328: step 156680, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-15 06:41:23.683465: step 156700, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 06:42:30.276304: step 156720, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 06:43:36.751024: step 156740, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 06:44:44.185787: step 156760, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-15 06:45:50.298841: step 156780, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 06:46:57.429662: step 156800, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 06:48:05.346314: step 156820, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-15 06:49:11.369376: step 156840, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 06:50:18.238950: step 156860, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 06:51:24.379937: step 156880, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 06:52:30.550296: step 156900, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 06:53:37.812977: step 156920, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 06:54:44.521698: step 156940, loss = 0.0060, acc = 0.9980 (18.5 examples/sec; 3.459 sec/batch)
2017-05-15 06:55:50.619001: step 156960, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 06:56:56.776544: step 156980, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 06:58:03.805330: step 157000, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
[Eval] 2017-05-15 06:58:18.314300: step 157000, acc = 0.9431, f1 = 0.9412
[Test] 2017-05-15 06:58:28.104657: step 157000, acc = 0.9309, f1 = 0.9303
[Status] 2017-05-15 06:58:28.104738: step 157000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 06:59:34.427918: step 157020, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 07:00:40.553420: step 157040, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 07:01:47.184953: step 157060, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 07:02:55.376891: step 157080, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-15 07:04:01.828082: step 157100, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 07:05:08.659242: step 157120, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-15 07:06:15.679228: step 157140, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 07:07:22.823245: step 157160, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-15 07:08:29.084368: step 157180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 07:09:36.494035: step 157200, loss = 0.0015, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-15 07:10:42.937513: step 157220, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 07:11:49.841508: step 157240, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 07:12:59.111012: step 157260, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-15 07:14:07.148898: step 157280, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-15 07:15:14.161413: step 157300, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 07:16:22.899676: step 157320, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 07:17:29.290610: step 157340, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 07:18:36.529687: step 157360, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 07:19:43.857815: step 157380, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 07:20:50.192754: step 157400, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 07:21:56.538963: step 157420, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 07:23:04.348598: step 157440, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 07:24:10.751015: step 157460, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 07:25:17.621277: step 157480, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 07:26:24.181277: step 157500, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 07:27:31.218365: step 157520, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-15 07:28:37.635915: step 157540, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 07:29:44.646370: step 157560, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 07:30:51.430614: step 157580, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 07:31:58.454143: step 157600, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 07:33:05.013980: step 157620, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 07:34:12.688722: step 157640, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 07:35:18.913241: step 157660, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-15 07:36:25.312120: step 157680, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 07:37:33.174315: step 157700, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-15 07:38:39.383791: step 157720, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 07:39:45.991257: step 157740, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 07:40:53.218857: step 157760, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 07:42:00.585680: step 157780, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 07:43:07.035978: step 157800, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 07:44:13.992622: step 157820, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-15 07:45:20.208398: step 157840, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 07:46:27.693000: step 157860, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-15 07:47:33.955674: step 157880, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 07:48:39.841011: step 157900, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 07:49:45.917925: step 157920, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 07:50:53.458189: step 157940, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 07:52:00.987545: step 157960, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-15 07:53:08.700345: step 157980, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 07:54:14.844243: step 158000, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
[Eval] 2017-05-15 07:54:30.386075: step 158000, acc = 0.9428, f1 = 0.9409
[Test] 2017-05-15 07:54:41.199494: step 158000, acc = 0.9309, f1 = 0.9303
[Status] 2017-05-15 07:54:41.199609: step 158000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 07:55:48.351168: step 158020, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-15 07:56:55.686274: step 158040, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-15 07:58:02.505943: step 158060, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-15 07:59:08.409605: step 158080, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 08:00:15.080690: step 158100, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 08:01:23.135405: step 158120, loss = 0.0026, acc = 0.9980 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 08:02:29.346983: step 158140, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 08:03:35.569427: step 158160, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 08:04:42.321303: step 158180, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 08:05:48.760059: step 158200, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 08:06:55.665951: step 158220, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 08:08:02.523725: step 158240, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 08:09:10.201476: step 158260, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 08:10:16.423194: step 158280, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 08:11:23.285812: step 158300, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-15 08:12:29.821595: step 158320, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-15 08:13:36.677851: step 158340, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 08:14:42.858842: step 158360, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-15 08:15:49.305479: step 158380, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 08:16:55.784442: step 158400, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 08:18:01.881440: step 158420, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 08:19:09.111724: step 158440, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-15 08:20:17.427742: step 158460, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-15 08:21:24.693078: step 158480, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 08:22:31.108363: step 158500, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-15 08:23:37.733470: step 158520, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 08:24:44.206697: step 158540, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 08:25:51.600763: step 158560, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-15 08:26:58.794469: step 158580, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-15 08:28:06.294124: step 158600, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-15 08:29:12.838135: step 158620, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 08:30:19.605011: step 158640, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-15 08:31:26.132209: step 158660, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 08:32:33.014876: step 158680, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 08:33:39.477619: step 158700, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 08:34:46.220581: step 158720, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 08:35:53.341431: step 158740, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 08:36:59.820456: step 158760, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-15 08:38:05.967432: step 158780, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 08:39:13.078435: step 158800, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 08:40:19.472234: step 158820, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 08:41:26.753543: step 158840, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-15 08:42:32.884227: step 158860, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 08:43:39.404450: step 158880, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-15 08:44:45.479127: step 158900, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 08:45:53.700890: step 158920, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 08:46:59.737176: step 158940, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 08:48:07.104279: step 158960, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-15 08:49:13.331636: step 158980, loss = 0.0024, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 08:50:20.688795: step 159000, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
[Eval] 2017-05-15 08:50:34.876170: step 159000, acc = 0.9431, f1 = 0.9412
[Test] 2017-05-15 08:50:44.651759: step 159000, acc = 0.9315, f1 = 0.9309
[Status] 2017-05-15 08:50:44.651830: step 159000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 08:51:51.848945: step 159020, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 08:52:58.288934: step 159040, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 08:54:04.369001: step 159060, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-15 08:55:10.533885: step 159080, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-15 08:56:17.465512: step 159100, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 08:57:24.267484: step 159120, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 08:58:31.874589: step 159140, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 08:59:37.983592: step 159160, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 09:00:44.740980: step 159180, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-15 09:01:52.623508: step 159200, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 09:03:01.373922: step 159220, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-15 09:04:09.051122: step 159240, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 09:05:16.297397: step 159260, loss = 0.0012, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-15 09:06:22.695450: step 159280, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 09:07:29.403693: step 159300, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-15 09:08:35.074996: step 159320, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 09:09:42.074269: step 159340, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 09:10:49.272100: step 159360, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-15 09:11:55.658730: step 159380, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 09:13:04.213266: step 159400, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-15 09:14:11.338189: step 159420, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-15 09:15:18.355563: step 159440, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 09:16:25.135338: step 159460, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-15 09:17:32.164194: step 159480, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 09:18:38.442802: step 159500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 09:19:44.948546: step 159520, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-15 09:20:52.223655: step 159540, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 09:21:58.556553: step 159560, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 09:23:07.418025: step 159580, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-15 09:24:14.047394: step 159600, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 09:25:20.610205: step 159620, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 09:26:26.844533: step 159640, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 09:27:33.274704: step 159660, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-15 09:28:39.850560: step 159680, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-15 09:29:45.514574: step 159700, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-15 09:30:52.308045: step 159720, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 09:31:58.931443: step 159740, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 09:33:06.392041: step 159760, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 09:34:13.303832: step 159780, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 09:35:19.978899: step 159800, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 09:36:27.513210: step 159820, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 09:37:34.782645: step 159840, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 09:38:41.040839: step 159860, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 09:39:48.586197: step 159880, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-15 09:40:55.782610: step 159900, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 09:42:02.956036: step 159920, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 09:43:08.999989: step 159940, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 09:44:15.321342: step 159960, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 09:45:22.498338: step 159980, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-15 09:46:30.835822: step 160000, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
[Eval] 2017-05-15 09:46:46.208506: step 160000, acc = 0.9432, f1 = 0.9414
[Test] 2017-05-15 09:46:56.985313: step 160000, acc = 0.9313, f1 = 0.9307
[Status] 2017-05-15 09:46:56.985401: step 160000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 09:48:03.767521: step 160020, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-15 09:49:09.908988: step 160040, loss = 0.0042, acc = 0.9980 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 09:50:15.943695: step 160060, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 09:51:22.423805: step 160080, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 09:52:30.392940: step 160100, loss = 0.0024, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-15 09:53:36.674002: step 160120, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-15 09:54:43.143530: step 160140, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-15 09:55:50.481851: step 160160, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-15 09:56:56.882390: step 160180, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 09:58:04.974530: step 160200, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.488 sec/batch)
2017-05-15 09:59:11.533181: step 160220, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-15 10:00:17.385380: step 160240, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 10:01:23.962649: step 160260, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 10:02:32.519361: step 160280, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-15 10:03:39.663723: step 160300, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 10:04:46.585161: step 160320, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-15 10:05:53.841103: step 160340, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 10:07:00.673598: step 160360, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 10:08:08.445852: step 160380, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-15 10:09:15.150124: step 160400, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 10:10:21.856348: step 160420, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 10:11:27.784255: step 160440, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 10:12:34.851846: step 160460, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 10:13:42.341156: step 160480, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-15 10:14:48.365769: step 160500, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-15 10:15:54.625453: step 160520, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 10:17:01.123137: step 160540, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-15 10:18:07.544508: step 160560, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-15 10:19:13.834690: step 160580, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 10:20:20.952346: step 160600, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 10:21:28.177120: step 160620, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-15 10:22:34.284672: step 160640, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 10:23:40.610891: step 160660, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 10:24:47.718773: step 160680, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 10:25:54.685359: step 160700, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 10:27:00.583963: step 160720, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 10:28:07.902807: step 160740, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-15 10:29:15.425816: step 160760, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-15 10:30:21.545300: step 160780, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 10:31:28.168789: step 160800, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-15 10:32:36.403919: step 160820, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-15 10:33:43.040914: step 160840, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-15 10:34:49.569810: step 160860, loss = 0.0026, acc = 0.9980 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 10:35:55.873165: step 160880, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 10:37:02.331647: step 160900, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-15 10:38:10.022116: step 160920, loss = 0.0030, acc = 0.9980 (18.3 examples/sec; 3.507 sec/batch)
2017-05-15 10:39:17.753122: step 160940, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 10:40:24.981788: step 160960, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 10:41:31.092744: step 160980, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 10:42:40.244895: step 161000, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-15 10:42:55.470168: step 161000, acc = 0.9436, f1 = 0.9417
[Test] 2017-05-15 10:43:05.552886: step 161000, acc = 0.9315, f1 = 0.9310
[Status] 2017-05-15 10:43:05.552960: step 161000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 10:44:12.146776: step 161020, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 10:45:17.947598: step 161040, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 10:46:26.360043: step 161060, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 10:47:32.728751: step 161080, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 10:48:39.158088: step 161100, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-15 10:49:47.433165: step 161120, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-15 10:50:55.066858: step 161140, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-15 10:52:01.732913: step 161160, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 10:53:08.639568: step 161180, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-15 10:54:15.654523: step 161200, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-15 10:55:22.072925: step 161220, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 10:56:29.649017: step 161240, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-15 10:57:36.840695: step 161260, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-15 10:58:44.954412: step 161280, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 10:59:53.394190: step 161300, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-15 11:01:01.027186: step 161320, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 11:02:07.697914: step 161340, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-15 11:03:15.945336: step 161360, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-15 11:04:23.611439: step 161380, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-15 11:05:30.382336: step 161400, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-15 11:06:37.506644: step 161420, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-15 11:07:43.887880: step 161440, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 11:08:51.217136: step 161460, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-15 11:09:57.349281: step 161480, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 11:11:04.011103: step 161500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 11:12:10.877231: step 161520, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-15 11:13:16.741019: step 161540, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-15 11:14:24.761963: step 161560, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-15 11:15:31.591812: step 161580, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 11:16:39.854338: step 161600, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-15 11:17:47.653950: step 161620, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-15 11:18:53.889729: step 161640, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-15 11:20:00.807145: step 161660, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 11:21:08.089304: step 161680, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-15 11:22:14.603695: step 161700, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 11:23:21.522847: step 161720, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-15 11:24:28.705699: step 161740, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 11:25:36.033357: step 161760, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-15 11:26:42.377836: step 161780, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 11:27:48.662351: step 161800, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-15 11:28:54.694285: step 161820, loss = 0.0010, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-15 11:30:01.851017: step 161840, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 11:31:08.760249: step 161860, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 11:32:15.354901: step 161880, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 11:33:21.973084: step 161900, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 11:34:28.960582: step 161920, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-15 11:35:35.934322: step 161940, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-15 11:36:42.886080: step 161960, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 11:37:49.738956: step 161980, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-15 11:38:56.216749: step 162000, loss = 0.0027, acc = 0.9980 (19.3 examples/sec; 3.309 sec/batch)
[Eval] 2017-05-15 11:39:11.683254: step 162000, acc = 0.9426, f1 = 0.9408
[Test] 2017-05-15 11:39:22.541717: step 162000, acc = 0.9308, f1 = 0.9303
[Status] 2017-05-15 11:39:22.541799: step 162000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 11:40:29.911184: step 162020, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-15 11:41:36.304481: step 162040, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 11:42:44.041076: step 162060, loss = 0.0020, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-15 11:43:50.052665: step 162080, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 11:44:56.678052: step 162100, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 11:46:03.511747: step 162120, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-15 11:47:10.237333: step 162140, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 11:48:16.533186: step 162160, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-15 11:49:23.704117: step 162180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 11:50:31.601116: step 162200, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 11:51:37.877749: step 162220, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 11:52:44.624042: step 162240, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 11:53:51.606243: step 162260, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 11:54:58.176281: step 162280, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 11:56:06.523568: step 162300, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-15 11:57:13.580999: step 162320, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 11:58:20.678418: step 162340, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 11:59:26.813468: step 162360, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 12:00:33.972867: step 162380, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-15 12:01:41.460675: step 162400, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-15 12:02:47.905646: step 162420, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 12:03:54.233959: step 162440, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 12:05:00.267255: step 162460, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 12:06:06.169796: step 162480, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 12:07:12.568151: step 162500, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-15 12:08:19.351286: step 162520, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-15 12:09:26.138687: step 162540, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 12:10:34.455117: step 162560, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 12:11:41.012682: step 162580, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-15 12:12:47.875965: step 162600, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-15 12:13:56.227445: step 162620, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-15 12:15:03.066304: step 162640, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 12:16:09.926194: step 162660, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-15 12:17:17.107751: step 162680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 12:18:23.638812: step 162700, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-15 12:19:31.631873: step 162720, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 12:20:38.576672: step 162740, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 12:21:45.551578: step 162760, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 12:22:52.108527: step 162780, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 12:23:58.449303: step 162800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 12:25:05.927229: step 162820, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 12:26:13.220752: step 162840, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-15 12:27:19.961122: step 162860, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 12:28:26.947066: step 162880, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-15 12:29:33.750611: step 162900, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 12:30:41.114249: step 162920, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 12:31:48.254882: step 162940, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-15 12:32:55.173199: step 162960, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-15 12:34:01.956649: step 162980, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-15 12:35:08.118469: step 163000, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
[Eval] 2017-05-15 12:35:23.701998: step 163000, acc = 0.9417, f1 = 0.9396
[Test] 2017-05-15 12:35:34.400810: step 163000, acc = 0.9291, f1 = 0.9285
[Status] 2017-05-15 12:35:34.400899: step 163000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 12:36:41.218916: step 163020, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 12:37:47.195320: step 163040, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 12:38:54.019147: step 163060, loss = 0.0023, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-15 12:39:59.978912: step 163080, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 12:41:07.737421: step 163100, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-15 12:42:14.398233: step 163120, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 12:43:21.036771: step 163140, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-15 12:44:27.351841: step 163160, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 12:45:34.173148: step 163180, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-15 12:46:40.732961: step 163200, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 12:47:47.192473: step 163220, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 12:48:53.268981: step 163240, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 12:49:59.522277: step 163260, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 12:51:07.237281: step 163280, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-15 12:52:15.117931: step 163300, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 12:53:21.577005: step 163320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 12:54:28.143383: step 163340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 12:55:34.618622: step 163360, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 12:56:41.492446: step 163380, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 12:57:47.594016: step 163400, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 12:58:54.768599: step 163420, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-15 13:00:00.892544: step 163440, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 13:01:07.148000: step 163460, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 13:02:14.072686: step 163480, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-15 13:03:20.225552: step 163500, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 13:04:27.153215: step 163520, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 13:05:35.746314: step 163540, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-15 13:06:42.019632: step 163560, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 13:07:48.584528: step 163580, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 13:08:55.549968: step 163600, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-15 13:10:01.803392: step 163620, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 13:11:08.441427: step 163640, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 13:12:15.581737: step 163660, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 13:13:21.491324: step 163680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 13:14:28.089328: step 163700, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 13:15:35.908238: step 163720, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 13:16:42.922600: step 163740, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 13:17:49.182635: step 163760, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-15 13:18:56.738240: step 163780, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-15 13:20:02.978769: step 163800, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 13:21:08.900760: step 163820, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 13:22:15.507722: step 163840, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 13:23:21.609208: step 163860, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 13:24:29.126037: step 163880, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-15 13:25:36.983457: step 163900, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-15 13:26:44.838004: step 163920, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-15 13:27:51.986662: step 163940, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 13:28:58.957365: step 163960, loss = 0.0017, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-15 13:30:05.875522: step 163980, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-15 13:31:12.162583: step 164000, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
[Eval] 2017-05-15 13:31:26.084312: step 164000, acc = 0.9435, f1 = 0.9417
[Test] 2017-05-15 13:31:36.386336: step 164000, acc = 0.9315, f1 = 0.9309
[Status] 2017-05-15 13:31:36.386436: step 164000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 13:32:43.180887: step 164020, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 13:33:50.940784: step 164040, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-15 13:34:57.452554: step 164060, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 13:36:03.903941: step 164080, loss = 0.0031, acc = 0.9980 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 13:37:10.081050: step 164100, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 13:38:16.770403: step 164120, loss = 0.0031, acc = 0.9980 (19.5 examples/sec; 3.275 sec/batch)
2017-05-15 13:39:24.386113: step 164140, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-15 13:40:31.050184: step 164160, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 13:41:37.287731: step 164180, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 13:42:44.490833: step 164200, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-15 13:43:50.407581: step 164220, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 13:44:56.777067: step 164240, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-15 13:46:03.728882: step 164260, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 13:47:10.527767: step 164280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 13:48:17.954645: step 164300, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.547 sec/batch)
2017-05-15 13:49:24.165352: step 164320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 13:50:30.380051: step 164340, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 13:51:36.970773: step 164360, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 13:52:43.242802: step 164380, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 13:53:49.968406: step 164400, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 13:54:56.541898: step 164420, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-15 13:56:03.170410: step 164440, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 13:57:10.694409: step 164460, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-15 13:58:18.201338: step 164480, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 13:59:25.877661: step 164500, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-15 14:00:32.314197: step 164520, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 14:01:39.905110: step 164540, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-15 14:02:46.694072: step 164560, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 14:03:55.279482: step 164580, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.538 sec/batch)
2017-05-15 14:05:01.754423: step 164600, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 14:06:08.905951: step 164620, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-15 14:07:15.186347: step 164640, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 14:08:22.540002: step 164660, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 14:09:30.115696: step 164680, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 14:10:38.087425: step 164700, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-15 14:11:45.621657: step 164720, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-15 14:12:52.393964: step 164740, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 14:13:58.833106: step 164760, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-15 14:15:06.212882: step 164780, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 14:16:13.148942: step 164800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 14:17:19.142990: step 164820, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-15 14:18:25.399544: step 164840, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 14:19:31.990630: step 164860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-15 14:20:39.186502: step 164880, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 14:21:45.990050: step 164900, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 14:22:53.010908: step 164920, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-15 14:23:58.990145: step 164940, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 14:25:05.687339: step 164960, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 14:26:12.434559: step 164980, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-15 14:27:19.186995: step 165000, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
[Eval] 2017-05-15 14:27:33.406789: step 165000, acc = 0.9433, f1 = 0.9416
[Test] 2017-05-15 14:27:43.171073: step 165000, acc = 0.9313, f1 = 0.9308
[Status] 2017-05-15 14:27:43.171182: step 165000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 14:28:49.290485: step 165020, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 14:29:56.884589: step 165040, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-15 14:31:02.500999: step 165060, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 14:32:09.523847: step 165080, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-15 14:33:16.953906: step 165100, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-15 14:34:23.161967: step 165120, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-15 14:35:30.032824: step 165140, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-15 14:36:36.195097: step 165160, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 14:37:42.557061: step 165180, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 14:38:49.133191: step 165200, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 14:39:56.844703: step 165220, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-15 14:41:04.121067: step 165240, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 14:42:10.633645: step 165260, loss = 0.0036, acc = 0.9980 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 14:43:17.057147: step 165280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 14:44:24.188563: step 165300, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-15 14:45:32.438866: step 165320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 14:46:39.932335: step 165340, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 14:47:46.454737: step 165360, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 14:48:53.340836: step 165380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 14:50:00.025803: step 165400, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 14:51:06.598699: step 165420, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-15 14:52:12.707810: step 165440, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 14:53:19.565553: step 165460, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-15 14:54:26.318692: step 165480, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 14:55:33.921590: step 165500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 14:56:41.181774: step 165520, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 14:57:47.876072: step 165540, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-15 14:58:54.300764: step 165560, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 15:00:00.509339: step 165580, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-15 15:01:06.910818: step 165600, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 15:02:13.720791: step 165620, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 15:03:19.942201: step 165640, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 15:04:27.059862: step 165660, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-15 15:05:33.545297: step 165680, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-15 15:06:39.443410: step 165700, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 15:07:46.438812: step 165720, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-15 15:08:52.630095: step 165740, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 15:09:59.195917: step 165760, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 15:11:05.917595: step 165780, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-15 15:12:12.640645: step 165800, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 15:13:19.790952: step 165820, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 15:14:26.558220: step 165840, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 15:15:33.258940: step 165860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 15:16:39.777806: step 165880, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 15:17:46.437143: step 165900, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 15:18:52.640889: step 165920, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-15 15:19:59.119337: step 165940, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 15:21:05.359991: step 165960, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 15:22:12.742407: step 165980, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-15 15:23:19.400643: step 166000, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
[Eval] 2017-05-15 15:23:34.932844: step 166000, acc = 0.9432, f1 = 0.9414
[Test] 2017-05-15 15:23:45.636413: step 166000, acc = 0.9314, f1 = 0.9309
[Status] 2017-05-15 15:23:45.636535: step 166000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 15:24:52.286521: step 166020, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-15 15:25:58.680952: step 166040, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 15:27:05.964999: step 166060, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-15 15:28:13.247582: step 166080, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-15 15:29:19.489944: step 166100, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 15:30:25.477253: step 166120, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 15:31:32.786164: step 166140, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 15:32:39.759663: step 166160, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-15 15:33:45.771531: step 166180, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 15:34:53.393722: step 166200, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 15:35:59.451534: step 166220, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 15:37:07.041002: step 166240, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 15:38:14.231990: step 166260, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-15 15:39:21.963776: step 166280, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-15 15:40:28.597998: step 166300, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 15:41:36.622869: step 166320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 15:42:44.160164: step 166340, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 15:43:50.691234: step 166360, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-15 15:44:57.068076: step 166380, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 15:46:04.714996: step 166400, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 15:47:11.767519: step 166420, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-15 15:48:18.378634: step 166440, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 15:49:24.815663: step 166460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 15:50:31.675159: step 166480, loss = 0.0024, acc = 0.9980 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 15:51:38.495622: step 166500, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 15:52:44.847672: step 166520, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-15 15:53:51.948365: step 166540, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-15 15:55:01.441479: step 166560, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-15 15:56:07.733603: step 166580, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-15 15:57:14.115771: step 166600, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-15 15:58:20.780630: step 166620, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 15:59:27.190335: step 166640, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 16:00:34.003160: step 166660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 16:01:40.308523: step 166680, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 16:02:46.770784: step 166700, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-15 16:03:53.449878: step 166720, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 16:05:00.706950: step 166740, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-15 16:06:07.236659: step 166760, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-15 16:07:14.169962: step 166780, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-15 16:08:21.191916: step 166800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 16:09:27.828526: step 166820, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-15 16:10:34.527666: step 166840, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 16:11:41.129966: step 166860, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-15 16:12:48.098261: step 166880, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-15 16:13:54.334625: step 166900, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 16:15:00.240473: step 166920, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 16:16:07.669654: step 166940, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-15 16:17:14.400183: step 166960, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-15 16:18:21.289110: step 166980, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-15 16:19:27.611946: step 167000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
[Eval] 2017-05-15 16:19:41.977396: step 167000, acc = 0.9436, f1 = 0.9419
[Test] 2017-05-15 16:19:51.863682: step 167000, acc = 0.9312, f1 = 0.9306
[Status] 2017-05-15 16:19:51.863780: step 167000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 16:20:58.657015: step 167020, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 16:22:06.161540: step 167040, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-15 16:23:13.837688: step 167060, loss = 0.0010, acc = 1.0000 (18.0 examples/sec; 3.561 sec/batch)
2017-05-15 16:24:20.932487: step 167080, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-15 16:25:28.323171: step 167100, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 16:26:34.784949: step 167120, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-15 16:27:41.353356: step 167140, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 16:28:47.288585: step 167160, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 16:29:54.015361: step 167180, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-15 16:31:00.440357: step 167200, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 16:32:06.810038: step 167220, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-15 16:33:13.926697: step 167240, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-15 16:34:20.174537: step 167260, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 16:35:27.223942: step 167280, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-15 16:36:34.309202: step 167300, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-15 16:37:40.387066: step 167320, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 16:38:49.583219: step 167340, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-15 16:39:56.698470: step 167360, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 16:41:03.325878: step 167380, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 16:42:09.458799: step 167400, loss = 0.0025, acc = 0.9980 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 16:43:15.623331: step 167420, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 16:44:22.495500: step 167440, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 16:45:28.586408: step 167460, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 16:46:34.821823: step 167480, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 16:47:42.114780: step 167500, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-15 16:48:49.633883: step 167520, loss = 0.0023, acc = 1.0000 (18.1 examples/sec; 3.531 sec/batch)
2017-05-15 16:49:57.094183: step 167540, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-15 16:51:03.095154: step 167560, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-15 16:52:10.380900: step 167580, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 16:53:18.722350: step 167600, loss = 0.0010, acc = 1.0000 (18.1 examples/sec; 3.526 sec/batch)
2017-05-15 16:54:25.710059: step 167620, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-15 16:55:33.151767: step 167640, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 16:56:40.437957: step 167660, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-15 16:57:47.160785: step 167680, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 16:58:53.663939: step 167700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 17:00:00.008846: step 167720, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 17:01:07.214920: step 167740, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 17:02:13.270730: step 167760, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-15 17:03:19.873681: step 167780, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 17:04:25.925591: step 167800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 17:05:32.383778: step 167820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 17:06:38.697909: step 167840, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-15 17:07:46.274103: step 167860, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 17:08:52.523170: step 167880, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 17:09:59.045055: step 167900, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-15 17:11:05.090765: step 167920, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 17:12:11.030557: step 167940, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 17:13:17.935877: step 167960, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-15 17:14:24.098483: step 167980, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 17:15:31.856973: step 168000, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
[Eval] 2017-05-15 17:15:47.076451: step 168000, acc = 0.9435, f1 = 0.9417
[Test] 2017-05-15 17:15:57.100637: step 168000, acc = 0.9312, f1 = 0.9306
[Status] 2017-05-15 17:15:57.100727: step 168000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 17:17:03.587553: step 168020, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 17:18:11.180822: step 168040, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 17:19:18.582570: step 168060, loss = 0.0011, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-15 17:20:25.638320: step 168080, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-15 17:21:32.734896: step 168100, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 17:22:39.557937: step 168120, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-15 17:23:46.562605: step 168140, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-15 17:24:53.548283: step 168160, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 17:26:00.306817: step 168180, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 17:27:07.485725: step 168200, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-15 17:28:14.207532: step 168220, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 17:29:20.728811: step 168240, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 17:30:27.750333: step 168260, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 17:31:34.607983: step 168280, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 17:32:40.890950: step 168300, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 17:33:47.391804: step 168320, loss = 0.0024, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 17:34:55.355617: step 168340, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-15 17:36:02.742471: step 168360, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 17:37:09.929813: step 168380, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-15 17:38:16.770658: step 168400, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 17:39:23.919712: step 168420, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 17:40:30.368556: step 168440, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-15 17:41:36.522666: step 168460, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 17:42:43.094146: step 168480, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-15 17:43:50.196287: step 168500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 17:44:57.077224: step 168520, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-15 17:46:04.456594: step 168540, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-15 17:47:12.619401: step 168560, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 17:48:19.584475: step 168580, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 17:49:26.330224: step 168600, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 17:50:32.993252: step 168620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 17:51:38.928839: step 168640, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-15 17:52:44.926532: step 168660, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-15 17:53:51.253634: step 168680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 17:54:57.287792: step 168700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 17:56:03.314769: step 168720, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 17:57:09.750285: step 168740, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 17:58:16.471964: step 168760, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 17:59:23.745393: step 168780, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 18:00:32.010800: step 168800, loss = 0.0020, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-15 18:01:38.093362: step 168820, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 18:02:44.859869: step 168840, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 18:03:51.552641: step 168860, loss = 0.0028, acc = 0.9980 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 18:04:58.146530: step 168880, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 18:06:05.672281: step 168900, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 18:07:12.004668: step 168920, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-15 18:08:20.120931: step 168940, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-15 18:09:27.448882: step 168960, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-15 18:10:33.712958: step 168980, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-15 18:11:40.735821: step 169000, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
[Eval] 2017-05-15 18:11:56.290302: step 169000, acc = 0.9436, f1 = 0.9418
[Test] 2017-05-15 18:12:06.600891: step 169000, acc = 0.9311, f1 = 0.9305
[Status] 2017-05-15 18:12:06.600985: step 169000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 18:13:14.029844: step 169020, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-15 18:14:20.738847: step 169040, loss = 0.0052, acc = 0.9980 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 18:15:27.404792: step 169060, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 18:16:33.750499: step 169080, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 18:17:40.602964: step 169100, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-15 18:18:46.667836: step 169120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 18:19:53.106564: step 169140, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-15 18:20:59.039027: step 169160, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 18:22:06.532784: step 169180, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 18:23:13.861257: step 169200, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-15 18:24:20.217109: step 169220, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 18:25:27.277938: step 169240, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 18:26:35.163794: step 169260, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-15 18:27:41.744943: step 169280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 18:28:48.671161: step 169300, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-15 18:29:54.867837: step 169320, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 18:31:02.816285: step 169340, loss = 0.0009, acc = 1.0000 (17.3 examples/sec; 3.707 sec/batch)
2017-05-15 18:32:09.502013: step 169360, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 18:33:16.176761: step 169380, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 18:34:23.421947: step 169400, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-15 18:35:30.060103: step 169420, loss = 0.0011, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-15 18:36:37.045998: step 169440, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 18:37:44.390377: step 169460, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 18:38:51.344288: step 169480, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 18:39:58.840562: step 169500, loss = 0.0026, acc = 0.9980 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 18:41:05.151316: step 169520, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-15 18:42:11.101128: step 169540, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 18:43:16.732371: step 169560, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-15 18:44:23.828918: step 169580, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-15 18:45:30.795261: step 169600, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-15 18:46:37.757547: step 169620, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 18:47:45.216512: step 169640, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-15 18:48:51.766809: step 169660, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-15 18:49:57.921744: step 169680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 18:51:04.486201: step 169700, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 18:52:12.586300: step 169720, loss = 0.0011, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-15 18:53:20.106789: step 169740, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 18:54:26.764752: step 169760, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-15 18:55:33.806179: step 169780, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 18:56:42.076110: step 169800, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 18:57:49.452858: step 169820, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 18:58:55.393860: step 169840, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 19:00:01.901099: step 169860, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-15 19:01:08.736532: step 169880, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 19:02:16.108909: step 169900, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-15 19:03:23.055881: step 169920, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-15 19:04:29.743221: step 169940, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-15 19:05:36.445125: step 169960, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 19:06:42.786529: step 169980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 19:07:49.486560: step 170000, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
[Eval] 2017-05-15 19:08:04.835603: step 170000, acc = 0.9434, f1 = 0.9417
[Test] 2017-05-15 19:08:14.815135: step 170000, acc = 0.9319, f1 = 0.9314
[Status] 2017-05-15 19:08:14.815247: step 170000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 19:09:21.810250: step 170020, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-15 19:10:28.598133: step 170040, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-15 19:11:35.184255: step 170060, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-15 19:12:42.171631: step 170080, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-15 19:13:49.017006: step 170100, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-15 19:14:55.565213: step 170120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 19:16:01.685770: step 170140, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 19:17:07.968283: step 170160, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 19:18:14.749363: step 170180, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-15 19:19:23.076741: step 170200, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-15 19:20:29.719566: step 170220, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 19:21:36.417605: step 170240, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 19:22:44.051903: step 170260, loss = 0.0028, acc = 0.9980 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 19:23:50.815802: step 170280, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-15 19:24:57.234320: step 170300, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-15 19:26:04.215923: step 170320, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-15 19:27:11.166335: step 170340, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-15 19:28:18.963231: step 170360, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 19:29:26.182145: step 170380, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 19:30:32.281240: step 170400, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-15 19:31:37.669301: step 170420, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 19:32:44.327034: step 170440, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 19:33:50.843641: step 170460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 19:34:58.262672: step 170480, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-15 19:36:04.749287: step 170500, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 19:37:11.815019: step 170520, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-15 19:38:17.972841: step 170540, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 19:39:25.185877: step 170560, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-15 19:40:31.592163: step 170580, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-15 19:41:38.455263: step 170600, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-15 19:42:45.038680: step 170620, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-15 19:43:51.393596: step 170640, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 19:44:58.370515: step 170660, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 19:46:05.454966: step 170680, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 19:47:11.738806: step 170700, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-15 19:48:19.204737: step 170720, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-15 19:49:27.187158: step 170740, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-15 19:50:34.125836: step 170760, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 19:51:40.748981: step 170780, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 19:52:47.579142: step 170800, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-15 19:53:54.372289: step 170820, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-15 19:55:01.792299: step 170840, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-15 19:56:09.419833: step 170860, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 19:57:16.160926: step 170880, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 19:58:22.735975: step 170900, loss = 0.0029, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 19:59:29.766983: step 170920, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-15 20:00:35.994692: step 170940, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-15 20:01:41.861133: step 170960, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 20:02:47.879440: step 170980, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 20:03:54.139733: step 171000, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
[Eval] 2017-05-15 20:04:08.313528: step 171000, acc = 0.9427, f1 = 0.9407
[Test] 2017-05-15 20:04:18.901730: step 171000, acc = 0.9297, f1 = 0.9291
[Status] 2017-05-15 20:04:18.901821: step 171000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 20:05:27.560265: step 171020, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 20:06:34.276455: step 171040, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 20:07:40.486524: step 171060, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 20:08:47.041685: step 171080, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-15 20:09:53.128411: step 171100, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-15 20:10:59.118152: step 171120, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-15 20:12:05.127484: step 171140, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 20:13:12.529665: step 171160, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 20:14:19.877670: step 171180, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 20:15:26.241186: step 171200, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 20:16:32.913693: step 171220, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-15 20:17:39.068568: step 171240, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 20:18:45.256097: step 171260, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-15 20:19:51.453895: step 171280, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 20:20:57.866959: step 171300, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-15 20:22:05.231389: step 171320, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-15 20:23:11.674214: step 171340, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-15 20:24:20.329297: step 171360, loss = 0.0017, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-15 20:25:27.747526: step 171380, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-15 20:26:34.292286: step 171400, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-15 20:27:40.225852: step 171420, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 20:28:47.374721: step 171440, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 20:29:53.591967: step 171460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 20:31:00.127230: step 171480, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-15 20:32:08.513001: step 171500, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-15 20:33:15.826350: step 171520, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 20:34:22.964158: step 171540, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 20:35:29.834595: step 171560, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 20:36:35.570829: step 171580, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-15 20:37:41.612835: step 171600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 20:38:47.968036: step 171620, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-15 20:39:54.136220: step 171640, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-15 20:41:01.112208: step 171660, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-15 20:42:07.850381: step 171680, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-15 20:43:14.710528: step 171700, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 20:44:21.920047: step 171720, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 20:45:28.494575: step 171740, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 20:46:35.462595: step 171760, loss = 0.0023, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 20:47:42.498370: step 171780, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-15 20:48:49.736669: step 171800, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-15 20:49:56.803900: step 171820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 20:51:03.333128: step 171840, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 20:52:09.746730: step 171860, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-15 20:53:16.119214: step 171880, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-15 20:54:22.785279: step 171900, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-15 20:55:29.109150: step 171920, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-15 20:56:35.529334: step 171940, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 20:57:42.823367: step 171960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 20:58:49.683191: step 171980, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 20:59:56.334703: step 172000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
[Eval] 2017-05-15 21:00:10.725474: step 172000, acc = 0.9437, f1 = 0.9419
[Test] 2017-05-15 21:00:21.224294: step 172000, acc = 0.9311, f1 = 0.9305
[Status] 2017-05-15 21:00:21.224373: step 172000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 21:01:27.687594: step 172020, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 21:02:34.794599: step 172040, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-15 21:03:41.922762: step 172060, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 21:04:48.554208: step 172080, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-15 21:05:55.696510: step 172100, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-15 21:07:01.970207: step 172120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 21:08:09.041188: step 172140, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 21:09:15.469450: step 172160, loss = 0.0023, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-15 21:10:21.926948: step 172180, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-15 21:11:28.597327: step 172200, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-15 21:12:34.776241: step 172220, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-15 21:13:41.705521: step 172240, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 21:14:49.394056: step 172260, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-15 21:15:55.546697: step 172280, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 21:17:02.700869: step 172300, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 21:18:09.264370: step 172320, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 21:19:16.647578: step 172340, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-15 21:20:24.161288: step 172360, loss = 0.0010, acc = 1.0000 (17.9 examples/sec; 3.574 sec/batch)
2017-05-15 21:21:32.403167: step 172380, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-15 21:22:38.632365: step 172400, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 21:23:45.380237: step 172420, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 21:24:52.368708: step 172440, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-15 21:25:59.127295: step 172460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 21:27:06.534849: step 172480, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-15 21:28:14.807004: step 172500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-15 21:29:21.563011: step 172520, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-15 21:30:28.670298: step 172540, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-15 21:31:36.377715: step 172560, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-15 21:32:42.304559: step 172580, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 21:33:48.543537: step 172600, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-15 21:34:54.837765: step 172620, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 21:36:03.298899: step 172640, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 21:37:10.189095: step 172660, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-15 21:38:16.816357: step 172680, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 21:39:23.014074: step 172700, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 21:40:29.124903: step 172720, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 21:41:36.315603: step 172740, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 21:42:42.618285: step 172760, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-15 21:43:50.551213: step 172780, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-15 21:44:57.737961: step 172800, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-15 21:46:04.321899: step 172820, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 21:47:11.331769: step 172840, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-15 21:48:18.259690: step 172860, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-15 21:49:24.380355: step 172880, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-15 21:50:31.242521: step 172900, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-15 21:51:38.233241: step 172920, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-15 21:52:45.954574: step 172940, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 21:53:52.752588: step 172960, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-15 21:55:00.122784: step 172980, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 21:56:06.515491: step 173000, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
[Eval] 2017-05-15 21:56:21.997261: step 173000, acc = 0.9433, f1 = 0.9415
[Test] 2017-05-15 21:56:32.622564: step 173000, acc = 0.9312, f1 = 0.9306
[Status] 2017-05-15 21:56:32.622679: step 173000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 21:57:38.948008: step 173020, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-15 21:58:45.606848: step 173040, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-15 21:59:52.414263: step 173060, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 22:00:58.608862: step 173080, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 22:02:05.083795: step 173100, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 22:03:12.598371: step 173120, loss = 0.0010, acc = 1.0000 (18.1 examples/sec; 3.537 sec/batch)
2017-05-15 22:04:19.055304: step 173140, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-15 22:05:26.399796: step 173160, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 22:06:32.456869: step 173180, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 22:07:40.121466: step 173200, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-15 22:08:46.977989: step 173220, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 22:09:53.313120: step 173240, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 22:11:00.824360: step 173260, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 22:12:07.443618: step 173280, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 22:13:14.508340: step 173300, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-15 22:14:20.947128: step 173320, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-15 22:15:27.717243: step 173340, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 22:16:34.630636: step 173360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 22:17:42.517624: step 173380, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-15 22:18:49.153980: step 173400, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-15 22:19:56.421405: step 173420, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-15 22:21:02.706500: step 173440, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-15 22:22:10.108099: step 173460, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-15 22:23:17.624717: step 173480, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 22:24:24.491346: step 173500, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 22:25:30.573941: step 173520, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 22:26:37.660259: step 173540, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-15 22:27:44.042964: step 173560, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 22:28:51.399695: step 173580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-15 22:29:57.892860: step 173600, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-15 22:31:04.406261: step 173620, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 22:32:10.559175: step 173640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-15 22:33:17.227677: step 173660, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 22:34:24.077772: step 173680, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-15 22:35:31.170600: step 173700, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-15 22:36:37.759524: step 173720, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-15 22:37:44.422998: step 173740, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 22:38:52.116748: step 173760, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-15 22:39:58.699305: step 173780, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-15 22:41:05.672442: step 173800, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-15 22:42:13.624677: step 173820, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-15 22:43:20.838814: step 173840, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-15 22:44:26.801142: step 173860, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-15 22:45:33.773307: step 173880, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-15 22:46:40.909464: step 173900, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-15 22:47:47.841887: step 173920, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-15 22:48:55.003206: step 173940, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-15 22:50:01.173863: step 173960, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 22:51:08.324573: step 173980, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-15 22:52:14.417264: step 174000, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
[Eval] 2017-05-15 22:52:29.869035: step 174000, acc = 0.9435, f1 = 0.9417
[Test] 2017-05-15 22:52:39.941298: step 174000, acc = 0.9310, f1 = 0.9305
[Status] 2017-05-15 22:52:39.941373: step 174000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 22:53:46.961466: step 174020, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-15 22:54:53.505123: step 174040, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-15 22:56:00.813488: step 174060, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 22:57:08.076653: step 174080, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-15 22:58:14.473317: step 174100, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-15 22:59:21.134378: step 174120, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-15 23:00:27.945523: step 174140, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-15 23:01:36.665314: step 174160, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-15 23:02:43.229648: step 174180, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-15 23:03:50.817974: step 174200, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-15 23:04:58.769228: step 174220, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-15 23:06:05.241034: step 174240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 23:07:13.030995: step 174260, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-15 23:08:19.248142: step 174280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 23:09:25.952268: step 174300, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-15 23:10:32.049797: step 174320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 23:11:39.757472: step 174340, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-15 23:12:46.296778: step 174360, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-15 23:13:55.121202: step 174380, loss = 0.0009, acc = 1.0000 (17.1 examples/sec; 3.734 sec/batch)
2017-05-15 23:15:03.112813: step 174400, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 23:16:09.354194: step 174420, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-15 23:17:16.023536: step 174440, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 23:18:22.497621: step 174460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 23:19:29.029280: step 174480, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 23:20:35.896309: step 174500, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-15 23:21:43.233785: step 174520, loss = 0.0066, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-15 23:22:50.122067: step 174540, loss = 0.0043, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-15 23:23:57.875863: step 174560, loss = 0.0056, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-15 23:25:03.631365: step 174580, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-15 23:26:09.705600: step 174600, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 23:27:16.439668: step 174620, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-15 23:28:23.286996: step 174640, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-15 23:29:30.076950: step 174660, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-15 23:30:37.319083: step 174680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-15 23:31:43.708993: step 174700, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-15 23:32:50.634038: step 174720, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-15 23:33:57.123588: step 174740, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-15 23:35:04.488327: step 174760, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-15 23:36:11.491063: step 174780, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-15 23:37:17.170324: step 174800, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-15 23:38:23.152271: step 174820, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 23:39:29.615119: step 174840, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-15 23:40:35.726177: step 174860, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 23:41:42.686479: step 174880, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-15 23:42:49.786544: step 174900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-15 23:43:56.409686: step 174920, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-15 23:45:04.087091: step 174940, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-15 23:46:10.805905: step 174960, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-15 23:47:18.115380: step 174980, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-15 23:48:25.687637: step 175000, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
[Eval] 2017-05-15 23:48:40.778254: step 175000, acc = 0.9439, f1 = 0.9421
[Test] 2017-05-15 23:48:51.338478: step 175000, acc = 0.9307, f1 = 0.9301
[Status] 2017-05-15 23:48:51.338575: step 175000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-15 23:49:58.225679: step 175020, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-15 23:51:05.473076: step 175040, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-15 23:52:12.078902: step 175060, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-15 23:53:18.645793: step 175080, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-15 23:54:25.801824: step 175100, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-15 23:55:32.541818: step 175120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-15 23:56:39.239503: step 175140, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-15 23:57:46.219939: step 175160, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-15 23:58:52.941468: step 175180, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 00:00:00.698232: step 175200, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-16 00:01:06.775561: step 175220, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 00:02:12.623629: step 175240, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 00:03:19.455948: step 175260, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 00:04:26.687515: step 175280, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 00:05:34.256661: step 175300, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-16 00:06:40.373478: step 175320, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 00:07:47.931106: step 175340, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-16 00:08:55.556614: step 175360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 00:10:01.355368: step 175380, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 00:11:08.856221: step 175400, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 00:12:15.961363: step 175420, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-16 00:13:21.910869: step 175440, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 00:14:28.175650: step 175460, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 00:15:36.255531: step 175480, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 00:16:42.866103: step 175500, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 00:17:49.607473: step 175520, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 00:18:57.273103: step 175540, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-16 00:20:04.104944: step 175560, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 00:21:10.573897: step 175580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 00:22:17.627927: step 175600, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-16 00:23:24.048851: step 175620, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 00:24:30.544241: step 175640, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 00:25:36.896392: step 175660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 00:26:44.893957: step 175680, loss = 0.0025, acc = 0.9980 (18.4 examples/sec; 3.487 sec/batch)
2017-05-16 00:27:52.333344: step 175700, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-16 00:28:59.002723: step 175720, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-16 00:30:06.393755: step 175740, loss = 0.0012, acc = 1.0000 (18.1 examples/sec; 3.544 sec/batch)
2017-05-16 00:31:13.357123: step 175760, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-16 00:32:21.057912: step 175780, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 00:33:27.333628: step 175800, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 00:34:34.028178: step 175820, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 00:35:40.750353: step 175840, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 00:36:48.283712: step 175860, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-16 00:37:54.483914: step 175880, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 00:39:01.014104: step 175900, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-16 00:40:07.762343: step 175920, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-16 00:41:16.215214: step 175940, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-16 00:42:22.458583: step 175960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 00:43:29.747487: step 175980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 00:44:36.255289: step 176000, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-16 00:44:51.460365: step 176000, acc = 0.9393, f1 = 0.9371
[Test] 2017-05-16 00:45:02.140818: step 176000, acc = 0.9247, f1 = 0.9240
[Status] 2017-05-16 00:45:02.140899: step 176000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 00:46:09.215865: step 176020, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 00:47:15.674788: step 176040, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 00:48:23.615441: step 176060, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-16 00:49:29.750719: step 176080, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-16 00:50:36.504472: step 176100, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 00:51:42.832149: step 176120, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-16 00:52:49.607181: step 176140, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 00:53:56.256349: step 176160, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-16 00:55:02.542979: step 176180, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 00:56:09.456184: step 176200, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 00:57:15.972976: step 176220, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-16 00:58:22.698682: step 176240, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-16 00:59:29.921900: step 176260, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-16 01:00:37.419767: step 176280, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-16 01:01:44.315292: step 176300, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 01:02:50.827919: step 176320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 01:03:57.547964: step 176340, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-16 01:05:03.587125: step 176360, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 01:06:10.687252: step 176380, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 01:07:18.785089: step 176400, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-16 01:08:26.102531: step 176420, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 01:09:31.933799: step 176440, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 01:10:37.935473: step 176460, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 01:11:45.552770: step 176480, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 01:12:52.772680: step 176500, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 01:13:59.734134: step 176520, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 01:15:07.429884: step 176540, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-16 01:16:14.532011: step 176560, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 01:17:21.374608: step 176580, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 01:18:30.327582: step 176600, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-16 01:19:36.960046: step 176620, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-16 01:20:44.461506: step 176640, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-16 01:21:50.550692: step 176660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 01:22:57.395622: step 176680, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-16 01:24:04.047089: step 176700, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 01:25:09.980697: step 176720, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 01:26:17.240197: step 176740, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-16 01:27:23.308123: step 176760, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 01:28:30.671602: step 176780, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 01:29:37.680569: step 176800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 01:30:44.185713: step 176820, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 01:31:50.982186: step 176840, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-16 01:32:58.959908: step 176860, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-16 01:34:06.503529: step 176880, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 01:35:13.284683: step 176900, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-16 01:36:20.288132: step 176920, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-16 01:37:26.787472: step 176940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 01:38:33.057148: step 176960, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 01:39:39.750889: step 176980, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 01:40:46.279630: step 177000, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
[Eval] 2017-05-16 01:41:01.559533: step 177000, acc = 0.9433, f1 = 0.9416
[Test] 2017-05-16 01:41:12.035224: step 177000, acc = 0.9309, f1 = 0.9304
[Status] 2017-05-16 01:41:12.035335: step 177000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 01:42:18.548838: step 177020, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 01:43:25.927651: step 177040, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-16 01:44:33.117692: step 177060, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 01:45:40.809448: step 177080, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-16 01:46:46.850298: step 177100, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 01:47:54.540614: step 177120, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-16 01:49:01.319688: step 177140, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-16 01:50:07.447673: step 177160, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 01:51:13.793083: step 177180, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-16 01:52:19.799012: step 177200, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 01:53:27.206341: step 177220, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 01:54:33.947578: step 177240, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 01:55:40.142915: step 177260, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 01:56:47.263087: step 177280, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 01:57:54.761712: step 177300, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 01:59:00.688181: step 177320, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 02:00:07.008895: step 177340, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 02:01:13.108714: step 177360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 02:02:20.273017: step 177380, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-16 02:03:27.358818: step 177400, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-16 02:04:36.542727: step 177420, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-16 02:05:44.033694: step 177440, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-16 02:06:50.550384: step 177460, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 02:07:57.373814: step 177480, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 02:09:03.512662: step 177500, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 02:10:09.617027: step 177520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 02:11:17.149097: step 177540, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 02:12:23.833780: step 177560, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-16 02:13:30.523864: step 177580, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 02:14:36.760195: step 177600, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-16 02:15:43.458818: step 177620, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 02:16:49.590530: step 177640, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 02:17:56.265854: step 177660, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 02:19:03.262956: step 177680, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 02:20:10.549920: step 177700, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-16 02:21:16.897858: step 177720, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 02:22:23.258840: step 177740, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 02:23:30.229331: step 177760, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-16 02:24:37.393789: step 177780, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-16 02:25:43.951584: step 177800, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 02:26:51.914403: step 177820, loss = 0.0043, acc = 0.9980 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 02:27:58.883734: step 177840, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 02:29:05.776802: step 177860, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 02:30:11.798356: step 177880, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 02:31:18.573232: step 177900, loss = 0.0533, acc = 0.9920 (19.3 examples/sec; 3.314 sec/batch)
2017-05-16 02:32:25.008201: step 177920, loss = 0.0908, acc = 0.9960 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 02:33:33.435586: step 177940, loss = 0.2127, acc = 0.9940 (18.4 examples/sec; 3.484 sec/batch)
2017-05-16 02:34:40.356883: step 177960, loss = 0.1666, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 02:35:47.192360: step 177980, loss = 0.1508, acc = 0.9980 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 02:36:53.233834: step 178000, loss = 0.1509, acc = 0.9980 (19.5 examples/sec; 3.289 sec/batch)
[Eval] 2017-05-16 02:37:08.753853: step 178000, acc = 0.9328, f1 = 0.9307
[Test] 2017-05-16 02:37:19.492027: step 178000, acc = 0.9199, f1 = 0.9192
[Status] 2017-05-16 02:37:19.492113: step 178000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 02:38:27.338113: step 178020, loss = 0.1380, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-16 02:39:34.158611: step 178040, loss = 0.1343, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-16 02:40:40.939194: step 178060, loss = 0.1440, acc = 0.9960 (18.6 examples/sec; 3.437 sec/batch)
2017-05-16 02:41:47.921789: step 178080, loss = 0.1255, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 02:42:54.179748: step 178100, loss = 0.1208, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 02:44:00.347335: step 178120, loss = 0.1160, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 02:45:06.663359: step 178140, loss = 0.1133, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-16 02:46:13.153834: step 178160, loss = 0.1114, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 02:47:20.084111: step 178180, loss = 0.1049, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 02:48:26.639315: step 178200, loss = 0.1014, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-16 02:49:33.867042: step 178220, loss = 0.0986, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 02:50:40.779040: step 178240, loss = 0.0958, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 02:51:47.467787: step 178260, loss = 0.0928, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-16 02:52:55.229950: step 178280, loss = 0.0903, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-16 02:54:01.982703: step 178300, loss = 0.0880, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 02:55:09.476209: step 178320, loss = 0.0856, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-16 02:56:15.713355: step 178340, loss = 0.0833, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 02:57:23.079461: step 178360, loss = 0.1093, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 02:58:29.656602: step 178380, loss = 0.0791, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-16 02:59:36.210458: step 178400, loss = 0.0809, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 03:00:43.465685: step 178420, loss = 0.0751, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 03:01:51.915413: step 178440, loss = 0.0733, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-16 03:03:00.215015: step 178460, loss = 0.0716, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-16 03:04:06.718260: step 178480, loss = 0.0699, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-16 03:05:15.222349: step 178500, loss = 0.0702, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 03:06:21.752589: step 178520, loss = 0.0665, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-16 03:07:28.534458: step 178540, loss = 0.0650, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-16 03:08:35.794986: step 178560, loss = 0.0634, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 03:09:42.544020: step 178580, loss = 0.0620, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 03:10:49.128544: step 178600, loss = 0.0606, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 03:11:56.401658: step 178620, loss = 0.0593, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 03:13:03.073598: step 178640, loss = 0.0580, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 03:14:09.799827: step 178660, loss = 0.0566, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 03:15:16.288828: step 178680, loss = 0.0553, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 03:16:22.975052: step 178700, loss = 0.0540, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-16 03:17:29.881933: step 178720, loss = 0.0528, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-16 03:18:37.212381: step 178740, loss = 0.0517, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-16 03:19:44.322345: step 178760, loss = 0.0507, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-16 03:20:51.187882: step 178780, loss = 0.0495, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-16 03:21:57.625180: step 178800, loss = 0.0483, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 03:23:05.846792: step 178820, loss = 0.0500, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 03:24:14.384124: step 178840, loss = 0.0462, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-16 03:25:20.536112: step 178860, loss = 0.0452, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 03:26:26.575696: step 178880, loss = 0.0443, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 03:27:33.754876: step 178900, loss = 0.0439, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 03:28:40.717923: step 178920, loss = 0.0423, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 03:29:47.365176: step 178940, loss = 0.0415, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-16 03:30:53.694241: step 178960, loss = 0.0415, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 03:32:01.171514: step 178980, loss = 0.0398, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 03:33:07.423191: step 179000, loss = 0.0388, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
[Eval] 2017-05-16 03:33:22.896472: step 179000, acc = 0.9365, f1 = 0.9344
[Test] 2017-05-16 03:33:33.596071: step 179000, acc = 0.9239, f1 = 0.9232
[Status] 2017-05-16 03:33:33.596147: step 179000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 03:34:40.028023: step 179020, loss = 0.0496, acc = 0.9980 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 03:35:47.518364: step 179040, loss = 0.0372, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 03:36:53.944648: step 179060, loss = 0.0364, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-16 03:38:00.743197: step 179080, loss = 0.0356, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 03:39:08.290229: step 179100, loss = 0.0354, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-16 03:40:14.656531: step 179120, loss = 0.0358, acc = 0.9980 (19.1 examples/sec; 3.355 sec/batch)
2017-05-16 03:41:21.688616: step 179140, loss = 0.0334, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 03:42:28.909621: step 179160, loss = 0.0327, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 03:43:35.708890: step 179180, loss = 0.0322, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-16 03:44:42.031780: step 179200, loss = 0.0313, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 03:45:48.482343: step 179220, loss = 0.0306, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 03:46:56.765328: step 179240, loss = 0.0300, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 03:48:04.225502: step 179260, loss = 0.0316, acc = 0.9980 (19.2 examples/sec; 3.342 sec/batch)
2017-05-16 03:49:10.674823: step 179280, loss = 0.0304, acc = 0.9980 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 03:50:17.949050: step 179300, loss = 0.0282, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-16 03:51:24.801149: step 179320, loss = 0.0275, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-16 03:52:31.459571: step 179340, loss = 0.0274, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 03:53:38.400339: step 179360, loss = 0.0264, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 03:54:45.697388: step 179380, loss = 0.0258, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-16 03:55:52.158977: step 179400, loss = 0.0254, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 03:57:00.014702: step 179420, loss = 0.0247, acc = 1.0000 (17.2 examples/sec; 3.723 sec/batch)
2017-05-16 03:58:07.226154: step 179440, loss = 0.0241, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 03:59:14.545367: step 179460, loss = 0.0236, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-16 04:00:21.287704: step 179480, loss = 0.0231, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-16 04:01:29.175103: step 179500, loss = 0.0226, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-16 04:02:36.317652: step 179520, loss = 0.0222, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 04:03:43.677252: step 179540, loss = 0.0220, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 04:04:51.610566: step 179560, loss = 0.0212, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 04:05:57.584697: step 179580, loss = 0.0208, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 04:07:05.371783: step 179600, loss = 0.0203, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-16 04:08:11.671885: step 179620, loss = 0.0200, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-16 04:09:19.149986: step 179640, loss = 0.0195, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-16 04:10:26.559626: step 179660, loss = 0.0191, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-16 04:11:34.798951: step 179680, loss = 0.0188, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-16 04:12:41.068971: step 179700, loss = 0.0183, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-16 04:13:48.682567: step 179720, loss = 0.0179, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 04:14:54.723720: step 179740, loss = 0.0190, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 04:16:01.978747: step 179760, loss = 0.0186, acc = 0.9980 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 04:17:09.494106: step 179780, loss = 0.0168, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 04:18:16.345547: step 179800, loss = 0.0165, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 04:19:23.450921: step 179820, loss = 0.0162, acc = 1.0000 (18.0 examples/sec; 3.563 sec/batch)
2017-05-16 04:20:31.413263: step 179840, loss = 0.0160, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 04:21:37.934136: step 179860, loss = 0.0155, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 04:22:44.179884: step 179880, loss = 0.0151, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 04:23:50.198580: step 179900, loss = 0.0148, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 04:24:58.325827: step 179920, loss = 0.0146, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-16 04:26:04.586560: step 179940, loss = 0.0142, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 04:27:10.544452: step 179960, loss = 0.0139, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 04:28:18.377893: step 179980, loss = 0.0137, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-16 04:29:24.641309: step 180000, loss = 0.0149, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
[Eval] 2017-05-16 04:29:40.186527: step 180000, acc = 0.9375, f1 = 0.9356
[Test] 2017-05-16 04:29:50.892049: step 180000, acc = 0.9253, f1 = 0.9247
[Status] 2017-05-16 04:29:50.892124: step 180000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 04:30:57.981279: step 180020, loss = 0.0130, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 04:32:04.715420: step 180040, loss = 0.0127, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 04:33:11.982742: step 180060, loss = 0.0126, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-16 04:34:18.304704: step 180080, loss = 0.0123, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 04:35:24.698556: step 180100, loss = 0.0121, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 04:36:31.025096: step 180120, loss = 0.0118, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 04:37:37.046834: step 180140, loss = 0.0114, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 04:38:43.743369: step 180160, loss = 0.0116, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-16 04:39:50.982413: step 180180, loss = 0.0110, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 04:40:59.171256: step 180200, loss = 0.0107, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 04:42:05.791602: step 180220, loss = 0.0105, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 04:43:13.298695: step 180240, loss = 0.0103, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 04:44:20.459162: step 180260, loss = 0.0102, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 04:45:26.714570: step 180280, loss = 0.0099, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 04:46:34.904235: step 180300, loss = 0.0096, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-16 04:47:41.174243: step 180320, loss = 0.0094, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 04:48:47.762293: step 180340, loss = 0.0093, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-16 04:49:54.741244: step 180360, loss = 0.0093, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-16 04:51:00.916944: step 180380, loss = 0.0089, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 04:52:08.328698: step 180400, loss = 0.0087, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 04:53:14.749105: step 180420, loss = 0.0085, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 04:54:23.991731: step 180440, loss = 0.0083, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-16 04:55:31.278134: step 180460, loss = 0.0081, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 04:56:38.117204: step 180480, loss = 0.0079, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-16 04:57:44.294604: step 180500, loss = 0.0078, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 04:58:51.081704: step 180520, loss = 0.0076, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 04:59:57.893590: step 180540, loss = 0.0075, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-16 05:01:04.559117: step 180560, loss = 0.0074, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 05:02:10.894759: step 180580, loss = 0.0072, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 05:03:17.244569: step 180600, loss = 0.0070, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 05:04:24.287573: step 180620, loss = 0.0071, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-16 05:05:32.465479: step 180640, loss = 0.0067, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-16 05:06:38.951756: step 180660, loss = 0.0065, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 05:07:45.375405: step 180680, loss = 0.0065, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 05:08:53.076141: step 180700, loss = 0.0063, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 05:09:59.495724: step 180720, loss = 0.0061, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 05:11:07.083230: step 180740, loss = 0.0059, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 05:12:14.306187: step 180760, loss = 0.0058, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-16 05:13:20.878249: step 180780, loss = 0.0057, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-16 05:14:27.266615: step 180800, loss = 0.0056, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-16 05:15:33.823704: step 180820, loss = 0.0055, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 05:16:40.292184: step 180840, loss = 0.0062, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 05:17:46.760694: step 180860, loss = 0.0052, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-16 05:18:54.197958: step 180880, loss = 0.0069, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 05:20:01.325901: step 180900, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 05:21:08.345232: step 180920, loss = 0.0051, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-16 05:22:15.835118: step 180940, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 05:23:22.466620: step 180960, loss = 0.0047, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 05:24:30.334658: step 180980, loss = 0.0046, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 05:25:37.262513: step 181000, loss = 0.0052, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
[Eval] 2017-05-16 05:25:52.659657: step 181000, acc = 0.9379, f1 = 0.9360
[Test] 2017-05-16 05:26:02.745532: step 181000, acc = 0.9257, f1 = 0.9250
[Status] 2017-05-16 05:26:02.745708: step 181000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 05:27:10.522671: step 181020, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 05:28:17.658618: step 181040, loss = 0.0058, acc = 0.9980 (18.1 examples/sec; 3.544 sec/batch)
2017-05-16 05:29:24.907346: step 181060, loss = 0.0042, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 05:30:31.627716: step 181080, loss = 0.0045, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 05:31:38.038511: step 181100, loss = 0.0041, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 05:32:44.433765: step 181120, loss = 0.0040, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 05:33:51.171091: step 181140, loss = 0.0042, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 05:34:59.354335: step 181160, loss = 0.0039, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-16 05:36:06.360679: step 181180, loss = 0.0038, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-16 05:37:13.055536: step 181200, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 05:38:19.756684: step 181220, loss = 0.0036, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 05:39:25.987118: step 181240, loss = 0.0035, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-16 05:40:32.798940: step 181260, loss = 0.0036, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-16 05:41:39.270263: step 181280, loss = 0.0034, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-16 05:42:45.554758: step 181300, loss = 0.0034, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 05:43:52.018665: step 181320, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 05:44:59.455195: step 181340, loss = 0.0044, acc = 1.0000 (18.0 examples/sec; 3.558 sec/batch)
2017-05-16 05:46:06.840253: step 181360, loss = 0.0034, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-16 05:47:14.032001: step 181380, loss = 0.0030, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-16 05:48:20.377425: step 181400, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 05:49:26.648194: step 181420, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 05:50:34.490358: step 181440, loss = 0.0083, acc = 0.9980 (18.9 examples/sec; 3.381 sec/batch)
2017-05-16 05:51:40.387281: step 181460, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 05:52:47.236995: step 181480, loss = 0.0028, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-16 05:53:54.503969: step 181500, loss = 0.0027, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 05:55:00.484004: step 181520, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 05:56:07.053768: step 181540, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 05:57:13.358340: step 181560, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 05:58:20.382685: step 181580, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 05:59:26.901590: step 181600, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 06:00:34.387141: step 181620, loss = 0.0024, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-16 06:01:41.390116: step 181640, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-16 06:02:49.617678: step 181660, loss = 0.0023, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-16 06:03:57.140307: step 181680, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 06:05:04.446707: step 181700, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 06:06:11.427777: step 181720, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 06:07:17.336289: step 181740, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 06:08:23.949735: step 181760, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-16 06:09:31.716782: step 181780, loss = 0.0022, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-16 06:10:38.767221: step 181800, loss = 0.0026, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-16 06:11:45.544417: step 181820, loss = 0.0022, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-16 06:12:52.235642: step 181840, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 06:14:00.195641: step 181860, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 06:15:08.219792: step 181880, loss = 0.0019, acc = 1.0000 (17.8 examples/sec; 3.586 sec/batch)
2017-05-16 06:16:13.924428: step 181900, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 06:17:20.833307: step 181920, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 06:18:28.262947: step 181940, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 06:19:34.562130: step 181960, loss = 0.0019, acc = 1.0000 (19.7 examples/sec; 3.250 sec/batch)
2017-05-16 06:20:41.138387: step 181980, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 06:21:48.386674: step 182000, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
[Eval] 2017-05-16 06:22:03.827961: step 182000, acc = 0.9391, f1 = 0.9372
[Test] 2017-05-16 06:22:14.379458: step 182000, acc = 0.9267, f1 = 0.9261
[Status] 2017-05-16 06:22:14.379533: step 182000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 06:23:20.882258: step 182020, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 06:24:27.897621: step 182040, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 06:25:35.382210: step 182060, loss = 0.0019, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-16 06:26:42.652707: step 182080, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 06:27:50.582025: step 182100, loss = 0.0037, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 06:28:57.353432: step 182120, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 06:30:04.822827: step 182140, loss = 0.0017, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-16 06:31:11.247594: step 182160, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 06:32:17.732928: step 182180, loss = 0.0031, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 06:33:24.034238: step 182200, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 06:34:30.718088: step 182220, loss = 0.0029, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-16 06:35:37.783663: step 182240, loss = 0.0047, acc = 0.9980 (18.6 examples/sec; 3.439 sec/batch)
2017-05-16 06:36:44.609547: step 182260, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-16 06:37:52.118545: step 182280, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-16 06:38:58.765738: step 182300, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 06:40:06.020605: step 182320, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 06:41:13.463713: step 182340, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 06:42:20.321085: step 182360, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 06:43:27.440036: step 182380, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-16 06:44:35.292439: step 182400, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-16 06:45:42.107202: step 182420, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-16 06:46:49.028653: step 182440, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 06:47:57.055072: step 182460, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 06:49:04.460197: step 182480, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 06:50:11.031305: step 182500, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-16 06:51:18.421159: step 182520, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 06:52:24.694755: step 182540, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-16 06:53:31.299008: step 182560, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 06:54:38.951268: step 182580, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-16 06:55:45.280654: step 182600, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 06:56:51.499770: step 182620, loss = 0.0014, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-16 06:57:58.350412: step 182640, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 06:59:05.407552: step 182660, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 07:00:12.354240: step 182680, loss = 0.0018, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-16 07:01:19.770100: step 182700, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 07:02:27.395178: step 182720, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-16 07:03:35.636809: step 182740, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-16 07:04:42.703569: step 182760, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 07:05:49.976882: step 182780, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 07:06:56.614037: step 182800, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 07:08:03.982960: step 182820, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-16 07:09:11.098108: step 182840, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 07:10:18.631753: step 182860, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 07:11:28.320700: step 182880, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-16 07:12:35.281278: step 182900, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-16 07:13:41.935738: step 182920, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 07:14:48.696671: step 182940, loss = 0.0028, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-16 07:15:54.773029: step 182960, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-16 07:17:01.691007: step 182980, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 07:18:08.455739: step 183000, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
[Eval] 2017-05-16 07:18:22.487323: step 183000, acc = 0.9396, f1 = 0.9377
[Test] 2017-05-16 07:18:32.691744: step 183000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-16 07:18:32.691820: step 183000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 07:19:39.137325: step 183020, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-16 07:20:46.114005: step 183040, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 07:21:52.155537: step 183060, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 07:22:58.723443: step 183080, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-16 07:24:05.909688: step 183100, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 07:25:13.385382: step 183120, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 07:26:20.295152: step 183140, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 07:27:26.746426: step 183160, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-16 07:28:33.690138: step 183180, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-16 07:29:40.128737: step 183200, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 07:30:47.160288: step 183220, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 07:31:53.797029: step 183240, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 07:32:59.631189: step 183260, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 07:34:06.652997: step 183280, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 07:35:13.154076: step 183300, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 07:36:20.419312: step 183320, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 07:37:26.872265: step 183340, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 07:38:33.229835: step 183360, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 07:39:39.727134: step 183380, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 07:40:45.985659: step 183400, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 07:41:51.929967: step 183420, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 07:43:00.760889: step 183440, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-16 07:44:08.333746: step 183460, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-16 07:45:14.624223: step 183480, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.290 sec/batch)
2017-05-16 07:46:20.998016: step 183500, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 07:47:27.125618: step 183520, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-16 07:48:33.942005: step 183540, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-16 07:49:40.683884: step 183560, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 07:50:47.452324: step 183580, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-16 07:51:53.728951: step 183600, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 07:53:01.730533: step 183620, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-16 07:54:08.732133: step 183640, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 07:55:15.859356: step 183660, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-16 07:56:21.945825: step 183680, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-16 07:57:29.234190: step 183700, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-16 07:58:37.424697: step 183720, loss = 0.0012, acc = 1.0000 (17.6 examples/sec; 3.632 sec/batch)
2017-05-16 07:59:44.614301: step 183740, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 08:00:51.358406: step 183760, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-16 08:01:58.776108: step 183780, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-16 08:03:05.525258: step 183800, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 08:04:13.184102: step 183820, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 08:05:20.249446: step 183840, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-16 08:06:26.547265: step 183860, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-16 08:07:33.001659: step 183880, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 08:08:39.471228: step 183900, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 08:09:45.688307: step 183920, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-16 08:10:52.649893: step 183940, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-16 08:11:59.197895: step 183960, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 08:13:05.458203: step 183980, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 08:14:12.126356: step 184000, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
[Eval] 2017-05-16 08:14:27.293979: step 184000, acc = 0.9405, f1 = 0.9387
[Test] 2017-05-16 08:14:37.267355: step 184000, acc = 0.9289, f1 = 0.9284
[Status] 2017-05-16 08:14:37.267444: step 184000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 08:15:43.515043: step 184020, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 08:16:50.738350: step 184040, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 08:17:57.706457: step 184060, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 08:19:04.547580: step 184080, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-16 08:20:10.708851: step 184100, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-16 08:21:18.170436: step 184120, loss = 0.0016, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-16 08:22:25.461045: step 184140, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 08:23:33.992359: step 184160, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-16 08:24:40.374611: step 184180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 08:25:46.958129: step 184200, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 08:26:54.142844: step 184220, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 08:28:00.336128: step 184240, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 08:29:06.243575: step 184260, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 08:30:13.098673: step 184280, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-16 08:31:19.389712: step 184300, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 08:32:26.493550: step 184320, loss = 0.0014, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-16 08:33:32.727589: step 184340, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 08:34:39.208046: step 184360, loss = 0.0012, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-16 08:35:46.192161: step 184380, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 08:36:53.025287: step 184400, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-16 08:38:00.481342: step 184420, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-16 08:39:08.204845: step 184440, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 08:40:16.959862: step 184460, loss = 0.0012, acc = 1.0000 (17.9 examples/sec; 3.577 sec/batch)
2017-05-16 08:41:22.941489: step 184480, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 08:42:29.378976: step 184500, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-16 08:43:36.288044: step 184520, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-16 08:44:42.685695: step 184540, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 08:45:49.433188: step 184560, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 08:46:57.473298: step 184580, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-16 08:48:03.459170: step 184600, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 08:49:09.755261: step 184620, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 08:50:16.553748: step 184640, loss = 0.0028, acc = 0.9980 (18.6 examples/sec; 3.444 sec/batch)
2017-05-16 08:51:24.132945: step 184660, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-16 08:52:31.440243: step 184680, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 08:53:37.774659: step 184700, loss = 0.0025, acc = 0.9980 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 08:54:45.267088: step 184720, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 08:55:52.090375: step 184740, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 08:56:58.587735: step 184760, loss = 0.0035, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 08:58:05.279901: step 184780, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-16 08:59:11.821768: step 184800, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 09:00:18.770751: step 184820, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 09:01:25.075714: step 184840, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 09:02:32.488749: step 184860, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-16 09:03:39.777455: step 184880, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-16 09:04:46.502764: step 184900, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 09:05:53.450844: step 184920, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 09:07:00.538155: step 184940, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 09:08:06.851507: step 184960, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 09:09:12.979282: step 184980, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 09:10:20.232245: step 185000, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
[Eval] 2017-05-16 09:10:35.585311: step 185000, acc = 0.9406, f1 = 0.9388
[Test] 2017-05-16 09:10:46.208635: step 185000, acc = 0.9295, f1 = 0.9289
[Status] 2017-05-16 09:10:46.208724: step 185000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 09:11:52.599925: step 185020, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-16 09:12:59.503752: step 185040, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 09:14:06.823900: step 185060, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-16 09:15:13.913971: step 185080, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-16 09:16:20.916921: step 185100, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-16 09:17:28.033043: step 185120, loss = 0.0028, acc = 0.9980 (18.7 examples/sec; 3.428 sec/batch)
2017-05-16 09:18:35.708835: step 185140, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 09:19:41.937324: step 185160, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 09:20:50.020030: step 185180, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-16 09:21:57.510023: step 185200, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-16 09:23:04.517828: step 185220, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 09:24:10.547170: step 185240, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 09:25:17.173909: step 185260, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 09:26:24.261761: step 185280, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 09:27:30.898222: step 185300, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 09:28:36.858116: step 185320, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 09:29:44.038772: step 185340, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-16 09:30:50.535608: step 185360, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-16 09:31:57.877228: step 185380, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-16 09:33:04.077320: step 185400, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 09:34:11.744374: step 185420, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 09:35:18.716123: step 185440, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-16 09:36:25.558554: step 185460, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 09:37:33.054161: step 185480, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-16 09:38:40.546761: step 185500, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-16 09:39:47.535614: step 185520, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-16 09:40:53.995037: step 185540, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-16 09:42:00.816384: step 185560, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-16 09:43:07.295933: step 185580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 09:44:13.432941: step 185600, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 09:45:21.117404: step 185620, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 09:46:27.879796: step 185640, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 09:47:35.167449: step 185660, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 09:48:42.830275: step 185680, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-16 09:49:50.293678: step 185700, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 09:50:57.077283: step 185720, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 09:52:04.097199: step 185740, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-16 09:53:10.200119: step 185760, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 09:54:17.223285: step 185780, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-16 09:55:24.173706: step 185800, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 09:56:31.599644: step 185820, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-16 09:57:38.548765: step 185840, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-16 09:58:44.878382: step 185860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 09:59:51.965049: step 185880, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 10:00:58.292622: step 185900, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 10:02:05.548355: step 185920, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-16 10:03:11.859044: step 185940, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 10:04:20.490556: step 185960, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-16 10:05:26.605420: step 185980, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 10:06:32.825994: step 186000, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
[Eval] 2017-05-16 10:06:48.189839: step 186000, acc = 0.9412, f1 = 0.9393
[Test] 2017-05-16 10:06:58.245489: step 186000, acc = 0.9297, f1 = 0.9291
[Status] 2017-05-16 10:06:58.245593: step 186000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 10:08:04.666716: step 186020, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 10:09:11.455034: step 186040, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 10:10:17.458864: step 186060, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 10:11:25.511714: step 186080, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-16 10:12:32.595326: step 186100, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-16 10:13:38.965328: step 186120, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-16 10:14:45.765688: step 186140, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-16 10:15:52.013938: step 186160, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 10:17:00.093013: step 186180, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 10:18:07.672510: step 186200, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 10:19:15.240252: step 186220, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-16 10:20:22.426496: step 186240, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 10:21:30.186439: step 186260, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 10:22:37.599190: step 186280, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 10:23:44.737401: step 186300, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-16 10:24:51.721690: step 186320, loss = 0.0038, acc = 0.9980 (18.5 examples/sec; 3.461 sec/batch)
2017-05-16 10:25:58.492878: step 186340, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-16 10:27:04.784242: step 186360, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 10:28:13.338498: step 186380, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-16 10:29:20.242101: step 186400, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-16 10:30:27.949704: step 186420, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 10:31:35.476127: step 186440, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-16 10:32:41.248349: step 186460, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-16 10:33:49.552276: step 186480, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 10:34:55.466221: step 186500, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 10:36:01.658512: step 186520, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 10:37:09.159952: step 186540, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-16 10:38:15.893044: step 186560, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 10:39:23.953762: step 186580, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-16 10:40:31.972133: step 186600, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 10:41:38.563367: step 186620, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 10:42:46.519400: step 186640, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-16 10:43:52.696051: step 186660, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 10:45:00.204087: step 186680, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-16 10:46:07.309597: step 186700, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-16 10:47:13.670924: step 186720, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-16 10:48:21.163720: step 186740, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-16 10:49:28.394272: step 186760, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 10:50:34.685507: step 186780, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 10:51:41.694044: step 186800, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-16 10:52:48.280937: step 186820, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 10:53:55.361887: step 186840, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-16 10:55:01.541974: step 186860, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-16 10:56:08.293515: step 186880, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 10:57:16.461837: step 186900, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 10:58:23.205841: step 186920, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-16 10:59:30.123310: step 186940, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 11:00:36.578843: step 186960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 11:01:43.218675: step 186980, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 11:02:49.920449: step 187000, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
[Eval] 2017-05-16 11:03:04.127376: step 187000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-16 11:03:14.671761: step 187000, acc = 0.9298, f1 = 0.9292
[Status] 2017-05-16 11:03:14.671880: step 187000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 11:04:20.876917: step 187020, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-16 11:05:29.477954: step 187040, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-16 11:06:36.097670: step 187060, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-16 11:07:43.760174: step 187080, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-16 11:08:51.116339: step 187100, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 11:09:58.411186: step 187120, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-16 11:11:05.555258: step 187140, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-16 11:12:12.934058: step 187160, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 11:13:19.529468: step 187180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 11:14:27.818333: step 187200, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-16 11:15:34.360468: step 187220, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-16 11:16:40.809047: step 187240, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-16 11:17:47.543605: step 187260, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-16 11:18:53.805802: step 187280, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 11:20:00.813686: step 187300, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-16 11:21:06.919930: step 187320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 11:22:14.033475: step 187340, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-16 11:23:20.349417: step 187360, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 11:24:27.718461: step 187380, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-16 11:25:34.188307: step 187400, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-16 11:26:40.771581: step 187420, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 11:27:47.546786: step 187440, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 11:28:54.306563: step 187460, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 11:30:00.847415: step 187480, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 11:31:09.089700: step 187500, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-16 11:32:17.394399: step 187520, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-16 11:33:23.320708: step 187540, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 11:34:29.928056: step 187560, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-16 11:35:36.366671: step 187580, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-16 11:36:43.778276: step 187600, loss = 0.0033, acc = 0.9980 (19.0 examples/sec; 3.371 sec/batch)
2017-05-16 11:37:50.138009: step 187620, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 11:38:57.139984: step 187640, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 11:40:04.022672: step 187660, loss = 0.0024, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 11:41:10.607615: step 187680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 11:42:16.735407: step 187700, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 11:43:23.885132: step 187720, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-16 11:44:30.856164: step 187740, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 11:45:37.598937: step 187760, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-16 11:46:43.833416: step 187780, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-16 11:47:50.438232: step 187800, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 11:48:57.794806: step 187820, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-16 11:50:03.982649: step 187840, loss = 0.0030, acc = 0.9980 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 11:51:10.559577: step 187860, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-16 11:52:16.893422: step 187880, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 11:53:23.304384: step 187900, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-16 11:54:30.184476: step 187920, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-16 11:55:36.738767: step 187940, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 11:56:43.013095: step 187960, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 11:57:49.534763: step 187980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 11:58:56.240174: step 188000, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
[Eval] 2017-05-16 11:59:11.495468: step 188000, acc = 0.9414, f1 = 0.9396
[Test] 2017-05-16 11:59:22.072021: step 188000, acc = 0.9301, f1 = 0.9295
[Status] 2017-05-16 11:59:22.072127: step 188000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 12:00:29.324093: step 188020, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 12:01:35.870124: step 188040, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 12:02:41.920893: step 188060, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 12:03:48.112714: step 188080, loss = 0.0028, acc = 0.9980 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 12:04:55.649372: step 188100, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-16 12:06:01.730372: step 188120, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 12:07:08.953806: step 188140, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-16 12:08:15.265769: step 188160, loss = 0.0044, acc = 0.9980 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 12:09:21.553179: step 188180, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 12:10:28.093019: step 188200, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 12:11:34.444523: step 188220, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-16 12:12:41.720866: step 188240, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 12:13:48.577328: step 188260, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 12:14:56.924179: step 188280, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 12:16:03.309318: step 188300, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-16 12:17:10.192590: step 188320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 12:18:17.735998: step 188340, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 12:19:24.874097: step 188360, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-16 12:20:34.762481: step 188380, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-16 12:21:41.356897: step 188400, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-16 12:22:47.908356: step 188420, loss = 0.0024, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-16 12:23:54.533049: step 188440, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-16 12:25:01.451326: step 188460, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 12:26:08.178859: step 188480, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 12:27:15.244529: step 188500, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 12:28:21.731588: step 188520, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 12:29:28.798815: step 188540, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-16 12:30:35.858187: step 188560, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 12:31:42.974627: step 188580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 12:32:50.631569: step 188600, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 12:33:57.170909: step 188620, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-16 12:35:04.036527: step 188640, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-16 12:36:10.977303: step 188660, loss = 0.0049, acc = 0.9980 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 12:37:18.275010: step 188680, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 12:38:24.732908: step 188700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 12:39:30.905536: step 188720, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 12:40:37.390669: step 188740, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 12:41:43.427962: step 188760, loss = 0.0026, acc = 0.9980 (19.6 examples/sec; 3.259 sec/batch)
2017-05-16 12:42:50.737774: step 188780, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-16 12:43:57.355933: step 188800, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-16 12:45:05.383658: step 188820, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 12:46:12.408277: step 188840, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-16 12:47:20.736300: step 188860, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 12:48:27.698760: step 188880, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 12:49:33.891618: step 188900, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 12:50:41.612263: step 188920, loss = 0.0010, acc = 1.0000 (18.0 examples/sec; 3.547 sec/batch)
2017-05-16 12:51:48.054672: step 188940, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-16 12:52:55.191142: step 188960, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 12:54:01.436710: step 188980, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 12:55:08.035938: step 189000, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
[Eval] 2017-05-16 12:55:22.243701: step 189000, acc = 0.9415, f1 = 0.9397
[Test] 2017-05-16 12:55:31.859424: step 189000, acc = 0.9302, f1 = 0.9296
[Status] 2017-05-16 12:55:31.859501: step 189000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 12:56:37.829316: step 189020, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 12:57:44.630931: step 189040, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 12:58:51.062380: step 189060, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 12:59:58.157341: step 189080, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-16 13:01:04.654562: step 189100, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-16 13:02:11.020133: step 189120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 13:03:17.297132: step 189140, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 13:04:24.518841: step 189160, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 13:05:30.950411: step 189180, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 13:06:38.317552: step 189200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 13:07:45.080717: step 189220, loss = 0.0025, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-16 13:08:50.918477: step 189240, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 13:09:56.892145: step 189260, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 13:11:03.453918: step 189280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 13:12:10.542148: step 189300, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 13:13:16.374028: step 189320, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 13:14:23.403628: step 189340, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 13:15:30.671095: step 189360, loss = 0.0034, acc = 0.9980 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 13:16:36.857860: step 189380, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 13:17:44.150228: step 189400, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-16 13:18:50.672198: step 189420, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 13:19:57.148965: step 189440, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-16 13:21:03.297327: step 189460, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-16 13:22:10.370586: step 189480, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 13:23:17.114023: step 189500, loss = 0.0034, acc = 0.9980 (18.5 examples/sec; 3.464 sec/batch)
2017-05-16 13:24:23.517207: step 189520, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 13:25:31.311469: step 189540, loss = 0.0011, acc = 1.0000 (18.0 examples/sec; 3.556 sec/batch)
2017-05-16 13:26:38.404133: step 189560, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 13:27:45.198852: step 189580, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 13:28:52.896815: step 189600, loss = 0.0010, acc = 1.0000 (18.0 examples/sec; 3.548 sec/batch)
2017-05-16 13:29:59.847220: step 189620, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-16 13:31:06.078961: step 189640, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 13:32:13.028653: step 189660, loss = 0.0025, acc = 0.9980 (19.1 examples/sec; 3.346 sec/batch)
2017-05-16 13:33:18.885696: step 189680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.290 sec/batch)
2017-05-16 13:34:25.773400: step 189700, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-16 13:35:32.472273: step 189720, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 13:36:39.673647: step 189740, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 13:37:46.741604: step 189760, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-16 13:38:52.914981: step 189780, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 13:39:59.433691: step 189800, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 13:41:07.354089: step 189820, loss = 0.0024, acc = 0.9980 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 13:42:13.831348: step 189840, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 13:43:20.450256: step 189860, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 13:44:28.211814: step 189880, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-16 13:45:35.103195: step 189900, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-16 13:46:42.100166: step 189920, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 13:47:48.465394: step 189940, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 13:48:55.571112: step 189960, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 13:50:04.129783: step 189980, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-16 13:51:11.869615: step 190000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-16 13:51:27.347124: step 190000, acc = 0.9415, f1 = 0.9396
[Test] 2017-05-16 13:51:37.965589: step 190000, acc = 0.9297, f1 = 0.9291
[Status] 2017-05-16 13:51:37.965673: step 190000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 13:52:45.493235: step 190020, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 13:53:52.379699: step 190040, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-16 13:54:59.656611: step 190060, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 13:56:06.176741: step 190080, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 13:57:11.901171: step 190100, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 13:58:18.817749: step 190120, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 13:59:24.780737: step 190140, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 14:00:31.964223: step 190160, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-16 14:01:37.942354: step 190180, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 14:02:44.071516: step 190200, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 14:03:51.415589: step 190220, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-16 14:04:57.947484: step 190240, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 14:06:05.119629: step 190260, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-16 14:07:11.975495: step 190280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 14:08:20.060135: step 190300, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-16 14:09:27.418017: step 190320, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-16 14:10:35.035608: step 190340, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 14:11:41.837199: step 190360, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-16 14:12:48.389814: step 190380, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 14:13:55.240627: step 190400, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 14:15:01.462954: step 190420, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 14:16:08.037214: step 190440, loss = 0.0029, acc = 0.9980 (19.0 examples/sec; 3.374 sec/batch)
2017-05-16 14:17:14.817736: step 190460, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 14:18:20.888011: step 190480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 14:19:27.145091: step 190500, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-16 14:20:35.558022: step 190520, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 14:21:42.184109: step 190540, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 14:22:49.641257: step 190560, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 14:23:56.024513: step 190580, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 14:25:03.085409: step 190600, loss = 0.0024, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-16 14:26:09.783275: step 190620, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-16 14:27:17.120641: step 190640, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-16 14:28:23.625886: step 190660, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 14:29:31.030313: step 190680, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 14:30:38.391680: step 190700, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 14:31:45.374775: step 190720, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-16 14:32:51.692908: step 190740, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 14:33:57.887086: step 190760, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-16 14:35:04.400066: step 190780, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 14:36:11.207858: step 190800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 14:37:20.530944: step 190820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 14:38:26.809099: step 190840, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 14:39:33.652249: step 190860, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 14:40:40.076324: step 190880, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 14:41:46.769219: step 190900, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-16 14:42:54.715258: step 190920, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-16 14:44:01.167299: step 190940, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 14:45:07.761833: step 190960, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 14:46:15.652099: step 190980, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-16 14:47:22.731798: step 191000, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
[Eval] 2017-05-16 14:47:38.340383: step 191000, acc = 0.9421, f1 = 0.9403
[Test] 2017-05-16 14:47:49.058583: step 191000, acc = 0.9306, f1 = 0.9301
[Status] 2017-05-16 14:47:49.058682: step 191000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 14:48:56.200395: step 191020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 14:50:02.549753: step 191040, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 14:51:08.559681: step 191060, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 14:52:15.313583: step 191080, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-16 14:53:22.204037: step 191100, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 14:54:31.660690: step 191120, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 14:55:39.274707: step 191140, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-16 14:56:45.971130: step 191160, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 14:57:53.820352: step 191180, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 14:59:01.586611: step 191200, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 15:00:08.884063: step 191220, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-16 15:01:15.555663: step 191240, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-16 15:02:22.052477: step 191260, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-16 15:03:30.246100: step 191280, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-16 15:04:36.985323: step 191300, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-16 15:05:43.608468: step 191320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 15:06:50.477590: step 191340, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 15:07:57.725652: step 191360, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 15:09:06.699986: step 191380, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-16 15:10:13.517472: step 191400, loss = 0.0024, acc = 0.9980 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 15:11:20.034190: step 191420, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-16 15:12:26.669642: step 191440, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 15:13:34.213217: step 191460, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-16 15:14:41.099828: step 191480, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 15:15:47.873059: step 191500, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 15:16:55.758967: step 191520, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-16 15:18:02.398451: step 191540, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 15:19:10.731205: step 191560, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.536 sec/batch)
2017-05-16 15:20:17.074419: step 191580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 15:21:24.098066: step 191600, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 15:22:30.728299: step 191620, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-16 15:23:37.777652: step 191640, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-16 15:24:44.646713: step 191660, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-16 15:25:52.220339: step 191680, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-16 15:26:58.686008: step 191700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 15:28:05.103574: step 191720, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 15:29:11.644086: step 191740, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 15:30:18.919600: step 191760, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 15:31:26.027013: step 191780, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 15:32:33.898274: step 191800, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-16 15:33:40.624938: step 191820, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-16 15:34:47.409828: step 191840, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-16 15:35:55.022806: step 191860, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-16 15:37:02.768740: step 191880, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 15:38:09.188991: step 191900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 15:39:16.154756: step 191920, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 15:40:23.748129: step 191940, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-16 15:41:30.321214: step 191960, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 15:42:37.809217: step 191980, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 15:43:44.744156: step 192000, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
[Eval] 2017-05-16 15:43:58.843827: step 192000, acc = 0.9422, f1 = 0.9403
[Test] 2017-05-16 15:44:08.617496: step 192000, acc = 0.9305, f1 = 0.9299
[Status] 2017-05-16 15:44:08.617599: step 192000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 15:45:15.607257: step 192020, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-16 15:46:22.903843: step 192040, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 15:47:28.884503: step 192060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 15:48:35.322777: step 192080, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 15:49:41.966413: step 192100, loss = 0.0024, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 15:50:48.258615: step 192120, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-16 15:51:55.752429: step 192140, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-16 15:53:02.656017: step 192160, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-16 15:54:09.353764: step 192180, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 15:55:16.391947: step 192200, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 15:56:23.193921: step 192220, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-16 15:57:29.485150: step 192240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 15:58:36.230366: step 192260, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-16 15:59:42.832481: step 192280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 16:00:50.372654: step 192300, loss = 0.0012, acc = 1.0000 (18.1 examples/sec; 3.542 sec/batch)
2017-05-16 16:01:57.017258: step 192320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 16:03:04.273651: step 192340, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-16 16:04:10.694786: step 192360, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-16 16:05:18.355394: step 192380, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 16:06:25.356000: step 192400, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 16:07:32.017501: step 192420, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-16 16:08:38.974551: step 192440, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-16 16:09:45.240884: step 192460, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 16:10:53.106456: step 192480, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-16 16:12:00.815099: step 192500, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-16 16:13:08.002218: step 192520, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 16:14:15.840643: step 192540, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 16:15:22.618076: step 192560, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-16 16:16:28.873645: step 192580, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-16 16:17:36.701781: step 192600, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 16:18:44.242185: step 192620, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-16 16:19:50.667019: step 192640, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 16:20:57.388480: step 192660, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 16:22:04.707528: step 192680, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 16:23:11.936206: step 192700, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 16:24:19.533046: step 192720, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-16 16:25:25.592611: step 192740, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.245 sec/batch)
2017-05-16 16:26:32.715531: step 192760, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 16:27:39.191024: step 192780, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 16:28:46.478212: step 192800, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-16 16:29:53.657384: step 192820, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-16 16:31:00.785612: step 192840, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 16:32:07.857824: step 192860, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 16:33:15.667732: step 192880, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 16:34:22.988955: step 192900, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 16:35:29.791888: step 192920, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-16 16:36:36.551350: step 192940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 16:37:43.144549: step 192960, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 16:38:49.642497: step 192980, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 16:39:56.221076: step 193000, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
[Eval] 2017-05-16 16:40:11.796707: step 193000, acc = 0.9420, f1 = 0.9402
[Test] 2017-05-16 16:40:22.478758: step 193000, acc = 0.9304, f1 = 0.9299
[Status] 2017-05-16 16:40:22.478862: step 193000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 16:41:29.653338: step 193020, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 16:42:37.262981: step 193040, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-16 16:43:43.418099: step 193060, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 16:44:50.088097: step 193080, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-16 16:45:56.656215: step 193100, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-16 16:47:03.489942: step 193120, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-16 16:48:09.744014: step 193140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 16:49:16.145365: step 193160, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-16 16:50:23.850252: step 193180, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-16 16:51:30.894235: step 193200, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 16:52:37.976135: step 193220, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-16 16:53:44.759316: step 193240, loss = 0.0023, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 16:54:51.263175: step 193260, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 16:55:57.779291: step 193280, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 16:57:04.223829: step 193300, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-16 16:58:10.993671: step 193320, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 16:59:17.865260: step 193340, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-16 17:00:24.608419: step 193360, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 17:01:32.198791: step 193380, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-16 17:02:38.426229: step 193400, loss = 0.0011, acc = 1.0000 (19.7 examples/sec; 3.252 sec/batch)
2017-05-16 17:03:45.000777: step 193420, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 17:04:51.991232: step 193440, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-16 17:05:58.066464: step 193460, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-16 17:07:05.031721: step 193480, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-16 17:08:11.016190: step 193500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 17:09:18.187822: step 193520, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 17:10:25.381863: step 193540, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 17:11:31.686437: step 193560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 17:12:37.527835: step 193580, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 17:13:45.144195: step 193600, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 17:14:51.866418: step 193620, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-16 17:15:59.123491: step 193640, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-16 17:17:06.078084: step 193660, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 17:18:13.273505: step 193680, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 17:19:20.706299: step 193700, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.561 sec/batch)
2017-05-16 17:20:27.783901: step 193720, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 17:21:34.632812: step 193740, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-16 17:22:41.511808: step 193760, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 17:23:48.530766: step 193780, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-16 17:24:54.796689: step 193800, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-16 17:26:02.685643: step 193820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 17:27:09.100848: step 193840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 17:28:17.747874: step 193860, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-16 17:29:23.951836: step 193880, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 17:30:30.159029: step 193900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 17:31:37.495121: step 193920, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-16 17:32:44.412717: step 193940, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 17:33:50.606650: step 193960, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 17:34:57.440637: step 193980, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 17:36:04.587262: step 194000, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
[Eval] 2017-05-16 17:36:19.922885: step 194000, acc = 0.9423, f1 = 0.9404
[Test] 2017-05-16 17:36:30.451581: step 194000, acc = 0.9308, f1 = 0.9302
[Status] 2017-05-16 17:36:30.451690: step 194000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 17:37:37.507377: step 194020, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 17:38:44.398849: step 194040, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-16 17:39:51.293185: step 194060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 17:40:58.198124: step 194080, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-16 17:42:04.644609: step 194100, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 17:43:10.623446: step 194120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 17:44:17.346715: step 194140, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 17:45:24.867697: step 194160, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 17:46:30.967768: step 194180, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 17:47:38.265262: step 194200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 17:48:43.855472: step 194220, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 17:49:49.907695: step 194240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 17:50:56.023488: step 194260, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 17:52:02.962729: step 194280, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-16 17:53:09.533096: step 194300, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-16 17:54:16.757858: step 194320, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-16 17:55:23.054716: step 194340, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 17:56:29.452460: step 194360, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 17:57:36.861164: step 194380, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-16 17:58:43.744627: step 194400, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-16 17:59:50.868868: step 194420, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 18:00:58.428322: step 194440, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-16 18:02:04.655747: step 194460, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-16 18:03:12.204820: step 194480, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-16 18:04:19.082826: step 194500, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-16 18:05:27.232601: step 194520, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 18:06:35.443953: step 194540, loss = 0.0030, acc = 0.9980 (17.5 examples/sec; 3.654 sec/batch)
2017-05-16 18:07:41.791266: step 194560, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 18:08:49.227113: step 194580, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-16 18:09:56.613419: step 194600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-16 18:11:02.795214: step 194620, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 18:12:09.635301: step 194640, loss = 0.0015, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-16 18:13:16.859249: step 194660, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 18:14:23.061484: step 194680, loss = 0.0026, acc = 0.9980 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 18:15:31.177146: step 194700, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-16 18:16:38.353853: step 194720, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-16 18:17:45.541665: step 194740, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-16 18:18:52.681792: step 194760, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 18:19:59.912077: step 194780, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 18:21:06.545554: step 194800, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 18:22:13.030173: step 194820, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 18:23:19.295782: step 194840, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 18:24:26.276774: step 194860, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 18:25:34.131081: step 194880, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-16 18:26:40.779362: step 194900, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 18:27:47.451831: step 194920, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 18:28:55.575962: step 194940, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 18:30:02.794031: step 194960, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-16 18:31:08.976598: step 194980, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 18:32:15.222975: step 195000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
[Eval] 2017-05-16 18:32:30.543725: step 195000, acc = 0.9426, f1 = 0.9407
[Test] 2017-05-16 18:32:41.206976: step 195000, acc = 0.9306, f1 = 0.9300
[Status] 2017-05-16 18:32:41.207052: step 195000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 18:33:48.098755: step 195020, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 18:34:54.142531: step 195040, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 18:36:01.115222: step 195060, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.507 sec/batch)
2017-05-16 18:37:07.476034: step 195080, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 18:38:13.825844: step 195100, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 18:39:20.092929: step 195120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 18:40:26.190824: step 195140, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 18:41:32.558742: step 195160, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 18:42:39.387102: step 195180, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-16 18:43:46.174351: step 195200, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-16 18:44:53.579496: step 195220, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-16 18:45:59.517245: step 195240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 18:47:05.795648: step 195260, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-16 18:48:12.098816: step 195280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 18:49:18.981740: step 195300, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-16 18:50:25.529116: step 195320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 18:51:31.809496: step 195340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-16 18:52:38.072714: step 195360, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 18:53:44.511197: step 195380, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 18:54:52.400309: step 195400, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-16 18:55:59.146173: step 195420, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 18:57:05.258078: step 195440, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 18:58:12.464106: step 195460, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 18:59:19.103803: step 195480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 19:00:26.007108: step 195500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 19:01:32.924260: step 195520, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-16 19:02:40.219162: step 195540, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 19:03:47.044254: step 195560, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-16 19:04:54.784920: step 195580, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-16 19:06:01.409985: step 195600, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-16 19:07:08.439180: step 195620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 19:08:15.147223: step 195640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 19:09:21.798699: step 195660, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 19:10:29.289375: step 195680, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-16 19:11:36.143010: step 195700, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-16 19:12:42.601785: step 195720, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 19:13:49.171158: step 195740, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-16 19:14:56.338097: step 195760, loss = 0.0043, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 19:16:02.407052: step 195780, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 19:17:08.956784: step 195800, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-16 19:18:17.332249: step 195820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 19:19:24.044349: step 195840, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 19:20:32.028910: step 195860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-16 19:21:38.619041: step 195880, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 19:22:45.731569: step 195900, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-16 19:23:52.292506: step 195920, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-16 19:24:59.311407: step 195940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 19:26:06.390220: step 195960, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 19:27:12.993213: step 195980, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-16 19:28:19.743000: step 196000, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
[Eval] 2017-05-16 19:28:35.011458: step 196000, acc = 0.9425, f1 = 0.9407
[Test] 2017-05-16 19:28:45.614588: step 196000, acc = 0.9308, f1 = 0.9303
[Status] 2017-05-16 19:28:45.614697: step 196000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 19:29:52.243998: step 196020, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-16 19:30:58.842913: step 196040, loss = 0.0028, acc = 0.9980 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 19:32:05.632192: step 196060, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-16 19:33:12.663349: step 196080, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 19:34:19.247094: step 196100, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 19:35:26.345337: step 196120, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-16 19:36:32.448131: step 196140, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-16 19:37:38.402033: step 196160, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 19:38:45.485064: step 196180, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-16 19:39:53.207395: step 196200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 19:40:59.571931: step 196220, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-16 19:42:07.251132: step 196240, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 19:43:14.553073: step 196260, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-16 19:44:21.839435: step 196280, loss = 0.0024, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-16 19:45:28.843872: step 196300, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-16 19:46:35.776474: step 196320, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 19:47:41.979847: step 196340, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 19:48:48.654883: step 196360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-16 19:49:55.365906: step 196380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 19:51:01.965438: step 196400, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-16 19:52:08.857597: step 196420, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 19:53:15.615959: step 196440, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 19:54:23.312974: step 196460, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-16 19:55:30.417767: step 196480, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-16 19:56:36.957699: step 196500, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-16 19:57:43.325068: step 196520, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-16 19:58:49.693142: step 196540, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-16 19:59:57.793245: step 196560, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 20:01:05.052819: step 196580, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-16 20:02:12.747284: step 196600, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-16 20:03:19.071771: step 196620, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-16 20:04:25.575877: step 196640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 20:05:32.547466: step 196660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 20:06:39.492917: step 196680, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-16 20:07:45.953187: step 196700, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 20:08:52.512918: step 196720, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 20:09:59.446405: step 196740, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-16 20:11:06.817082: step 196760, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 20:12:13.952013: step 196780, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-16 20:13:19.854129: step 196800, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 20:14:27.622364: step 196820, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-16 20:15:35.598143: step 196840, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 20:16:42.467290: step 196860, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 20:17:49.384265: step 196880, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 20:18:56.172038: step 196900, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 20:20:02.692319: step 196920, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 20:21:08.901688: step 196940, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 20:22:15.309759: step 196960, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-16 20:23:21.985167: step 196980, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 20:24:28.401116: step 197000, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-16 20:24:43.943351: step 197000, acc = 0.9417, f1 = 0.9399
[Test] 2017-05-16 20:24:54.635709: step 197000, acc = 0.9299, f1 = 0.9293
[Status] 2017-05-16 20:24:54.635810: step 197000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 20:26:03.683427: step 197020, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-16 20:27:10.147780: step 197040, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-16 20:28:16.444513: step 197060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 20:29:23.564919: step 197080, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-16 20:30:29.714983: step 197100, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 20:31:37.552514: step 197120, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-16 20:32:45.619724: step 197140, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-16 20:33:53.305336: step 197160, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 20:34:59.745921: step 197180, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-16 20:36:07.226710: step 197200, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-16 20:37:15.103712: step 197220, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 20:38:21.071635: step 197240, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-16 20:39:28.296221: step 197260, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-16 20:40:34.658316: step 197280, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-16 20:41:40.722923: step 197300, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-16 20:42:46.890106: step 197320, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 20:43:54.656147: step 197340, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-16 20:45:00.687403: step 197360, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 20:46:07.411921: step 197380, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 20:47:14.471606: step 197400, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 20:48:20.820493: step 197420, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 20:49:28.560597: step 197440, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-16 20:50:35.142944: step 197460, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 20:51:41.714932: step 197480, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 20:52:49.378775: step 197500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 20:53:55.826163: step 197520, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 20:55:03.839768: step 197540, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 20:56:10.219164: step 197560, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 20:57:17.165729: step 197580, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 20:58:24.389468: step 197600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-16 20:59:30.977254: step 197620, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-16 21:00:37.807214: step 197640, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-16 21:01:43.892783: step 197660, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 21:02:50.563117: step 197680, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-16 21:03:58.169744: step 197700, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-16 21:05:06.310088: step 197720, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-16 21:06:13.987699: step 197740, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-16 21:07:21.848803: step 197760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 21:08:29.128237: step 197780, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 21:09:35.872068: step 197800, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-16 21:10:43.075735: step 197820, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-16 21:11:48.953532: step 197840, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-16 21:12:55.837390: step 197860, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 21:14:02.712901: step 197880, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-16 21:15:09.205364: step 197900, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 21:16:16.708610: step 197920, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 21:17:23.826910: step 197940, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-16 21:18:29.971730: step 197960, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 21:19:36.680818: step 197980, loss = 0.0023, acc = 0.9980 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 21:20:43.333027: step 198000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
[Eval] 2017-05-16 21:20:58.742879: step 198000, acc = 0.9421, f1 = 0.9402
[Test] 2017-05-16 21:21:09.357775: step 198000, acc = 0.9301, f1 = 0.9296
[Status] 2017-05-16 21:21:09.357892: step 198000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 21:22:15.611894: step 198020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 21:23:21.629577: step 198040, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 21:24:28.563785: step 198060, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-16 21:25:34.370666: step 198080, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-16 21:26:40.788617: step 198100, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-16 21:27:48.142102: step 198120, loss = 0.0046, acc = 0.9980 (18.6 examples/sec; 3.446 sec/batch)
2017-05-16 21:28:54.973339: step 198140, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 21:30:02.580550: step 198160, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-16 21:31:08.856688: step 198180, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 21:32:15.141674: step 198200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 21:33:21.483526: step 198220, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-16 21:34:27.637866: step 198240, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 21:35:34.046346: step 198260, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-16 21:36:40.430757: step 198280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 21:37:47.374688: step 198300, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 21:38:53.583638: step 198320, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 21:39:59.707970: step 198340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 21:41:06.625490: step 198360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 21:42:12.844682: step 198380, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-16 21:43:19.742623: step 198400, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-16 21:44:26.130339: step 198420, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-16 21:45:32.079520: step 198440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 21:46:38.397339: step 198460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 21:47:45.185312: step 198480, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 21:48:52.469406: step 198500, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-16 21:49:59.716324: step 198520, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-16 21:51:05.965277: step 198540, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-16 21:52:11.910426: step 198560, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-16 21:53:20.813284: step 198580, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-16 21:54:26.989937: step 198600, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-16 21:55:32.810022: step 198620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-16 21:56:40.458896: step 198640, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-16 21:57:47.232575: step 198660, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 21:58:53.755616: step 198680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-16 21:59:59.847097: step 198700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 22:01:06.941836: step 198720, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-16 22:02:12.965928: step 198740, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 22:03:19.151828: step 198760, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 22:04:25.802827: step 198780, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 22:05:31.949227: step 198800, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 22:06:38.729796: step 198820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 22:07:45.017042: step 198840, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-16 22:08:51.554178: step 198860, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-16 22:10:00.088189: step 198880, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-16 22:11:06.429218: step 198900, loss = 0.0037, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 22:12:12.765341: step 198920, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 22:13:19.181411: step 198940, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-16 22:14:26.854303: step 198960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-16 22:15:34.812193: step 198980, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-16 22:16:41.657426: step 199000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
[Eval] 2017-05-16 22:16:55.822790: step 199000, acc = 0.9424, f1 = 0.9407
[Test] 2017-05-16 22:17:06.306537: step 199000, acc = 0.9306, f1 = 0.9300
[Status] 2017-05-16 22:17:06.306630: step 199000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 22:18:12.994799: step 199020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 22:19:21.334925: step 199040, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-16 22:20:27.720961: step 199060, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-16 22:21:35.200655: step 199080, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 22:22:41.323262: step 199100, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-16 22:23:48.536498: step 199120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 22:24:55.693735: step 199140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-16 22:26:04.050429: step 199160, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-16 22:27:10.195263: step 199180, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-16 22:28:17.505265: step 199200, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 22:29:23.659322: step 199220, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-16 22:30:31.387500: step 199240, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-16 22:31:38.213363: step 199260, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-16 22:32:45.096966: step 199280, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 22:33:51.739947: step 199300, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 22:34:58.183122: step 199320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 22:36:04.785750: step 199340, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-16 22:37:11.950578: step 199360, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 22:38:20.111954: step 199380, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-16 22:39:26.273617: step 199400, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-16 22:40:32.421239: step 199420, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-16 22:41:39.108814: step 199440, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 22:42:45.312274: step 199460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 22:43:53.272747: step 199480, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-16 22:44:59.858293: step 199500, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-16 22:46:06.844218: step 199520, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-16 22:47:13.147012: step 199540, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-16 22:48:19.660336: step 199560, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-16 22:49:28.241361: step 199580, loss = 0.0010, acc = 1.0000 (17.6 examples/sec; 3.641 sec/batch)
2017-05-16 22:50:35.301154: step 199600, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-16 22:51:41.238091: step 199620, loss = 0.0023, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 22:52:47.790644: step 199640, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-16 22:53:54.605386: step 199660, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 22:55:01.545869: step 199680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-16 22:56:07.721862: step 199700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-16 22:57:16.428837: step 199720, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-16 22:58:22.901803: step 199740, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-16 22:59:29.407435: step 199760, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-16 23:00:36.424410: step 199780, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-16 23:01:42.378186: step 199800, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-16 23:02:50.052705: step 199820, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-16 23:03:57.423256: step 199840, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-16 23:05:04.443633: step 199860, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-16 23:06:10.815639: step 199880, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 23:07:17.507397: step 199900, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-16 23:08:24.049787: step 199920, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-16 23:09:30.789865: step 199940, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-16 23:10:36.814502: step 199960, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-16 23:11:43.806892: step 199980, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-16 23:12:50.176790: step 200000, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
[Eval] 2017-05-16 23:13:05.744217: step 200000, acc = 0.9432, f1 = 0.9413
[Test] 2017-05-16 23:13:16.331051: step 200000, acc = 0.9307, f1 = 0.9301
[Status] 2017-05-16 23:13:16.331149: step 200000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-16 23:14:23.674428: step 200020, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-16 23:15:30.788620: step 200040, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-16 23:16:37.999306: step 200060, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-16 23:17:45.808924: step 200080, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-16 23:18:52.143933: step 200100, loss = 0.0030, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 23:19:59.329774: step 200120, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-16 23:21:05.725711: step 200140, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-16 23:22:12.723833: step 200160, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-16 23:23:19.765635: step 200180, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-16 23:24:27.075119: step 200200, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-16 23:25:33.622491: step 200220, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-16 23:26:40.145610: step 200240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 23:27:47.261842: step 200260, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-16 23:28:54.274421: step 200280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-16 23:30:00.433362: step 200300, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-16 23:31:07.606291: step 200320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-16 23:32:13.627346: step 200340, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-16 23:33:21.679008: step 200360, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-16 23:34:27.576879: step 200380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 23:35:34.315684: step 200400, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-16 23:36:41.256214: step 200420, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-16 23:37:48.743429: step 200440, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-16 23:38:55.569639: step 200460, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-16 23:40:02.423774: step 200480, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-16 23:41:09.953547: step 200500, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-16 23:42:17.125256: step 200520, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-16 23:43:23.450865: step 200540, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-16 23:44:30.656173: step 200560, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-16 23:45:37.858443: step 200580, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-16 23:46:46.121748: step 200600, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-16 23:47:52.474773: step 200620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-16 23:48:58.582738: step 200640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-16 23:50:04.888576: step 200660, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-16 23:51:13.356430: step 200680, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-16 23:52:20.238493: step 200700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-16 23:53:27.279322: step 200720, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-16 23:54:34.763790: step 200740, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-16 23:55:41.072818: step 200760, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-16 23:56:47.351320: step 200780, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-16 23:57:54.606804: step 200800, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-16 23:59:01.373308: step 200820, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-17 00:00:07.851949: step 200840, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-17 00:01:14.830613: step 200860, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-17 00:02:20.940360: step 200880, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 00:03:27.479919: step 200900, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 00:04:34.110260: step 200920, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 00:05:41.109611: step 200940, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-17 00:06:48.106290: step 200960, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 00:07:55.670928: step 200980, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-17 00:09:02.397252: step 201000, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
[Eval] 2017-05-17 00:09:16.834909: step 201000, acc = 0.9427, f1 = 0.9408
[Test] 2017-05-17 00:09:26.680418: step 201000, acc = 0.9311, f1 = 0.9306
[Status] 2017-05-17 00:09:26.680513: step 201000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 00:10:32.604676: step 201020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 00:11:38.558001: step 201040, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 00:12:45.181758: step 201060, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 00:13:51.193284: step 201080, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 00:14:57.976275: step 201100, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 00:16:03.898946: step 201120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 00:17:12.023986: step 201140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 00:18:18.280793: step 201160, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 00:19:24.504262: step 201180, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 00:20:31.481880: step 201200, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 00:21:39.123461: step 201220, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 00:22:47.426432: step 201240, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-17 00:23:54.211349: step 201260, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-17 00:25:02.405533: step 201280, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-17 00:26:09.362132: step 201300, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-17 00:27:15.967370: step 201320, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-17 00:28:23.622672: step 201340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 00:29:30.392684: step 201360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 00:30:37.583318: step 201380, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-17 00:31:44.277680: step 201400, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 00:32:52.357693: step 201420, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-17 00:33:59.092542: step 201440, loss = 0.0044, acc = 0.9980 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 00:35:06.777768: step 201460, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-17 00:36:13.752748: step 201480, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-17 00:37:20.372701: step 201500, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 00:38:26.667677: step 201520, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 00:39:33.225241: step 201540, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-17 00:40:39.877538: step 201560, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-17 00:41:45.713518: step 201580, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 00:42:53.165526: step 201600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 00:43:59.833619: step 201620, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-17 00:45:06.551955: step 201640, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-17 00:46:14.489385: step 201660, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 00:47:22.184349: step 201680, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-17 00:48:29.595923: step 201700, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 00:49:36.181777: step 201720, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 00:50:43.145739: step 201740, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.432 sec/batch)
2017-05-17 00:51:49.962173: step 201760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 00:52:56.392641: step 201780, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-17 00:54:03.545343: step 201800, loss = 0.0009, acc = 1.0000 (17.9 examples/sec; 3.574 sec/batch)
2017-05-17 00:55:10.190675: step 201820, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 00:56:17.404800: step 201840, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 00:57:23.928823: step 201860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 00:58:32.357354: step 201880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 00:59:39.518220: step 201900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 01:00:45.758373: step 201920, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-17 01:01:53.386518: step 201940, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-17 01:03:00.223700: step 201960, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 01:04:07.024753: step 201980, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-17 01:05:13.309881: step 202000, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
[Eval] 2017-05-17 01:05:27.383647: step 202000, acc = 0.9431, f1 = 0.9413
[Test] 2017-05-17 01:05:37.990905: step 202000, acc = 0.9311, f1 = 0.9305
[Status] 2017-05-17 01:05:37.991013: step 202000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 01:06:44.241823: step 202020, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 01:07:52.003171: step 202040, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 01:08:58.757403: step 202060, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 01:10:06.171155: step 202080, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-17 01:11:14.285096: step 202100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 01:12:20.700947: step 202120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 01:13:27.261012: step 202140, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 01:14:33.332139: step 202160, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 01:15:39.763438: step 202180, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 01:16:45.908387: step 202200, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 01:17:52.529871: step 202220, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-17 01:18:58.714115: step 202240, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 01:20:05.603352: step 202260, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-17 01:21:12.203984: step 202280, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 01:22:17.818936: step 202300, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 01:23:24.612709: step 202320, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 01:24:32.613537: step 202340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 01:25:39.124127: step 202360, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 01:26:45.996810: step 202380, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 01:27:52.474542: step 202400, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 01:28:59.322719: step 202420, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 01:30:06.442118: step 202440, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 01:31:13.421900: step 202460, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 01:32:19.514312: step 202480, loss = 0.0008, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-17 01:33:26.322217: step 202500, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 01:34:33.717484: step 202520, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 01:35:40.298918: step 202540, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 01:36:46.840492: step 202560, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 01:37:53.935727: step 202580, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 01:39:00.406561: step 202600, loss = 0.0024, acc = 0.9980 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 01:40:08.774442: step 202620, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 01:41:16.085124: step 202640, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 01:42:22.732332: step 202660, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 01:43:28.950459: step 202680, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 01:44:36.352503: step 202700, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-17 01:45:44.042150: step 202720, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-17 01:46:51.961553: step 202740, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 01:47:58.212277: step 202760, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-17 01:49:06.058283: step 202780, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-17 01:50:13.011050: step 202800, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 01:51:19.808226: step 202820, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 01:52:26.766727: step 202840, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-17 01:53:33.229443: step 202860, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 01:54:39.274990: step 202880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 01:55:46.292641: step 202900, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 01:56:52.886490: step 202920, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 01:57:59.626260: step 202940, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 01:59:07.296611: step 202960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 02:00:13.926478: step 202980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 02:01:20.438203: step 203000, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
[Eval] 2017-05-17 02:01:35.676410: step 203000, acc = 0.9428, f1 = 0.9410
[Test] 2017-05-17 02:01:45.855760: step 203000, acc = 0.9303, f1 = 0.9297
[Status] 2017-05-17 02:01:45.855871: step 203000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 02:02:53.823504: step 203020, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-17 02:03:59.726583: step 203040, loss = 0.0022, acc = 0.9980 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 02:05:06.470690: step 203060, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-17 02:06:13.023331: step 203080, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 02:07:20.652360: step 203100, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-17 02:08:28.169079: step 203120, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-17 02:09:34.834098: step 203140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 02:10:41.283877: step 203160, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 02:11:47.818067: step 203180, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 02:12:53.858430: step 203200, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-17 02:14:01.012122: step 203220, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-17 02:15:08.273984: step 203240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 02:16:15.233041: step 203260, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 02:17:22.251080: step 203280, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-17 02:18:29.325737: step 203300, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 02:19:36.179773: step 203320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 02:20:42.705024: step 203340, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-17 02:21:48.807155: step 203360, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 02:22:55.204414: step 203380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 02:24:02.463309: step 203400, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 02:25:09.009791: step 203420, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-17 02:26:15.649947: step 203440, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 02:27:21.581220: step 203460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 02:28:28.377243: step 203480, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 02:29:34.542201: step 203500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 02:30:41.363174: step 203520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 02:31:49.326509: step 203540, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-17 02:32:56.055591: step 203560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 02:34:03.098016: step 203580, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-17 02:35:09.113000: step 203600, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 02:36:16.979503: step 203620, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-17 02:37:23.465102: step 203640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 02:38:30.731756: step 203660, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-17 02:39:37.357329: step 203680, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-17 02:40:43.545486: step 203700, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-17 02:41:50.892455: step 203720, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-17 02:42:57.301980: step 203740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 02:44:04.394873: step 203760, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-17 02:45:10.804452: step 203780, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 02:46:17.354371: step 203800, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 02:47:24.455465: step 203820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 02:48:32.405808: step 203840, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-17 02:49:39.197532: step 203860, loss = 0.0036, acc = 0.9980 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 02:50:46.085647: step 203880, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 02:51:53.635937: step 203900, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-17 02:53:00.659238: step 203920, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 02:54:07.406593: step 203940, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-17 02:55:15.197287: step 203960, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 02:56:23.542289: step 203980, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-17 02:57:30.531466: step 204000, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
[Eval] 2017-05-17 02:57:44.788180: step 204000, acc = 0.9429, f1 = 0.9411
[Test] 2017-05-17 02:57:55.270150: step 204000, acc = 0.9307, f1 = 0.9301
[Status] 2017-05-17 02:57:55.270251: step 204000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 02:59:02.481139: step 204020, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-17 03:00:09.638290: step 204040, loss = 0.0008, acc = 1.0000 (18.0 examples/sec; 3.546 sec/batch)
2017-05-17 03:01:16.067452: step 204060, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 03:02:22.146512: step 204080, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 03:03:29.705880: step 204100, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-17 03:04:35.568384: step 204120, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-17 03:05:42.061042: step 204140, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 03:06:48.969593: step 204160, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 03:07:55.494323: step 204180, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 03:09:02.444965: step 204200, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-17 03:10:08.747133: step 204220, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 03:11:16.170464: step 204240, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-17 03:12:22.559996: step 204260, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-17 03:13:29.914534: step 204280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 03:14:36.454054: step 204300, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 03:15:42.281506: step 204320, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 03:16:49.479338: step 204340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 03:17:57.799808: step 204360, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 03:19:04.760079: step 204380, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-17 03:20:11.781909: step 204400, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-17 03:21:18.439166: step 204420, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-17 03:22:25.512496: step 204440, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 03:23:33.712840: step 204460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 03:24:39.715557: step 204480, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 03:25:46.340256: step 204500, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 03:26:52.565329: step 204520, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 03:27:58.613641: step 204540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 03:29:05.447892: step 204560, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-17 03:30:12.774355: step 204580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 03:31:18.999426: step 204600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 03:32:27.272222: step 204620, loss = 0.0012, acc = 1.0000 (18.0 examples/sec; 3.564 sec/batch)
2017-05-17 03:33:34.889515: step 204640, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 03:34:43.588792: step 204660, loss = 0.0011, acc = 1.0000 (18.1 examples/sec; 3.535 sec/batch)
2017-05-17 03:35:49.888153: step 204680, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 03:36:56.409239: step 204700, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 03:38:03.275246: step 204720, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-17 03:39:09.944616: step 204740, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 03:40:18.062768: step 204760, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-17 03:41:25.113874: step 204780, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 03:42:32.566879: step 204800, loss = 0.0027, acc = 0.9980 (18.4 examples/sec; 3.473 sec/batch)
2017-05-17 03:43:39.487749: step 204820, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-17 03:44:46.157880: step 204840, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 03:45:52.615227: step 204860, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 03:46:59.267752: step 204880, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 03:48:06.383639: step 204900, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 03:49:12.857809: step 204920, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-17 03:50:20.199890: step 204940, loss = 0.0032, acc = 0.9980 (18.5 examples/sec; 3.458 sec/batch)
2017-05-17 03:51:26.538257: step 204960, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 03:52:32.571808: step 204980, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 03:53:38.584312: step 205000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
[Eval] 2017-05-17 03:53:52.646283: step 205000, acc = 0.9421, f1 = 0.9402
[Test] 2017-05-17 03:54:03.179279: step 205000, acc = 0.9300, f1 = 0.9294
[Status] 2017-05-17 03:54:03.179376: step 205000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 03:55:10.232327: step 205020, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-17 03:56:17.030516: step 205040, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 03:57:23.647875: step 205060, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-17 03:58:31.165104: step 205080, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 03:59:37.938394: step 205100, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 04:00:46.042794: step 205120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 04:01:52.996401: step 205140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 04:03:00.238094: step 205160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-17 04:04:07.308813: step 205180, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 04:05:14.338607: step 205200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 04:06:21.474458: step 205220, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 04:07:27.937010: step 205240, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 04:08:35.715346: step 205260, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 04:09:42.193031: step 205280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 04:10:48.932277: step 205300, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 04:11:54.893599: step 205320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 04:13:01.488066: step 205340, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 04:14:08.922630: step 205360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 04:15:15.072573: step 205380, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 04:16:22.044530: step 205400, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-17 04:17:28.406517: step 205420, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 04:18:34.180500: step 205440, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-17 04:19:40.604766: step 205460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 04:20:48.707179: step 205480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 04:21:55.357209: step 205500, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-17 04:23:01.527256: step 205520, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 04:24:08.009715: step 205540, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-17 04:25:14.279258: step 205560, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-17 04:26:21.944990: step 205580, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-17 04:27:29.040786: step 205600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-17 04:28:36.203218: step 205620, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 04:29:43.554039: step 205640, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-17 04:30:50.898369: step 205660, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-17 04:31:57.533921: step 205680, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-17 04:33:03.556478: step 205700, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-17 04:34:11.143108: step 205720, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-17 04:35:17.342157: step 205740, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 04:36:24.286030: step 205760, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-17 04:37:31.608833: step 205780, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-17 04:38:38.706212: step 205800, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-17 04:39:44.965418: step 205820, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 04:40:52.066114: step 205840, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-17 04:41:58.483344: step 205860, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 04:43:05.594239: step 205880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 04:44:12.607378: step 205900, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 04:45:19.384297: step 205920, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-17 04:46:25.990897: step 205940, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-17 04:47:33.386382: step 205960, loss = 0.0015, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-17 04:48:41.157894: step 205980, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-17 04:49:48.021717: step 206000, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
[Eval] 2017-05-17 04:50:03.217507: step 206000, acc = 0.9414, f1 = 0.9396
[Test] 2017-05-17 04:50:13.695115: step 206000, acc = 0.9291, f1 = 0.9285
[Status] 2017-05-17 04:50:13.695221: step 206000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 04:51:19.895852: step 206020, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-17 04:52:27.748431: step 206040, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-17 04:53:33.628062: step 206060, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 04:54:39.615721: step 206080, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 04:55:46.130342: step 206100, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-17 04:56:52.715183: step 206120, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 04:57:59.453991: step 206140, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-17 04:59:05.707339: step 206160, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 05:00:13.277150: step 206180, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-17 05:01:19.507832: step 206200, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-17 05:02:25.809752: step 206220, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 05:03:32.128015: step 206240, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 05:04:40.160370: step 206260, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-17 05:05:46.079287: step 206280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 05:06:52.548581: step 206300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 05:07:58.759276: step 206320, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 05:09:04.962928: step 206340, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 05:10:11.563790: step 206360, loss = 0.0008, acc = 1.0000 (19.7 examples/sec; 3.254 sec/batch)
2017-05-17 05:11:18.519670: step 206380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 05:12:25.640943: step 206400, loss = 0.0048, acc = 0.9980 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 05:13:33.118941: step 206420, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-17 05:14:40.312657: step 206440, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 05:15:46.886615: step 206460, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-17 05:16:53.010336: step 206480, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-17 05:18:01.440353: step 206500, loss = 0.0012, acc = 1.0000 (17.5 examples/sec; 3.659 sec/batch)
2017-05-17 05:19:07.733895: step 206520, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 05:20:15.332958: step 206540, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-17 05:21:21.696903: step 206560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 05:22:29.596925: step 206580, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-17 05:23:36.654267: step 206600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 05:24:43.541244: step 206620, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-17 05:25:51.124316: step 206640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 05:26:58.387959: step 206660, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-17 05:28:05.129067: step 206680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 05:29:12.468404: step 206700, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-17 05:30:20.655444: step 206720, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 05:31:27.973986: step 206740, loss = 0.0031, acc = 0.9980 (18.7 examples/sec; 3.415 sec/batch)
2017-05-17 05:32:34.864583: step 206760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 05:33:41.166971: step 206780, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 05:34:48.123424: step 206800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-17 05:35:55.270097: step 206820, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-17 05:37:02.987902: step 206840, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 05:38:10.063535: step 206860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 05:39:17.162362: step 206880, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-17 05:40:24.286199: step 206900, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 05:41:32.098616: step 206920, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-17 05:42:39.186955: step 206940, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-17 05:43:45.980666: step 206960, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-17 05:44:53.606688: step 206980, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-17 05:46:00.213895: step 207000, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
[Eval] 2017-05-17 05:46:15.789343: step 207000, acc = 0.9430, f1 = 0.9411
[Test] 2017-05-17 05:46:26.507953: step 207000, acc = 0.9303, f1 = 0.9297
[Status] 2017-05-17 05:46:26.508054: step 207000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 05:47:32.617633: step 207020, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 05:48:38.909856: step 207040, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 05:49:45.470735: step 207060, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 05:50:52.100725: step 207080, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 05:51:58.766079: step 207100, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 05:53:05.349514: step 207120, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-17 05:54:11.981713: step 207140, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 05:55:18.619416: step 207160, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-17 05:56:24.728801: step 207180, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 05:57:31.032834: step 207200, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 05:58:37.778570: step 207220, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 05:59:44.468276: step 207240, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 06:00:51.796331: step 207260, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-17 06:01:58.185361: step 207280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 06:03:04.580569: step 207300, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-17 06:04:11.515022: step 207320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 06:05:17.818007: step 207340, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-17 06:06:24.314615: step 207360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 06:07:30.833838: step 207380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 06:08:38.107020: step 207400, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-17 06:09:45.166102: step 207420, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-17 06:10:51.554283: step 207440, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 06:11:58.172210: step 207460, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 06:13:04.883199: step 207480, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 06:14:11.277901: step 207500, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 06:15:19.091278: step 207520, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 06:16:27.771584: step 207540, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 06:17:34.232091: step 207560, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 06:18:41.670749: step 207580, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-17 06:19:49.773478: step 207600, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-17 06:20:56.013949: step 207620, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 06:22:03.569352: step 207640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 06:23:11.108038: step 207660, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 06:24:17.180290: step 207680, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 06:25:23.216691: step 207700, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-17 06:26:30.589071: step 207720, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-17 06:27:36.932663: step 207740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 06:28:43.187451: step 207760, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 06:29:50.952385: step 207780, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 06:30:58.857055: step 207800, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 06:32:05.576643: step 207820, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 06:33:12.811118: step 207840, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 06:34:19.549438: step 207860, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 06:35:25.542919: step 207880, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 06:36:33.312786: step 207900, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-17 06:37:40.372077: step 207920, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 06:38:47.065667: step 207940, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 06:39:53.268346: step 207960, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 06:41:00.260439: step 207980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 06:42:06.400240: step 208000, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
[Eval] 2017-05-17 06:42:21.061840: step 208000, acc = 0.9422, f1 = 0.9404
[Test] 2017-05-17 06:42:31.480455: step 208000, acc = 0.9301, f1 = 0.9295
[Status] 2017-05-17 06:42:31.480546: step 208000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 06:43:38.133398: step 208020, loss = 0.0039, acc = 0.9980 (19.0 examples/sec; 3.371 sec/batch)
2017-05-17 06:44:44.677703: step 208040, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-17 06:45:50.718110: step 208060, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 06:46:57.304816: step 208080, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 06:48:03.736754: step 208100, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-17 06:49:10.184574: step 208120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 06:50:17.436483: step 208140, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-17 06:51:23.628327: step 208160, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 06:52:31.120398: step 208180, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-17 06:53:37.986478: step 208200, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 06:54:45.234444: step 208220, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-17 06:55:51.534701: step 208240, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-17 06:56:57.860557: step 208260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 06:58:05.220514: step 208280, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-17 06:59:11.619437: step 208300, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 07:00:18.764945: step 208320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 07:01:25.728027: step 208340, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 07:02:31.631195: step 208360, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 07:03:38.532161: step 208380, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-17 07:04:45.964608: step 208400, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 07:05:53.105234: step 208420, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 07:07:00.808506: step 208440, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 07:08:07.882411: step 208460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 07:09:14.955164: step 208480, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-17 07:10:21.520382: step 208500, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 07:11:28.920689: step 208520, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-17 07:12:35.168539: step 208540, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 07:13:41.909761: step 208560, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 07:14:48.954600: step 208580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 07:15:55.639347: step 208600, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-17 07:17:02.548581: step 208620, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 07:18:08.745206: step 208640, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-17 07:19:17.430177: step 208660, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 07:20:23.997541: step 208680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 07:21:31.617103: step 208700, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-17 07:22:38.360705: step 208720, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-17 07:23:44.683244: step 208740, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-17 07:24:51.642987: step 208760, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-17 07:25:58.712782: step 208780, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-17 07:27:05.290766: step 208800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 07:28:11.225135: step 208820, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-17 07:29:17.423163: step 208840, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 07:30:25.454398: step 208860, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 07:31:32.793954: step 208880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 07:32:40.495376: step 208900, loss = 0.0021, acc = 1.0000 (18.0 examples/sec; 3.559 sec/batch)
2017-05-17 07:33:48.099403: step 208920, loss = 0.0036, acc = 0.9980 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 07:34:54.390127: step 208940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 07:36:01.923110: step 208960, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-17 07:37:08.702015: step 208980, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-17 07:38:15.159073: step 209000, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
[Eval] 2017-05-17 07:38:30.746594: step 209000, acc = 0.9430, f1 = 0.9412
[Test] 2017-05-17 07:38:41.510781: step 209000, acc = 0.9310, f1 = 0.9305
[Status] 2017-05-17 07:38:41.510889: step 209000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 07:39:47.848281: step 209020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 07:40:54.504348: step 209040, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-17 07:42:01.925870: step 209060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 07:43:09.611266: step 209080, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 07:44:15.787121: step 209100, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-17 07:45:22.650587: step 209120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 07:46:28.975516: step 209140, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 07:47:35.221243: step 209160, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-17 07:48:43.720375: step 209180, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-17 07:49:51.149797: step 209200, loss = 0.0034, acc = 0.9980 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 07:50:57.613337: step 209220, loss = 0.0025, acc = 0.9980 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 07:52:04.439425: step 209240, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 07:53:10.765822: step 209260, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 07:54:16.785979: step 209280, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-17 07:55:23.100835: step 209300, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 07:56:29.818697: step 209320, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-17 07:57:36.519615: step 209340, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-17 07:58:44.089020: step 209360, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-17 07:59:51.658907: step 209380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 08:00:58.546022: step 209400, loss = 0.0029, acc = 0.9980 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 08:02:05.045641: step 209420, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-17 08:03:11.840891: step 209440, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-17 08:04:18.410715: step 209460, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 08:05:24.616663: step 209480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 08:06:31.714964: step 209500, loss = 0.0022, acc = 0.9980 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 08:07:38.589789: step 209520, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 08:08:45.190933: step 209540, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 08:09:51.687175: step 209560, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-17 08:10:58.076923: step 209580, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 08:12:04.872316: step 209600, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 08:13:12.293854: step 209620, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 08:14:18.641191: step 209640, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 08:15:26.653753: step 209660, loss = 0.0036, acc = 0.9980 (17.2 examples/sec; 3.725 sec/batch)
2017-05-17 08:16:34.184330: step 209680, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-17 08:17:40.380007: step 209700, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-17 08:18:47.267114: step 209720, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-17 08:19:53.305380: step 209740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 08:21:00.032476: step 209760, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 08:22:07.158615: step 209780, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-17 08:23:14.542991: step 209800, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 08:24:21.538397: step 209820, loss = 0.0023, acc = 0.9980 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 08:25:28.271964: step 209840, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-17 08:26:34.771756: step 209860, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 08:27:42.511164: step 209880, loss = 0.0009, acc = 1.0000 (17.7 examples/sec; 3.621 sec/batch)
2017-05-17 08:28:49.461878: step 209900, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-17 08:29:56.439840: step 209920, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-17 08:31:03.456446: step 209940, loss = 0.0017, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-17 08:32:10.029321: step 209960, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-17 08:33:16.919660: step 209980, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-17 08:34:23.314275: step 210000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
[Eval] 2017-05-17 08:34:38.749593: step 210000, acc = 0.9404, f1 = 0.9385
[Test] 2017-05-17 08:34:49.419447: step 210000, acc = 0.9281, f1 = 0.9275
[Status] 2017-05-17 08:34:49.419545: step 210000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 08:35:55.542236: step 210020, loss = 0.0023, acc = 0.9980 (19.1 examples/sec; 3.355 sec/batch)
2017-05-17 08:37:02.182328: step 210040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 08:38:08.318924: step 210060, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 08:39:15.055910: step 210080, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 08:40:21.599376: step 210100, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-17 08:41:28.555202: step 210120, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-17 08:42:35.396431: step 210140, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-17 08:43:42.393424: step 210160, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-17 08:44:48.575381: step 210180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 08:45:55.324295: step 210200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 08:47:01.998585: step 210220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 08:48:07.965105: step 210240, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 08:49:15.481540: step 210260, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 08:50:21.514161: step 210280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 08:51:27.744271: step 210300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 08:52:34.191368: step 210320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 08:53:40.572510: step 210340, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-17 08:54:47.768519: step 210360, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-17 08:55:55.087288: step 210380, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-17 08:57:01.334383: step 210400, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 08:58:07.531122: step 210420, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 08:59:14.258684: step 210440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 09:00:21.153866: step 210460, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 09:01:27.722877: step 210480, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-17 09:02:34.170345: step 210500, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-17 09:03:41.454746: step 210520, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 09:04:47.751917: step 210540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-17 09:05:55.323849: step 210560, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-17 09:07:02.293787: step 210580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 09:08:09.098562: step 210600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 09:09:16.368083: step 210620, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-17 09:10:23.491035: step 210640, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-17 09:11:30.079120: step 210660, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 09:12:37.902393: step 210680, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-17 09:13:44.655752: step 210700, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-17 09:14:51.855508: step 210720, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 09:15:58.701784: step 210740, loss = 0.0013, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-17 09:17:06.548193: step 210760, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-17 09:18:13.834933: step 210780, loss = 0.0010, acc = 1.0000 (18.1 examples/sec; 3.545 sec/batch)
2017-05-17 09:19:20.381359: step 210800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 09:20:27.491955: step 210820, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-17 09:21:35.519327: step 210840, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-17 09:22:41.834879: step 210860, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 09:23:47.934400: step 210880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 09:24:54.925021: step 210900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 09:26:01.144350: step 210920, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 09:27:07.814466: step 210940, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-17 09:28:14.723713: step 210960, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 09:29:21.613671: step 210980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 09:30:28.375469: step 211000, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
[Eval] 2017-05-17 09:30:42.963804: step 211000, acc = 0.9429, f1 = 0.9410
[Test] 2017-05-17 09:30:53.048895: step 211000, acc = 0.9303, f1 = 0.9297
[Status] 2017-05-17 09:30:53.049003: step 211000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 09:31:59.853409: step 211020, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-17 09:33:05.767465: step 211040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-17 09:34:13.228180: step 211060, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.546 sec/batch)
2017-05-17 09:35:20.433221: step 211080, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-17 09:36:26.910005: step 211100, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-17 09:37:33.303946: step 211120, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 09:38:39.317579: step 211140, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 09:39:45.846037: step 211160, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 09:40:52.461433: step 211180, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-17 09:41:58.917844: step 211200, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 09:43:05.411740: step 211220, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 09:44:11.722590: step 211240, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 09:45:18.344650: step 211260, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 09:46:25.096733: step 211280, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 09:47:32.018230: step 211300, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 09:48:39.348527: step 211320, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 09:49:45.849468: step 211340, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 09:50:53.182832: step 211360, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-17 09:52:00.517919: step 211380, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-17 09:53:07.552340: step 211400, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 09:54:14.544488: step 211420, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.522 sec/batch)
2017-05-17 09:55:21.842066: step 211440, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 09:56:28.073991: step 211460, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 09:57:34.630076: step 211480, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 09:58:40.476866: step 211500, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 09:59:47.993447: step 211520, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-17 10:00:54.339883: step 211540, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 10:02:00.880820: step 211560, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 10:03:07.582292: step 211580, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-17 10:04:14.320704: step 211600, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 10:05:20.983809: step 211620, loss = 3.6579, acc = 0.7240 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 10:06:27.660275: step 211640, loss = 0.4040, acc = 0.9580 (19.0 examples/sec; 3.365 sec/batch)
2017-05-17 10:07:34.181935: step 211660, loss = 0.2311, acc = 0.9880 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 10:08:41.133402: step 211680, loss = 0.2061, acc = 0.9960 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 10:09:47.182601: step 211700, loss = 0.2500, acc = 0.9940 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 10:10:53.259341: step 211720, loss = 0.1884, acc = 0.9980 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 10:12:00.251120: step 211740, loss = 0.2629, acc = 0.9940 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 10:13:06.481088: step 211760, loss = 0.1827, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 10:14:12.634069: step 211780, loss = 0.2431, acc = 0.9980 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 10:15:19.630238: step 211800, loss = 0.1649, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 10:16:27.201837: step 211820, loss = 0.1594, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-17 10:17:33.291888: step 211840, loss = 0.1592, acc = 0.9980 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 10:18:40.758044: step 211860, loss = 0.1862, acc = 0.9980 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 10:19:47.734491: step 211880, loss = 0.1460, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-17 10:20:54.523566: step 211900, loss = 0.1424, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-17 10:22:02.433217: step 211920, loss = 0.1392, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 10:23:09.942818: step 211940, loss = 0.1378, acc = 0.9980 (18.3 examples/sec; 3.499 sec/batch)
2017-05-17 10:24:17.615753: step 211960, loss = 0.1330, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-17 10:25:24.603646: step 211980, loss = 0.1304, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-17 10:26:31.820578: step 212000, loss = 0.1275, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
[Eval] 2017-05-17 10:26:47.323593: step 212000, acc = 0.9329, f1 = 0.9307
[Test] 2017-05-17 10:26:58.047268: step 212000, acc = 0.9196, f1 = 0.9189
[Status] 2017-05-17 10:26:58.047364: step 212000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 10:28:05.225937: step 212020, loss = 0.1782, acc = 0.9980 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 10:29:11.737787: step 212040, loss = 0.1563, acc = 0.9980 (18.8 examples/sec; 3.410 sec/batch)
2017-05-17 10:30:18.521419: step 212060, loss = 0.1200, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-17 10:31:24.622951: step 212080, loss = 0.1177, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 10:32:30.546413: step 212100, loss = 0.1157, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 10:33:37.020718: step 212120, loss = 0.1231, acc = 0.9960 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 10:34:43.143652: step 212140, loss = 0.1111, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 10:35:50.652916: step 212160, loss = 0.1091, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 10:36:57.339374: step 212180, loss = 0.1094, acc = 0.9980 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 10:38:03.467218: step 212200, loss = 0.1051, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-17 10:39:11.078057: step 212220, loss = 0.1032, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-17 10:40:18.222646: step 212240, loss = 0.1180, acc = 0.9980 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 10:41:25.212008: step 212260, loss = 0.0995, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 10:42:31.132393: step 212280, loss = 0.0978, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 10:43:38.644511: step 212300, loss = 0.1171, acc = 0.9980 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 10:44:44.750761: step 212320, loss = 0.0945, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 10:45:50.777848: step 212340, loss = 0.1182, acc = 0.9960 (19.2 examples/sec; 3.341 sec/batch)
2017-05-17 10:46:57.239723: step 212360, loss = 0.0914, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 10:48:04.323515: step 212380, loss = 0.0897, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-17 10:49:11.339025: step 212400, loss = 0.1109, acc = 0.9980 (19.1 examples/sec; 3.353 sec/batch)
2017-05-17 10:50:18.327196: step 212420, loss = 0.0885, acc = 0.9980 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 10:51:24.948981: step 212440, loss = 0.0851, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 10:52:31.461973: step 212460, loss = 0.0917, acc = 0.9980 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 10:53:38.202569: step 212480, loss = 0.0822, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-17 10:54:46.181348: step 212500, loss = 0.0808, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-17 10:55:52.738645: step 212520, loss = 0.0795, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-17 10:57:00.535861: step 212540, loss = 0.0782, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-17 10:58:07.810747: step 212560, loss = 0.0768, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 10:59:14.043215: step 212580, loss = 0.0755, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-17 11:00:20.532173: step 212600, loss = 0.0743, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-17 11:01:28.418984: step 212620, loss = 0.0730, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-17 11:02:36.095939: step 212640, loss = 0.0718, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-17 11:03:43.033134: step 212660, loss = 0.0706, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 11:04:49.693172: step 212680, loss = 0.0695, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 11:05:57.581663: step 212700, loss = 0.0683, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 11:07:04.174337: step 212720, loss = 0.0672, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 11:08:11.893374: step 212740, loss = 0.0661, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 11:09:19.073534: step 212760, loss = 0.0650, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-17 11:10:26.254072: step 212780, loss = 0.0640, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 11:11:33.007319: step 212800, loss = 0.0629, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 11:12:40.080169: step 212820, loss = 0.0619, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 11:13:47.254989: step 212840, loss = 0.0609, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-17 11:14:53.562950: step 212860, loss = 0.0599, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 11:16:00.559286: step 212880, loss = 0.0589, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-17 11:17:07.740741: step 212900, loss = 0.0579, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-17 11:18:14.597314: step 212920, loss = 0.0570, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 11:19:22.610581: step 212940, loss = 0.0561, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 11:20:28.836631: step 212960, loss = 0.0552, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 11:21:34.998015: step 212980, loss = 0.0543, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 11:22:42.974748: step 213000, loss = 0.0534, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
[Eval] 2017-05-17 11:22:58.356822: step 213000, acc = 0.9345, f1 = 0.9323
[Test] 2017-05-17 11:23:08.263438: step 213000, acc = 0.9214, f1 = 0.9207
[Status] 2017-05-17 11:23:08.263531: step 213000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 11:24:14.921673: step 213020, loss = 0.0528, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-17 11:25:21.581954: step 213040, loss = 0.0516, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-17 11:26:27.539976: step 213060, loss = 0.0522, acc = 0.9980 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 11:27:34.282032: step 213080, loss = 0.0500, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 11:28:40.681290: step 213100, loss = 0.0492, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 11:29:47.700409: step 213120, loss = 0.0484, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 11:30:53.841620: step 213140, loss = 0.0476, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 11:31:59.891852: step 213160, loss = 0.0468, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 11:33:05.866360: step 213180, loss = 0.0460, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 11:34:12.595401: step 213200, loss = 0.0453, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 11:35:19.434082: step 213220, loss = 0.0445, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-17 11:36:26.379751: step 213240, loss = 0.0438, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-17 11:37:32.780230: step 213260, loss = 0.0431, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-17 11:38:39.460815: step 213280, loss = 0.0424, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 11:39:45.808712: step 213300, loss = 0.0417, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 11:40:52.723310: step 213320, loss = 0.0412, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-17 11:41:58.950942: step 213340, loss = 0.0403, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 11:43:05.037466: step 213360, loss = 0.0397, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 11:44:11.909344: step 213380, loss = 0.0390, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-17 11:45:18.585421: step 213400, loss = 0.0403, acc = 0.9980 (19.0 examples/sec; 3.375 sec/batch)
2017-05-17 11:46:25.449388: step 213420, loss = 0.0378, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 11:47:32.766728: step 213440, loss = 0.0371, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 11:48:39.460546: step 213460, loss = 0.0366, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 11:49:46.450528: step 213480, loss = 0.0380, acc = 0.9980 (19.2 examples/sec; 3.334 sec/batch)
2017-05-17 11:50:52.596457: step 213500, loss = 0.0353, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 11:51:59.969849: step 213520, loss = 0.0348, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 11:53:07.495845: step 213540, loss = 0.0342, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-17 11:54:13.919122: step 213560, loss = 0.0336, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 11:55:20.369593: step 213580, loss = 0.0331, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-17 11:56:26.793173: step 213600, loss = 0.0325, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 11:57:33.581250: step 213620, loss = 0.0320, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 11:58:39.771879: step 213640, loss = 0.0314, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 11:59:46.565786: step 213660, loss = 0.0312, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 12:00:53.233782: step 213680, loss = 0.0306, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 12:02:01.312747: step 213700, loss = 0.0299, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-17 12:03:08.287694: step 213720, loss = 0.0294, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 12:04:15.494709: step 213740, loss = 0.0290, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-17 12:05:22.181905: step 213760, loss = 0.0284, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 12:06:29.631156: step 213780, loss = 0.0280, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-17 12:07:35.472967: step 213800, loss = 0.0280, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 12:08:44.138189: step 213820, loss = 0.0270, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-17 12:09:50.753308: step 213840, loss = 0.0266, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 12:10:57.816547: step 213860, loss = 0.0261, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 12:12:04.850691: step 213880, loss = 0.0257, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 12:13:11.326406: step 213900, loss = 0.0253, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-17 12:14:17.907730: step 213920, loss = 0.0248, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 12:15:24.917177: step 213940, loss = 0.0244, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-17 12:16:31.440064: step 213960, loss = 0.0240, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 12:17:38.660904: step 213980, loss = 0.0236, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-17 12:18:44.666424: step 214000, loss = 0.0232, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
[Eval] 2017-05-17 12:18:58.766531: step 214000, acc = 0.9346, f1 = 0.9324
[Test] 2017-05-17 12:19:09.197826: step 214000, acc = 0.9212, f1 = 0.9205
[Status] 2017-05-17 12:19:09.197908: step 214000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 12:20:15.389025: step 214020, loss = 0.0228, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 12:21:21.397606: step 214040, loss = 0.0224, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 12:22:27.492106: step 214060, loss = 0.0221, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 12:23:33.915764: step 214080, loss = 0.0217, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 12:24:40.143685: step 214100, loss = 0.0213, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 12:25:46.650342: step 214120, loss = 0.0211, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 12:26:53.825601: step 214140, loss = 0.0206, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 12:28:00.967016: step 214160, loss = 0.0203, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-17 12:29:07.869459: step 214180, loss = 0.0199, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-17 12:30:16.054562: step 214200, loss = 0.0196, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-17 12:31:22.892552: step 214220, loss = 0.0192, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-17 12:32:31.275691: step 214240, loss = 0.0189, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 12:33:37.851461: step 214260, loss = 0.0187, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 12:34:44.364977: step 214280, loss = 0.0183, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 12:35:51.480209: step 214300, loss = 0.0180, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-17 12:36:58.757437: step 214320, loss = 0.0180, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-17 12:38:05.814550: step 214340, loss = 0.0174, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-17 12:39:12.029358: step 214360, loss = 0.0170, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-17 12:40:19.353735: step 214380, loss = 0.0167, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-17 12:41:26.561120: step 214400, loss = 0.0164, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 12:42:32.581954: step 214420, loss = 0.0162, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 12:43:40.113822: step 214440, loss = 0.0163, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 12:44:46.904744: step 214460, loss = 0.0156, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 12:45:53.983276: step 214480, loss = 0.0153, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 12:47:01.292957: step 214500, loss = 0.0151, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 12:48:07.843835: step 214520, loss = 0.0148, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 12:49:15.698434: step 214540, loss = 0.0146, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-17 12:50:21.940168: step 214560, loss = 0.0143, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 12:51:28.345709: step 214580, loss = 0.0142, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-17 12:52:35.127916: step 214600, loss = 0.0138, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-17 12:53:42.515153: step 214620, loss = 0.0136, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 12:54:50.910346: step 214640, loss = 0.0134, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-17 12:55:59.237139: step 214660, loss = 0.0131, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-17 12:57:07.417943: step 214680, loss = 0.0129, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-17 12:58:15.099643: step 214700, loss = 0.0126, acc = 1.0000 (17.8 examples/sec; 3.586 sec/batch)
2017-05-17 12:59:22.212451: step 214720, loss = 0.0124, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 13:00:28.953375: step 214740, loss = 0.0122, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-17 13:01:34.640938: step 214760, loss = 0.0120, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 13:02:40.816211: step 214780, loss = 0.0117, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-17 13:03:48.435100: step 214800, loss = 0.0119, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-17 13:04:56.595594: step 214820, loss = 0.0114, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-17 13:06:03.085425: step 214840, loss = 0.0113, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 13:07:09.958584: step 214860, loss = 0.0109, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-17 13:08:17.612583: step 214880, loss = 0.0112, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-17 13:09:25.743026: step 214900, loss = 0.0106, acc = 1.0000 (18.0 examples/sec; 3.553 sec/batch)
2017-05-17 13:10:32.506681: step 214920, loss = 0.0104, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 13:11:40.799360: step 214940, loss = 0.0103, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 13:12:48.008291: step 214960, loss = 0.0101, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 13:13:54.605245: step 214980, loss = 0.0099, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 13:15:01.953820: step 215000, loss = 0.0100, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
[Eval] 2017-05-17 13:15:17.147878: step 215000, acc = 0.9344, f1 = 0.9322
[Test] 2017-05-17 13:15:27.738377: step 215000, acc = 0.9210, f1 = 0.9203
[Status] 2017-05-17 13:15:27.738494: step 215000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 13:16:35.405623: step 215020, loss = 0.0095, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 13:17:42.023265: step 215040, loss = 0.0093, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 13:18:48.335896: step 215060, loss = 0.0093, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-17 13:19:54.893838: step 215080, loss = 0.0090, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-17 13:21:02.061284: step 215100, loss = 0.0088, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-17 13:22:09.108455: step 215120, loss = 0.0087, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 13:23:16.589155: step 215140, loss = 0.0099, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 13:24:22.585241: step 215160, loss = 0.0104, acc = 0.9980 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 13:25:29.370660: step 215180, loss = 0.0082, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 13:26:35.926692: step 215200, loss = 0.0081, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-17 13:27:41.939912: step 215220, loss = 0.0082, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 13:28:48.956627: step 215240, loss = 0.0077, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-17 13:29:55.547452: step 215260, loss = 0.0077, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-17 13:31:02.513853: step 215280, loss = 0.0075, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-17 13:32:10.680143: step 215300, loss = 0.0073, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-17 13:33:17.803436: step 215320, loss = 0.0072, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-17 13:34:24.398749: step 215340, loss = 0.0071, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 13:35:30.322736: step 215360, loss = 0.0071, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-17 13:36:37.937093: step 215380, loss = 0.0068, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 13:37:44.773208: step 215400, loss = 0.0081, acc = 0.9980 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 13:38:50.692976: step 215420, loss = 0.0065, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 13:39:57.776084: step 215440, loss = 0.0064, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-17 13:41:05.567861: step 215460, loss = 0.0074, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 13:42:14.649109: step 215480, loss = 0.0062, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-17 13:43:21.097134: step 215500, loss = 0.0077, acc = 0.9980 (19.2 examples/sec; 3.341 sec/batch)
2017-05-17 13:44:27.452218: step 215520, loss = 0.0060, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-17 13:45:34.034581: step 215540, loss = 0.0059, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 13:46:42.083637: step 215560, loss = 0.0058, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-17 13:47:48.919816: step 215580, loss = 0.0058, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-17 13:48:55.113381: step 215600, loss = 0.0056, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 13:50:01.194774: step 215620, loss = 0.0055, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 13:51:08.225738: step 215640, loss = 0.0056, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 13:52:14.385308: step 215660, loss = 0.0052, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-17 13:53:21.223440: step 215680, loss = 0.0051, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 13:54:27.660455: step 215700, loss = 0.0050, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-17 13:55:35.022221: step 215720, loss = 0.0051, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 13:56:41.864518: step 215740, loss = 0.0049, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-17 13:57:47.801601: step 215760, loss = 0.0050, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 13:58:55.015529: step 215780, loss = 0.0047, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 14:00:01.634109: step 215800, loss = 0.0046, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 14:01:08.517271: step 215820, loss = 0.0046, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-17 14:02:15.023754: step 215840, loss = 0.0045, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 14:03:22.518320: step 215860, loss = 0.0044, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-17 14:04:30.313485: step 215880, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 14:05:37.516201: step 215900, loss = 0.0043, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-17 14:06:44.028847: step 215920, loss = 0.0042, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-17 14:07:50.674295: step 215940, loss = 0.0042, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-17 14:08:56.618845: step 215960, loss = 0.0041, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-17 14:10:04.718075: step 215980, loss = 0.0042, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-17 14:11:11.089477: step 216000, loss = 0.0039, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
[Eval] 2017-05-17 14:11:26.569524: step 216000, acc = 0.9355, f1 = 0.9333
[Test] 2017-05-17 14:11:36.662138: step 216000, acc = 0.9228, f1 = 0.9221
[Status] 2017-05-17 14:11:36.662220: step 216000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 14:12:44.114348: step 216020, loss = 0.0037, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 14:13:51.148402: step 216040, loss = 0.0038, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-17 14:14:57.896534: step 216060, loss = 0.0036, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 14:16:04.142361: step 216080, loss = 0.0037, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-17 14:17:10.614798: step 216100, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 14:18:16.755152: step 216120, loss = 0.0035, acc = 1.0000 (19.4 examples/sec; 3.290 sec/batch)
2017-05-17 14:19:22.695415: step 216140, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 14:20:28.916628: step 216160, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 14:21:36.678138: step 216180, loss = 0.0033, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-17 14:22:43.349769: step 216200, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 14:23:49.728223: step 216220, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 14:24:56.143214: step 216240, loss = 0.0032, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 14:26:04.314745: step 216260, loss = 0.0031, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-17 14:27:11.477039: step 216280, loss = 0.0033, acc = 1.0000 (18.7 examples/sec; 3.413 sec/batch)
2017-05-17 14:28:19.760035: step 216300, loss = 0.0029, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-17 14:29:27.851481: step 216320, loss = 0.0032, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-17 14:30:34.528424: step 216340, loss = 0.0029, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 14:31:41.341529: step 216360, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 14:32:48.834206: step 216380, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-17 14:33:55.331652: step 216400, loss = 0.0028, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 14:35:02.989802: step 216420, loss = 0.0029, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-17 14:36:10.004586: step 216440, loss = 0.0027, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-17 14:37:16.239872: step 216460, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 14:38:22.275639: step 216480, loss = 0.0032, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-17 14:39:28.741856: step 216500, loss = 0.0027, acc = 1.0000 (19.7 examples/sec; 3.255 sec/batch)
2017-05-17 14:40:35.769759: step 216520, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 14:41:41.755283: step 216540, loss = 0.0026, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 14:42:49.082911: step 216560, loss = 0.0025, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 14:43:56.502484: step 216580, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 14:45:02.902710: step 216600, loss = 0.0041, acc = 0.9980 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 14:46:08.910736: step 216620, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 14:47:15.854559: step 216640, loss = 0.0023, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-17 14:48:23.729865: step 216660, loss = 0.0023, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-17 14:49:30.033645: step 216680, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 14:50:36.980474: step 216700, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 14:51:45.304193: step 216720, loss = 0.0021, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-17 14:52:52.769756: step 216740, loss = 0.0028, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-17 14:53:59.666948: step 216760, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-17 14:55:06.397644: step 216780, loss = 0.0033, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 14:56:12.916603: step 216800, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 14:57:20.008152: step 216820, loss = 0.0034, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-17 14:58:26.772893: step 216840, loss = 0.0019, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-17 14:59:33.239685: step 216860, loss = 0.0025, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 15:00:41.440446: step 216880, loss = 0.0024, acc = 1.0000 (18.2 examples/sec; 3.507 sec/batch)
2017-05-17 15:01:47.882022: step 216900, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 15:02:55.115530: step 216920, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 15:04:01.315405: step 216940, loss = 0.0029, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 15:05:08.056780: step 216960, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 15:06:14.847430: step 216980, loss = 0.0026, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-17 15:07:21.594328: step 217000, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
[Eval] 2017-05-17 15:07:36.964512: step 217000, acc = 0.9378, f1 = 0.9357
[Test] 2017-05-17 15:07:47.609946: step 217000, acc = 0.9256, f1 = 0.9250
[Status] 2017-05-17 15:07:47.610043: step 217000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 15:08:54.346480: step 217020, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 15:10:01.899081: step 217040, loss = 0.0021, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-17 15:11:09.430985: step 217060, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 15:12:15.571030: step 217080, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 15:13:21.839355: step 217100, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 15:14:28.232191: step 217120, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 15:15:34.472164: step 217140, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-17 15:16:41.082321: step 217160, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 15:17:48.155929: step 217180, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 15:18:55.065091: step 217200, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-17 15:20:02.242561: step 217220, loss = 0.0022, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 15:21:08.898408: step 217240, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 15:22:15.383125: step 217260, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-17 15:23:21.959138: step 217280, loss = 0.0031, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-17 15:24:28.080780: step 217300, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 15:25:34.503202: step 217320, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 15:26:40.722199: step 217340, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 15:27:47.122380: step 217360, loss = 0.0030, acc = 0.9980 (19.0 examples/sec; 3.368 sec/batch)
2017-05-17 15:28:53.604151: step 217380, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-17 15:30:00.386173: step 217400, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 15:31:06.522822: step 217420, loss = 0.0026, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-17 15:32:14.246761: step 217440, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-17 15:33:20.497239: step 217460, loss = 0.0019, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-17 15:34:27.405822: step 217480, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 15:35:34.122674: step 217500, loss = 0.0018, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-17 15:36:40.961042: step 217520, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 15:37:47.991922: step 217540, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-17 15:38:54.894719: step 217560, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 15:40:02.027958: step 217580, loss = 0.0014, acc = 1.0000 (18.0 examples/sec; 3.559 sec/batch)
2017-05-17 15:41:09.439044: step 217600, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 15:42:17.704437: step 217620, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-17 15:43:23.880368: step 217640, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 15:44:31.262611: step 217660, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-17 15:45:38.422193: step 217680, loss = 0.0020, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-17 15:46:45.665972: step 217700, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 15:47:53.116161: step 217720, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 15:49:01.076202: step 217740, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 15:50:07.795435: step 217760, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 15:51:13.885249: step 217780, loss = 0.0016, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-17 15:52:20.423470: step 217800, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 15:53:26.665659: step 217820, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 15:54:35.592119: step 217840, loss = 0.0014, acc = 1.0000 (18.1 examples/sec; 3.536 sec/batch)
2017-05-17 15:55:43.698657: step 217860, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-17 15:56:49.958883: step 217880, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-17 15:57:56.756885: step 217900, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-17 15:59:04.758949: step 217920, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-17 16:00:11.081792: step 217940, loss = 0.0035, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 16:01:19.105293: step 217960, loss = 0.0018, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-17 16:02:25.916894: step 217980, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 16:03:34.501398: step 218000, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.523 sec/batch)
[Eval] 2017-05-17 16:03:48.904863: step 218000, acc = 0.9395, f1 = 0.9375
[Test] 2017-05-17 16:03:59.397695: step 218000, acc = 0.9269, f1 = 0.9263
[Status] 2017-05-17 16:03:59.397796: step 218000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 16:05:06.783452: step 218020, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-17 16:06:13.595276: step 218040, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-17 16:07:20.547352: step 218060, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 16:08:27.689246: step 218080, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 16:09:34.612691: step 218100, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-17 16:10:42.177055: step 218120, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-17 16:11:48.507696: step 218140, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-17 16:12:55.555468: step 218160, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-17 16:14:01.843991: step 218180, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 16:15:09.317619: step 218200, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 16:16:16.104795: step 218220, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 16:17:23.035705: step 218240, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-17 16:18:29.942801: step 218260, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 16:19:36.952260: step 218280, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-17 16:20:43.600100: step 218300, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 16:21:50.422901: step 218320, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 16:22:57.605305: step 218340, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-17 16:24:04.066058: step 218360, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 16:25:11.649164: step 218380, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-17 16:26:18.986604: step 218400, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 16:27:25.170926: step 218420, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 16:28:32.401801: step 218440, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-17 16:29:40.235401: step 218460, loss = 0.0015, acc = 1.0000 (17.9 examples/sec; 3.576 sec/batch)
2017-05-17 16:30:48.089684: step 218480, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-17 16:31:54.526019: step 218500, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 16:33:01.382764: step 218520, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 16:34:08.751797: step 218540, loss = 0.0013, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-17 16:35:15.728255: step 218560, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 16:36:22.385014: step 218580, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 16:37:29.124181: step 218600, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 16:38:35.836412: step 218620, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 16:39:43.058256: step 218640, loss = 0.0025, acc = 0.9980 (18.4 examples/sec; 3.475 sec/batch)
2017-05-17 16:40:50.072391: step 218660, loss = 0.0020, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-17 16:41:56.207893: step 218680, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 16:43:02.571388: step 218700, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 16:44:09.326928: step 218720, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 16:45:17.844510: step 218740, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 16:46:25.895906: step 218760, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-17 16:47:33.242320: step 218780, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 16:48:40.078952: step 218800, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 16:49:47.376279: step 218820, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-17 16:50:54.135225: step 218840, loss = 0.0017, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-17 16:52:00.518015: step 218860, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 16:53:07.265660: step 218880, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-17 16:54:14.188568: step 218900, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 16:55:21.530178: step 218920, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-17 16:56:28.276732: step 218940, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 16:57:34.989491: step 218960, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 16:58:41.475242: step 218980, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-17 16:59:47.711377: step 219000, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
[Eval] 2017-05-17 17:00:01.755652: step 219000, acc = 0.9407, f1 = 0.9387
[Test] 2017-05-17 17:00:11.503802: step 219000, acc = 0.9279, f1 = 0.9273
[Status] 2017-05-17 17:00:11.503892: step 219000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 17:01:18.460505: step 219020, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 17:02:25.870692: step 219040, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 17:03:32.309792: step 219060, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 17:04:38.885511: step 219080, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 17:05:45.347263: step 219100, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 17:06:51.963594: step 219120, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 17:07:59.931127: step 219140, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-17 17:09:06.171088: step 219160, loss = 0.0015, acc = 1.0000 (19.7 examples/sec; 3.247 sec/batch)
2017-05-17 17:10:13.049202: step 219180, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-17 17:11:19.532350: step 219200, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-17 17:12:26.145812: step 219220, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 17:13:33.152540: step 219240, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 17:14:39.838263: step 219260, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-17 17:15:47.093274: step 219280, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-17 17:16:53.612087: step 219300, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 17:18:00.321150: step 219320, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 17:19:07.176762: step 219340, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 17:20:13.574435: step 219360, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 17:21:19.809202: step 219380, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 17:22:26.878457: step 219400, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 17:23:34.538190: step 219420, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 17:24:40.907440: step 219440, loss = 0.0027, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 17:25:47.666448: step 219460, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-17 17:26:54.341048: step 219480, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 17:28:01.884367: step 219500, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 17:29:08.384431: step 219520, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 17:30:15.580303: step 219540, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 17:31:22.805214: step 219560, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-17 17:32:29.388044: step 219580, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 17:33:35.820776: step 219600, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 17:34:43.217000: step 219620, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-17 17:35:49.710265: step 219640, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 17:36:56.260462: step 219660, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 17:38:02.824177: step 219680, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 17:39:11.279310: step 219700, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-17 17:40:18.148938: step 219720, loss = 0.0034, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 17:41:25.692720: step 219740, loss = 0.0010, acc = 1.0000 (17.9 examples/sec; 3.566 sec/batch)
2017-05-17 17:42:33.253519: step 219760, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-17 17:43:41.059196: step 219780, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-17 17:44:47.305414: step 219800, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 17:45:53.401824: step 219820, loss = 0.0030, acc = 0.9980 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 17:47:00.674442: step 219840, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-17 17:48:06.707506: step 219860, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 17:49:13.788140: step 219880, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 17:50:19.936089: step 219900, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 17:51:27.057221: step 219920, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-17 17:52:33.660734: step 219940, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 17:53:39.890839: step 219960, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 17:54:46.691779: step 219980, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 17:55:53.348253: step 220000, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
[Eval] 2017-05-17 17:56:08.763350: step 220000, acc = 0.9410, f1 = 0.9391
[Test] 2017-05-17 17:56:19.282880: step 220000, acc = 0.9278, f1 = 0.9272
[Status] 2017-05-17 17:56:19.282976: step 220000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 17:57:26.395717: step 220020, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 17:58:32.632817: step 220040, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 17:59:39.649733: step 220060, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 18:00:45.954820: step 220080, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-17 18:01:52.319700: step 220100, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 18:02:59.120844: step 220120, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 18:04:06.089834: step 220140, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 18:05:12.569750: step 220160, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 18:06:19.615523: step 220180, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 18:07:26.142516: step 220200, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 18:08:32.329287: step 220220, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 18:09:38.530745: step 220240, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 18:10:45.089749: step 220260, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-17 18:11:51.921500: step 220280, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 18:12:58.155141: step 220300, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 18:14:04.960331: step 220320, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 18:15:11.433828: step 220340, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 18:16:18.106048: step 220360, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 18:17:24.497718: step 220380, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 18:18:30.728872: step 220400, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 18:19:37.495476: step 220420, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 18:20:43.757175: step 220440, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 18:21:51.780062: step 220460, loss = 0.0011, acc = 1.0000 (18.1 examples/sec; 3.533 sec/batch)
2017-05-17 18:22:58.508081: step 220480, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 18:24:05.479719: step 220500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 18:25:13.041014: step 220520, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-17 18:26:19.324291: step 220540, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 18:27:26.457132: step 220560, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 18:28:33.635968: step 220580, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-17 18:29:41.295266: step 220600, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 18:30:48.274406: step 220620, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-17 18:31:56.223333: step 220640, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 18:33:03.015633: step 220660, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 18:34:08.946803: step 220680, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 18:35:15.420791: step 220700, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 18:36:21.447404: step 220720, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 18:37:27.698532: step 220740, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 18:38:36.073174: step 220760, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-17 18:39:42.995335: step 220780, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-17 18:40:50.167579: step 220800, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 18:41:58.474021: step 220820, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-17 18:43:05.716375: step 220840, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-17 18:44:12.137022: step 220860, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-17 18:45:19.759075: step 220880, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-17 18:46:26.307962: step 220900, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 18:47:33.055419: step 220920, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-17 18:48:39.583853: step 220940, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 18:49:46.078315: step 220960, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 18:50:52.478679: step 220980, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 18:52:00.003506: step 221000, loss = 0.0016, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
[Eval] 2017-05-17 18:52:14.633783: step 221000, acc = 0.9413, f1 = 0.9394
[Test] 2017-05-17 18:52:24.926064: step 221000, acc = 0.9281, f1 = 0.9275
[Status] 2017-05-17 18:52:24.926175: step 221000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 18:53:31.339492: step 221020, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 18:54:37.630123: step 221040, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 18:55:44.070064: step 221060, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 18:56:49.957594: step 221080, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-17 18:57:56.956212: step 221100, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-17 18:59:03.422062: step 221120, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 19:00:09.228313: step 221140, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 19:01:15.469268: step 221160, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 19:02:22.330372: step 221180, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-17 19:03:28.943263: step 221200, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-17 19:04:35.321345: step 221220, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 19:05:41.915899: step 221240, loss = 0.0025, acc = 0.9980 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 19:06:49.037975: step 221260, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 19:07:55.917307: step 221280, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 19:09:02.094625: step 221300, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 19:10:10.589797: step 221320, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-17 19:11:17.140312: step 221340, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 19:12:24.070208: step 221360, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 19:13:30.624432: step 221380, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-17 19:14:37.066270: step 221400, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-17 19:15:43.469713: step 221420, loss = 0.0029, acc = 0.9980 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 19:16:50.474864: step 221440, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 19:17:58.083987: step 221460, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 19:19:05.838718: step 221480, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-17 19:20:12.155838: step 221500, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 19:21:18.854368: step 221520, loss = 0.0025, acc = 0.9980 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 19:22:26.126139: step 221540, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 19:23:34.073497: step 221560, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 19:24:40.595385: step 221580, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-17 19:25:46.427268: step 221600, loss = 0.0025, acc = 0.9980 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 19:26:53.256228: step 221620, loss = 0.0013, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-17 19:28:00.655945: step 221640, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-17 19:29:06.797325: step 221660, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 19:30:14.476193: step 221680, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-17 19:31:22.523167: step 221700, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 19:32:29.345975: step 221720, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 19:33:36.583709: step 221740, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-17 19:34:43.477870: step 221760, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 19:35:50.014771: step 221780, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-17 19:36:56.983568: step 221800, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 19:38:05.001343: step 221820, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-17 19:39:11.266761: step 221840, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 19:40:17.553613: step 221860, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-17 19:41:24.321383: step 221880, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 19:42:31.101823: step 221900, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 19:43:38.112354: step 221920, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 19:44:45.007246: step 221940, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 19:45:52.463219: step 221960, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-17 19:46:58.579794: step 221980, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 19:48:05.547417: step 222000, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
[Eval] 2017-05-17 19:48:21.003601: step 222000, acc = 0.9415, f1 = 0.9396
[Test] 2017-05-17 19:48:31.021372: step 222000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-17 19:48:31.021445: step 222000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 19:49:37.768279: step 222020, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 19:50:44.131249: step 222040, loss = 0.0015, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 19:51:51.068762: step 222060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 19:52:57.402619: step 222080, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 19:54:04.507698: step 222100, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 19:55:11.498219: step 222120, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-17 19:56:17.729437: step 222140, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 19:57:24.267171: step 222160, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 19:58:30.182006: step 222180, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 19:59:36.290593: step 222200, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 20:00:43.566856: step 222220, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 20:01:50.203329: step 222240, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 20:02:57.297304: step 222260, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-17 20:04:03.909188: step 222280, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 20:05:10.291716: step 222300, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 20:06:17.304166: step 222320, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-17 20:07:23.857785: step 222340, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-17 20:08:30.191970: step 222360, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-17 20:09:36.795581: step 222380, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-17 20:10:43.544877: step 222400, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 20:11:50.415147: step 222420, loss = 0.0023, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-17 20:12:57.484132: step 222440, loss = 0.0026, acc = 0.9980 (18.6 examples/sec; 3.445 sec/batch)
2017-05-17 20:14:04.365149: step 222460, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-17 20:15:11.384888: step 222480, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-17 20:16:18.019886: step 222500, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 20:17:24.155750: step 222520, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 20:18:31.257656: step 222540, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-17 20:19:37.789715: step 222560, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 20:20:45.505595: step 222580, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-17 20:21:52.252026: step 222600, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-17 20:22:59.265408: step 222620, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-17 20:24:06.220382: step 222640, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 20:25:13.675642: step 222660, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 20:26:20.633038: step 222680, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 20:27:27.889059: step 222700, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-17 20:28:34.689369: step 222720, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 20:29:41.540753: step 222740, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 20:30:47.682533: step 222760, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 20:31:55.713106: step 222780, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 20:33:02.939109: step 222800, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 20:34:09.135599: step 222820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 20:35:15.774118: step 222840, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 20:36:22.110400: step 222860, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 20:37:30.056387: step 222880, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 20:38:37.238132: step 222900, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 20:39:43.429035: step 222920, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-17 20:40:50.632243: step 222940, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-17 20:41:57.352218: step 222960, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-17 20:43:03.510553: step 222980, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-17 20:44:10.298471: step 223000, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
[Eval] 2017-05-17 20:44:25.816371: step 223000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-17 20:44:36.521914: step 223000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-17 20:44:36.522019: step 223000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 20:45:43.176219: step 223020, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-17 20:46:49.391932: step 223040, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 20:47:57.280413: step 223060, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-17 20:49:04.644664: step 223080, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
2017-05-17 20:50:11.175245: step 223100, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-17 20:51:17.856957: step 223120, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 20:52:23.845753: step 223140, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-17 20:53:30.361736: step 223160, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-17 20:54:37.453287: step 223180, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 20:55:43.746969: step 223200, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 20:56:51.386788: step 223220, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 20:57:58.315787: step 223240, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 20:59:05.799110: step 223260, loss = 0.0011, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-17 21:00:12.288470: step 223280, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-17 21:01:19.696584: step 223300, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-17 21:02:26.979476: step 223320, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-17 21:03:34.897455: step 223340, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-17 21:04:42.512937: step 223360, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 21:05:49.702557: step 223380, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 21:06:56.431015: step 223400, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-17 21:08:03.723400: step 223420, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-17 21:09:11.214870: step 223440, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-17 21:10:17.943873: step 223460, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-17 21:11:25.258761: step 223480, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-17 21:12:33.542899: step 223500, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.523 sec/batch)
2017-05-17 21:13:39.855900: step 223520, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-17 21:14:46.802503: step 223540, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-17 21:15:53.613289: step 223560, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-17 21:16:59.909300: step 223580, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 21:18:06.847362: step 223600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 21:19:14.761149: step 223620, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-17 21:20:22.909898: step 223640, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-17 21:21:29.826346: step 223660, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 21:22:36.577366: step 223680, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 21:23:42.955734: step 223700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 21:24:49.155814: step 223720, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 21:25:55.530526: step 223740, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-17 21:27:03.286317: step 223760, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-17 21:28:10.638548: step 223780, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-17 21:29:16.932203: step 223800, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-17 21:30:23.629487: step 223820, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-17 21:31:29.889757: step 223840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-17 21:32:36.778979: step 223860, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-17 21:33:43.233436: step 223880, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 21:34:50.299071: step 223900, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-17 21:35:57.423359: step 223920, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-17 21:37:04.159968: step 223940, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-17 21:38:11.731724: step 223960, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 21:39:18.433350: step 223980, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-17 21:40:25.109920: step 224000, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
[Eval] 2017-05-17 21:40:40.434832: step 224000, acc = 0.9415, f1 = 0.9396
[Test] 2017-05-17 21:40:50.972411: step 224000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-17 21:40:50.972501: step 224000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 21:41:57.314030: step 224020, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-17 21:43:04.444843: step 224040, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 21:44:10.627759: step 224060, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-17 21:45:17.978600: step 224080, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-17 21:46:25.080173: step 224100, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-17 21:47:31.505191: step 224120, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-17 21:48:37.695120: step 224140, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-17 21:49:43.851810: step 224160, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 21:50:51.106924: step 224180, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 21:51:58.203795: step 224200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 21:53:04.195651: step 224220, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-17 21:54:11.295730: step 224240, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 21:55:17.444945: step 224260, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-17 21:56:24.701435: step 224280, loss = 0.0017, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-17 21:57:32.491986: step 224300, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-17 21:58:39.457181: step 224320, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 21:59:46.094237: step 224340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-17 22:00:53.510922: step 224360, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-17 22:01:59.811305: step 224380, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 22:03:07.745726: step 224400, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-17 22:04:14.872640: step 224420, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 22:05:21.181817: step 224440, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-17 22:06:27.819790: step 224460, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-17 22:07:34.363427: step 224480, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-17 22:08:40.868123: step 224500, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-17 22:09:46.985417: step 224520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 22:10:53.742064: step 224540, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-17 22:12:00.719625: step 224560, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-17 22:13:07.148279: step 224580, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 22:14:13.247797: step 224600, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 22:15:20.483365: step 224620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 22:16:27.676808: step 224640, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-17 22:17:34.112429: step 224660, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 22:18:41.410568: step 224680, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 22:19:47.937599: step 224700, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-17 22:20:55.556221: step 224720, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-17 22:22:02.003318: step 224740, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 22:23:08.318593: step 224760, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-17 22:24:16.692275: step 224780, loss = 0.0010, acc = 1.0000 (17.7 examples/sec; 3.616 sec/batch)
2017-05-17 22:25:23.372695: step 224800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 22:26:32.137683: step 224820, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-17 22:27:39.583244: step 224840, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-17 22:28:45.509317: step 224860, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 22:29:52.895261: step 224880, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 22:30:59.549748: step 224900, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-17 22:32:06.413043: step 224920, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-17 22:33:12.762602: step 224940, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 22:34:18.878987: step 224960, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-17 22:35:26.354147: step 224980, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-17 22:36:33.042468: step 225000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
[Eval] 2017-05-17 22:36:48.530758: step 225000, acc = 0.9419, f1 = 0.9401
[Test] 2017-05-17 22:36:58.598654: step 225000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-17 22:36:58.598746: step 225000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 22:38:06.005238: step 225020, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.360 sec/batch)
2017-05-17 22:39:13.348462: step 225040, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-17 22:40:19.758410: step 225060, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-17 22:41:26.200043: step 225080, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 22:42:34.122145: step 225100, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 22:43:40.607947: step 225120, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 22:44:47.681386: step 225140, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-17 22:45:54.815631: step 225160, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-17 22:47:02.089543: step 225180, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-17 22:48:09.382701: step 225200, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 22:49:15.543166: step 225220, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-17 22:50:21.960195: step 225240, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-17 22:51:28.915883: step 225260, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-17 22:52:36.438882: step 225280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 22:53:42.962635: step 225300, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-17 22:54:49.767391: step 225320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-17 22:55:55.910218: step 225340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-17 22:57:02.145474: step 225360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-17 22:58:09.303683: step 225380, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-17 22:59:16.317329: step 225400, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-17 23:00:25.176599: step 225420, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-17 23:01:31.458463: step 225440, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-17 23:02:37.961769: step 225460, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 23:03:44.342756: step 225480, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-17 23:04:50.777853: step 225500, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-17 23:05:59.778745: step 225520, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-17 23:07:06.143304: step 225540, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 23:08:12.684007: step 225560, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-17 23:09:20.147742: step 225580, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-17 23:10:27.605042: step 225600, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-17 23:11:34.860097: step 225620, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-17 23:12:41.312297: step 225640, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-17 23:13:47.894776: step 225660, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-17 23:14:54.716346: step 225680, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-17 23:16:01.424046: step 225700, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-17 23:17:07.692680: step 225720, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-17 23:18:15.302506: step 225740, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-17 23:19:22.133882: step 225760, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-17 23:20:28.211255: step 225780, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 23:21:36.332745: step 225800, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-17 23:22:43.184424: step 225820, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-17 23:23:49.408893: step 225840, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 23:24:56.572640: step 225860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-17 23:26:03.211311: step 225880, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-17 23:27:10.309982: step 225900, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-17 23:28:16.202006: step 225920, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-17 23:29:24.349786: step 225940, loss = 0.0010, acc = 1.0000 (18.0 examples/sec; 3.565 sec/batch)
2017-05-17 23:30:30.834350: step 225960, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-17 23:31:36.539923: step 225980, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-17 23:32:43.275603: step 226000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
[Eval] 2017-05-17 23:32:58.519390: step 226000, acc = 0.9417, f1 = 0.9399
[Test] 2017-05-17 23:33:08.591746: step 226000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-17 23:33:08.591839: step 226000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-17 23:34:15.159454: step 226020, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-17 23:35:23.036814: step 226040, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-17 23:36:30.467313: step 226060, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-17 23:37:37.996389: step 226080, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-17 23:38:45.496941: step 226100, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-17 23:39:53.076652: step 226120, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-17 23:40:59.630526: step 226140, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-17 23:42:05.775700: step 226160, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-17 23:43:12.303176: step 226180, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 23:44:18.526501: step 226200, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-17 23:45:25.690711: step 226220, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-17 23:46:32.461664: step 226240, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-17 23:47:39.124640: step 226260, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-17 23:48:46.373810: step 226280, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-17 23:49:54.035100: step 226300, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-17 23:51:01.332901: step 226320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-17 23:52:08.674229: step 226340, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-17 23:53:15.399412: step 226360, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-17 23:54:21.214654: step 226380, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-17 23:55:27.462309: step 226400, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-17 23:56:33.945968: step 226420, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-17 23:57:40.728439: step 226440, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-17 23:58:47.409805: step 226460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-17 23:59:54.132552: step 226480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 00:01:02.522153: step 226500, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-18 00:02:10.199424: step 226520, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-18 00:03:17.166569: step 226540, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-18 00:04:23.590595: step 226560, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 00:05:30.133093: step 226580, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-18 00:06:36.206683: step 226600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 00:07:43.137983: step 226620, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-18 00:08:51.701831: step 226640, loss = 0.0038, acc = 0.9980 (18.4 examples/sec; 3.475 sec/batch)
2017-05-18 00:09:58.893693: step 226660, loss = 0.0012, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-18 00:11:05.879526: step 226680, loss = 0.0033, acc = 0.9980 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 00:12:12.353141: step 226700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 00:13:19.589968: step 226720, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 00:14:26.579595: step 226740, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 00:15:33.149514: step 226760, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 00:16:39.876551: step 226780, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-18 00:17:47.249253: step 226800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 00:18:54.096723: step 226820, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 00:20:01.822871: step 226840, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-18 00:21:09.338167: step 226860, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-18 00:22:17.098831: step 226880, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-18 00:23:23.678064: step 226900, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 00:24:30.308020: step 226920, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 00:25:36.392509: step 226940, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 00:26:43.694086: step 226960, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-18 00:27:49.786356: step 226980, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 00:28:57.441461: step 227000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
[Eval] 2017-05-18 00:29:11.372069: step 227000, acc = 0.9416, f1 = 0.9399
[Test] 2017-05-18 00:29:20.955236: step 227000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-18 00:29:20.955328: step 227000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 00:30:28.015151: step 227020, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 00:31:35.297926: step 227040, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-18 00:32:41.568169: step 227060, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 00:33:48.246327: step 227080, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 00:34:55.843615: step 227100, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 00:36:01.870728: step 227120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-18 00:37:09.373844: step 227140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 00:38:15.388889: step 227160, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 00:39:21.747053: step 227180, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-18 00:40:29.144486: step 227200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 00:41:36.255104: step 227220, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 00:42:43.317661: step 227240, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-18 00:43:49.927475: step 227260, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 00:44:57.235851: step 227280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 00:46:03.762713: step 227300, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 00:47:09.869326: step 227320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 00:48:16.599094: step 227340, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 00:49:22.733122: step 227360, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 00:50:29.248022: step 227380, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 00:51:35.216445: step 227400, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 00:52:41.744280: step 227420, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-18 00:53:48.764285: step 227440, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-18 00:54:55.655633: step 227460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 00:56:01.916630: step 227480, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-18 00:57:08.086793: step 227500, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 00:58:14.488468: step 227520, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-18 00:59:22.763954: step 227540, loss = 0.0015, acc = 1.0000 (17.9 examples/sec; 3.576 sec/batch)
2017-05-18 01:00:29.495493: step 227560, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 01:01:36.422364: step 227580, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-18 01:02:43.599026: step 227600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 01:03:49.714603: step 227620, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 01:04:56.586211: step 227640, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 01:06:03.716033: step 227660, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-18 01:07:10.380205: step 227680, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 01:08:18.028829: step 227700, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-18 01:09:24.902666: step 227720, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-18 01:10:34.268914: step 227740, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-18 01:11:40.815289: step 227760, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 01:12:47.417683: step 227780, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-18 01:13:54.050718: step 227800, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 01:15:01.501142: step 227820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 01:16:08.097054: step 227840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 01:17:14.859992: step 227860, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 01:18:21.358056: step 227880, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-18 01:19:28.752731: step 227900, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-18 01:20:35.893837: step 227920, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 01:21:42.158281: step 227940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 01:22:48.214083: step 227960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 01:23:55.764932: step 227980, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-18 01:25:02.085588: step 228000, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
[Eval] 2017-05-18 01:25:17.413539: step 228000, acc = 0.9420, f1 = 0.9402
[Test] 2017-05-18 01:25:28.035285: step 228000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-18 01:25:28.035380: step 228000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 01:26:34.674169: step 228020, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 01:27:42.096526: step 228040, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 01:28:49.019351: step 228060, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-18 01:29:55.373955: step 228080, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 01:31:01.667722: step 228100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 01:32:09.063535: step 228120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 01:33:16.866851: step 228140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 01:34:23.622651: step 228160, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-18 01:35:29.785209: step 228180, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 01:36:36.452709: step 228200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 01:37:43.978134: step 228220, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 01:38:50.198842: step 228240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 01:39:57.184618: step 228260, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-18 01:41:03.491711: step 228280, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-18 01:42:09.760260: step 228300, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-18 01:43:16.717696: step 228320, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-18 01:44:23.925781: step 228340, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 01:45:31.707601: step 228360, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-18 01:46:38.572962: step 228380, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-18 01:47:46.466559: step 228400, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-18 01:48:53.496366: step 228420, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 01:49:59.877675: step 228440, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 01:51:06.912177: step 228460, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-18 01:52:14.138992: step 228480, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-18 01:53:20.751310: step 228500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 01:54:27.867709: step 228520, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 01:55:35.381367: step 228540, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-18 01:56:42.690805: step 228560, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-18 01:57:48.869718: step 228580, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 01:58:56.295566: step 228600, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.544 sec/batch)
2017-05-18 02:00:02.931785: step 228620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 02:01:10.026887: step 228640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 02:02:17.240581: step 228660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 02:03:24.031576: step 228680, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 02:04:31.304820: step 228700, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 02:05:38.537830: step 228720, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 02:06:45.883762: step 228740, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-18 02:07:53.261607: step 228760, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 02:09:01.345607: step 228780, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 02:10:08.137398: step 228800, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-18 02:11:16.321405: step 228820, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-18 02:12:23.361864: step 228840, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 02:13:29.421696: step 228860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 02:14:39.073191: step 228880, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.563 sec/batch)
2017-05-18 02:15:45.511892: step 228900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 02:16:52.916948: step 228920, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 02:17:59.232145: step 228940, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 02:19:06.270189: step 228960, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-18 02:20:13.810837: step 228980, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-18 02:21:20.360332: step 229000, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-18 02:21:34.424161: step 229000, acc = 0.9419, f1 = 0.9401
[Test] 2017-05-18 02:21:44.022462: step 229000, acc = 0.9287, f1 = 0.9281
[Status] 2017-05-18 02:21:44.022550: step 229000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 02:22:50.207004: step 229020, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 02:23:57.227903: step 229040, loss = 0.0026, acc = 0.9980 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 02:25:03.926699: step 229060, loss = 0.0031, acc = 0.9980 (19.0 examples/sec; 3.360 sec/batch)
2017-05-18 02:26:10.437453: step 229080, loss = 0.0023, acc = 0.9980 (19.1 examples/sec; 3.351 sec/batch)
2017-05-18 02:27:16.927991: step 229100, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-18 02:28:24.349921: step 229120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-18 02:29:30.320154: step 229140, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 02:30:37.179988: step 229160, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 02:31:44.960111: step 229180, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 02:32:51.919445: step 229200, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-18 02:33:58.369435: step 229220, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-18 02:35:04.238556: step 229240, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 02:36:10.631190: step 229260, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-18 02:37:17.945776: step 229280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 02:38:25.299599: step 229300, loss = 0.0024, acc = 0.9980 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 02:39:31.770004: step 229320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 02:40:38.967210: step 229340, loss = 0.0024, acc = 0.9980 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 02:41:45.768223: step 229360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 02:42:52.376129: step 229380, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-18 02:43:58.674872: step 229400, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 02:45:04.686042: step 229420, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 02:46:11.517511: step 229440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 02:47:19.185578: step 229460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 02:48:26.438028: step 229480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 02:49:32.893528: step 229500, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-18 02:50:39.560122: step 229520, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 02:51:46.977365: step 229540, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 02:52:55.035788: step 229560, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-18 02:54:02.552479: step 229580, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 02:55:09.417398: step 229600, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-18 02:56:16.494762: step 229620, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-18 02:57:23.605419: step 229640, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 02:58:30.301347: step 229660, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-18 02:59:36.572775: step 229680, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-18 03:00:44.033995: step 229700, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-18 03:01:51.099436: step 229720, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-18 03:02:58.945248: step 229740, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-18 03:04:05.655422: step 229760, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-18 03:05:12.749502: step 229780, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 03:06:19.597372: step 229800, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 03:07:27.542651: step 229820, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-18 03:08:34.925711: step 229840, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-18 03:09:42.762127: step 229860, loss = 0.0012, acc = 1.0000 (17.5 examples/sec; 3.649 sec/batch)
2017-05-18 03:10:50.184782: step 229880, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.507 sec/batch)
2017-05-18 03:11:56.657577: step 229900, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 03:13:03.821189: step 229920, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.251 sec/batch)
2017-05-18 03:14:10.380142: step 229940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 03:15:17.392623: step 229960, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-18 03:16:23.611032: step 229980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 03:17:29.896920: step 230000, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-18 03:17:44.052757: step 230000, acc = 0.9420, f1 = 0.9402
[Test] 2017-05-18 03:17:54.431085: step 230000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-18 03:17:54.431167: step 230000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 03:19:01.712385: step 230020, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.413 sec/batch)
2017-05-18 03:20:08.457772: step 230040, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-18 03:21:15.043479: step 230060, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-18 03:22:21.481375: step 230080, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 03:23:30.019096: step 230100, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-18 03:24:37.215868: step 230120, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-18 03:25:44.834981: step 230140, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-18 03:26:52.501685: step 230160, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 03:27:59.392157: step 230180, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-18 03:29:06.242890: step 230200, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-18 03:30:13.682411: step 230220, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-18 03:31:20.369009: step 230240, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.377 sec/batch)
2017-05-18 03:32:27.338261: step 230260, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 03:33:34.565897: step 230280, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 03:34:42.439745: step 230300, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-18 03:35:50.230910: step 230320, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-18 03:36:56.716858: step 230340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 03:38:04.158290: step 230360, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-18 03:39:11.190453: step 230380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 03:40:17.699825: step 230400, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-18 03:41:25.049318: step 230420, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 03:42:32.586999: step 230440, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 03:43:40.435613: step 230460, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-18 03:44:46.482949: step 230480, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 03:45:53.522241: step 230500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 03:47:00.862021: step 230520, loss = 0.0010, acc = 1.0000 (18.1 examples/sec; 3.544 sec/batch)
2017-05-18 03:48:07.871522: step 230540, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-18 03:49:15.121810: step 230560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 03:50:21.873841: step 230580, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 03:51:30.281384: step 230600, loss = 0.0009, acc = 1.0000 (17.7 examples/sec; 3.609 sec/batch)
2017-05-18 03:52:38.740322: step 230620, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-18 03:53:45.514964: step 230640, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 03:54:52.820756: step 230660, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-18 03:55:59.137392: step 230680, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 03:57:05.491844: step 230700, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 03:58:11.318545: step 230720, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 03:59:18.351467: step 230740, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-18 04:00:24.406427: step 230760, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 04:01:31.008713: step 230780, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 04:02:38.182123: step 230800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 04:03:45.418552: step 230820, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-18 04:04:52.672056: step 230840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 04:06:00.004654: step 230860, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-18 04:07:07.358037: step 230880, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-18 04:08:15.096957: step 230900, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-18 04:09:21.508290: step 230920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 04:10:28.825816: step 230940, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-18 04:11:35.942492: step 230960, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-18 04:12:43.256588: step 230980, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-18 04:13:49.585841: step 231000, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
[Eval] 2017-05-18 04:14:03.866479: step 231000, acc = 0.9425, f1 = 0.9407
[Test] 2017-05-18 04:14:13.977366: step 231000, acc = 0.9290, f1 = 0.9284
[Status] 2017-05-18 04:14:13.977462: step 231000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 04:15:20.410825: step 231020, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 04:16:26.594413: step 231040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 04:17:33.899556: step 231060, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-18 04:18:41.001473: step 231080, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 04:19:47.120537: step 231100, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 04:20:54.085375: step 231120, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 04:22:00.565807: step 231140, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-18 04:23:07.507139: step 231160, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 04:24:13.988268: step 231180, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-18 04:25:21.161472: step 231200, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-18 04:26:27.666278: step 231220, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-18 04:27:35.336483: step 231240, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-18 04:28:41.782430: step 231260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 04:29:48.799694: step 231280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 04:30:56.152620: step 231300, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 04:32:02.933406: step 231320, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 04:33:09.569992: step 231340, loss = 0.0023, acc = 0.9980 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 04:34:17.017596: step 231360, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-18 04:35:24.808409: step 231380, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-18 04:36:30.732748: step 231400, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 04:37:39.488006: step 231420, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 04:38:45.724497: step 231440, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 04:39:53.125871: step 231460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 04:40:59.424331: step 231480, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-18 04:42:06.212230: step 231500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 04:43:12.888810: step 231520, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 04:44:20.771776: step 231540, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-18 04:45:28.056130: step 231560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 04:46:34.652537: step 231580, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-18 04:47:40.643865: step 231600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 04:48:46.593280: step 231620, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 04:49:54.773309: step 231640, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 04:51:01.585707: step 231660, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 04:52:09.104256: step 231680, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 04:53:15.351151: step 231700, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-18 04:54:24.080202: step 231720, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 04:55:31.804593: step 231740, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 04:56:39.190592: step 231760, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-18 04:57:47.228873: step 231780, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-18 04:58:54.880952: step 231800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 05:00:01.011302: step 231820, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 05:01:08.529331: step 231840, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-18 05:02:15.891101: step 231860, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 05:03:22.518959: step 231880, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 05:04:29.266353: step 231900, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-18 05:05:35.873212: step 231920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 05:06:42.436840: step 231940, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-18 05:07:49.317998: step 231960, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 05:08:56.392833: step 231980, loss = 0.0022, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-18 05:10:03.156927: step 232000, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
[Eval] 2017-05-18 05:10:18.780306: step 232000, acc = 0.9420, f1 = 0.9402
[Test] 2017-05-18 05:10:29.509053: step 232000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-18 05:10:29.509138: step 232000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 05:11:36.191317: step 232020, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-18 05:12:42.199808: step 232040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 05:13:49.456973: step 232060, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-18 05:14:56.060536: step 232080, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 05:16:03.873066: step 232100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-18 05:17:11.085711: step 232120, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 05:18:18.335812: step 232140, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-18 05:19:28.036953: step 232160, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-18 05:20:34.354883: step 232180, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 05:21:40.371460: step 232200, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-18 05:22:47.946542: step 232220, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-18 05:23:54.291852: step 232240, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 05:25:00.454775: step 232260, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 05:26:06.480608: step 232280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 05:27:13.197697: step 232300, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-18 05:28:19.172972: step 232320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 05:29:26.237872: step 232340, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 05:30:32.368772: step 232360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 05:31:38.792521: step 232380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-18 05:32:45.694774: step 232400, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 05:33:52.800265: step 232420, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 05:35:00.459093: step 232440, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-18 05:36:06.514754: step 232460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 05:37:14.198436: step 232480, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-18 05:38:20.960563: step 232500, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 05:39:27.682656: step 232520, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 05:40:35.283580: step 232540, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 05:41:43.166586: step 232560, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-18 05:42:50.513703: step 232580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 05:43:56.297331: step 232600, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 05:45:03.656128: step 232620, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 05:46:09.783135: step 232640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 05:47:16.222695: step 232660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 05:48:23.270040: step 232680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 05:49:29.896970: step 232700, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 05:50:36.744591: step 232720, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 05:51:45.210043: step 232740, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-18 05:52:52.100342: step 232760, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 05:53:59.163490: step 232780, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-18 05:55:05.626081: step 232800, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-18 05:56:11.698894: step 232820, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 05:57:18.410979: step 232840, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-18 05:58:26.290405: step 232860, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 05:59:33.514630: step 232880, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-18 06:00:39.991580: step 232900, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-18 06:01:47.330944: step 232920, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-18 06:02:55.049799: step 232940, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-18 06:04:02.514863: step 232960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 06:05:09.554773: step 232980, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 06:06:16.238104: step 233000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-18 06:06:30.342128: step 233000, acc = 0.9416, f1 = 0.9398
[Test] 2017-05-18 06:06:39.997597: step 233000, acc = 0.9287, f1 = 0.9281
[Status] 2017-05-18 06:06:39.997711: step 233000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 06:07:47.464087: step 233020, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-18 06:08:55.838036: step 233040, loss = 0.0013, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-18 06:10:02.184700: step 233060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 06:11:08.830863: step 233080, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 06:12:15.391182: step 233100, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-18 06:13:21.393976: step 233120, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-18 06:14:28.198642: step 233140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 06:15:34.372881: step 233160, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 06:16:41.325228: step 233180, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-18 06:17:48.889756: step 233200, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-18 06:18:55.859051: step 233220, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-18 06:20:03.674411: step 233240, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 06:21:09.889091: step 233260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 06:22:17.345798: step 233280, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-18 06:23:24.697923: step 233300, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.540 sec/batch)
2017-05-18 06:24:32.504013: step 233320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 06:25:39.527166: step 233340, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-18 06:26:45.902366: step 233360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 06:27:52.024591: step 233380, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 06:28:59.661465: step 233400, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 06:30:07.008224: step 233420, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 06:31:13.787754: step 233440, loss = 0.0037, acc = 0.9980 (19.5 examples/sec; 3.282 sec/batch)
2017-05-18 06:32:21.449052: step 233460, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-18 06:33:28.336667: step 233480, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 06:34:35.162183: step 233500, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-18 06:35:41.410155: step 233520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 06:36:48.023541: step 233540, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 06:37:55.249319: step 233560, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-18 06:39:01.520255: step 233580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 06:40:07.953460: step 233600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 06:41:15.541817: step 233620, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-18 06:42:22.337986: step 233640, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-18 06:43:30.493784: step 233660, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-18 06:44:38.667013: step 233680, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 06:45:45.496582: step 233700, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-18 06:46:52.372292: step 233720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 06:47:58.705530: step 233740, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 06:49:06.269607: step 233760, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-18 06:50:12.707294: step 233780, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 06:51:19.071954: step 233800, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 06:52:25.334302: step 233820, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 06:53:33.240785: step 233840, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-18 06:54:41.338178: step 233860, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-18 06:55:49.689893: step 233880, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-18 06:56:56.515733: step 233900, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-18 06:58:03.335924: step 233920, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 06:59:10.043862: step 233940, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 07:00:16.305151: step 233960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 07:01:23.853821: step 233980, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 07:02:30.198764: step 234000, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
[Eval] 2017-05-18 07:02:45.336023: step 234000, acc = 0.9424, f1 = 0.9406
[Test] 2017-05-18 07:02:55.292561: step 234000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-18 07:02:55.292647: step 234000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 07:04:02.067937: step 234020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 07:05:10.283799: step 234040, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-18 07:06:16.541465: step 234060, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 07:07:22.926597: step 234080, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 07:08:29.598634: step 234100, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 07:09:36.242434: step 234120, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 07:10:43.189067: step 234140, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 07:11:50.800540: step 234160, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 07:12:57.083027: step 234180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 07:14:03.176139: step 234200, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 07:15:10.058237: step 234220, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-18 07:16:16.405262: step 234240, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 07:17:23.124271: step 234260, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 07:18:30.491789: step 234280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 07:19:36.349598: step 234300, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 07:20:42.551552: step 234320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 07:21:49.344985: step 234340, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 07:22:55.836718: step 234360, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-18 07:24:02.088127: step 234380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 07:25:08.449590: step 234400, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-18 07:26:15.621648: step 234420, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-18 07:27:21.832648: step 234440, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 07:28:30.764281: step 234460, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-18 07:29:38.016487: step 234480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 07:30:44.462924: step 234500, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 07:31:51.558779: step 234520, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 07:32:57.887025: step 234540, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 07:34:04.033430: step 234560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 07:35:10.980112: step 234580, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 07:36:17.730131: step 234600, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-18 07:37:25.332966: step 234620, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 07:38:33.571162: step 234640, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-18 07:39:40.746092: step 234660, loss = 0.0095, acc = 0.9980 (18.5 examples/sec; 3.468 sec/batch)
2017-05-18 07:40:47.271004: step 234680, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-18 07:41:54.904224: step 234700, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-18 07:43:01.983260: step 234720, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-18 07:44:08.255599: step 234740, loss = 0.0032, acc = 0.9980 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 07:45:14.568666: step 234760, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 07:46:22.029625: step 234780, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-18 07:47:29.536872: step 234800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 07:48:36.123902: step 234820, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 07:49:43.397339: step 234840, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 07:50:50.613580: step 234860, loss = 0.0008, acc = 1.0000 (18.0 examples/sec; 3.564 sec/batch)
2017-05-18 07:51:57.699381: step 234880, loss = 0.0010, acc = 1.0000 (18.1 examples/sec; 3.545 sec/batch)
2017-05-18 07:53:03.916272: step 234900, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 07:54:10.212345: step 234920, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 07:55:17.577037: step 234940, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-18 07:56:25.185626: step 234960, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-18 07:57:32.878828: step 234980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 07:58:39.796631: step 235000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-18 07:58:54.358262: step 235000, acc = 0.9422, f1 = 0.9404
[Test] 2017-05-18 07:59:04.956217: step 235000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-18 07:59:04.956296: step 235000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 08:00:11.593699: step 235020, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-18 08:01:18.288210: step 235040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 08:02:25.153835: step 235060, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-18 08:03:31.403629: step 235080, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 08:04:37.973352: step 235100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 08:05:44.281535: step 235120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 08:06:51.484131: step 235140, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-18 08:07:59.276389: step 235160, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 08:09:06.199007: step 235180, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 08:10:13.202255: step 235200, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-18 08:11:20.408000: step 235220, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-18 08:12:27.369089: step 235240, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-18 08:13:34.736870: step 235260, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 08:14:42.608580: step 235280, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-18 08:15:48.687447: step 235300, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-18 08:16:54.973113: step 235320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 08:18:01.399291: step 235340, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 08:19:08.138275: step 235360, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-18 08:20:14.193995: step 235380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 08:21:20.437855: step 235400, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 08:22:27.115172: step 235420, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 08:23:34.299037: step 235440, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-18 08:24:40.922174: step 235460, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-18 08:25:47.191516: step 235480, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-18 08:26:54.090055: step 235500, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-18 08:28:00.288677: step 235520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 08:29:06.426891: step 235540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 08:30:13.878333: step 235560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 08:31:20.804401: step 235580, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-18 08:32:27.496804: step 235600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 08:33:34.275416: step 235620, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-18 08:34:40.594643: step 235640, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 08:35:48.254612: step 235660, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 08:36:54.828138: step 235680, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 08:38:02.109680: step 235700, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-18 08:39:08.262492: step 235720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 08:40:16.216059: step 235740, loss = 0.0008, acc = 1.0000 (17.7 examples/sec; 3.609 sec/batch)
2017-05-18 08:41:22.288434: step 235760, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 08:42:28.708448: step 235780, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-18 08:43:35.384258: step 235800, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-18 08:44:41.951839: step 235820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 08:45:48.686796: step 235840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 08:46:55.719604: step 235860, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 08:48:03.775884: step 235880, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-18 08:49:10.600155: step 235900, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-18 08:50:19.204186: step 235920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 08:51:27.081858: step 235940, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-18 08:52:33.896535: step 235960, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-18 08:53:41.088844: step 235980, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 08:54:47.681514: step 236000, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
[Eval] 2017-05-18 08:55:03.025990: step 236000, acc = 0.9427, f1 = 0.9408
[Test] 2017-05-18 08:55:13.664488: step 236000, acc = 0.9289, f1 = 0.9284
[Status] 2017-05-18 08:55:13.664569: step 236000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 08:56:21.467887: step 236020, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-18 08:57:27.993963: step 236040, loss = 0.0013, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-18 08:58:35.419277: step 236060, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 08:59:42.566920: step 236080, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 09:00:48.921675: step 236100, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-18 09:01:55.518126: step 236120, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 09:03:02.372484: step 236140, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 09:04:08.960799: step 236160, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 09:05:16.117455: step 236180, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-18 09:06:22.950101: step 236200, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 09:07:29.921436: step 236220, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-18 09:08:36.375285: step 236240, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-18 09:09:43.180738: step 236260, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 09:10:49.840573: step 236280, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 09:11:56.235427: step 236300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 09:13:03.954531: step 236320, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-18 09:14:10.138202: step 236340, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-18 09:15:16.850351: step 236360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 09:16:23.694376: step 236380, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-18 09:17:29.626256: step 236400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-18 09:18:35.977276: step 236420, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 09:19:42.784011: step 236440, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-18 09:20:49.282417: step 236460, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 09:21:57.679278: step 236480, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-18 09:23:06.208478: step 236500, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 09:24:13.045089: step 236520, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-18 09:25:19.788785: step 236540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 09:26:26.908853: step 236560, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 09:27:33.449046: step 236580, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-18 09:28:40.147404: step 236600, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 09:29:46.862294: step 236620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 09:30:53.761816: step 236640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 09:32:00.763790: step 236660, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-18 09:33:07.516013: step 236680, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 09:34:14.330015: step 236700, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-18 09:35:21.699002: step 236720, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-18 09:36:28.281917: step 236740, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 09:37:34.947093: step 236760, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 09:38:41.320439: step 236780, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 09:39:48.278568: step 236800, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 09:40:55.475834: step 236820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 09:42:03.176224: step 236840, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 09:43:09.921127: step 236860, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 09:44:16.743362: step 236880, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 09:45:23.518776: step 236900, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 09:46:30.195313: step 236920, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 09:47:36.322066: step 236940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 09:48:44.579775: step 236960, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-18 09:49:50.849401: step 236980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 09:50:58.075839: step 237000, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
[Eval] 2017-05-18 09:51:13.642316: step 237000, acc = 0.9428, f1 = 0.9410
[Test] 2017-05-18 09:51:24.267659: step 237000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-18 09:51:24.267760: step 237000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 09:52:31.139264: step 237020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 09:53:37.521658: step 237040, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 09:54:43.354618: step 237060, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-18 09:55:49.777778: step 237080, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 09:56:56.429946: step 237100, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 09:58:04.201378: step 237120, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-18 09:59:10.513291: step 237140, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-18 10:00:18.308287: step 237160, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-18 10:01:25.974515: step 237180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 10:02:33.214846: step 237200, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-18 10:03:39.347314: step 237220, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-18 10:04:45.751193: step 237240, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-18 10:05:52.550023: step 237260, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-18 10:06:59.527963: step 237280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 10:08:07.262681: step 237300, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-18 10:09:14.564574: step 237320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 10:10:22.127110: step 237340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 10:11:30.212669: step 237360, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-18 10:12:36.608784: step 237380, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 10:13:43.789848: step 237400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 10:14:50.138758: step 237420, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 10:15:56.865376: step 237440, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-18 10:17:03.479362: step 237460, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 10:18:11.377290: step 237480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 10:19:17.356724: step 237500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 10:20:23.954123: step 237520, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 10:21:30.546266: step 237540, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 10:22:36.972406: step 237560, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-18 10:23:44.105951: step 237580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 10:24:50.615689: step 237600, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 10:25:56.904936: step 237620, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 10:27:03.074377: step 237640, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 10:28:09.589633: step 237660, loss = 0.0017, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-18 10:29:16.540160: step 237680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 10:30:23.891547: step 237700, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 10:31:30.168203: step 237720, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 10:32:36.399865: step 237740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 10:33:42.627602: step 237760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 10:34:50.462325: step 237780, loss = 0.0008, acc = 1.0000 (17.8 examples/sec; 3.598 sec/batch)
2017-05-18 10:35:57.505139: step 237800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 10:37:05.125868: step 237820, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 10:38:11.612450: step 237840, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-18 10:39:17.900952: step 237860, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 10:40:25.194585: step 237880, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-18 10:41:32.928964: step 237900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 10:42:39.100569: step 237920, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-18 10:43:45.358051: step 237940, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 10:44:52.032179: step 237960, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-18 10:45:58.830768: step 237980, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-18 10:47:05.028196: step 238000, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
[Eval] 2017-05-18 10:47:20.623117: step 238000, acc = 0.9422, f1 = 0.9404
[Test] 2017-05-18 10:47:31.365542: step 238000, acc = 0.9292, f1 = 0.9286
[Status] 2017-05-18 10:47:31.365652: step 238000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 10:48:37.973062: step 238020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 10:49:45.401765: step 238040, loss = 0.0023, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 10:50:53.269808: step 238060, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-18 10:51:59.476424: step 238080, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 10:53:06.488171: step 238100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 10:54:13.583111: step 238120, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-18 10:55:21.524824: step 238140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 10:56:27.618304: step 238160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 10:57:34.177143: step 238180, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 10:58:40.764508: step 238200, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 10:59:47.339604: step 238220, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 11:00:54.200163: step 238240, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-18 11:02:01.520065: step 238260, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 11:03:08.517874: step 238280, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-18 11:04:15.946573: step 238300, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 11:05:23.895490: step 238320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 11:06:30.332308: step 238340, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 11:07:36.472691: step 238360, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 11:08:42.858394: step 238380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 11:09:49.523703: step 238400, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 11:10:56.339022: step 238420, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-18 11:12:02.919082: step 238440, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 11:13:09.712714: step 238460, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-18 11:14:17.091928: step 238480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 11:15:23.920327: step 238500, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-18 11:16:31.087356: step 238520, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 11:17:37.317189: step 238540, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-18 11:18:44.167964: step 238560, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 11:19:52.027352: step 238580, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-18 11:20:58.988276: step 238600, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.522 sec/batch)
2017-05-18 11:22:06.193616: step 238620, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-18 11:23:13.456022: step 238640, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-18 11:24:20.295263: step 238660, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 11:25:26.398158: step 238680, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 11:26:34.191873: step 238700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 11:27:42.039252: step 238720, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-18 11:28:48.469728: step 238740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 11:29:55.048544: step 238760, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 11:31:01.978496: step 238780, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-18 11:32:08.250387: step 238800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 11:33:15.042991: step 238820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 11:34:21.391428: step 238840, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-18 11:35:27.504458: step 238860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 11:36:34.577354: step 238880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 11:37:41.849033: step 238900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 11:38:49.860256: step 238920, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-18 11:39:56.351861: step 238940, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-18 11:41:03.491922: step 238960, loss = 0.0026, acc = 0.9980 (18.8 examples/sec; 3.409 sec/batch)
2017-05-18 11:42:09.489424: step 238980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 11:43:17.692227: step 239000, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
[Eval] 2017-05-18 11:43:31.881467: step 239000, acc = 0.9427, f1 = 0.9408
[Test] 2017-05-18 11:43:41.663855: step 239000, acc = 0.9290, f1 = 0.9284
[Status] 2017-05-18 11:43:41.663935: step 239000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 11:44:48.202404: step 239020, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-18 11:45:54.513916: step 239040, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 11:47:03.108943: step 239060, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-18 11:48:09.512594: step 239080, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-18 11:49:16.332627: step 239100, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 11:50:22.588513: step 239120, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 11:51:30.367220: step 239140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 11:52:36.412080: step 239160, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-18 11:53:42.933407: step 239180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 11:54:50.036164: step 239200, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-18 11:55:56.272320: step 239220, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 11:57:03.698574: step 239240, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-18 11:58:11.323232: step 239260, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-18 11:59:18.988111: step 239280, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 12:00:26.162241: step 239300, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 12:01:34.291007: step 239320, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-18 12:02:41.095281: step 239340, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-18 12:03:47.882897: step 239360, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 12:04:55.208306: step 239380, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-18 12:06:01.578060: step 239400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 12:07:07.725504: step 239420, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 12:08:15.365855: step 239440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 12:09:22.515853: step 239460, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-18 12:10:28.976682: step 239480, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 12:11:35.628943: step 239500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 12:12:41.671627: step 239520, loss = 0.0022, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 12:13:48.757018: step 239540, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 12:14:56.908180: step 239560, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-18 12:16:03.079747: step 239580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 12:17:09.593114: step 239600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 12:18:17.401819: step 239620, loss = 0.0022, acc = 0.9980 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 12:19:23.901513: step 239640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 12:20:30.719631: step 239660, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 12:21:37.980705: step 239680, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 12:22:44.067898: step 239700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 12:23:50.837231: step 239720, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 12:24:57.085404: step 239740, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-18 12:26:04.617915: step 239760, loss = 0.0023, acc = 0.9980 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 12:27:12.073433: step 239780, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-18 12:28:18.856767: step 239800, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-18 12:29:26.498800: step 239820, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 12:30:33.073623: step 239840, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-18 12:31:39.739877: step 239860, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-18 12:32:46.896163: step 239880, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-18 12:33:54.645222: step 239900, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-18 12:35:01.629980: step 239920, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 12:36:08.173685: step 239940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 12:37:16.226919: step 239960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 12:38:23.208557: step 239980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-18 12:39:30.520667: step 240000, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
[Eval] 2017-05-18 12:39:45.871647: step 240000, acc = 0.9424, f1 = 0.9406
[Test] 2017-05-18 12:39:56.571743: step 240000, acc = 0.9291, f1 = 0.9285
[Status] 2017-05-18 12:39:56.571865: step 240000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 12:41:02.803545: step 240020, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-18 12:42:09.132364: step 240040, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-18 12:43:16.584784: step 240060, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-18 12:44:23.517502: step 240080, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-18 12:45:29.938774: step 240100, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-18 12:46:37.910573: step 240120, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-18 12:47:44.390589: step 240140, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 12:48:51.069174: step 240160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-18 12:49:57.520981: step 240180, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 12:51:06.843158: step 240200, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-18 12:52:14.522584: step 240220, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 12:53:22.690862: step 240240, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-18 12:54:29.311718: step 240260, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-18 12:55:35.517181: step 240280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 12:56:42.042010: step 240300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 12:57:48.692504: step 240320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 12:58:55.937689: step 240340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 13:00:02.393299: step 240360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-18 13:01:08.796188: step 240380, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-18 13:02:16.118961: step 240400, loss = 0.0018, acc = 1.0000 (18.2 examples/sec; 3.520 sec/batch)
2017-05-18 13:03:22.139595: step 240420, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 13:04:28.879884: step 240440, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 13:05:35.393430: step 240460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-18 13:06:42.185359: step 240480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 13:07:48.371747: step 240500, loss = 0.0032, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 13:08:55.705357: step 240520, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 13:10:01.750747: step 240540, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-18 13:11:08.156181: step 240560, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-18 13:12:14.861840: step 240580, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-18 13:13:21.405184: step 240600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 13:14:28.559842: step 240620, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-18 13:15:35.178990: step 240640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 13:16:42.606282: step 240660, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-18 13:17:49.900668: step 240680, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 13:18:56.350331: step 240700, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-18 13:20:03.798398: step 240720, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-18 13:21:11.352576: step 240740, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-18 13:22:17.949914: step 240760, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-18 13:23:26.066532: step 240780, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-18 13:24:33.904032: step 240800, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-18 13:25:40.847085: step 240820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-18 13:26:47.780060: step 240840, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-18 13:27:54.691399: step 240860, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 13:29:01.029042: step 240880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 13:30:07.775151: step 240900, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-18 13:31:15.371193: step 240920, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-18 13:32:22.777092: step 240940, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-18 13:33:28.913212: step 240960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 13:34:36.014444: step 240980, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-18 13:35:43.440376: step 241000, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
[Eval] 2017-05-18 13:35:57.616264: step 241000, acc = 0.9423, f1 = 0.9405
[Test] 2017-05-18 13:36:07.312482: step 241000, acc = 0.9288, f1 = 0.9283
[Status] 2017-05-18 13:36:07.312555: step 241000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 13:37:14.153578: step 241020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 13:38:20.812827: step 241040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 13:39:27.980577: step 241060, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 13:40:34.918854: step 241080, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-18 13:41:41.573775: step 241100, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 13:42:48.577389: step 241120, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 13:43:55.283119: step 241140, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 13:45:02.416401: step 241160, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-18 13:46:08.550577: step 241180, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-18 13:47:15.884887: step 241200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 13:48:22.175411: step 241220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 13:49:28.541184: step 241240, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 13:50:34.990165: step 241260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 13:51:41.313655: step 241280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 13:52:47.981320: step 241300, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-18 13:53:55.901623: step 241320, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-18 13:55:02.815628: step 241340, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-18 13:56:09.640660: step 241360, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 13:57:16.460902: step 241380, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-18 13:58:23.399791: step 241400, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 13:59:30.706782: step 241420, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-18 14:00:38.015338: step 241440, loss = 0.0033, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 14:01:44.848530: step 241460, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 14:02:52.359028: step 241480, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-18 14:03:58.838253: step 241500, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 14:05:05.439459: step 241520, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-18 14:06:12.176185: step 241540, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-18 14:07:20.311615: step 241560, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-18 14:08:26.479431: step 241580, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 14:09:34.013016: step 241600, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 14:10:41.211064: step 241620, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-18 14:11:48.529061: step 241640, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 14:12:55.407725: step 241660, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 14:14:01.657627: step 241680, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 14:15:08.726826: step 241700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 14:16:16.255766: step 241720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 14:17:23.229149: step 241740, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-18 14:18:29.628677: step 241760, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 14:19:36.558519: step 241780, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 14:20:44.327843: step 241800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 14:21:50.633087: step 241820, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 14:22:58.913019: step 241840, loss = 0.0008, acc = 1.0000 (18.0 examples/sec; 3.560 sec/batch)
2017-05-18 14:24:05.719907: step 241860, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 14:25:13.282808: step 241880, loss = 0.0022, acc = 0.9980 (19.2 examples/sec; 3.342 sec/batch)
2017-05-18 14:26:20.965606: step 241900, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 14:27:29.334560: step 241920, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-18 14:28:36.481038: step 241940, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-18 14:29:44.278065: step 241960, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 14:30:50.781124: step 241980, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 14:31:57.461382: step 242000, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
[Eval] 2017-05-18 14:32:12.907995: step 242000, acc = 0.9409, f1 = 0.9391
[Test] 2017-05-18 14:32:23.284366: step 242000, acc = 0.9286, f1 = 0.9279
[Status] 2017-05-18 14:32:23.284456: step 242000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 14:33:29.841182: step 242020, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 14:34:37.263003: step 242040, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-18 14:35:43.771221: step 242060, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-18 14:36:51.071072: step 242080, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-18 14:37:57.920927: step 242100, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 14:39:04.547053: step 242120, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 14:40:11.304372: step 242140, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 14:41:18.670011: step 242160, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-18 14:42:25.542408: step 242180, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-18 14:43:31.860755: step 242200, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 14:44:38.981887: step 242220, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 14:45:45.135812: step 242240, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 14:46:52.085060: step 242260, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-18 14:47:58.605207: step 242280, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-18 14:49:05.903448: step 242300, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-18 14:50:12.393407: step 242320, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-18 14:51:18.943926: step 242340, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 14:52:25.397738: step 242360, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-18 14:53:31.951643: step 242380, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 14:54:39.197273: step 242400, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 14:55:46.654766: step 242420, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 14:56:53.324130: step 242440, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 14:57:59.866246: step 242460, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-18 14:59:06.093988: step 242480, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 15:00:12.987892: step 242500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 15:01:20.453638: step 242520, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-18 15:02:28.282697: step 242540, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 15:03:34.726064: step 242560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 15:04:41.979096: step 242580, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-18 15:05:49.030280: step 242600, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-18 15:06:55.797963: step 242620, loss = 0.0023, acc = 0.9980 (18.4 examples/sec; 3.478 sec/batch)
2017-05-18 15:08:03.584737: step 242640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 15:09:10.260041: step 242660, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 15:10:16.399589: step 242680, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 15:11:23.060449: step 242700, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-18 15:12:30.008294: step 242720, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-18 15:13:37.942361: step 242740, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-18 15:14:44.546523: step 242760, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 15:15:50.756555: step 242780, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 15:16:57.720321: step 242800, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-18 15:18:03.997177: step 242820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-18 15:19:10.182632: step 242840, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-18 15:20:17.315946: step 242860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 15:21:23.260229: step 242880, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 15:22:29.231359: step 242900, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 15:23:36.422595: step 242920, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-18 15:24:44.672847: step 242940, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 15:25:51.500014: step 242960, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 15:26:58.450359: step 242980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 15:28:05.031108: step 243000, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
[Eval] 2017-05-18 15:28:20.540760: step 243000, acc = 0.9427, f1 = 0.9409
[Test] 2017-05-18 15:28:31.204360: step 243000, acc = 0.9297, f1 = 0.9291
[Status] 2017-05-18 15:28:31.204454: step 243000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 15:29:38.238491: step 243020, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-18 15:30:45.160877: step 243040, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 15:31:52.328727: step 243060, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-18 15:32:59.822356: step 243080, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-18 15:34:06.684275: step 243100, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 15:35:13.786140: step 243120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 15:36:20.859400: step 243140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 15:37:27.608318: step 243160, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-18 15:38:34.423469: step 243180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 15:39:41.352023: step 243200, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 15:40:47.738911: step 243220, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 15:41:54.460083: step 243240, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 15:43:01.150438: step 243260, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 15:44:08.413251: step 243280, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 15:45:15.398640: step 243300, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-18 15:46:22.665055: step 243320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 15:47:28.912325: step 243340, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 15:48:35.653221: step 243360, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 15:49:42.316736: step 243380, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 15:50:48.245219: step 243400, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 15:51:55.036764: step 243420, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 15:53:01.428752: step 243440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 15:54:08.792459: step 243460, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-18 15:55:15.538093: step 243480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 15:56:23.195132: step 243500, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 15:57:30.245677: step 243520, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 15:58:37.180836: step 243540, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-18 15:59:45.470922: step 243560, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 16:00:51.814197: step 243580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 16:01:59.020056: step 243600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 16:03:06.011506: step 243620, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 16:04:12.720769: step 243640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 16:05:19.975533: step 243660, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 16:06:26.062104: step 243680, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-18 16:07:32.457390: step 243700, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-18 16:08:38.354014: step 243720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 16:09:45.718823: step 243740, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-18 16:10:51.741893: step 243760, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 16:11:57.535163: step 243780, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 16:13:04.167892: step 243800, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-18 16:14:11.269538: step 243820, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 16:15:17.473731: step 243840, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 16:16:23.844568: step 243860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 16:17:29.717938: step 243880, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 16:18:36.726377: step 243900, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-18 16:19:44.093145: step 243920, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 16:20:51.960798: step 243940, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 16:21:58.780844: step 243960, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-18 16:23:06.032863: step 243980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-18 16:24:12.046802: step 244000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
[Eval] 2017-05-18 16:24:27.555575: step 244000, acc = 0.9426, f1 = 0.9407
[Test] 2017-05-18 16:24:38.186336: step 244000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-18 16:24:38.186449: step 244000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 16:25:44.542670: step 244020, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 16:26:52.730491: step 244040, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 16:27:59.667005: step 244060, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-18 16:29:05.760348: step 244080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 16:30:12.172136: step 244100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 16:31:18.195525: step 244120, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 16:32:24.859497: step 244140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 16:33:31.841803: step 244160, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-18 16:34:38.136380: step 244180, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 16:35:44.304573: step 244200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 16:36:50.524225: step 244220, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 16:37:57.171730: step 244240, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 16:39:04.106065: step 244260, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-18 16:40:10.942167: step 244280, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 16:41:17.358119: step 244300, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 16:42:24.851754: step 244320, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-18 16:43:31.653935: step 244340, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 16:44:39.059765: step 244360, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-18 16:45:45.195786: step 244380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 16:46:51.577448: step 244400, loss = 0.0022, acc = 0.9980 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 16:47:57.965450: step 244420, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 16:49:04.812561: step 244440, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-18 16:50:10.742767: step 244460, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-18 16:51:18.286239: step 244480, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-18 16:52:24.960369: step 244500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 16:53:33.620202: step 244520, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-18 16:54:41.048510: step 244540, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-18 16:55:47.669483: step 244560, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 16:56:55.293157: step 244580, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-18 16:58:01.744802: step 244600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 16:59:08.054689: step 244620, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 17:00:15.298900: step 244640, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-18 17:01:23.354618: step 244660, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-18 17:02:30.165549: step 244680, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-18 17:03:37.244134: step 244700, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-18 17:04:44.097085: step 244720, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-18 17:05:51.227364: step 244740, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-18 17:06:58.079005: step 244760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 17:08:04.252059: step 244780, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 17:09:10.969486: step 244800, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-18 17:10:17.804815: step 244820, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-18 17:11:23.917778: step 244840, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 17:12:30.834427: step 244860, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-18 17:13:38.053236: step 244880, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-18 17:14:44.958841: step 244900, loss = 0.0011, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-18 17:15:52.204016: step 244920, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-18 17:16:59.289906: step 244940, loss = 0.0009, acc = 1.0000 (17.9 examples/sec; 3.573 sec/batch)
2017-05-18 17:18:06.891963: step 244960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 17:19:14.592389: step 244980, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-18 17:20:23.609422: step 245000, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
[Eval] 2017-05-18 17:20:39.116137: step 245000, acc = 0.9421, f1 = 0.9403
[Test] 2017-05-18 17:20:49.601305: step 245000, acc = 0.9291, f1 = 0.9285
[Status] 2017-05-18 17:20:49.601388: step 245000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 17:21:56.436521: step 245020, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 17:23:02.992243: step 245040, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-18 17:24:08.793519: step 245060, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-18 17:25:16.245770: step 245080, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 17:26:22.598091: step 245100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 17:27:29.458299: step 245120, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-18 17:28:36.075952: step 245140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 17:29:42.858887: step 245160, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-18 17:30:49.767988: step 245180, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-18 17:31:56.004257: step 245200, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 17:33:02.450980: step 245220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-18 17:34:09.103838: step 245240, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-18 17:35:15.030894: step 245260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 17:36:21.850415: step 245280, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-18 17:37:28.716601: step 245300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 17:38:35.350195: step 245320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 17:39:42.445458: step 245340, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-18 17:40:49.497617: step 245360, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 17:41:55.655217: step 245380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 17:43:03.100575: step 245400, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-18 17:44:09.805540: step 245420, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-18 17:45:17.593182: step 245440, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 17:46:24.410664: step 245460, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 17:47:31.593888: step 245480, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-18 17:48:38.280832: step 245500, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 17:49:45.006051: step 245520, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 17:50:51.387307: step 245540, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 17:51:59.115122: step 245560, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-18 17:53:05.548682: step 245580, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-18 17:54:11.912790: step 245600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 17:55:18.366215: step 245620, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 17:56:24.731352: step 245640, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-18 17:57:31.244122: step 245660, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-18 17:58:37.350209: step 245680, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-18 17:59:43.316918: step 245700, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 18:00:50.795912: step 245720, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-18 18:01:57.123436: step 245740, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 18:03:03.766454: step 245760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 18:04:10.246248: step 245780, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 18:05:16.312484: step 245800, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 18:06:22.857113: step 245820, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-18 18:07:29.018403: step 245840, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-18 18:08:35.708966: step 245860, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 18:09:41.957350: step 245880, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 18:10:48.147267: step 245900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 18:11:54.941981: step 245920, loss = 0.0024, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-18 18:13:01.816799: step 245940, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 18:14:09.449431: step 245960, loss = 0.0019, acc = 1.0000 (18.8 examples/sec; 3.395 sec/batch)
2017-05-18 18:15:16.263689: step 245980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 18:16:22.996215: step 246000, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
[Eval] 2017-05-18 18:16:38.410398: step 246000, acc = 0.9427, f1 = 0.9408
[Test] 2017-05-18 18:16:48.899991: step 246000, acc = 0.9292, f1 = 0.9286
[Status] 2017-05-18 18:16:48.900106: step 246000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 18:17:54.901527: step 246020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 18:19:02.614846: step 246040, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 18:20:10.000368: step 246060, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-18 18:21:17.042157: step 246080, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-18 18:22:23.471369: step 246100, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 18:23:30.381936: step 246120, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 18:24:37.253842: step 246140, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-18 18:25:44.036266: step 246160, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-18 18:26:52.927250: step 246180, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-18 18:27:58.818720: step 246200, loss = 0.0021, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-18 18:29:06.944563: step 246220, loss = 0.0010, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-18 18:30:14.199241: step 246240, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 18:31:20.209141: step 246260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 18:32:27.728731: step 246280, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-18 18:33:35.125741: step 246300, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 18:34:41.898853: step 246320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 18:35:49.019727: step 246340, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-18 18:36:55.488184: step 246360, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-18 18:38:03.414857: step 246380, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 18:39:09.288501: step 246400, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-18 18:40:17.508437: step 246420, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 18:41:24.803555: step 246440, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-18 18:42:31.399421: step 246460, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 18:43:39.101927: step 246480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 18:44:45.988271: step 246500, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 18:45:52.829512: step 246520, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-18 18:46:59.085696: step 246540, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 18:48:07.923880: step 246560, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-18 18:49:14.804060: step 246580, loss = 0.0023, acc = 0.9980 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 18:50:21.633460: step 246600, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-18 18:51:29.190451: step 246620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 18:52:36.264411: step 246640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 18:53:43.917402: step 246660, loss = 0.0008, acc = 1.0000 (17.7 examples/sec; 3.609 sec/batch)
2017-05-18 18:54:50.281347: step 246680, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-18 18:55:57.736347: step 246700, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-18 18:57:04.329279: step 246720, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 18:58:11.378941: step 246740, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-18 18:59:19.373129: step 246760, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-18 19:00:26.020098: step 246780, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-18 19:01:33.317537: step 246800, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-18 19:02:39.875964: step 246820, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-18 19:03:46.108761: step 246840, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 19:04:52.615704: step 246860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 19:05:59.311611: step 246880, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-18 19:07:05.916224: step 246900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 19:08:12.185631: step 246920, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-18 19:09:20.093898: step 246940, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-18 19:10:27.278673: step 246960, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 19:11:34.165684: step 246980, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-18 19:12:40.328626: step 247000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-18 19:12:55.607471: step 247000, acc = 0.9399, f1 = 0.9378
[Test] 2017-05-18 19:13:06.181596: step 247000, acc = 0.9260, f1 = 0.9254
[Status] 2017-05-18 19:13:06.181704: step 247000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 19:14:12.559441: step 247020, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 19:15:19.685012: step 247040, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 19:16:26.690465: step 247060, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 19:17:32.541828: step 247080, loss = 0.0008, acc = 1.0000 (19.7 examples/sec; 3.252 sec/batch)
2017-05-18 19:18:38.810217: step 247100, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-18 19:19:45.530738: step 247120, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 19:20:52.467760: step 247140, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-18 19:21:58.637858: step 247160, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-18 19:23:05.754749: step 247180, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-18 19:24:12.656411: step 247200, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-18 19:25:19.698126: step 247220, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-18 19:26:26.974296: step 247240, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-18 19:27:33.360265: step 247260, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-18 19:28:40.926508: step 247280, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-18 19:29:48.618496: step 247300, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-18 19:30:55.164384: step 247320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 19:32:01.393769: step 247340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 19:33:08.822882: step 247360, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-18 19:34:14.945198: step 247380, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 19:35:21.301456: step 247400, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-18 19:36:27.323353: step 247420, loss = 0.0022, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 19:37:34.391251: step 247440, loss = 0.0014, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-18 19:38:41.166392: step 247460, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 19:39:50.109163: step 247480, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-18 19:40:56.452381: step 247500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-18 19:42:03.203165: step 247520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 19:43:09.921573: step 247540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 19:44:16.929355: step 247560, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 19:45:23.610634: step 247580, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-18 19:46:30.018156: step 247600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 19:47:37.976468: step 247620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 19:48:44.930919: step 247640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 19:49:50.987671: step 247660, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-18 19:50:58.128588: step 247680, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-18 19:52:04.669320: step 247700, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 19:53:11.735119: step 247720, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 19:54:19.028324: step 247740, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-18 19:55:25.445777: step 247760, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 19:56:32.184190: step 247780, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-18 19:57:38.771178: step 247800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 19:58:45.793050: step 247820, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 19:59:52.876361: step 247840, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 20:00:59.063735: step 247860, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-18 20:02:07.748944: step 247880, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-18 20:03:14.342541: step 247900, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-18 20:04:22.036611: step 247920, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 20:05:28.909653: step 247940, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-18 20:06:36.310922: step 247960, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-18 20:07:43.580491: step 247980, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 20:08:50.075667: step 248000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
[Eval] 2017-05-18 20:09:05.558351: step 248000, acc = 0.9422, f1 = 0.9402
[Test] 2017-05-18 20:09:16.122359: step 248000, acc = 0.9287, f1 = 0.9281
[Status] 2017-05-18 20:09:16.122458: step 248000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 20:10:22.925776: step 248020, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 20:11:29.764740: step 248040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 20:12:37.003533: step 248060, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 20:13:43.814488: step 248080, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 20:14:50.587395: step 248100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 20:15:56.903825: step 248120, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-18 20:17:03.948496: step 248140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 20:18:11.394717: step 248160, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-18 20:19:17.983574: step 248180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 20:20:25.433977: step 248200, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-18 20:21:32.816909: step 248220, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-18 20:22:40.064708: step 248240, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-18 20:23:46.734443: step 248260, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 20:24:52.928705: step 248280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-18 20:25:58.898355: step 248300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 20:27:05.585782: step 248320, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-18 20:28:11.990968: step 248340, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 20:29:18.036193: step 248360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 20:30:24.651276: step 248380, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 20:31:30.963814: step 248400, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-18 20:32:37.417189: step 248420, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 20:33:44.671047: step 248440, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-18 20:34:51.936145: step 248460, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 20:35:58.973224: step 248480, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-18 20:37:06.122632: step 248500, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 20:38:13.873898: step 248520, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-18 20:39:20.819112: step 248540, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-18 20:40:27.361576: step 248560, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-18 20:41:34.423824: step 248580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 20:42:40.901653: step 248600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 20:43:47.127706: step 248620, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 20:44:53.245856: step 248640, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 20:46:00.307107: step 248660, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 20:47:07.491159: step 248680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 20:48:15.330125: step 248700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-18 20:49:21.230877: step 248720, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-18 20:50:28.360459: step 248740, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 20:51:34.136357: step 248760, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 20:52:41.822384: step 248780, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-18 20:53:47.858473: step 248800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-18 20:54:53.961769: step 248820, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-18 20:56:00.873655: step 248840, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-18 20:57:07.312573: step 248860, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 20:58:14.158358: step 248880, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-18 20:59:21.760540: step 248900, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-18 21:00:28.560928: step 248920, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-18 21:01:35.392875: step 248940, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-18 21:02:42.015564: step 248960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 21:03:50.434030: step 248980, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-18 21:04:56.230351: step 249000, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
[Eval] 2017-05-18 21:05:11.624185: step 249000, acc = 0.9412, f1 = 0.9392
[Test] 2017-05-18 21:05:22.300040: step 249000, acc = 0.9277, f1 = 0.9271
[Status] 2017-05-18 21:05:22.300144: step 249000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 21:06:29.089127: step 249020, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-18 21:07:36.533277: step 249040, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-18 21:08:43.357356: step 249060, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-18 21:09:49.436172: step 249080, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-18 21:10:57.559420: step 249100, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-18 21:12:05.611590: step 249120, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-18 21:13:12.345854: step 249140, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-18 21:14:20.543503: step 249160, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-18 21:15:26.553154: step 249180, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 21:16:34.136698: step 249200, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-18 21:17:40.215829: step 249220, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 21:18:46.792351: step 249240, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-18 21:19:54.131408: step 249260, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-18 21:21:02.090582: step 249280, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-18 21:22:09.310625: step 249300, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-18 21:23:16.038904: step 249320, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 21:24:23.371010: step 249340, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-18 21:25:30.894278: step 249360, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 21:26:38.226552: step 249380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-18 21:27:47.087020: step 249400, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-18 21:28:53.295045: step 249420, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-18 21:29:59.914918: step 249440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 21:31:06.926207: step 249460, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-18 21:32:13.432808: step 249480, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-18 21:33:19.580510: step 249500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-18 21:34:26.933489: step 249520, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-18 21:35:34.170692: step 249540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 21:36:40.769345: step 249560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 21:37:46.684409: step 249580, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-18 21:38:52.986882: step 249600, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-18 21:40:00.244313: step 249620, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-18 21:41:06.647151: step 249640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 21:42:14.280024: step 249660, loss = 0.0022, acc = 0.9980 (18.4 examples/sec; 3.487 sec/batch)
2017-05-18 21:43:21.282695: step 249680, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-18 21:44:28.039283: step 249700, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 21:45:35.403563: step 249720, loss = 0.0015, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-18 21:46:42.136237: step 249740, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 21:47:49.298297: step 249760, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-18 21:48:56.359467: step 249780, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-18 21:50:03.654806: step 249800, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 21:51:10.121612: step 249820, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 21:52:17.037448: step 249840, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-18 21:53:24.022884: step 249860, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-18 21:54:31.435433: step 249880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 21:55:38.446548: step 249900, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-18 21:56:45.523373: step 249920, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-18 21:57:52.016710: step 249940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 21:58:59.497656: step 249960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 22:00:07.795520: step 249980, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.540 sec/batch)
2017-05-18 22:01:14.606096: step 250000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
[Eval] 2017-05-18 22:01:30.127892: step 250000, acc = 0.9423, f1 = 0.9404
[Test] 2017-05-18 22:01:40.643209: step 250000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-18 22:01:40.643309: step 250000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 22:02:49.062812: step 250020, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-18 22:03:55.601693: step 250040, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 22:05:02.868593: step 250060, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 22:06:10.234047: step 250080, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-18 22:07:17.128794: step 250100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 22:08:23.455854: step 250120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 22:09:30.689613: step 250140, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-18 22:10:38.299978: step 250160, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 22:11:44.320611: step 250180, loss = 0.0150, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-18 22:12:50.395225: step 250200, loss = 0.1052, acc = 0.9940 (19.3 examples/sec; 3.319 sec/batch)
2017-05-18 22:13:56.474591: step 250220, loss = 0.0857, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 22:15:03.231846: step 250240, loss = 0.0993, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-18 22:16:09.145903: step 250260, loss = 0.1052, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 22:17:16.869796: step 250280, loss = 0.1067, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-18 22:18:22.985602: step 250300, loss = 0.1470, acc = 0.9980 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 22:19:31.471295: step 250320, loss = 0.1055, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-18 22:20:38.389600: step 250340, loss = 0.1134, acc = 0.9980 (19.4 examples/sec; 3.294 sec/batch)
2017-05-18 22:21:44.374145: step 250360, loss = 0.0965, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-18 22:22:51.581694: step 250380, loss = 0.0938, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-18 22:23:58.216675: step 250400, loss = 0.0907, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-18 22:25:04.777384: step 250420, loss = 0.0874, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-18 22:26:11.591102: step 250440, loss = 0.0854, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-18 22:27:18.450694: step 250460, loss = 0.0824, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 22:28:26.458631: step 250480, loss = 0.0801, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 22:29:33.043026: step 250500, loss = 0.0779, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 22:30:40.898008: step 250520, loss = 0.0761, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 22:31:47.302096: step 250540, loss = 0.0739, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-18 22:32:54.683698: step 250560, loss = 0.0720, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-18 22:34:01.321074: step 250580, loss = 0.0703, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-18 22:35:08.620498: step 250600, loss = 0.0687, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 22:36:15.088349: step 250620, loss = 0.0697, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
2017-05-18 22:37:20.867929: step 250640, loss = 0.0655, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-18 22:38:27.375711: step 250660, loss = 0.0639, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-18 22:39:34.791843: step 250680, loss = 0.0625, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-18 22:40:41.315839: step 250700, loss = 0.0610, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-18 22:41:49.558982: step 250720, loss = 0.0596, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-18 22:42:56.705655: step 250740, loss = 0.0582, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-18 22:44:03.163633: step 250760, loss = 0.0569, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 22:45:09.662181: step 250780, loss = 0.0571, acc = 0.9980 (19.0 examples/sec; 3.376 sec/batch)
2017-05-18 22:46:16.199239: step 250800, loss = 0.0544, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-18 22:47:22.426025: step 250820, loss = 0.0531, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 22:48:29.491790: step 250840, loss = 0.0520, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-18 22:49:37.220920: step 250860, loss = 0.0520, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-18 22:50:43.348079: step 250880, loss = 0.0498, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-18 22:51:50.970212: step 250900, loss = 0.0487, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-18 22:52:57.273070: step 250920, loss = 0.0489, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-18 22:54:04.268717: step 250940, loss = 0.0468, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-18 22:55:10.571259: step 250960, loss = 0.0457, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-18 22:56:16.859523: step 250980, loss = 0.0448, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 22:57:24.991511: step 251000, loss = 0.0439, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
[Eval] 2017-05-18 22:57:40.550143: step 251000, acc = 0.9281, f1 = 0.9263
[Test] 2017-05-18 22:57:51.292687: step 251000, acc = 0.9153, f1 = 0.9145
[Status] 2017-05-18 22:57:51.292765: step 251000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 22:58:57.955272: step 251020, loss = 0.0429, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 23:00:04.510860: step 251040, loss = 0.0425, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-18 23:01:12.189441: step 251060, loss = 0.0411, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-18 23:02:19.306871: step 251080, loss = 0.0415, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-18 23:03:27.823270: step 251100, loss = 0.0394, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-18 23:04:35.558388: step 251120, loss = 0.0385, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-18 23:05:41.651826: step 251140, loss = 0.0377, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-18 23:06:48.697839: step 251160, loss = 0.0370, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-18 23:07:54.773956: step 251180, loss = 0.0362, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 23:09:01.300556: step 251200, loss = 0.0355, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-18 23:10:08.834606: step 251220, loss = 0.0348, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 23:11:15.917801: step 251240, loss = 0.0341, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-18 23:12:23.057842: step 251260, loss = 0.0333, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-18 23:13:30.380414: step 251280, loss = 0.0326, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-18 23:14:37.665609: step 251300, loss = 0.0320, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-18 23:15:44.550648: step 251320, loss = 0.0324, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-18 23:16:50.409708: step 251340, loss = 0.0307, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 23:17:56.831871: step 251360, loss = 0.0301, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-18 23:19:03.200957: step 251380, loss = 0.0295, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-18 23:20:10.634644: step 251400, loss = 0.0289, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-18 23:21:17.887029: step 251420, loss = 0.0284, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-18 23:22:25.091383: step 251440, loss = 0.0277, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-18 23:23:31.553727: step 251460, loss = 0.0272, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-18 23:24:37.813062: step 251480, loss = 0.0267, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-18 23:25:45.079236: step 251500, loss = 0.0261, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-18 23:26:51.570298: step 251520, loss = 0.0257, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-18 23:27:58.373962: step 251540, loss = 0.0252, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-18 23:29:05.765415: step 251560, loss = 0.0246, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-18 23:30:12.607490: step 251580, loss = 0.0252, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-18 23:31:19.478110: step 251600, loss = 0.0236, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-18 23:32:26.865335: step 251620, loss = 0.0233, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-18 23:33:33.670021: step 251640, loss = 0.0230, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-18 23:34:40.345873: step 251660, loss = 0.0222, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 23:35:47.539030: step 251680, loss = 0.0217, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-18 23:36:54.913989: step 251700, loss = 0.0213, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-18 23:38:02.126981: step 251720, loss = 0.0208, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-18 23:39:08.831429: step 251740, loss = 0.0204, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-18 23:40:15.153860: step 251760, loss = 0.0200, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-18 23:41:22.984927: step 251780, loss = 0.0196, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-18 23:42:29.825816: step 251800, loss = 0.0192, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-18 23:43:35.639782: step 251820, loss = 0.0193, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-18 23:44:41.865894: step 251840, loss = 0.0186, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-18 23:45:48.805583: step 251860, loss = 0.0181, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-18 23:46:54.916691: step 251880, loss = 0.0177, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-18 23:48:01.914361: step 251900, loss = 0.0174, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-18 23:49:08.703271: step 251920, loss = 0.0170, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-18 23:50:16.234225: step 251940, loss = 0.0166, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-18 23:51:23.632498: step 251960, loss = 0.0163, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-18 23:52:30.759639: step 251980, loss = 0.0160, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 23:53:38.285949: step 252000, loss = 0.0157, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
[Eval] 2017-05-18 23:53:53.881731: step 252000, acc = 0.9288, f1 = 0.9268
[Test] 2017-05-18 23:54:04.722852: step 252000, acc = 0.9170, f1 = 0.9162
[Status] 2017-05-18 23:54:04.722965: step 252000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-18 23:55:12.104378: step 252020, loss = 0.0153, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-18 23:56:19.330939: step 252040, loss = 0.0150, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-18 23:57:26.833110: step 252060, loss = 0.0147, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-18 23:58:34.010263: step 252080, loss = 0.0144, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-18 23:59:40.558825: step 252100, loss = 0.0141, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 00:00:47.325592: step 252120, loss = 0.0138, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-19 00:01:53.421210: step 252140, loss = 0.0135, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 00:03:01.581286: step 252160, loss = 0.0132, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-19 00:04:08.071513: step 252180, loss = 0.0130, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 00:05:15.513550: step 252200, loss = 0.0127, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 00:06:23.186497: step 252220, loss = 0.0126, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 00:07:29.465515: step 252240, loss = 0.0122, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 00:08:36.902875: step 252260, loss = 0.0119, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 00:09:44.404039: step 252280, loss = 0.0117, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 00:10:51.056519: step 252300, loss = 0.0114, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 00:11:59.559094: step 252320, loss = 0.0112, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-19 00:13:06.417119: step 252340, loss = 0.0110, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 00:14:13.535198: step 252360, loss = 0.0108, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 00:15:19.437940: step 252380, loss = 0.0105, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-19 00:16:25.441271: step 252400, loss = 0.0103, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 00:17:31.804191: step 252420, loss = 0.0101, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-19 00:18:38.684437: step 252440, loss = 0.0099, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-19 00:19:46.086288: step 252460, loss = 0.0111, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 00:20:52.709838: step 252480, loss = 0.0098, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 00:21:59.310707: step 252500, loss = 0.0093, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 00:23:06.562538: step 252520, loss = 0.0091, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 00:24:14.024200: step 252540, loss = 0.0105, acc = 0.9980 (18.4 examples/sec; 3.477 sec/batch)
2017-05-19 00:25:20.783372: step 252560, loss = 0.0087, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-19 00:26:27.246030: step 252580, loss = 0.0086, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 00:27:34.463174: step 252600, loss = 0.0084, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 00:28:40.480396: step 252620, loss = 0.0082, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 00:29:48.028155: step 252640, loss = 0.0357, acc = 0.9980 (18.6 examples/sec; 3.446 sec/batch)
2017-05-19 00:30:54.464452: step 252660, loss = 0.0079, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 00:32:01.393230: step 252680, loss = 0.0077, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-19 00:33:08.236237: step 252700, loss = 0.0075, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 00:34:15.882148: step 252720, loss = 0.0077, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 00:35:23.537402: step 252740, loss = 0.0072, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-19 00:36:31.741404: step 252760, loss = 0.0071, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-19 00:37:38.444944: step 252780, loss = 0.0069, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 00:38:45.556605: step 252800, loss = 0.0068, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 00:39:53.072784: step 252820, loss = 0.0067, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-19 00:40:59.750022: step 252840, loss = 0.0065, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 00:42:07.115954: step 252860, loss = 0.0064, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-19 00:43:14.056631: step 252880, loss = 0.0062, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-19 00:44:21.096897: step 252900, loss = 0.0061, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 00:45:27.409301: step 252920, loss = 0.0061, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 00:46:35.266689: step 252940, loss = 0.0062, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 00:47:42.208521: step 252960, loss = 0.0058, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 00:48:49.599927: step 252980, loss = 0.0057, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-19 00:49:55.649621: step 253000, loss = 0.0055, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
[Eval] 2017-05-19 00:50:11.019617: step 253000, acc = 0.9286, f1 = 0.9267
[Test] 2017-05-19 00:50:21.715883: step 253000, acc = 0.9168, f1 = 0.9160
[Status] 2017-05-19 00:50:21.715982: step 253000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 00:51:28.744006: step 253020, loss = 0.0055, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-19 00:52:36.116683: step 253040, loss = 0.0052, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 00:53:42.679192: step 253060, loss = 0.0054, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 00:54:48.533680: step 253080, loss = 0.0051, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 00:55:55.319589: step 253100, loss = 0.0049, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 00:57:02.161086: step 253120, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 00:58:09.660496: step 253140, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 00:59:17.674239: step 253160, loss = 0.0047, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 01:00:25.379679: step 253180, loss = 0.0046, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-19 01:01:31.288929: step 253200, loss = 0.0045, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 01:02:37.755503: step 253220, loss = 0.0043, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-19 01:03:44.683168: step 253240, loss = 0.0046, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-19 01:04:51.176562: step 253260, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 01:05:57.881135: step 253280, loss = 0.0041, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 01:07:05.724338: step 253300, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 01:08:12.577978: step 253320, loss = 0.0039, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-19 01:09:19.294177: step 253340, loss = 0.0039, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-19 01:10:25.941591: step 253360, loss = 0.0038, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-19 01:11:33.498625: step 253380, loss = 0.0037, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 01:12:40.639243: step 253400, loss = 0.0038, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 01:13:46.758504: step 253420, loss = 0.0036, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 01:14:53.686852: step 253440, loss = 0.0034, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-19 01:16:00.631898: step 253460, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 01:17:07.802505: step 253480, loss = 0.0034, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 01:18:15.502556: step 253500, loss = 0.0032, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 01:19:21.325368: step 253520, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 01:20:28.098209: step 253540, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 01:21:34.114763: step 253560, loss = 0.0031, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 01:22:40.299829: step 253580, loss = 0.0030, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 01:23:48.506345: step 253600, loss = 0.0033, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-19 01:24:56.558879: step 253620, loss = 0.0029, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 01:26:02.988369: step 253640, loss = 0.0029, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 01:27:08.952512: step 253660, loss = 0.0027, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 01:28:16.919977: step 253680, loss = 0.0028, acc = 1.0000 (18.3 examples/sec; 3.507 sec/batch)
2017-05-19 01:29:23.480692: step 253700, loss = 0.0027, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 01:30:29.421967: step 253720, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 01:31:36.514626: step 253740, loss = 0.0043, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 01:32:43.847043: step 253760, loss = 0.0027, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-19 01:33:51.270464: step 253780, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 01:34:58.364712: step 253800, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 01:36:05.723813: step 253820, loss = 0.0026, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-19 01:37:12.011092: step 253840, loss = 0.0024, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 01:38:18.980264: step 253860, loss = 0.0023, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-19 01:39:25.685470: step 253880, loss = 0.0037, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-19 01:40:32.292520: step 253900, loss = 0.0034, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 01:41:39.581207: step 253920, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 01:42:46.222284: step 253940, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 01:43:53.151585: step 253960, loss = 0.0021, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 01:45:00.001079: step 253980, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 01:46:06.867724: step 254000, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
[Eval] 2017-05-19 01:46:22.095792: step 254000, acc = 0.9346, f1 = 0.9327
[Test] 2017-05-19 01:46:32.737682: step 254000, acc = 0.9219, f1 = 0.9213
[Status] 2017-05-19 01:46:32.737795: step 254000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 01:47:41.479127: step 254020, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 01:48:47.399454: step 254040, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-19 01:49:53.923409: step 254060, loss = 0.0020, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-19 01:51:00.651537: step 254080, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-19 01:52:08.111486: step 254100, loss = 0.0021, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 01:53:14.907526: step 254120, loss = 0.0018, acc = 1.0000 (18.1 examples/sec; 3.539 sec/batch)
2017-05-19 01:54:22.875634: step 254140, loss = 0.0019, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-19 01:55:30.233449: step 254160, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-19 01:56:37.430619: step 254180, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 01:57:43.264814: step 254200, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 01:58:50.159493: step 254220, loss = 0.0017, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-19 01:59:58.117184: step 254240, loss = 0.0028, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 02:01:04.988535: step 254260, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 02:02:12.845857: step 254280, loss = 0.0027, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-19 02:03:20.833694: step 254300, loss = 0.0027, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-19 02:04:27.848792: step 254320, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 02:05:34.940171: step 254340, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 02:06:41.793845: step 254360, loss = 0.0015, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-19 02:07:48.571106: step 254380, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 02:08:56.245160: step 254400, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 02:10:03.217727: step 254420, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 02:11:09.880045: step 254440, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 02:12:16.966014: step 254460, loss = 0.0014, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-19 02:13:23.650664: step 254480, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 02:14:30.156151: step 254500, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 02:15:37.772177: step 254520, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 02:16:44.658022: step 254540, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-19 02:17:51.473643: step 254560, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-19 02:18:59.712320: step 254580, loss = 0.0015, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-19 02:20:07.245082: step 254600, loss = 0.0032, acc = 0.9980 (18.8 examples/sec; 3.411 sec/batch)
2017-05-19 02:21:13.617892: step 254620, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 02:22:22.369847: step 254640, loss = 0.0014, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-19 02:23:28.514152: step 254660, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 02:24:35.786359: step 254680, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 02:25:42.544559: step 254700, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 02:26:49.724213: step 254720, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 02:27:56.365667: step 254740, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-19 02:29:03.663629: step 254760, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 02:30:10.897637: step 254780, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 02:31:17.999547: step 254800, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 02:32:24.919352: step 254820, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 02:33:32.186245: step 254840, loss = 0.0013, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-19 02:34:38.552626: step 254860, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 02:35:45.202860: step 254880, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-19 02:36:52.380562: step 254900, loss = 0.0022, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-19 02:37:59.521403: step 254920, loss = 0.0019, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-19 02:39:05.915624: step 254940, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 02:40:13.105394: step 254960, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 02:41:20.377889: step 254980, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 02:42:26.764302: step 255000, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
[Eval] 2017-05-19 02:42:42.021452: step 255000, acc = 0.9383, f1 = 0.9364
[Test] 2017-05-19 02:42:52.551872: step 255000, acc = 0.9256, f1 = 0.9249
[Status] 2017-05-19 02:42:52.551976: step 255000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 02:44:01.034198: step 255020, loss = 0.0015, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-19 02:45:07.553075: step 255040, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 02:46:14.334828: step 255060, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 02:47:21.919477: step 255080, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 02:48:28.984769: step 255100, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 02:49:34.877590: step 255120, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 02:50:40.883542: step 255140, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 02:51:48.029699: step 255160, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-19 02:52:55.005875: step 255180, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 02:54:01.713756: step 255200, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 02:55:08.062952: step 255220, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 02:56:14.209931: step 255240, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 02:57:21.788382: step 255260, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-19 02:58:28.663130: step 255280, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 02:59:35.099549: step 255300, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 03:00:41.724513: step 255320, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 03:01:48.754456: step 255340, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 03:02:55.084707: step 255360, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-19 03:04:01.698970: step 255380, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 03:05:09.735874: step 255400, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 03:06:17.310697: step 255420, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 03:07:23.606967: step 255440, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 03:08:31.380066: step 255460, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 03:09:37.715432: step 255480, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 03:10:44.231568: step 255500, loss = 0.0014, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-19 03:11:50.653474: step 255520, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 03:12:57.216801: step 255540, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 03:14:03.727234: step 255560, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-19 03:15:10.978948: step 255580, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-19 03:16:18.066421: step 255600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 03:17:25.274937: step 255620, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 03:18:31.630377: step 255640, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 03:19:38.219416: step 255660, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-19 03:20:44.903520: step 255680, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-19 03:21:51.525664: step 255700, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-19 03:22:57.603236: step 255720, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 03:24:04.451884: step 255740, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 03:25:10.781763: step 255760, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 03:26:16.826182: step 255780, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 03:27:23.476087: step 255800, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 03:28:30.501948: step 255820, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 03:29:36.972816: step 255840, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 03:30:44.020001: step 255860, loss = 0.0027, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-19 03:31:50.775276: step 255880, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-19 03:32:57.610744: step 255900, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-19 03:34:04.916522: step 255920, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 03:35:11.941041: step 255940, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-19 03:36:18.275780: step 255960, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 03:37:24.472576: step 255980, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 03:38:30.976521: step 256000, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
[Eval] 2017-05-19 03:38:45.562018: step 256000, acc = 0.9398, f1 = 0.9379
[Test] 2017-05-19 03:38:56.198890: step 256000, acc = 0.9271, f1 = 0.9265
[Status] 2017-05-19 03:38:56.198984: step 256000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 03:40:02.822997: step 256020, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-19 03:41:10.464417: step 256040, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-19 03:42:16.724801: step 256060, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 03:43:24.476748: step 256080, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-19 03:44:31.699662: step 256100, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 03:45:38.204574: step 256120, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 03:46:45.611671: step 256140, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-19 03:47:51.846762: step 256160, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 03:48:58.902042: step 256180, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 03:50:06.008203: step 256200, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 03:51:12.595891: step 256220, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 03:52:19.853173: step 256240, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-19 03:53:28.065328: step 256260, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-19 03:54:35.400094: step 256280, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-19 03:55:42.867019: step 256300, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 03:56:49.430328: step 256320, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 03:57:56.551538: step 256340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 03:59:03.060470: step 256360, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 04:00:09.752264: step 256380, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-19 04:01:16.776488: step 256400, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-19 04:02:24.094456: step 256420, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-19 04:03:30.944228: step 256440, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 04:04:37.854973: step 256460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 04:05:44.866685: step 256480, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-19 04:06:52.243596: step 256500, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-19 04:07:59.118347: step 256520, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 04:09:06.574599: step 256540, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-19 04:10:13.349631: step 256560, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 04:11:20.995335: step 256580, loss = 0.0026, acc = 0.9980 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 04:12:27.577292: step 256600, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 04:13:34.372510: step 256620, loss = 0.0022, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-19 04:14:41.535159: step 256640, loss = 0.0009, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-19 04:15:48.574589: step 256660, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 04:16:55.521661: step 256680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 04:18:03.271590: step 256700, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 04:19:09.693222: step 256720, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-19 04:20:16.913383: step 256740, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 04:21:24.500376: step 256760, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 04:22:30.557382: step 256780, loss = 0.0024, acc = 0.9980 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 04:23:38.211890: step 256800, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 04:24:44.886114: step 256820, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-19 04:25:51.850521: step 256840, loss = 0.0023, acc = 0.9980 (18.2 examples/sec; 3.509 sec/batch)
2017-05-19 04:26:58.075983: step 256860, loss = 0.0078, acc = 0.9980 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 04:28:06.562728: step 256880, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-19 04:29:14.101037: step 256900, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 04:30:20.492446: step 256920, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 04:31:27.299882: step 256940, loss = 0.0066, acc = 0.9980 (18.8 examples/sec; 3.397 sec/batch)
2017-05-19 04:32:34.324544: step 256960, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 04:33:41.186124: step 256980, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 04:34:48.012275: step 257000, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
[Eval] 2017-05-19 04:35:03.426113: step 257000, acc = 0.9403, f1 = 0.9384
[Test] 2017-05-19 04:35:14.075581: step 257000, acc = 0.9277, f1 = 0.9271
[Status] 2017-05-19 04:35:14.075682: step 257000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 04:36:21.204227: step 257020, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-19 04:37:28.891942: step 257040, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 04:38:35.376859: step 257060, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 04:39:42.009016: step 257080, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 04:40:48.640319: step 257100, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 04:41:55.143367: step 257120, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 04:43:01.875715: step 257140, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 04:44:08.397014: step 257160, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-19 04:45:15.248070: step 257180, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-19 04:46:22.362203: step 257200, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-19 04:47:28.718041: step 257220, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 04:48:34.786476: step 257240, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 04:49:41.682909: step 257260, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-19 04:50:47.698871: step 257280, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 04:51:54.611182: step 257300, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 04:53:00.586911: step 257320, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 04:54:07.305970: step 257340, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 04:55:13.348798: step 257360, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 04:56:21.343432: step 257380, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-19 04:57:27.356375: step 257400, loss = 0.0024, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 04:58:34.186292: step 257420, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 04:59:40.223486: step 257440, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 05:00:47.745216: step 257460, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-19 05:01:56.168701: step 257480, loss = 0.0024, acc = 0.9980 (19.0 examples/sec; 3.375 sec/batch)
2017-05-19 05:03:02.739313: step 257500, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 05:04:09.203470: step 257520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 05:05:15.214465: step 257540, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-19 05:06:21.456637: step 257560, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 05:07:28.196018: step 257580, loss = 0.0010, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-19 05:08:35.018187: step 257600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 05:09:42.541554: step 257620, loss = 0.0011, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-19 05:10:48.870593: step 257640, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 05:11:55.454360: step 257660, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-19 05:13:03.032669: step 257680, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-19 05:14:09.827962: step 257700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 05:15:16.110455: step 257720, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 05:16:22.201747: step 257740, loss = 0.0020, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 05:17:29.189656: step 257760, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 05:18:35.416045: step 257780, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-19 05:19:41.785605: step 257800, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-19 05:20:48.511644: step 257820, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 05:21:55.665880: step 257840, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.413 sec/batch)
2017-05-19 05:23:01.893780: step 257860, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 05:24:09.071073: step 257880, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-19 05:25:15.836013: step 257900, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 05:26:23.009664: step 257920, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 05:27:28.949302: step 257940, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 05:28:35.684952: step 257960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 05:29:41.546941: step 257980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 05:30:47.580175: step 258000, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
[Eval] 2017-05-19 05:31:01.789005: step 258000, acc = 0.9407, f1 = 0.9388
[Test] 2017-05-19 05:31:11.979694: step 258000, acc = 0.9274, f1 = 0.9268
[Status] 2017-05-19 05:31:11.979791: step 258000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 05:32:18.344999: step 258020, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 05:33:24.678389: step 258040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 05:34:31.883533: step 258060, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 05:35:38.200953: step 258080, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 05:36:46.529942: step 258100, loss = 0.0010, acc = 1.0000 (17.9 examples/sec; 3.575 sec/batch)
2017-05-19 05:37:53.529593: step 258120, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-19 05:38:59.766931: step 258140, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 05:40:06.276937: step 258160, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 05:41:13.238817: step 258180, loss = 0.0022, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-19 05:42:19.902458: step 258200, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 05:43:26.423636: step 258220, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 05:44:33.655069: step 258240, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-19 05:45:40.980815: step 258260, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.526 sec/batch)
2017-05-19 05:46:48.719822: step 258280, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-19 05:47:55.403886: step 258300, loss = 0.0052, acc = 0.9980 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 05:49:01.731239: step 258320, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-19 05:50:08.255596: step 258340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 05:51:14.410063: step 258360, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 05:52:21.166227: step 258380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 05:53:27.935636: step 258400, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 05:54:35.347212: step 258420, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 05:55:41.546079: step 258440, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-19 05:56:48.738239: step 258460, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.514 sec/batch)
2017-05-19 05:57:55.368177: step 258480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 05:59:01.460686: step 258500, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 06:00:08.122434: step 258520, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-19 06:01:15.666757: step 258540, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 06:02:21.826817: step 258560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 06:03:28.702412: step 258580, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 06:04:35.721148: step 258600, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 06:05:43.448021: step 258620, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-19 06:06:49.443375: step 258640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 06:07:56.430281: step 258660, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-19 06:09:02.774066: step 258680, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 06:10:10.138162: step 258700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 06:11:18.854655: step 258720, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-19 06:12:25.822954: step 258740, loss = 0.0012, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 06:13:32.347736: step 258760, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-19 06:14:39.995498: step 258780, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-19 06:15:46.863415: step 258800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 06:16:54.048837: step 258820, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-19 06:18:00.336808: step 258840, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 06:19:07.158160: step 258860, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-19 06:20:14.671663: step 258880, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 06:21:20.774683: step 258900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 06:22:27.595575: step 258920, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 06:23:35.729099: step 258940, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-19 06:24:42.820330: step 258960, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-19 06:25:49.168048: step 258980, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 06:26:56.641529: step 259000, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
[Eval] 2017-05-19 06:27:11.896750: step 259000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-19 06:27:22.248399: step 259000, acc = 0.9278, f1 = 0.9272
[Status] 2017-05-19 06:27:22.248466: step 259000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 06:28:28.552220: step 259020, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-19 06:29:34.873023: step 259040, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 06:30:42.667422: step 259060, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-19 06:31:49.768530: step 259080, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-19 06:32:56.164854: step 259100, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 06:34:02.383469: step 259120, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 06:35:08.908978: step 259140, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-19 06:36:14.991074: step 259160, loss = 0.0033, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 06:37:21.529213: step 259180, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-19 06:38:30.116109: step 259200, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 06:39:36.439513: step 259220, loss = 0.0023, acc = 0.9980 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 06:40:42.857448: step 259240, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 06:41:49.754994: step 259260, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-19 06:42:56.135475: step 259280, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 06:44:02.316149: step 259300, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 06:45:08.852759: step 259320, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-19 06:46:15.759108: step 259340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-19 06:47:23.138604: step 259360, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-19 06:48:30.301294: step 259380, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-19 06:49:37.202104: step 259400, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-19 06:50:43.526349: step 259420, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 06:51:50.441996: step 259440, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-19 06:52:56.762353: step 259460, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-19 06:54:03.541731: step 259480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 06:55:10.172725: step 259500, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 06:56:16.995451: step 259520, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 06:57:23.977481: step 259540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 06:58:30.716362: step 259560, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-19 06:59:37.704209: step 259580, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 07:00:44.229936: step 259600, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 07:01:50.444321: step 259620, loss = 0.0026, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 07:02:57.236180: step 259640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 07:04:04.013448: step 259660, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-19 07:05:11.818446: step 259680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 07:06:18.538172: step 259700, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 07:07:26.534233: step 259720, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-19 07:08:33.514401: step 259740, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-19 07:09:40.140548: step 259760, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 07:10:46.803394: step 259780, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 07:11:53.518223: step 259800, loss = 0.0055, acc = 0.9980 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 07:13:00.137291: step 259820, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-19 07:14:08.322701: step 259840, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 07:15:15.138250: step 259860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 07:16:22.119862: step 259880, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 07:17:28.633153: step 259900, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 07:18:35.266510: step 259920, loss = 0.0024, acc = 0.9980 (18.8 examples/sec; 3.408 sec/batch)
2017-05-19 07:19:42.111969: step 259940, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 07:20:48.097201: step 259960, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 07:21:55.327077: step 259980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 07:23:02.497853: step 260000, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
[Eval] 2017-05-19 07:23:17.999991: step 260000, acc = 0.9411, f1 = 0.9392
[Test] 2017-05-19 07:23:28.679641: step 260000, acc = 0.9275, f1 = 0.9269
[Status] 2017-05-19 07:23:28.679730: step 260000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 07:24:35.371393: step 260020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 07:25:42.962310: step 260040, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 07:26:51.039715: step 260060, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-19 07:27:57.723384: step 260080, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 07:29:04.995914: step 260100, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-19 07:30:12.028548: step 260120, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-19 07:31:19.122184: step 260140, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 07:32:26.413953: step 260160, loss = 0.0016, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 07:33:33.670775: step 260180, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 07:34:40.943706: step 260200, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-19 07:35:47.721012: step 260220, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 07:36:53.715194: step 260240, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 07:38:00.351898: step 260260, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-19 07:39:06.498083: step 260280, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 07:40:13.972097: step 260300, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-19 07:41:19.981969: step 260320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 07:42:27.258837: step 260340, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-19 07:43:33.772462: step 260360, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-19 07:44:40.985525: step 260380, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 07:45:47.276629: step 260400, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-19 07:46:53.865452: step 260420, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-19 07:48:00.427407: step 260440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 07:49:07.700145: step 260460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 07:50:13.626760: step 260480, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 07:51:20.325252: step 260500, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-19 07:52:27.580514: step 260520, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-19 07:53:34.840592: step 260540, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 07:54:41.519172: step 260560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 07:55:49.003126: step 260580, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 07:56:56.865151: step 260600, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 07:58:03.332938: step 260620, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-19 07:59:09.730379: step 260640, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-19 08:00:16.882670: step 260660, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-19 08:01:23.890872: step 260680, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 08:02:30.119538: step 260700, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 08:03:37.495522: step 260720, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-19 08:04:43.545720: step 260740, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-19 08:05:49.840516: step 260760, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 08:06:55.835432: step 260780, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 08:08:02.614048: step 260800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 08:09:08.382866: step 260820, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 08:10:15.057390: step 260840, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-19 08:11:21.904162: step 260860, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-19 08:12:29.004640: step 260880, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-19 08:13:36.429286: step 260900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 08:14:42.840615: step 260920, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 08:15:49.904265: step 260940, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-19 08:16:57.226123: step 260960, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 08:18:03.937052: step 260980, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-19 08:19:11.019246: step 261000, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
[Eval] 2017-05-19 08:19:26.267199: step 261000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-19 08:19:36.850873: step 261000, acc = 0.9276, f1 = 0.9270
[Status] 2017-05-19 08:19:36.850944: step 261000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 08:20:43.645121: step 261020, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-19 08:21:50.058922: step 261040, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 08:22:57.819125: step 261060, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-19 08:24:06.017286: step 261080, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 08:25:12.759486: step 261100, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-19 08:26:20.059012: step 261120, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-19 08:27:27.492451: step 261140, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-19 08:28:33.901021: step 261160, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 08:29:40.284708: step 261180, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 08:30:47.594822: step 261200, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-19 08:31:54.940836: step 261220, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-19 08:33:03.394696: step 261240, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 08:34:09.688611: step 261260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 08:35:17.098420: step 261280, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-19 08:36:23.855476: step 261300, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 08:37:29.798481: step 261320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 08:38:36.251152: step 261340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 08:39:43.210267: step 261360, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 08:40:49.607219: step 261380, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 08:41:57.543388: step 261400, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-19 08:43:03.823145: step 261420, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 08:44:11.504481: step 261440, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-19 08:45:18.081411: step 261460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 08:46:24.535456: step 261480, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 08:47:31.283571: step 261500, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-19 08:48:38.409376: step 261520, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-19 08:49:44.400676: step 261540, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 08:50:50.819302: step 261560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 08:51:58.081312: step 261580, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 08:53:04.430643: step 261600, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 08:54:12.217709: step 261620, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-19 08:55:18.671632: step 261640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-19 08:56:25.951790: step 261660, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 08:57:31.866435: step 261680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 08:58:38.841546: step 261700, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-19 08:59:45.583305: step 261720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 09:00:52.780818: step 261740, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 09:02:00.250635: step 261760, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-19 09:03:07.349526: step 261780, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 09:04:14.197356: step 261800, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 09:05:21.290333: step 261820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 09:06:29.518279: step 261840, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 09:07:36.984031: step 261860, loss = 0.0008, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-19 09:08:43.658158: step 261880, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 09:09:49.977726: step 261900, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-19 09:10:56.161819: step 261920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 09:12:03.516924: step 261940, loss = 0.0027, acc = 0.9980 (19.0 examples/sec; 3.365 sec/batch)
2017-05-19 09:13:10.341670: step 261960, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 09:14:17.826520: step 261980, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-19 09:15:24.119515: step 262000, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
[Eval] 2017-05-19 09:15:39.553352: step 262000, acc = 0.9414, f1 = 0.9396
[Test] 2017-05-19 09:15:49.590439: step 262000, acc = 0.9281, f1 = 0.9275
[Status] 2017-05-19 09:15:49.590531: step 262000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 09:16:57.303269: step 262020, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-19 09:18:03.571248: step 262040, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-19 09:19:10.835906: step 262060, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 09:20:19.969411: step 262080, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-19 09:21:26.584492: step 262100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 09:22:32.734750: step 262120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-19 09:23:40.139856: step 262140, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-19 09:24:46.332869: step 262160, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 09:25:53.115392: step 262180, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-19 09:26:59.367724: step 262200, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 09:28:05.747813: step 262220, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-19 09:29:12.523765: step 262240, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 09:30:20.936507: step 262260, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-19 09:31:27.659725: step 262280, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-19 09:32:35.084737: step 262300, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-19 09:33:41.331881: step 262320, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 09:34:49.411963: step 262340, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 09:35:56.534540: step 262360, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-19 09:37:03.393993: step 262380, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 09:38:10.356049: step 262400, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 09:39:17.570391: step 262420, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-19 09:40:24.564962: step 262440, loss = 0.0022, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-19 09:41:31.202794: step 262460, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-19 09:42:37.686088: step 262480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 09:43:43.888469: step 262500, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 09:44:50.436227: step 262520, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 09:45:57.624487: step 262540, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 09:47:05.376049: step 262560, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-19 09:48:11.334633: step 262580, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-19 09:49:17.462396: step 262600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 09:50:24.021893: step 262620, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 09:51:30.345833: step 262640, loss = 0.0022, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-19 09:52:37.340639: step 262660, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-19 09:53:43.742330: step 262680, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 09:54:49.991210: step 262700, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 09:55:56.971481: step 262720, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 09:57:04.078166: step 262740, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-19 09:58:10.066698: step 262760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 09:59:17.209861: step 262780, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 10:00:25.519836: step 262800, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-19 10:01:33.268877: step 262820, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-19 10:02:39.842774: step 262840, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-19 10:03:47.026448: step 262860, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-19 10:04:54.363490: step 262880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 10:06:01.474129: step 262900, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 10:07:08.856945: step 262920, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-19 10:08:15.257918: step 262940, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-19 10:09:21.899322: step 262960, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 10:10:29.882967: step 262980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 10:11:37.125854: step 263000, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
[Eval] 2017-05-19 10:11:52.687405: step 263000, acc = 0.9415, f1 = 0.9396
[Test] 2017-05-19 10:12:03.182316: step 263000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-19 10:12:03.182394: step 263000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 10:13:11.795873: step 263020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 10:14:18.811519: step 263040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 10:15:25.364245: step 263060, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 10:16:31.630638: step 263080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 10:17:39.634327: step 263100, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 10:18:45.957721: step 263120, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 10:19:53.134480: step 263140, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-19 10:20:59.769910: step 263160, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 10:22:06.119640: step 263180, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 10:23:13.658362: step 263200, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-19 10:24:20.215082: step 263220, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 10:25:27.833400: step 263240, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.511 sec/batch)
2017-05-19 10:26:35.539048: step 263260, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 10:27:42.670141: step 263280, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-19 10:28:49.133989: step 263300, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 10:29:56.987855: step 263320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 10:31:04.783852: step 263340, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-19 10:32:11.809808: step 263360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 10:33:18.899886: step 263380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 10:34:25.409666: step 263400, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-19 10:35:32.340492: step 263420, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-19 10:36:38.357458: step 263440, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 10:37:44.800003: step 263460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 10:38:51.994957: step 263480, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 10:39:58.613973: step 263500, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 10:41:04.848640: step 263520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 10:42:12.577634: step 263540, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-19 10:43:18.545397: step 263560, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 10:44:24.664638: step 263580, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-19 10:45:30.948276: step 263600, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 10:46:38.091198: step 263620, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 10:47:45.397672: step 263640, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.488 sec/batch)
2017-05-19 10:48:51.413349: step 263660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 10:49:57.602437: step 263680, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-19 10:51:04.033542: step 263700, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-19 10:52:10.754517: step 263720, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 10:53:17.407298: step 263740, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-19 10:54:24.202594: step 263760, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 10:55:31.481755: step 263780, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-19 10:56:38.508171: step 263800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 10:57:45.763300: step 263820, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 10:58:52.927086: step 263840, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 10:59:59.974756: step 263860, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-19 11:01:07.183046: step 263880, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 11:02:15.559499: step 263900, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-19 11:03:22.425756: step 263920, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 11:04:29.285861: step 263940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 11:05:35.674547: step 263960, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 11:06:41.998751: step 263980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 11:07:48.750692: step 264000, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
[Eval] 2017-05-19 11:08:02.812164: step 264000, acc = 0.9419, f1 = 0.9400
[Test] 2017-05-19 11:08:13.005318: step 264000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-19 11:08:13.005434: step 264000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 11:09:20.174427: step 264020, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 11:10:28.120985: step 264040, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-19 11:11:34.166091: step 264060, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 11:12:40.231483: step 264080, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 11:13:47.335822: step 264100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 11:14:54.486741: step 264120, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 11:16:02.440035: step 264140, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 11:17:09.034809: step 264160, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-19 11:18:15.840342: step 264180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 11:19:22.932146: step 264200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 11:20:29.062988: step 264220, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-19 11:21:35.469763: step 264240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 11:22:43.768911: step 264260, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-19 11:23:50.990264: step 264280, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 11:24:58.856502: step 264300, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-19 11:26:06.477975: step 264320, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-19 11:27:12.777984: step 264340, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 11:28:19.498509: step 264360, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-19 11:29:26.097186: step 264380, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-19 11:30:32.528491: step 264400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 11:31:39.381766: step 264420, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-19 11:32:45.989186: step 264440, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 11:33:52.144212: step 264460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 11:35:00.066240: step 264480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 11:36:07.337693: step 264500, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 11:37:13.964822: step 264520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 11:38:20.360983: step 264540, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 11:39:27.289551: step 264560, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 11:40:33.557591: step 264580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 11:41:40.846599: step 264600, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-19 11:42:47.314016: step 264620, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 11:43:54.134231: step 264640, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-19 11:45:01.187061: step 264660, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-19 11:46:08.602044: step 264680, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 11:47:15.594693: step 264700, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 11:48:22.593759: step 264720, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 11:49:29.200753: step 264740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 11:50:35.649426: step 264760, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 11:51:42.744445: step 264780, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-19 11:52:48.993163: step 264800, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 11:53:55.861453: step 264820, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 11:55:03.294948: step 264840, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-19 11:56:09.642959: step 264860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 11:57:17.033164: step 264880, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 11:58:24.657584: step 264900, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-19 11:59:31.880247: step 264920, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 12:00:38.625000: step 264940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 12:01:45.631723: step 264960, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 12:02:51.868832: step 264980, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 12:03:58.180393: step 265000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
[Eval] 2017-05-19 12:04:13.368457: step 265000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-19 12:04:24.189115: step 265000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-19 12:04:24.189216: step 265000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 12:05:31.389097: step 265020, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-19 12:06:39.000589: step 265040, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 12:07:46.490995: step 265060, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 12:08:52.848714: step 265080, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 12:09:59.920251: step 265100, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.565 sec/batch)
2017-05-19 12:11:06.954260: step 265120, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-19 12:12:13.628263: step 265140, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-19 12:13:20.179971: step 265160, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 12:14:27.024594: step 265180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 12:15:34.078254: step 265200, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 12:16:40.740509: step 265220, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-19 12:17:47.050089: step 265240, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 12:18:54.806932: step 265260, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-19 12:20:01.750860: step 265280, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 12:21:08.838555: step 265300, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-19 12:22:15.711844: step 265320, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-19 12:23:22.247894: step 265340, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-19 12:24:29.245888: step 265360, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 12:25:36.280032: step 265380, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-19 12:26:43.125153: step 265400, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 12:27:49.653957: step 265420, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-19 12:28:55.683198: step 265440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 12:30:02.058818: step 265460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 12:31:09.105169: step 265480, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 12:32:15.942838: step 265500, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 12:33:22.502228: step 265520, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-19 12:34:29.403351: step 265540, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 12:35:37.131370: step 265560, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-19 12:36:43.755151: step 265580, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 12:37:50.979069: step 265600, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-19 12:38:58.324077: step 265620, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 12:40:05.806391: step 265640, loss = 0.0012, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-19 12:41:13.199249: step 265660, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-19 12:42:20.705457: step 265680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 12:43:27.714709: step 265700, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 12:44:34.252628: step 265720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 12:45:40.940835: step 265740, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-19 12:46:47.081769: step 265760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 12:47:53.802062: step 265780, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-19 12:49:00.977455: step 265800, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 12:50:07.995390: step 265820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 12:51:14.203253: step 265840, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 12:52:21.152163: step 265860, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-19 12:53:27.951501: step 265880, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 12:54:34.494096: step 265900, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-19 12:55:42.190772: step 265920, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 12:56:49.198312: step 265940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 12:57:55.890288: step 265960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 12:59:02.283355: step 265980, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 13:00:09.249011: step 266000, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
[Eval] 2017-05-19 13:00:24.656603: step 266000, acc = 0.9420, f1 = 0.9401
[Test] 2017-05-19 13:00:34.609417: step 266000, acc = 0.9286, f1 = 0.9280
[Status] 2017-05-19 13:00:34.609485: step 266000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 13:01:41.165008: step 266020, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-19 13:02:49.420651: step 266040, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-19 13:03:56.274190: step 266060, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-19 13:05:04.162501: step 266080, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 13:06:11.364984: step 266100, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 13:07:18.470015: step 266120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 13:08:25.842656: step 266140, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-19 13:09:32.942060: step 266160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 13:10:39.274677: step 266180, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-19 13:11:46.859468: step 266200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 13:12:54.946820: step 266220, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-19 13:14:01.153376: step 266240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 13:15:08.759851: step 266260, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-19 13:16:15.316841: step 266280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 13:17:21.793708: step 266300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 13:18:28.532037: step 266320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 13:19:35.400686: step 266340, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 13:20:42.568664: step 266360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 13:21:50.467510: step 266380, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-19 13:22:57.964767: step 266400, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 13:24:04.376600: step 266420, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 13:25:10.444558: step 266440, loss = 0.0013, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-19 13:26:17.003584: step 266460, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-19 13:27:23.994417: step 266480, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-19 13:28:31.336606: step 266500, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-19 13:29:38.791074: step 266520, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 13:30:45.565113: step 266540, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-19 13:31:52.052292: step 266560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 13:32:58.977702: step 266580, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-19 13:34:06.527657: step 266600, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-19 13:35:13.933073: step 266620, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 13:36:20.418956: step 266640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 13:37:29.047414: step 266660, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 13:38:37.335109: step 266680, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-19 13:39:43.636693: step 266700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 13:40:50.191371: step 266720, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 13:41:57.798925: step 266740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 13:43:03.634171: step 266760, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-19 13:44:10.120158: step 266780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 13:45:17.075443: step 266800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-19 13:46:24.513206: step 266820, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 13:47:31.370013: step 266840, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.522 sec/batch)
2017-05-19 13:48:38.268863: step 266860, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-19 13:49:44.619947: step 266880, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-19 13:50:52.759467: step 266900, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-19 13:51:59.125693: step 266920, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 13:53:06.329291: step 266940, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 13:54:13.391774: step 266960, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 13:55:19.951171: step 266980, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 13:56:26.173478: step 267000, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-19 13:56:41.670818: step 267000, acc = 0.9420, f1 = 0.9401
[Test] 2017-05-19 13:56:51.617913: step 267000, acc = 0.9290, f1 = 0.9284
[Status] 2017-05-19 13:56:51.618014: step 267000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 13:57:57.751528: step 267020, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 13:59:04.184450: step 267040, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-19 14:00:10.568181: step 267060, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-19 14:01:17.083741: step 267080, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 14:02:23.309663: step 267100, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-19 14:03:31.241956: step 267120, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-19 14:04:38.480998: step 267140, loss = 0.0017, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-19 14:05:47.369200: step 267160, loss = 0.0008, acc = 1.0000 (17.6 examples/sec; 3.632 sec/batch)
2017-05-19 14:06:55.543650: step 267180, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-19 14:08:03.369381: step 267200, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-19 14:09:10.096579: step 267220, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-19 14:10:17.387116: step 267240, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-19 14:11:23.290647: step 267260, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-19 14:12:30.076832: step 267280, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 14:13:37.469640: step 267300, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-19 14:14:43.863159: step 267320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 14:15:51.751994: step 267340, loss = 0.0008, acc = 1.0000 (17.7 examples/sec; 3.608 sec/batch)
2017-05-19 14:16:59.648365: step 267360, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-19 14:18:05.804020: step 267380, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 14:19:13.222662: step 267400, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-19 14:20:19.833634: step 267420, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 14:21:27.019549: step 267440, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 14:22:33.831214: step 267460, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 14:23:41.079907: step 267480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 14:24:47.852121: step 267500, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-19 14:25:54.648134: step 267520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 14:27:01.030410: step 267540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 14:28:07.781816: step 267560, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 14:29:15.089686: step 267580, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 14:30:21.194079: step 267600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 14:31:28.470322: step 267620, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-19 14:32:35.252635: step 267640, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-19 14:33:43.613897: step 267660, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-19 14:34:51.174872: step 267680, loss = 0.0008, acc = 1.0000 (18.0 examples/sec; 3.552 sec/batch)
2017-05-19 14:35:57.471342: step 267700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 14:37:03.656446: step 267720, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 14:38:10.951776: step 267740, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-19 14:39:17.494828: step 267760, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 14:40:23.942597: step 267780, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 14:41:31.022430: step 267800, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-19 14:42:38.024861: step 267820, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 14:43:45.125375: step 267840, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-19 14:44:52.456508: step 267860, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-19 14:45:59.683077: step 267880, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 14:47:05.978118: step 267900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 14:48:12.722877: step 267920, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 14:49:18.905752: step 267940, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 14:50:26.751591: step 267960, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 14:51:32.818292: step 267980, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 14:52:38.963867: step 268000, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
[Eval] 2017-05-19 14:52:54.259507: step 268000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-19 14:53:04.819637: step 268000, acc = 0.9286, f1 = 0.9279
[Status] 2017-05-19 14:53:04.819735: step 268000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 14:54:11.820664: step 268020, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-19 14:55:18.829149: step 268040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 14:56:25.992337: step 268060, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-19 14:57:31.848826: step 268080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 14:58:37.843415: step 268100, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 14:59:44.794514: step 268120, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 15:00:52.646297: step 268140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 15:01:59.581948: step 268160, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 15:03:05.768455: step 268180, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-19 15:04:11.628471: step 268200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 15:05:17.749673: step 268220, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-19 15:06:25.345363: step 268240, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-19 15:07:31.563672: step 268260, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 15:08:38.087648: step 268280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 15:09:44.361483: step 268300, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 15:10:51.451216: step 268320, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.507 sec/batch)
2017-05-19 15:11:58.010357: step 268340, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 15:13:05.388493: step 268360, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-19 15:14:13.251124: step 268380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 15:15:19.945744: step 268400, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-19 15:16:26.233381: step 268420, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 15:17:33.038445: step 268440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 15:18:39.598275: step 268460, loss = 0.0024, acc = 0.9980 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 15:19:46.377724: step 268480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-19 15:20:52.292840: step 268500, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-19 15:21:59.401935: step 268520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 15:23:07.069711: step 268540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 15:24:14.674732: step 268560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 15:25:22.921096: step 268580, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.531 sec/batch)
2017-05-19 15:26:29.682379: step 268600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 15:27:36.815854: step 268620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 15:28:43.339636: step 268640, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-19 15:29:50.053698: step 268660, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 15:30:56.287557: step 268680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 15:32:04.095127: step 268700, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-19 15:33:11.306661: step 268720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 15:34:17.409866: step 268740, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-19 15:35:24.606734: step 268760, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-19 15:36:31.667146: step 268780, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-19 15:37:39.226343: step 268800, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-19 15:38:46.137344: step 268820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 15:39:52.896264: step 268840, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-19 15:40:59.656562: step 268860, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-19 15:42:06.787729: step 268880, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-19 15:43:13.239221: step 268900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 15:44:19.542671: step 268920, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 15:45:26.564558: step 268940, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 15:46:33.913984: step 268960, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 15:47:41.335560: step 268980, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 15:48:47.925852: step 269000, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
[Eval] 2017-05-19 15:49:03.373554: step 269000, acc = 0.9423, f1 = 0.9404
[Test] 2017-05-19 15:49:13.651698: step 269000, acc = 0.9289, f1 = 0.9284
[Status] 2017-05-19 15:49:13.651806: step 269000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 15:50:20.501994: step 269020, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 15:51:27.529856: step 269040, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-19 15:52:34.294920: step 269060, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-19 15:53:41.314050: step 269080, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 15:54:48.582466: step 269100, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 15:55:55.690675: step 269120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 15:57:04.608630: step 269140, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-19 15:58:12.224553: step 269160, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-19 15:59:19.049292: step 269180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 16:00:25.697838: step 269200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 16:01:33.165881: step 269220, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-19 16:02:39.755254: step 269240, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 16:03:46.392296: step 269260, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 16:04:52.401008: step 269280, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 16:05:59.019543: step 269300, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-19 16:07:06.152787: step 269320, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-19 16:08:13.189921: step 269340, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 16:09:20.921415: step 269360, loss = 0.0012, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-19 16:10:28.425215: step 269380, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-19 16:11:35.161975: step 269400, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-19 16:12:42.850183: step 269420, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-19 16:13:50.465760: step 269440, loss = 0.0007, acc = 1.0000 (17.6 examples/sec; 3.629 sec/batch)
2017-05-19 16:14:58.219351: step 269460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 16:16:04.649059: step 269480, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-19 16:17:12.360061: step 269500, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-19 16:18:19.477091: step 269520, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 16:19:26.931473: step 269540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 16:20:33.268673: step 269560, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 16:21:39.139143: step 269580, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 16:22:45.555361: step 269600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 16:23:51.858371: step 269620, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 16:24:59.388819: step 269640, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-19 16:26:06.150064: step 269660, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 16:27:13.436312: step 269680, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-19 16:28:20.888099: step 269700, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 16:29:28.419288: step 269720, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-19 16:30:35.922251: step 269740, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-19 16:31:42.550279: step 269760, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 16:32:50.766533: step 269780, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-19 16:33:57.471147: step 269800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 16:35:04.356456: step 269820, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 16:36:11.468765: step 269840, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 16:37:18.679648: step 269860, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-19 16:38:25.343144: step 269880, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-19 16:39:32.474839: step 269900, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 16:40:38.860456: step 269920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 16:41:45.699717: step 269940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 16:42:52.473238: step 269960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 16:43:59.829099: step 269980, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-19 16:45:07.173450: step 270000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
[Eval] 2017-05-19 16:45:22.616394: step 270000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-19 16:45:33.216851: step 270000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-19 16:45:33.216940: step 270000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 16:46:40.598110: step 270020, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 16:47:48.493961: step 270040, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-19 16:48:55.177791: step 270060, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 16:50:02.502795: step 270080, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-19 16:51:08.437033: step 270100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 16:52:14.684283: step 270120, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 16:53:22.112266: step 270140, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-19 16:54:28.488180: step 270160, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 16:55:35.206617: step 270180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 16:56:42.037499: step 270200, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 16:57:48.664172: step 270220, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-19 16:58:55.185531: step 270240, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 17:00:01.285494: step 270260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 17:01:07.952136: step 270280, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-19 17:02:15.632166: step 270300, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-19 17:03:22.600175: step 270320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 17:04:28.801045: step 270340, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 17:05:35.261890: step 270360, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 17:06:42.245152: step 270380, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-19 17:07:49.713003: step 270400, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-19 17:08:56.207782: step 270420, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-19 17:10:01.967099: step 270440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 17:11:09.072183: step 270460, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-19 17:12:17.455280: step 270480, loss = 0.0008, acc = 1.0000 (17.8 examples/sec; 3.595 sec/batch)
2017-05-19 17:13:23.469266: step 270500, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-19 17:14:29.587297: step 270520, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 17:15:37.600589: step 270540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 17:16:45.947658: step 270560, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-19 17:17:52.615296: step 270580, loss = 0.0031, acc = 0.9980 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 17:18:58.785212: step 270600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 17:20:07.361076: step 270620, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-19 17:21:13.809980: step 270640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 17:22:21.183449: step 270660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-19 17:23:28.425034: step 270680, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-19 17:24:36.405738: step 270700, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-19 17:25:42.383541: step 270720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 17:26:48.694593: step 270740, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 17:27:55.686561: step 270760, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 17:29:02.488262: step 270780, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 17:30:10.042222: step 270800, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-19 17:31:16.871561: step 270820, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 17:32:24.558173: step 270840, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 17:33:31.675318: step 270860, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-19 17:34:40.160376: step 270880, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 17:35:47.660128: step 270900, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-19 17:36:53.910779: step 270920, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 17:38:00.500800: step 270940, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 17:39:08.061362: step 270960, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-19 17:40:15.605153: step 270980, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-19 17:41:21.836702: step 271000, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
[Eval] 2017-05-19 17:41:37.412943: step 271000, acc = 0.9420, f1 = 0.9401
[Test] 2017-05-19 17:41:48.007507: step 271000, acc = 0.9290, f1 = 0.9284
[Status] 2017-05-19 17:41:48.007583: step 271000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 17:42:54.298681: step 271020, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-19 17:44:01.024714: step 271040, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 17:45:07.353533: step 271060, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-19 17:46:15.009193: step 271080, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-19 17:47:21.347449: step 271100, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-19 17:48:29.071468: step 271120, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-19 17:49:36.544169: step 271140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-19 17:50:44.320448: step 271160, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-19 17:51:51.112384: step 271180, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 17:52:58.834667: step 271200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 17:54:05.894719: step 271220, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 17:55:13.718281: step 271240, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.525 sec/batch)
2017-05-19 17:56:20.437181: step 271260, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 17:57:27.039939: step 271280, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-19 17:58:33.886107: step 271300, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-19 17:59:41.149673: step 271320, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-19 18:00:48.264308: step 271340, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-19 18:01:54.449691: step 271360, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 18:03:02.142052: step 271380, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.507 sec/batch)
2017-05-19 18:04:08.667185: step 271400, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-19 18:05:16.455250: step 271420, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-19 18:06:23.061720: step 271440, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-19 18:07:30.072331: step 271460, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-19 18:08:36.035050: step 271480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 18:09:43.042960: step 271500, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.507 sec/batch)
2017-05-19 18:10:49.606298: step 271520, loss = 0.0023, acc = 0.9980 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 18:11:55.655247: step 271540, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 18:13:02.634829: step 271560, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-19 18:14:09.090134: step 271580, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 18:15:15.992011: step 271600, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 18:16:22.385001: step 271620, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 18:17:29.015691: step 271640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 18:18:36.199184: step 271660, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 18:19:42.701939: step 271680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 18:20:50.297649: step 271700, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 18:21:56.273022: step 271720, loss = 0.0043, acc = 0.9980 (19.4 examples/sec; 3.297 sec/batch)
2017-05-19 18:23:03.281676: step 271740, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 18:24:10.410075: step 271760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 18:25:16.847880: step 271780, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 18:26:24.944139: step 271800, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.516 sec/batch)
2017-05-19 18:27:31.522017: step 271820, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 18:28:38.157972: step 271840, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 18:29:45.918687: step 271860, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-19 18:30:52.795780: step 271880, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-19 18:32:00.375257: step 271900, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-19 18:33:06.881028: step 271920, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 18:34:13.070221: step 271940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 18:35:20.852972: step 271960, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 18:36:27.942543: step 271980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 18:37:34.744513: step 272000, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
[Eval] 2017-05-19 18:37:50.287088: step 272000, acc = 0.9421, f1 = 0.9402
[Test] 2017-05-19 18:38:00.823177: step 272000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-19 18:38:00.823271: step 272000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 18:39:08.155531: step 272020, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-19 18:40:14.120285: step 272040, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 18:41:21.435463: step 272060, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-19 18:42:27.758409: step 272080, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 18:43:35.058440: step 272100, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-19 18:44:41.601524: step 272120, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 18:45:47.626373: step 272140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 18:46:54.765059: step 272160, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 18:48:02.202919: step 272180, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-19 18:49:09.244146: step 272200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 18:50:15.896689: step 272220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 18:51:22.420938: step 272240, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 18:52:30.309171: step 272260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 18:53:37.122190: step 272280, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 18:54:43.223161: step 272300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 18:55:49.530194: step 272320, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 18:56:56.588173: step 272340, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 18:58:02.712878: step 272360, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 18:59:09.474413: step 272380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 19:00:16.466582: step 272400, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-19 19:01:24.922716: step 272420, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-19 19:02:31.503392: step 272440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 19:03:39.883205: step 272460, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-19 19:04:46.654203: step 272480, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.415 sec/batch)
2017-05-19 19:05:54.053793: step 272500, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-19 19:07:01.677178: step 272520, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-19 19:08:08.850633: step 272540, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-19 19:09:15.810986: step 272560, loss = 0.0021, acc = 0.9980 (18.4 examples/sec; 3.481 sec/batch)
2017-05-19 19:10:22.555335: step 272580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 19:11:29.508939: step 272600, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-19 19:12:37.949506: step 272620, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-19 19:13:44.247912: step 272640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 19:14:51.008193: step 272660, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-19 19:15:57.634689: step 272680, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-19 19:17:04.530724: step 272700, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-19 19:18:11.305954: step 272720, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 19:19:17.741795: step 272740, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-19 19:20:24.485951: step 272760, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-19 19:21:32.348559: step 272780, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-19 19:22:39.292959: step 272800, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-19 19:23:47.138919: step 272820, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-19 19:24:53.249886: step 272840, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 19:26:00.071870: step 272860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 19:27:07.962610: step 272880, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 19:28:15.650900: step 272900, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 19:29:22.788449: step 272920, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-19 19:30:29.262199: step 272940, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-19 19:31:35.487392: step 272960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 19:32:42.429241: step 272980, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 19:33:49.655453: step 273000, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
[Eval] 2017-05-19 19:34:03.786292: step 273000, acc = 0.9419, f1 = 0.9400
[Test] 2017-05-19 19:34:14.430744: step 273000, acc = 0.9290, f1 = 0.9284
[Status] 2017-05-19 19:34:14.430855: step 273000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 19:35:20.687653: step 273020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-19 19:36:27.784610: step 273040, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 19:37:34.441825: step 273060, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-19 19:38:40.681025: step 273080, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-19 19:39:48.495267: step 273100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 19:40:55.567044: step 273120, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 19:42:02.348246: step 273140, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 19:43:08.785414: step 273160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-19 19:44:17.741830: step 273180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 19:45:24.447930: step 273200, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 19:46:30.844614: step 273220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 19:47:36.960024: step 273240, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 19:48:43.767372: step 273260, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-19 19:49:50.204003: step 273280, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 19:50:56.733662: step 273300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 19:52:03.882727: step 273320, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-19 19:53:10.985064: step 273340, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 19:54:17.840624: step 273360, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-19 19:55:24.778511: step 273380, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.575 sec/batch)
2017-05-19 19:56:31.875293: step 273400, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.551 sec/batch)
2017-05-19 19:57:40.716162: step 273420, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 19:58:47.547322: step 273440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 19:59:55.310185: step 273460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 20:01:01.582945: step 273480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 20:02:08.307727: step 273500, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-19 20:03:15.312963: step 273520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 20:04:21.377593: step 273540, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 20:05:27.865588: step 273560, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 20:06:34.445239: step 273580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 20:07:41.454861: step 273600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-19 20:08:49.098574: step 273620, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-19 20:09:55.696843: step 273640, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 20:11:01.810481: step 273660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 20:12:07.573522: step 273680, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 20:13:14.362216: step 273700, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 20:14:21.183748: step 273720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 20:15:27.339822: step 273740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 20:16:33.525343: step 273760, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 20:17:39.253627: step 273780, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 20:18:45.272870: step 273800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 20:19:52.050468: step 273820, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 20:20:59.704172: step 273840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-19 20:22:06.382253: step 273860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 20:23:13.945596: step 273880, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 20:24:22.216369: step 273900, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-19 20:25:29.147202: step 273920, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 20:26:36.863727: step 273940, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-19 20:27:43.418860: step 273960, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-19 20:28:50.677887: step 273980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 20:29:57.169464: step 274000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
[Eval] 2017-05-19 20:30:12.714415: step 274000, acc = 0.9424, f1 = 0.9405
[Test] 2017-05-19 20:30:23.236374: step 274000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-19 20:30:23.236461: step 274000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 20:31:30.716525: step 274020, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-19 20:32:37.376579: step 274040, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 20:33:44.902647: step 274060, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-19 20:34:51.176933: step 274080, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 20:35:59.087356: step 274100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-19 20:37:05.540907: step 274120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 20:38:12.578686: step 274140, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-19 20:39:19.086853: step 274160, loss = 0.0033, acc = 0.9980 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 20:40:26.813840: step 274180, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-19 20:41:33.075747: step 274200, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-19 20:42:40.081885: step 274220, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 20:43:46.626489: step 274240, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-19 20:44:52.657309: step 274260, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-19 20:45:59.691084: step 274280, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-19 20:47:06.516791: step 274300, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-19 20:48:13.937033: step 274320, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 20:49:20.589981: step 274340, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-19 20:50:28.190871: step 274360, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-19 20:51:35.297135: step 274380, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-19 20:52:42.551380: step 274400, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-19 20:53:48.973747: step 274420, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 20:54:55.995034: step 274440, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 20:56:02.394656: step 274460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 20:57:09.001923: step 274480, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 20:58:16.148410: step 274500, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-19 20:59:23.042119: step 274520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-19 21:00:31.334674: step 274540, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-19 21:01:37.603576: step 274560, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 21:02:43.687001: step 274580, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 21:03:51.139044: step 274600, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-19 21:04:58.932195: step 274620, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-19 21:06:06.449680: step 274640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 21:07:13.771952: step 274660, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 21:08:20.437699: step 274680, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-19 21:09:27.988902: step 274700, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 21:10:35.055050: step 274720, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-19 21:11:41.678702: step 274740, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-19 21:12:48.458943: step 274760, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 21:13:55.745500: step 274780, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-19 21:15:02.464154: step 274800, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-19 21:16:09.049200: step 274820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 21:17:16.100224: step 274840, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 21:18:22.345297: step 274860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-19 21:19:29.097048: step 274880, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-19 21:20:36.808933: step 274900, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-19 21:21:43.285220: step 274920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 21:22:49.772850: step 274940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 21:23:56.172802: step 274960, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-19 21:25:04.065239: step 274980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 21:26:11.290324: step 275000, loss = 0.0011, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
[Eval] 2017-05-19 21:26:26.681632: step 275000, acc = 0.9421, f1 = 0.9403
[Test] 2017-05-19 21:26:37.282071: step 275000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-19 21:26:37.282165: step 275000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 21:27:43.323358: step 275020, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-19 21:28:50.798488: step 275040, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 21:29:57.040912: step 275060, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 21:31:05.264114: step 275080, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 21:32:11.863041: step 275100, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 21:33:17.933900: step 275120, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-19 21:34:24.542941: step 275140, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-19 21:35:31.501993: step 275160, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 21:36:39.884104: step 275180, loss = 0.0007, acc = 1.0000 (17.8 examples/sec; 3.603 sec/batch)
2017-05-19 21:37:46.793180: step 275200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-19 21:38:53.415342: step 275220, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-19 21:40:01.383491: step 275240, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-19 21:41:08.027041: step 275260, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-19 21:42:13.832751: step 275280, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 21:43:21.692959: step 275300, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-19 21:44:29.256680: step 275320, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-19 21:45:35.281786: step 275340, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 21:46:41.439706: step 275360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 21:47:48.554053: step 275380, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-19 21:48:54.635616: step 275400, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 21:50:00.850018: step 275420, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 21:51:08.604238: step 275440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 21:52:15.394629: step 275460, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-19 21:53:21.737230: step 275480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 21:54:29.043278: step 275500, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-19 21:55:35.265801: step 275520, loss = 0.0021, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 21:56:42.747677: step 275540, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-19 21:57:50.518578: step 275560, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-19 21:58:57.858334: step 275580, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-19 22:00:04.896641: step 275600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 22:01:11.496914: step 275620, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 22:02:18.701045: step 275640, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-19 22:03:26.053145: step 275660, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-19 22:04:33.547647: step 275680, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-19 22:05:41.273685: step 275700, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-19 22:06:48.034221: step 275720, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 22:07:55.959136: step 275740, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-19 22:09:03.106185: step 275760, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-19 22:10:09.870109: step 275780, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-19 22:11:16.171608: step 275800, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 22:12:23.930345: step 275820, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-19 22:13:30.836324: step 275840, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-19 22:14:37.035923: step 275860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-19 22:15:43.467284: step 275880, loss = 0.0030, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-19 22:16:50.744307: step 275900, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 22:17:57.303072: step 275920, loss = 0.0023, acc = 0.9980 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 22:19:03.529608: step 275940, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 22:20:09.850963: step 275960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-19 22:21:17.130878: step 275980, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-19 22:22:24.666101: step 276000, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
[Eval] 2017-05-19 22:22:38.837067: step 276000, acc = 0.9420, f1 = 0.9400
[Test] 2017-05-19 22:22:48.642903: step 276000, acc = 0.9293, f1 = 0.9287
[Status] 2017-05-19 22:22:48.642962: step 276000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 22:23:55.177075: step 276020, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 22:25:01.528110: step 276040, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-19 22:26:07.763718: step 276060, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-19 22:27:14.003344: step 276080, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 22:28:20.448817: step 276100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-19 22:29:27.184619: step 276120, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-19 22:30:34.514767: step 276140, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-19 22:31:41.051903: step 276160, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 22:32:47.238191: step 276180, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-19 22:33:55.260362: step 276200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 22:35:01.692135: step 276220, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-19 22:36:07.991266: step 276240, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-19 22:37:16.065987: step 276260, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
2017-05-19 22:38:22.928203: step 276280, loss = 0.0027, acc = 0.9980 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 22:39:30.249037: step 276300, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-19 22:40:37.501284: step 276320, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-19 22:41:44.122907: step 276340, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-19 22:42:51.730007: step 276360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 22:43:58.280627: step 276380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 22:45:04.616245: step 276400, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-19 22:46:11.108100: step 276420, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-19 22:47:18.842920: step 276440, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-19 22:48:26.047112: step 276460, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 22:49:32.410473: step 276480, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-19 22:50:39.336912: step 276500, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-19 22:51:45.682879: step 276520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 22:52:52.236191: step 276540, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-19 22:53:59.853377: step 276560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-19 22:55:05.998696: step 276580, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 22:56:12.567951: step 276600, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-19 22:57:19.168341: step 276620, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-19 22:58:25.323599: step 276640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 22:59:32.441160: step 276660, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-19 23:00:38.655914: step 276680, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-19 23:01:45.637601: step 276700, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-19 23:02:53.254295: step 276720, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-19 23:03:59.844189: step 276740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-19 23:05:07.584287: step 276760, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-19 23:06:14.755975: step 276780, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-19 23:07:21.250701: step 276800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-19 23:08:27.808698: step 276820, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-19 23:09:34.541072: step 276840, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 23:10:42.327977: step 276860, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-19 23:11:48.760658: step 276880, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-19 23:12:54.990392: step 276900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-19 23:14:02.838850: step 276920, loss = 0.0027, acc = 0.9980 (19.2 examples/sec; 3.331 sec/batch)
2017-05-19 23:15:09.984964: step 276940, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-19 23:16:16.740524: step 276960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-19 23:17:23.470384: step 276980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-19 23:18:29.668507: step 277000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
[Eval] 2017-05-19 23:18:45.093612: step 277000, acc = 0.9425, f1 = 0.9406
[Test] 2017-05-19 23:18:55.714391: step 277000, acc = 0.9291, f1 = 0.9285
[Status] 2017-05-19 23:18:55.714490: step 277000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-19 23:20:02.871228: step 277020, loss = 0.0039, acc = 0.9980 (18.3 examples/sec; 3.506 sec/batch)
2017-05-19 23:21:08.971273: step 277040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-19 23:22:16.185656: step 277060, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 23:23:22.556770: step 277080, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-19 23:24:29.728628: step 277100, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-19 23:25:36.955083: step 277120, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-19 23:26:43.968063: step 277140, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-19 23:27:50.465103: step 277160, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-19 23:28:57.218350: step 277180, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-19 23:30:04.978907: step 277200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-19 23:31:11.677830: step 277220, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-19 23:32:18.736844: step 277240, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 23:33:26.248467: step 277260, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-19 23:34:32.487299: step 277280, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-19 23:35:39.259830: step 277300, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-19 23:36:45.579149: step 277320, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-19 23:37:52.787617: step 277340, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-19 23:39:00.143193: step 277360, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-19 23:40:07.268775: step 277380, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 23:41:13.555298: step 277400, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-19 23:42:19.910684: step 277420, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-19 23:43:26.867978: step 277440, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-19 23:44:33.672113: step 277460, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-19 23:45:40.746379: step 277480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-19 23:46:47.947699: step 277500, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-19 23:47:54.767573: step 277520, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-19 23:49:01.429489: step 277540, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-19 23:50:08.526387: step 277560, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-19 23:51:15.433234: step 277580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-19 23:52:22.426757: step 277600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-19 23:53:29.618818: step 277620, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-19 23:54:37.741011: step 277640, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-19 23:55:45.036531: step 277660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-19 23:56:52.336621: step 277680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-19 23:57:58.885799: step 277700, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-19 23:59:04.961549: step 277720, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 00:00:12.293792: step 277740, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-20 00:01:20.471242: step 277760, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-20 00:02:27.357888: step 277780, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-20 00:03:33.997303: step 277800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 00:04:40.284093: step 277820, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 00:05:46.245651: step 277840, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 00:06:52.720147: step 277860, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-20 00:07:58.867598: step 277880, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 00:09:05.990017: step 277900, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 00:10:12.975935: step 277920, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 00:11:21.965920: step 277940, loss = 0.0008, acc = 1.0000 (17.7 examples/sec; 3.608 sec/batch)
2017-05-20 00:12:29.313858: step 277960, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-20 00:13:36.131978: step 277980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 00:14:42.910057: step 278000, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
[Eval] 2017-05-20 00:14:58.188093: step 278000, acc = 0.9428, f1 = 0.9410
[Test] 2017-05-20 00:15:08.875778: step 278000, acc = 0.9293, f1 = 0.9287
[Status] 2017-05-20 00:15:08.875890: step 278000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 00:16:16.337359: step 278020, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 00:17:23.002320: step 278040, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-20 00:18:30.830619: step 278060, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 00:19:37.512477: step 278080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 00:20:44.169621: step 278100, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 00:21:51.387582: step 278120, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 00:22:58.926651: step 278140, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.499 sec/batch)
2017-05-20 00:24:05.344661: step 278160, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 00:25:11.891995: step 278180, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 00:26:19.333294: step 278200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 00:27:27.276053: step 278220, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-20 00:28:33.849800: step 278240, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-20 00:29:40.200291: step 278260, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-20 00:30:46.993917: step 278280, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 00:31:53.635282: step 278300, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 00:32:59.848995: step 278320, loss = 0.0022, acc = 0.9980 (19.6 examples/sec; 3.273 sec/batch)
2017-05-20 00:34:07.001915: step 278340, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 00:35:13.716388: step 278360, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 00:36:21.151289: step 278380, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-20 00:37:28.341756: step 278400, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 00:38:34.797463: step 278420, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 00:39:41.857776: step 278440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 00:40:48.117785: step 278460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 00:41:54.394271: step 278480, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-20 00:43:01.615157: step 278500, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-20 00:44:07.735758: step 278520, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 00:45:15.649072: step 278540, loss = 0.0021, acc = 0.9980 (19.0 examples/sec; 3.375 sec/batch)
2017-05-20 00:46:23.104306: step 278560, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-20 00:47:30.222387: step 278580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 00:48:37.465130: step 278600, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-20 00:49:44.983078: step 278620, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.528 sec/batch)
2017-05-20 00:50:52.267370: step 278640, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-20 00:51:59.145986: step 278660, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-20 00:53:06.279211: step 278680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 00:54:12.317731: step 278700, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-20 00:55:18.794719: step 278720, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 00:56:25.399283: step 278740, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-20 00:57:32.065573: step 278760, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-20 00:58:38.652506: step 278780, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-20 00:59:46.135750: step 278800, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-20 01:00:52.616046: step 278820, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 01:01:59.418599: step 278840, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-20 01:03:06.408787: step 278860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 01:04:13.326828: step 278880, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 01:05:21.613789: step 278900, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-20 01:06:28.181099: step 278920, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-20 01:07:35.857273: step 278940, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
2017-05-20 01:08:42.613994: step 278960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 01:09:49.522211: step 278980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 01:10:56.337497: step 279000, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
[Eval] 2017-05-20 01:11:11.778118: step 279000, acc = 0.9420, f1 = 0.9400
[Test] 2017-05-20 01:11:22.601154: step 279000, acc = 0.9289, f1 = 0.9283
[Status] 2017-05-20 01:11:22.601236: step 279000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 01:12:29.160393: step 279020, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 01:13:37.047021: step 279040, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 01:14:43.975539: step 279060, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-20 01:15:50.225714: step 279080, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 01:16:57.682460: step 279100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 01:18:05.157836: step 279120, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 01:19:12.676053: step 279140, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-20 01:20:19.209603: step 279160, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 01:21:26.598695: step 279180, loss = 0.0007, acc = 1.0000 (17.8 examples/sec; 3.605 sec/batch)
2017-05-20 01:22:32.807713: step 279200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 01:23:40.195791: step 279220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 01:24:46.705871: step 279240, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 01:25:53.049429: step 279260, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 01:26:59.962345: step 279280, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-20 01:28:07.769277: step 279300, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-20 01:29:14.310160: step 279320, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-20 01:30:21.682353: step 279340, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 01:31:28.146258: step 279360, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-20 01:32:35.903893: step 279380, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 01:33:42.554663: step 279400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 01:34:48.523142: step 279420, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 01:35:56.076574: step 279440, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-20 01:37:02.883082: step 279460, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 01:38:11.322096: step 279480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 01:39:18.270941: step 279500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 01:40:24.827935: step 279520, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-20 01:41:31.976239: step 279540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 01:42:38.764253: step 279560, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 01:43:45.509156: step 279580, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-20 01:44:52.973278: step 279600, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 01:45:59.623275: step 279620, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 01:47:06.174161: step 279640, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 01:48:13.583121: step 279660, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 01:49:19.793012: step 279680, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 01:50:26.964851: step 279700, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 01:51:34.301188: step 279720, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 01:52:41.451770: step 279740, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-20 01:53:48.004159: step 279760, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-20 01:54:54.393473: step 279780, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 01:56:01.572714: step 279800, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-20 01:57:07.582824: step 279820, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-20 01:58:14.879741: step 279840, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-20 01:59:21.348580: step 279860, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 02:00:28.124792: step 279880, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 02:01:34.305428: step 279900, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 02:02:41.234069: step 279920, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-20 02:03:47.715166: step 279940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 02:04:54.036842: step 279960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 02:06:00.894209: step 279980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 02:07:07.135196: step 280000, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
[Eval] 2017-05-20 02:07:21.507219: step 280000, acc = 0.9425, f1 = 0.9406
[Test] 2017-05-20 02:07:31.187564: step 280000, acc = 0.9290, f1 = 0.9284
[Status] 2017-05-20 02:07:31.187644: step 280000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 02:08:39.185782: step 280020, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-20 02:09:45.427796: step 280040, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-20 02:10:52.726852: step 280060, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 02:11:59.388324: step 280080, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 02:13:06.785834: step 280100, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-20 02:14:13.046463: step 280120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 02:15:19.160533: step 280140, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 02:16:26.265211: step 280160, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 02:17:32.535619: step 280180, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 02:18:39.388188: step 280200, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-20 02:19:47.749489: step 280220, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-20 02:20:55.131065: step 280240, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 02:22:02.176334: step 280260, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-20 02:23:09.112797: step 280280, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-20 02:24:16.093357: step 280300, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-20 02:25:22.308834: step 280320, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 02:26:28.449008: step 280340, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 02:27:34.983531: step 280360, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-20 02:28:42.152475: step 280380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 02:29:48.881765: step 280400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 02:30:55.323397: step 280420, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 02:32:02.290765: step 280440, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-20 02:33:10.202708: step 280460, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 02:34:17.320246: step 280480, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-20 02:35:25.498993: step 280500, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-20 02:36:31.941139: step 280520, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-20 02:37:38.770172: step 280540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 02:38:45.378943: step 280560, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-20 02:39:52.606980: step 280580, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-20 02:41:00.381594: step 280600, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.523 sec/batch)
2017-05-20 02:42:06.651435: step 280620, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 02:43:12.881014: step 280640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 02:44:18.806040: step 280660, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 02:45:25.238155: step 280680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 02:46:31.785136: step 280700, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-20 02:47:37.718654: step 280720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 02:48:44.443666: step 280740, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 02:49:50.888692: step 280760, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 02:50:58.817227: step 280780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 02:52:05.671563: step 280800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 02:53:11.631101: step 280820, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 02:54:18.609496: step 280840, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-20 02:55:25.161906: step 280860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 02:56:31.559344: step 280880, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-20 02:57:39.653057: step 280900, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-20 02:58:47.280532: step 280920, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.520 sec/batch)
2017-05-20 02:59:53.854420: step 280940, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-20 03:01:00.330968: step 280960, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 03:02:06.965290: step 280980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 03:03:14.525580: step 281000, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.518 sec/batch)
[Eval] 2017-05-20 03:03:30.108684: step 281000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-20 03:03:40.803646: step 281000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-20 03:03:40.803716: step 281000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 03:04:48.223692: step 281020, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 03:05:54.578755: step 281040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 03:07:02.223708: step 281060, loss = 0.0022, acc = 0.9980 (19.0 examples/sec; 3.371 sec/batch)
2017-05-20 03:08:09.036018: step 281080, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 03:09:16.535339: step 281100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 03:10:23.794252: step 281120, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 03:11:30.490882: step 281140, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 03:12:37.850283: step 281160, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-20 03:13:44.062235: step 281180, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 03:14:50.349191: step 281200, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 03:15:58.161706: step 281220, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 03:17:07.586159: step 281240, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-20 03:18:14.229029: step 281260, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 03:19:20.701220: step 281280, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-20 03:20:27.886456: step 281300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 03:21:34.600524: step 281320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 03:22:41.390414: step 281340, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-20 03:23:49.156837: step 281360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 03:24:56.594315: step 281380, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-20 03:26:03.524670: step 281400, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-20 03:27:11.567538: step 281420, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-20 03:28:17.968407: step 281440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 03:29:24.151135: step 281460, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 03:30:30.823473: step 281480, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 03:31:37.946119: step 281500, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 03:32:44.814728: step 281520, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-20 03:33:51.690074: step 281540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 03:34:58.980356: step 281560, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-20 03:36:05.075283: step 281580, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 03:37:11.744355: step 281600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 03:38:18.727982: step 281620, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 03:39:25.572460: step 281640, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-20 03:40:31.839428: step 281660, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 03:41:39.499133: step 281680, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-20 03:42:46.258212: step 281700, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 03:43:52.769016: step 281720, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-20 03:45:00.140950: step 281740, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 03:46:06.946061: step 281760, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-20 03:47:13.412814: step 281780, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 03:48:21.105447: step 281800, loss = 0.0018, acc = 1.0000 (17.8 examples/sec; 3.599 sec/batch)
2017-05-20 03:49:27.883155: step 281820, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 03:50:35.036324: step 281840, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-20 03:51:41.234142: step 281860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 03:52:48.871674: step 281880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 03:53:55.141144: step 281900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 03:55:04.114364: step 281920, loss = 0.0007, acc = 1.0000 (17.5 examples/sec; 3.652 sec/batch)
2017-05-20 03:56:10.862234: step 281940, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 03:57:17.981386: step 281960, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 03:58:24.450254: step 281980, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-20 03:59:31.790425: step 282000, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
[Eval] 2017-05-20 03:59:47.167953: step 282000, acc = 0.9330, f1 = 0.9312
[Test] 2017-05-20 03:59:57.811956: step 282000, acc = 0.9209, f1 = 0.9201
[Status] 2017-05-20 03:59:57.812058: step 282000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 04:01:05.843447: step 282020, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-20 04:02:13.254611: step 282040, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 04:03:20.608790: step 282060, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-20 04:04:27.673406: step 282080, loss = 0.0013, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-20 04:05:34.487652: step 282100, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-20 04:06:40.615300: step 282120, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 04:07:47.779015: step 282140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 04:08:55.869052: step 282160, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-20 04:10:02.626821: step 282180, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 04:11:09.157004: step 282200, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-20 04:12:16.177407: step 282220, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 04:13:23.731470: step 282240, loss = 0.0019, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 04:14:30.915942: step 282260, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 04:15:37.042046: step 282280, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 04:16:43.827871: step 282300, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-20 04:17:52.391392: step 282320, loss = 0.0021, acc = 0.9980 (18.7 examples/sec; 3.424 sec/batch)
2017-05-20 04:18:59.959250: step 282340, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-20 04:20:06.682183: step 282360, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-20 04:21:12.765628: step 282380, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 04:22:19.163979: step 282400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 04:23:25.332059: step 282420, loss = 0.0015, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 04:24:31.853226: step 282440, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-20 04:25:38.882732: step 282460, loss = 0.0007, acc = 1.0000 (19.7 examples/sec; 3.250 sec/batch)
2017-05-20 04:26:45.670649: step 282480, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-20 04:27:52.458670: step 282500, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 04:28:58.802361: step 282520, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 04:30:06.327469: step 282540, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 04:31:13.551638: step 282560, loss = 0.0024, acc = 0.9980 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 04:32:20.446945: step 282580, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 04:33:26.192552: step 282600, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 04:34:33.130035: step 282620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 04:35:40.025084: step 282640, loss = 0.0018, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-20 04:36:47.341436: step 282660, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-20 04:37:54.188576: step 282680, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-20 04:39:00.778413: step 282700, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-20 04:40:08.392919: step 282720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-20 04:41:15.596140: step 282740, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 04:42:22.112819: step 282760, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 04:43:28.493215: step 282780, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 04:44:34.437271: step 282800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-20 04:45:41.867295: step 282820, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.523 sec/batch)
2017-05-20 04:46:48.234189: step 282840, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 04:47:56.330202: step 282860, loss = 0.0012, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-20 04:49:02.858687: step 282880, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 04:50:09.841061: step 282900, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 04:51:16.369650: step 282920, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 04:52:22.846655: step 282940, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 04:53:30.316267: step 282960, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 04:54:37.082529: step 282980, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-20 04:55:44.585741: step 283000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
[Eval] 2017-05-20 04:56:00.071367: step 283000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-20 04:56:10.655431: step 283000, acc = 0.9279, f1 = 0.9273
[Status] 2017-05-20 04:56:10.655521: step 283000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 04:57:17.552020: step 283020, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-20 04:58:24.111905: step 283040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-20 04:59:31.250956: step 283060, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 05:00:38.339647: step 283080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 05:01:45.842500: step 283100, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-20 05:02:52.283367: step 283120, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-20 05:04:01.597731: step 283140, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-20 05:05:08.050650: step 283160, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 05:06:15.587470: step 283180, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.360 sec/batch)
2017-05-20 05:07:22.458949: step 283200, loss = 0.0051, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 05:08:31.499000: step 283220, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 05:09:38.044802: step 283240, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 05:10:46.103980: step 283260, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-20 05:11:52.468431: step 283280, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 05:12:59.583421: step 283300, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-20 05:14:06.428299: step 283320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 05:15:13.242412: step 283340, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 05:16:20.008446: step 283360, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-20 05:17:27.547015: step 283380, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-20 05:18:36.072006: step 283400, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 05:19:42.230875: step 283420, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 05:20:48.370335: step 283440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 05:21:54.962590: step 283460, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 05:23:02.867493: step 283480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 05:24:10.729369: step 283500, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-20 05:25:16.922730: step 283520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 05:26:23.094554: step 283540, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 05:27:31.330764: step 283560, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-20 05:28:38.412438: step 283580, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 05:29:45.843361: step 283600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 05:30:53.615904: step 283620, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 05:32:00.138686: step 283640, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-20 05:33:08.060651: step 283660, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 05:34:14.797861: step 283680, loss = 0.0021, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-20 05:35:23.521859: step 283700, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.515 sec/batch)
2017-05-20 05:36:31.633238: step 283720, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-20 05:37:38.512051: step 283740, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-20 05:38:45.405927: step 283760, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 05:39:52.766403: step 283780, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 05:40:59.482448: step 283800, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 05:42:06.169274: step 283820, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-20 05:43:12.592435: step 283840, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 05:44:18.813762: step 283860, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 05:45:24.901733: step 283880, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 05:46:31.215457: step 283900, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 05:47:38.214747: step 283920, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 05:48:44.735167: step 283940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 05:49:51.203252: step 283960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 05:50:57.454943: step 283980, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 05:52:04.065339: step 284000, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
[Eval] 2017-05-20 05:52:19.688412: step 284000, acc = 0.9426, f1 = 0.9408
[Test] 2017-05-20 05:52:30.377285: step 284000, acc = 0.9292, f1 = 0.9286
[Status] 2017-05-20 05:52:30.377378: step 284000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 05:53:37.864205: step 284020, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-20 05:54:45.014480: step 284040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 05:55:52.688724: step 284060, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.507 sec/batch)
2017-05-20 05:56:59.038946: step 284080, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 05:58:05.693066: step 284100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 05:59:12.287703: step 284120, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 06:00:20.434688: step 284140, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-20 06:01:26.840162: step 284160, loss = 0.0006, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 06:02:33.165468: step 284180, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-20 06:03:39.135017: step 284200, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 06:04:45.178488: step 284220, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 06:05:52.598188: step 284240, loss = 0.0030, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-20 06:06:59.508058: step 284260, loss = 0.0082, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-20 06:08:05.801098: step 284280, loss = 0.0435, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-20 06:09:11.932607: step 284300, loss = 0.0721, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 06:10:18.106153: step 284320, loss = 0.0876, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 06:11:24.968896: step 284340, loss = 0.0963, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 06:12:32.795063: step 284360, loss = 0.0927, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-20 06:13:39.568027: step 284380, loss = 0.0981, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 06:14:45.709882: step 284400, loss = 0.0852, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 06:15:52.348753: step 284420, loss = 0.0807, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-20 06:16:58.906738: step 284440, loss = 0.0762, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 06:18:05.819905: step 284460, loss = 0.0720, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-20 06:19:13.043995: step 284480, loss = 0.0676, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-20 06:20:20.737273: step 284500, loss = 0.0637, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 06:21:28.021922: step 284520, loss = 0.0601, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 06:22:35.489410: step 284540, loss = 0.0568, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 06:23:42.681927: step 284560, loss = 0.0537, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 06:24:48.886668: step 284580, loss = 0.0508, acc = 1.0000 (19.7 examples/sec; 3.257 sec/batch)
2017-05-20 06:25:55.622053: step 284600, loss = 0.0481, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 06:27:02.749350: step 284620, loss = 0.0456, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-20 06:28:10.612566: step 284640, loss = 0.0433, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-20 06:29:16.849149: step 284660, loss = 0.0410, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-20 06:30:23.146977: step 284680, loss = 0.0389, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-20 06:31:31.133070: step 284700, loss = 0.0384, acc = 0.9980 (18.4 examples/sec; 3.482 sec/batch)
2017-05-20 06:32:38.120906: step 284720, loss = 0.0351, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-20 06:33:45.939976: step 284740, loss = 0.0333, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-20 06:34:52.967649: step 284760, loss = 0.0316, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-20 06:35:59.063422: step 284780, loss = 0.0301, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-20 06:37:06.850355: step 284800, loss = 0.0299, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-20 06:38:13.796051: step 284820, loss = 0.0272, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 06:39:19.712487: step 284840, loss = 0.0271, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 06:40:26.273023: step 284860, loss = 0.0245, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 06:41:32.823492: step 284880, loss = 0.0233, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-20 06:42:40.292774: step 284900, loss = 0.0222, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 06:43:46.896783: step 284920, loss = 0.0211, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 06:44:53.836735: step 284940, loss = 0.0201, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 06:46:00.772021: step 284960, loss = 0.0192, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 06:47:08.212548: step 284980, loss = 0.0182, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 06:48:15.310740: step 285000, loss = 0.0173, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
[Eval] 2017-05-20 06:48:30.354205: step 285000, acc = 0.9372, f1 = 0.9351
[Test] 2017-05-20 06:48:40.990994: step 285000, acc = 0.9238, f1 = 0.9231
[Status] 2017-05-20 06:48:40.991108: step 285000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 06:49:48.140492: step 285020, loss = 0.0164, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 06:50:56.248677: step 285040, loss = 0.0156, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-20 06:52:02.980927: step 285060, loss = 0.0149, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 06:53:09.254850: step 285080, loss = 0.0141, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 06:54:15.966731: step 285100, loss = 0.0134, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 06:55:23.064131: step 285120, loss = 0.0132, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 06:56:30.447444: step 285140, loss = 0.0126, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 06:57:37.494788: step 285160, loss = 0.0116, acc = 1.0000 (18.1 examples/sec; 3.529 sec/batch)
2017-05-20 06:58:44.596557: step 285180, loss = 0.0112, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 06:59:51.780048: step 285200, loss = 0.0105, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-20 07:00:59.909115: step 285220, loss = 0.0114, acc = 0.9980 (18.2 examples/sec; 3.516 sec/batch)
2017-05-20 07:02:06.991921: step 285240, loss = 0.0095, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 07:03:15.476795: step 285260, loss = 0.0090, acc = 1.0000 (17.4 examples/sec; 3.682 sec/batch)
2017-05-20 07:04:23.498042: step 285280, loss = 0.0086, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 07:05:30.450304: step 285300, loss = 0.0082, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-20 07:06:36.914771: step 285320, loss = 0.0078, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 07:07:43.273028: step 285340, loss = 0.0074, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 07:08:51.470900: step 285360, loss = 0.0071, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-20 07:09:58.716219: step 285380, loss = 0.0069, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-20 07:11:06.565095: step 285400, loss = 0.0064, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 07:12:13.963092: step 285420, loss = 0.0061, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-20 07:13:21.905890: step 285440, loss = 0.0059, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 07:14:30.185886: step 285460, loss = 0.0056, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-20 07:15:36.353896: step 285480, loss = 0.0054, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 07:16:42.776243: step 285500, loss = 0.0050, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 07:17:49.692510: step 285520, loss = 0.0048, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-20 07:18:56.365838: step 285540, loss = 0.0046, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-20 07:20:02.837627: step 285560, loss = 0.0045, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-20 07:21:09.804977: step 285580, loss = 0.0042, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 07:22:16.461206: step 285600, loss = 0.0054, acc = 0.9980 (18.6 examples/sec; 3.449 sec/batch)
2017-05-20 07:23:22.992059: step 285620, loss = 0.0037, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 07:24:29.511294: step 285640, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 07:25:37.036421: step 285660, loss = 0.0034, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-20 07:26:44.072888: step 285680, loss = 0.0033, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 07:27:50.736000: step 285700, loss = 0.0031, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 07:28:57.109490: step 285720, loss = 0.0030, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-20 07:30:03.498577: step 285740, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 07:31:09.619075: step 285760, loss = 0.0028, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 07:32:16.398920: step 285780, loss = 0.0025, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 07:33:22.717597: step 285800, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 07:34:29.997318: step 285820, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 07:35:36.587862: step 285840, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 07:36:44.372965: step 285860, loss = 0.0030, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-20 07:37:51.090582: step 285880, loss = 0.0021, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-20 07:38:57.541847: step 285900, loss = 0.0019, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-20 07:40:04.783247: step 285920, loss = 0.0019, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-20 07:41:11.333324: step 285940, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 07:42:17.947076: step 285960, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 07:43:24.604432: step 285980, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 07:44:31.095895: step 286000, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
[Eval] 2017-05-20 07:44:45.988757: step 286000, acc = 0.9384, f1 = 0.9364
[Test] 2017-05-20 07:44:56.678156: step 286000, acc = 0.9248, f1 = 0.9241
[Status] 2017-05-20 07:44:56.678279: step 286000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 07:46:04.032326: step 286020, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 07:47:11.214096: step 286040, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 07:48:18.074461: step 286060, loss = 0.0014, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-20 07:49:25.751809: step 286080, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-20 07:50:32.541196: step 286100, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-20 07:51:38.877776: step 286120, loss = 0.0013, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 07:52:45.442339: step 286140, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 07:53:52.066141: step 286160, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 07:55:01.999287: step 286180, loss = 0.0012, acc = 1.0000 (17.1 examples/sec; 3.752 sec/batch)
2017-05-20 07:56:09.277390: step 286200, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 07:57:17.877867: step 286220, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-20 07:58:25.276219: step 286240, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-20 07:59:32.024936: step 286260, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-20 08:00:40.416654: step 286280, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-20 08:01:47.534772: step 286300, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 08:02:54.706777: step 286320, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 08:04:02.451246: step 286340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 08:05:08.584912: step 286360, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 08:06:16.990928: step 286380, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-20 08:07:23.846297: step 286400, loss = 0.0011, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-20 08:08:30.268717: step 286420, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 08:09:36.155357: step 286440, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 08:10:42.040921: step 286460, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 08:11:48.507117: step 286480, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 08:12:55.687747: step 286500, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-20 08:14:03.321225: step 286520, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-20 08:15:09.345298: step 286540, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-20 08:16:16.998007: step 286560, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-20 08:17:24.464819: step 286580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 08:18:30.529052: step 286600, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 08:19:37.345602: step 286620, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-20 08:20:44.839656: step 286640, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-20 08:21:51.690936: step 286660, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 08:22:59.555822: step 286680, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 08:24:06.193048: step 286700, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 08:25:13.537250: step 286720, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-20 08:26:20.726756: step 286740, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 08:27:27.098112: step 286760, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-20 08:28:33.283854: step 286780, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 08:29:39.578185: step 286800, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-20 08:30:45.746006: step 286820, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 08:31:53.227601: step 286840, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 08:32:59.598785: step 286860, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 08:34:05.446452: step 286880, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 08:35:13.381132: step 286900, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-20 08:36:19.770564: step 286920, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 08:37:26.127562: step 286940, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 08:38:32.341373: step 286960, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 08:39:39.390966: step 286980, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 08:40:45.903789: step 287000, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-20 08:41:01.416444: step 287000, acc = 0.9393, f1 = 0.9374
[Test] 2017-05-20 08:41:12.123866: step 287000, acc = 0.9253, f1 = 0.9247
[Status] 2017-05-20 08:41:12.123979: step 287000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 08:42:18.953637: step 287020, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 08:43:25.623405: step 287040, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 08:44:32.720404: step 287060, loss = 0.0010, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-20 08:45:39.100147: step 287080, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 08:46:46.322220: step 287100, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-20 08:47:55.359046: step 287120, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 08:49:01.652893: step 287140, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 08:50:08.003538: step 287160, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-20 08:51:14.851384: step 287180, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 08:52:22.249075: step 287200, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 08:53:28.149546: step 287220, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 08:54:35.450023: step 287240, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-20 08:55:42.826553: step 287260, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-20 08:56:50.296162: step 287280, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 08:57:56.329257: step 287300, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 08:59:04.342228: step 287320, loss = 0.0032, acc = 0.9980 (18.4 examples/sec; 3.475 sec/batch)
2017-05-20 09:00:10.585813: step 287340, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 09:01:17.935743: step 287360, loss = 0.0024, acc = 0.9980 (19.1 examples/sec; 3.359 sec/batch)
2017-05-20 09:02:25.920001: step 287380, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-20 09:03:34.966687: step 287400, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-20 09:04:41.612099: step 287420, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 09:05:49.808836: step 287440, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 09:06:56.977893: step 287460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 09:08:04.431138: step 287480, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-20 09:09:11.946298: step 287500, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 09:10:18.879625: step 287520, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 09:11:26.454610: step 287540, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-20 09:12:32.603243: step 287560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 09:13:38.956140: step 287580, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 09:14:45.347318: step 287600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 09:15:52.562851: step 287620, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-20 09:17:00.631665: step 287640, loss = 0.0008, acc = 1.0000 (17.9 examples/sec; 3.570 sec/batch)
2017-05-20 09:18:06.731429: step 287660, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 09:19:13.520051: step 287680, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 09:20:20.343725: step 287700, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-20 09:21:26.676503: step 287720, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-20 09:22:33.080357: step 287740, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-20 09:23:39.930947: step 287760, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-20 09:24:46.277703: step 287780, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 09:25:53.503063: step 287800, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-20 09:27:00.302445: step 287820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 09:28:06.558899: step 287840, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 09:29:12.692839: step 287860, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 09:30:20.761133: step 287880, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-20 09:31:27.689779: step 287900, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-20 09:32:36.055831: step 287920, loss = 0.0009, acc = 1.0000 (17.8 examples/sec; 3.591 sec/batch)
2017-05-20 09:33:43.782942: step 287940, loss = 0.0016, acc = 1.0000 (18.1 examples/sec; 3.530 sec/batch)
2017-05-20 09:34:51.222150: step 287960, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 09:35:57.678758: step 287980, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-20 09:37:05.987648: step 288000, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
[Eval] 2017-05-20 09:37:21.536322: step 288000, acc = 0.9403, f1 = 0.9384
[Test] 2017-05-20 09:37:32.096035: step 288000, acc = 0.9265, f1 = 0.9259
[Status] 2017-05-20 09:37:32.096120: step 288000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 09:38:40.214228: step 288020, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-20 09:39:47.269266: step 288040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 09:40:55.580728: step 288060, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-20 09:42:02.838833: step 288080, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-20 09:43:10.250112: step 288100, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 09:44:16.548771: step 288120, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 09:45:23.222018: step 288140, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 09:46:29.455500: step 288160, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 09:47:36.035553: step 288180, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 09:48:44.359879: step 288200, loss = 0.0010, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 09:49:51.434729: step 288220, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 09:50:58.720872: step 288240, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 09:52:05.324495: step 288260, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 09:53:12.135381: step 288280, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-20 09:54:19.499687: step 288300, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-20 09:55:26.106146: step 288320, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.377 sec/batch)
2017-05-20 09:56:32.948100: step 288340, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-20 09:57:40.127348: step 288360, loss = 0.0022, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 09:58:46.901370: step 288380, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-20 09:59:53.187429: step 288400, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-20 10:01:00.208645: step 288420, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.476 sec/batch)
2017-05-20 10:02:07.245160: step 288440, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-20 10:03:13.863878: step 288460, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 10:04:20.511759: step 288480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 10:05:28.016266: step 288500, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 10:06:35.974904: step 288520, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-20 10:07:42.321331: step 288540, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 10:08:49.218124: step 288560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 10:09:56.668545: step 288580, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 10:11:03.900687: step 288600, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 10:12:10.530958: step 288620, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 10:13:18.410124: step 288640, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-20 10:14:25.425501: step 288660, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-20 10:15:32.377905: step 288680, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-20 10:16:39.201671: step 288700, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 10:17:45.692156: step 288720, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-20 10:18:52.888361: step 288740, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-20 10:19:59.900979: step 288760, loss = 0.0015, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-20 10:21:07.171184: step 288780, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-20 10:22:14.480181: step 288800, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-20 10:23:20.749488: step 288820, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 10:24:28.486682: step 288840, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 10:25:36.087068: step 288860, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-20 10:26:44.092579: step 288880, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 10:27:50.456495: step 288900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 10:28:57.278031: step 288920, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 10:30:04.283565: step 288940, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 10:31:11.435839: step 288960, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-20 10:32:17.879409: step 288980, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 10:33:25.780765: step 289000, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
[Eval] 2017-05-20 10:33:41.439533: step 289000, acc = 0.9406, f1 = 0.9386
[Test] 2017-05-20 10:33:52.251863: step 289000, acc = 0.9268, f1 = 0.9262
[Status] 2017-05-20 10:33:52.251970: step 289000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 10:34:59.088862: step 289020, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-20 10:36:05.732036: step 289040, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-20 10:37:13.050219: step 289060, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 10:38:20.594586: step 289080, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 10:39:28.175614: step 289100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 10:40:35.522345: step 289120, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-20 10:41:42.767615: step 289140, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-20 10:42:50.184296: step 289160, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 10:43:56.845474: step 289180, loss = 0.0009, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-20 10:45:04.225528: step 289200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 10:46:11.268578: step 289220, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 10:47:18.295423: step 289240, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 10:48:24.912902: step 289260, loss = 0.0022, acc = 0.9980 (18.8 examples/sec; 3.399 sec/batch)
2017-05-20 10:49:30.981089: step 289280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 10:50:38.532222: step 289300, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-20 10:51:45.324848: step 289320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 10:52:51.606326: step 289340, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 10:53:57.767149: step 289360, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 10:55:05.802719: step 289380, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 10:56:13.788212: step 289400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 10:57:20.700407: step 289420, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 10:58:26.888005: step 289440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 10:59:34.076379: step 289460, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.543 sec/batch)
2017-05-20 11:00:40.261173: step 289480, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 11:01:47.915581: step 289500, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 11:02:53.778102: step 289520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 11:04:00.670577: step 289540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 11:05:08.228448: step 289560, loss = 0.0014, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 11:06:16.202346: step 289580, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-20 11:07:23.283917: step 289600, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-20 11:08:29.425136: step 289620, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 11:09:35.608245: step 289640, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.260 sec/batch)
2017-05-20 11:10:43.307655: step 289660, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 11:11:50.546346: step 289680, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 11:12:57.902764: step 289700, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
2017-05-20 11:14:06.070783: step 289720, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 11:15:12.786781: step 289740, loss = 0.0021, acc = 0.9980 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 11:16:19.383481: step 289760, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 11:17:27.003780: step 289780, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-20 11:18:34.768843: step 289800, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-20 11:19:42.202066: step 289820, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-20 11:20:49.049870: step 289840, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-20 11:21:55.747284: step 289860, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-20 11:23:02.797773: step 289880, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 11:24:10.055632: step 289900, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.535 sec/batch)
2017-05-20 11:25:17.220536: step 289920, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 11:26:23.830082: step 289940, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 11:27:32.745648: step 289960, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.553 sec/batch)
2017-05-20 11:28:39.360704: step 289980, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-20 11:29:46.171298: step 290000, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
[Eval] 2017-05-20 11:30:01.661521: step 290000, acc = 0.9408, f1 = 0.9389
[Test] 2017-05-20 11:30:12.412989: step 290000, acc = 0.9274, f1 = 0.9268
[Status] 2017-05-20 11:30:12.413116: step 290000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 11:31:19.433394: step 290020, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 11:32:26.304696: step 290040, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 11:33:34.301440: step 290060, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-20 11:34:40.526505: step 290080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 11:35:47.622851: step 290100, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 11:36:55.867574: step 290120, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.505 sec/batch)
2017-05-20 11:38:03.593590: step 290140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 11:39:09.790542: step 290160, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 11:40:16.293937: step 290180, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 11:41:23.394151: step 290200, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 11:42:30.689357: step 290220, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-20 11:43:37.677388: step 290240, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 11:44:45.536008: step 290260, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 11:45:51.659861: step 290280, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 11:46:59.267737: step 290300, loss = 0.0008, acc = 1.0000 (17.9 examples/sec; 3.577 sec/batch)
2017-05-20 11:48:07.126684: step 290320, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-20 11:49:13.669814: step 290340, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-20 11:50:21.113699: step 290360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 11:51:27.937790: step 290380, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 11:52:35.543016: step 290400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 11:53:42.337496: step 290420, loss = 0.0028, acc = 0.9980 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 11:54:48.682246: step 290440, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-20 11:55:55.094186: step 290460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 11:57:02.174441: step 290480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 11:58:08.442577: step 290500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 11:59:14.715519: step 290520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 12:00:20.896986: step 290540, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 12:01:29.310368: step 290560, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-20 12:02:35.849981: step 290580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 12:03:42.665285: step 290600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 12:04:49.413900: step 290620, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-20 12:05:55.545050: step 290640, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-20 12:07:02.148707: step 290660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-20 12:08:10.077391: step 290680, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-20 12:09:17.322989: step 290700, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 12:10:23.709948: step 290720, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 12:11:30.187478: step 290740, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 12:12:37.444584: step 290760, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-20 12:13:44.018225: step 290780, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 12:14:51.015652: step 290800, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 12:15:58.055997: step 290820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 12:17:05.010750: step 290840, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 12:18:11.890763: step 290860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 12:19:19.344132: step 290880, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 12:20:26.336961: step 290900, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-20 12:21:32.580807: step 290920, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-20 12:22:39.105084: step 290940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-20 12:23:45.641493: step 290960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 12:24:51.846798: step 290980, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 12:25:58.603194: step 291000, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
[Eval] 2017-05-20 12:26:13.801143: step 291000, acc = 0.9409, f1 = 0.9390
[Test] 2017-05-20 12:26:24.392815: step 291000, acc = 0.9276, f1 = 0.9269
[Status] 2017-05-20 12:26:24.392888: step 291000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 12:27:32.194954: step 291020, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-20 12:28:39.573196: step 291040, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 12:29:46.340536: step 291060, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 12:30:52.950996: step 291080, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 12:31:59.350132: step 291100, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 12:33:05.662261: step 291120, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 12:34:11.949165: step 291140, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-20 12:35:19.299741: step 291160, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
2017-05-20 12:36:25.973816: step 291180, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 12:37:32.192758: step 291200, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 12:38:39.215649: step 291220, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-20 12:39:45.574275: step 291240, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-20 12:40:52.470256: step 291260, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 12:41:59.500496: step 291280, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 12:43:06.458611: step 291300, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-20 12:44:14.437865: step 291320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 12:45:21.490714: step 291340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-20 12:46:27.571112: step 291360, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 12:47:35.082403: step 291380, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-20 12:48:42.237005: step 291400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 12:49:48.851968: step 291420, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-20 12:50:55.452468: step 291440, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 12:52:02.138534: step 291460, loss = 0.0021, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 12:53:08.899331: step 291480, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 12:54:14.991344: step 291500, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 12:55:21.079454: step 291520, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 12:56:28.768673: step 291540, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-20 12:57:35.738172: step 291560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 12:58:42.274703: step 291580, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 12:59:49.881236: step 291600, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-20 13:00:57.177797: step 291620, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-20 13:02:03.628645: step 291640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 13:03:09.581871: step 291660, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 13:04:16.480106: step 291680, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-20 13:05:22.503347: step 291700, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 13:06:29.013781: step 291720, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-20 13:07:35.300765: step 291740, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 13:08:42.621388: step 291760, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 13:09:48.893963: step 291780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 13:10:54.857576: step 291800, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 13:12:01.630210: step 291820, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-20 13:13:08.224318: step 291840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 13:14:15.221670: step 291860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 13:15:21.877882: step 291880, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 13:16:27.973363: step 291900, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 13:17:35.387206: step 291920, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-20 13:18:43.601010: step 291940, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-20 13:19:50.196903: step 291960, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-20 13:20:56.671321: step 291980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 13:22:03.169494: step 292000, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
[Eval] 2017-05-20 13:22:18.599654: step 292000, acc = 0.9412, f1 = 0.9393
[Test] 2017-05-20 13:22:29.302525: step 292000, acc = 0.9273, f1 = 0.9267
[Status] 2017-05-20 13:22:29.302654: step 292000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 13:23:35.101450: step 292020, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 13:24:41.448949: step 292040, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 13:25:48.705705: step 292060, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-20 13:26:56.130764: step 292080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-20 13:28:03.965812: step 292100, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-20 13:29:10.378900: step 292120, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 13:30:17.545861: step 292140, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 13:31:25.084767: step 292160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 13:32:31.459135: step 292180, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 13:33:38.995009: step 292200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 13:34:45.865824: step 292220, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 13:35:52.927590: step 292240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 13:36:59.281863: step 292260, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-20 13:38:05.586574: step 292280, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 13:39:11.718668: step 292300, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 13:40:19.088180: step 292320, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 13:41:26.814094: step 292340, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 13:42:33.529967: step 292360, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 13:43:40.394890: step 292380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 13:44:47.126702: step 292400, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-20 13:45:54.344223: step 292420, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-20 13:47:01.081817: step 292440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 13:48:07.128533: step 292460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 13:49:13.557906: step 292480, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-20 13:50:21.397943: step 292500, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 13:51:28.259144: step 292520, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-20 13:52:35.034700: step 292540, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 13:53:42.393857: step 292560, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 13:54:50.545259: step 292580, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-20 13:55:56.550692: step 292600, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-20 13:57:02.827454: step 292620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 13:58:10.027208: step 292640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 13:59:17.618076: step 292660, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-20 14:00:24.144171: step 292680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 14:01:30.736758: step 292700, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 14:02:37.225121: step 292720, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 14:03:43.800624: step 292740, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 14:04:50.028537: step 292760, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 14:05:56.689338: step 292780, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 14:07:04.318654: step 292800, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-20 14:08:11.182618: step 292820, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 14:09:17.432404: step 292840, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 14:10:24.926435: step 292860, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-20 14:11:31.460794: step 292880, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 14:12:38.403467: step 292900, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-20 14:13:44.707661: step 292920, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-20 14:14:50.673722: step 292940, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-20 14:15:57.585744: step 292960, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-20 14:17:04.912154: step 292980, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 14:18:12.341201: step 293000, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
[Eval] 2017-05-20 14:18:27.603771: step 293000, acc = 0.9412, f1 = 0.9392
[Test] 2017-05-20 14:18:38.163601: step 293000, acc = 0.9272, f1 = 0.9266
[Status] 2017-05-20 14:18:38.163674: step 293000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 14:19:45.094234: step 293020, loss = 0.0021, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-20 14:20:51.674349: step 293040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 14:21:58.646946: step 293060, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 14:23:06.439749: step 293080, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 14:24:14.855247: step 293100, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 14:25:21.318418: step 293120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-20 14:26:27.626443: step 293140, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-20 14:27:33.580775: step 293160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 14:28:39.697534: step 293180, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 14:29:46.863803: step 293200, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.527 sec/batch)
2017-05-20 14:30:53.484167: step 293220, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-20 14:31:59.389626: step 293240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 14:33:06.251324: step 293260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 14:34:12.825945: step 293280, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-20 14:35:18.822463: step 293300, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 14:36:26.103341: step 293320, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 14:37:33.813283: step 293340, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-20 14:38:40.227317: step 293360, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 14:39:46.785515: step 293380, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 14:40:54.243268: step 293400, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 14:42:00.264157: step 293420, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 14:43:07.165011: step 293440, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-20 14:44:14.592115: step 293460, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-20 14:45:22.352220: step 293480, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-20 14:46:29.193432: step 293500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 14:47:36.045229: step 293520, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-20 14:48:42.627072: step 293540, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 14:49:48.740659: step 293560, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 14:50:55.208072: step 293580, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-20 14:52:01.404672: step 293600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 14:53:07.423117: step 293620, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 14:54:14.746126: step 293640, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 14:55:22.005518: step 293660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 14:56:28.519470: step 293680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 14:57:35.556274: step 293700, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 14:58:42.713161: step 293720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 14:59:49.429302: step 293740, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-20 15:00:55.487327: step 293760, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 15:02:02.370467: step 293780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 15:03:09.854693: step 293800, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 15:04:15.956396: step 293820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 15:05:22.354878: step 293840, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 15:06:29.637015: step 293860, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-20 15:07:36.618209: step 293880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 15:08:42.515188: step 293900, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 15:09:49.436659: step 293920, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 15:10:55.894753: step 293940, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 15:12:03.136011: step 293960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 15:13:10.554415: step 293980, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 15:14:16.962451: step 294000, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
[Eval] 2017-05-20 15:14:32.048065: step 294000, acc = 0.9412, f1 = 0.9393
[Test] 2017-05-20 15:14:42.556342: step 294000, acc = 0.9277, f1 = 0.9271
[Status] 2017-05-20 15:14:42.556476: step 294000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 15:15:49.758139: step 294020, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-20 15:16:56.763974: step 294040, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-20 15:18:04.380064: step 294060, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-20 15:19:11.251607: step 294080, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-20 15:20:18.016523: step 294100, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-20 15:21:25.027513: step 294120, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-20 15:22:31.607850: step 294140, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 15:23:39.318281: step 294160, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-20 15:24:46.107622: step 294180, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-20 15:25:53.298654: step 294200, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 15:27:00.122294: step 294220, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 15:28:06.892674: step 294240, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 15:29:14.443336: step 294260, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-20 15:30:21.499950: step 294280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 15:31:28.055904: step 294300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 15:32:35.168133: step 294320, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-20 15:33:41.974974: step 294340, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 15:34:49.035376: step 294360, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-20 15:35:55.428831: step 294380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 15:37:03.038813: step 294400, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-20 15:38:11.327476: step 294420, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 15:39:18.624117: step 294440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 15:40:24.754056: step 294460, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 15:41:31.122139: step 294480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 15:42:38.816797: step 294500, loss = 0.0007, acc = 1.0000 (17.2 examples/sec; 3.719 sec/batch)
2017-05-20 15:43:45.280588: step 294520, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-20 15:44:53.031111: step 294540, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.488 sec/batch)
2017-05-20 15:45:59.993030: step 294560, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 15:47:07.394396: step 294580, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 15:48:14.707862: step 294600, loss = 0.0011, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-20 15:49:20.955175: step 294620, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 15:50:28.358218: step 294640, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-20 15:51:35.533023: step 294660, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-20 15:52:42.853927: step 294680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 15:53:50.252199: step 294700, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-20 15:54:56.708514: step 294720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 15:56:02.873764: step 294740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 15:57:09.462601: step 294760, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 15:58:15.353313: step 294780, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 15:59:22.710983: step 294800, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-20 16:00:30.858716: step 294820, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-20 16:01:37.813495: step 294840, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-20 16:02:45.000376: step 294860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 16:03:51.801474: step 294880, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 16:04:58.970507: step 294900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 16:06:05.326053: step 294920, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 16:07:11.643157: step 294940, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-20 16:08:18.611338: step 294960, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-20 16:09:24.463083: step 294980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 16:10:31.970688: step 295000, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
[Eval] 2017-05-20 16:10:46.293597: step 295000, acc = 0.9415, f1 = 0.9396
[Test] 2017-05-20 16:10:56.101409: step 295000, acc = 0.9278, f1 = 0.9272
[Status] 2017-05-20 16:10:56.101489: step 295000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 16:12:02.804002: step 295020, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 16:13:09.096714: step 295040, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 16:14:15.897487: step 295060, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 16:15:22.587053: step 295080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 16:16:29.976919: step 295100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 16:17:38.197695: step 295120, loss = 0.0007, acc = 1.0000 (17.5 examples/sec; 3.661 sec/batch)
2017-05-20 16:18:45.159618: step 295140, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-20 16:19:51.924008: step 295160, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-20 16:20:58.586420: step 295180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 16:22:04.677627: step 295200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 16:23:10.905836: step 295220, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 16:24:18.254442: step 295240, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-20 16:25:25.186701: step 295260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-20 16:26:32.442898: step 295280, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.471 sec/batch)
2017-05-20 16:27:38.636452: step 295300, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 16:28:45.412653: step 295320, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 16:29:52.910061: step 295340, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.579 sec/batch)
2017-05-20 16:31:01.311484: step 295360, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-20 16:32:08.326407: step 295380, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-20 16:33:15.773194: step 295400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 16:34:21.627791: step 295420, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 16:35:30.141720: step 295440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 16:36:37.127757: step 295460, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-20 16:37:43.555049: step 295480, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-20 16:38:50.005740: step 295500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 16:39:56.531322: step 295520, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 16:41:03.953934: step 295540, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-20 16:42:11.331875: step 295560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 16:43:17.821718: step 295580, loss = 0.0035, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 16:44:24.566499: step 295600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 16:45:32.154669: step 295620, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-20 16:46:38.452333: step 295640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 16:47:45.311168: step 295660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 16:48:51.488943: step 295680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 16:49:58.353325: step 295700, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-20 16:51:04.885355: step 295720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 16:52:12.324517: step 295740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 16:53:18.665387: step 295760, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 16:54:25.465644: step 295780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 16:55:31.832785: step 295800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 16:56:38.774637: step 295820, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-20 16:57:46.148380: step 295840, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-20 16:58:52.469100: step 295860, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 16:59:59.782180: step 295880, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 17:01:06.344841: step 295900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 17:02:13.834940: step 295920, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-20 17:03:20.673873: step 295940, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 17:04:27.563379: step 295960, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-20 17:05:34.460646: step 295980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 17:06:42.305919: step 296000, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
[Eval] 2017-05-20 17:06:57.905130: step 296000, acc = 0.9417, f1 = 0.9398
[Test] 2017-05-20 17:07:08.604779: step 296000, acc = 0.9277, f1 = 0.9271
[Status] 2017-05-20 17:07:08.604877: step 296000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 17:08:15.214036: step 296020, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 17:09:21.711202: step 296040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-20 17:10:28.288020: step 296060, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-20 17:11:34.524711: step 296080, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-20 17:12:41.448653: step 296100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 17:13:47.742992: step 296120, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-20 17:14:54.139580: step 296140, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-20 17:16:00.585754: step 296160, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-20 17:17:07.293976: step 296180, loss = 0.0017, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-20 17:18:15.169759: step 296200, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 17:19:22.060464: step 296220, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 17:20:28.456771: step 296240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 17:21:35.115176: step 296260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 17:22:42.359858: step 296280, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-20 17:23:49.651737: step 296300, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-20 17:24:57.493157: step 296320, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-20 17:26:04.734266: step 296340, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-20 17:27:12.424885: step 296360, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.435 sec/batch)
2017-05-20 17:28:19.554406: step 296380, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 17:29:26.162197: step 296400, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-20 17:30:34.023703: step 296420, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 17:31:40.848683: step 296440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 17:32:48.128035: step 296460, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.363 sec/batch)
2017-05-20 17:33:55.058226: step 296480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 17:35:02.168373: step 296500, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-20 17:36:08.435765: step 296520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 17:37:14.713395: step 296540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 17:38:21.008905: step 296560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 17:39:27.099387: step 296580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 17:40:34.334723: step 296600, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 17:41:40.403025: step 296620, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-20 17:42:47.400564: step 296640, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-20 17:43:53.897198: step 296660, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 17:45:00.785958: step 296680, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-20 17:46:07.121051: step 296700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 17:47:15.681946: step 296720, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-20 17:48:21.933393: step 296740, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 17:49:28.010982: step 296760, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 17:50:35.147438: step 296780, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-20 17:51:42.095069: step 296800, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 17:52:49.196982: step 296820, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 17:53:58.515850: step 296840, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.512 sec/batch)
2017-05-20 17:55:05.005289: step 296860, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 17:56:11.497936: step 296880, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-20 17:57:17.874957: step 296900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 17:58:25.610008: step 296920, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-20 17:59:31.713327: step 296940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 18:00:37.941522: step 296960, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-20 18:01:44.528174: step 296980, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 18:02:51.160212: step 297000, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
[Eval] 2017-05-20 18:03:06.465457: step 297000, acc = 0.9401, f1 = 0.9382
[Test] 2017-05-20 18:03:17.061862: step 297000, acc = 0.9262, f1 = 0.9256
[Status] 2017-05-20 18:03:17.061951: step 297000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 18:04:23.783667: step 297020, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 18:05:30.106800: step 297040, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-20 18:06:36.304979: step 297060, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 18:07:44.998978: step 297080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-20 18:08:51.480851: step 297100, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 18:09:58.411705: step 297120, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 18:11:06.821835: step 297140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 18:12:14.096878: step 297160, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-20 18:13:20.703021: step 297180, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-20 18:14:26.842682: step 297200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 18:15:33.031593: step 297220, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-20 18:16:39.625988: step 297240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 18:17:46.214577: step 297260, loss = 0.0021, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-20 18:18:53.617158: step 297280, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-20 18:20:00.520413: step 297300, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-20 18:21:07.552288: step 297320, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 18:22:14.541789: step 297340, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 18:23:22.145715: step 297360, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-20 18:24:29.768047: step 297380, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-20 18:25:36.497739: step 297400, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-20 18:26:43.529007: step 297420, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 18:27:49.993028: step 297440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-20 18:28:56.662788: step 297460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 18:30:03.935760: step 297480, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 18:31:10.395079: step 297500, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 18:32:16.253942: step 297520, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 18:33:23.487572: step 297540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 18:34:30.388891: step 297560, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 18:35:37.445465: step 297580, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-20 18:36:43.627458: step 297600, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 18:37:50.075025: step 297620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 18:38:56.364079: step 297640, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 18:40:03.245343: step 297660, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-20 18:41:10.544958: step 297680, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-20 18:42:17.052914: step 297700, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 18:43:23.519899: step 297720, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 18:44:29.495768: step 297740, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 18:45:36.550571: step 297760, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 18:46:42.882412: step 297780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 18:47:49.733887: step 297800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 18:48:55.867467: step 297820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 18:50:02.305174: step 297840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 18:51:09.198479: step 297860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 18:52:16.758063: step 297880, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 18:53:24.100514: step 297900, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-20 18:54:31.818107: step 297920, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.531 sec/batch)
2017-05-20 18:55:38.356129: step 297940, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 18:56:45.484976: step 297960, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.482 sec/batch)
2017-05-20 18:57:51.304687: step 297980, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-20 18:58:58.117142: step 298000, loss = 0.0016, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
[Eval] 2017-05-20 18:59:13.682997: step 298000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-20 18:59:24.351952: step 298000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-20 18:59:24.352068: step 298000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 19:00:30.665180: step 298020, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-20 19:01:36.948142: step 298040, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-20 19:02:43.690814: step 298060, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 19:03:51.191285: step 298080, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-20 19:04:57.490777: step 298100, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 19:06:05.062880: step 298120, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-20 19:07:11.295584: step 298140, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-20 19:08:18.316070: step 298160, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 19:09:25.297939: step 298180, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-20 19:10:32.180084: step 298200, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-20 19:11:38.693676: step 298220, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 19:12:45.125676: step 298240, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 19:13:51.949155: step 298260, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-20 19:15:00.181176: step 298280, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.568 sec/batch)
2017-05-20 19:16:07.245755: step 298300, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 19:17:13.738236: step 298320, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 19:18:20.184915: step 298340, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-20 19:19:26.564797: step 298360, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 19:20:35.006256: step 298380, loss = 0.0006, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-20 19:21:42.127691: step 298400, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-20 19:22:49.200428: step 298420, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 19:23:55.623690: step 298440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 19:25:03.597758: step 298460, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-20 19:26:10.899066: step 298480, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-20 19:27:17.531824: step 298500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 19:28:24.329697: step 298520, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-20 19:29:32.180585: step 298540, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 19:30:39.134902: step 298560, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-20 19:31:45.843483: step 298580, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-20 19:32:52.193312: step 298600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 19:33:59.482049: step 298620, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 19:35:06.762842: step 298640, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-20 19:36:13.443630: step 298660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 19:37:20.277513: step 298680, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 19:38:27.857028: step 298700, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 19:39:34.911157: step 298720, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-20 19:40:42.635644: step 298740, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 19:41:49.500708: step 298760, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-20 19:42:56.743111: step 298780, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-20 19:44:03.726012: step 298800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 19:45:10.395500: step 298820, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-20 19:46:16.542646: step 298840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 19:47:22.770729: step 298860, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 19:48:30.197708: step 298880, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 19:49:37.367870: step 298900, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 19:50:44.121506: step 298920, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-20 19:51:51.632930: step 298940, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-20 19:52:58.091522: step 298960, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-20 19:54:05.551316: step 298980, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 19:55:11.894274: step 299000, loss = 0.0006, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
[Eval] 2017-05-20 19:55:27.438938: step 299000, acc = 0.9413, f1 = 0.9394
[Test] 2017-05-20 19:55:38.138941: step 299000, acc = 0.9282, f1 = 0.9275
[Status] 2017-05-20 19:55:38.139043: step 299000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 19:56:44.677374: step 299020, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 19:57:51.637928: step 299040, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 19:58:58.883009: step 299060, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 20:00:05.546699: step 299080, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-20 20:01:12.036835: step 299100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 20:02:19.289264: step 299120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-20 20:03:26.757890: step 299140, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-20 20:04:32.968476: step 299160, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 20:05:38.948931: step 299180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 20:06:45.813716: step 299200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 20:07:52.941663: step 299220, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-20 20:08:59.004560: step 299240, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 20:10:05.729105: step 299260, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-20 20:11:13.568706: step 299280, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 20:12:19.882440: step 299300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 20:13:26.264938: step 299320, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 20:14:32.685223: step 299340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 20:15:40.222434: step 299360, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-20 20:16:47.550572: step 299380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 20:17:54.040965: step 299400, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-20 20:19:00.913913: step 299420, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 20:20:07.753827: step 299440, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 20:21:14.688570: step 299460, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 20:22:20.730169: step 299480, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 20:23:27.327233: step 299500, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-20 20:24:34.577213: step 299520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-20 20:25:41.859863: step 299540, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 20:26:48.407403: step 299560, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 20:27:54.528127: step 299580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 20:29:01.301457: step 299600, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-20 20:30:07.525270: step 299620, loss = 0.0022, acc = 0.9980 (19.6 examples/sec; 3.266 sec/batch)
2017-05-20 20:31:14.996359: step 299640, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-20 20:32:22.571660: step 299660, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-20 20:33:29.066311: step 299680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 20:34:36.622185: step 299700, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-20 20:35:44.109391: step 299720, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-20 20:36:52.382061: step 299740, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 20:37:59.443095: step 299760, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-20 20:39:06.614034: step 299780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-20 20:40:14.185865: step 299800, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-20 20:41:19.775473: step 299820, loss = 0.0007, acc = 1.0000 (19.7 examples/sec; 3.254 sec/batch)
2017-05-20 20:42:27.329824: step 299840, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-20 20:43:34.073037: step 299860, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-20 20:44:41.006008: step 299880, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.262 sec/batch)
2017-05-20 20:45:47.618214: step 299900, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-20 20:46:55.299524: step 299920, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-20 20:48:02.548208: step 299940, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 20:49:09.363792: step 299960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-20 20:50:16.006674: step 299980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 20:51:23.074561: step 300000, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
[Eval] 2017-05-20 20:51:38.272555: step 300000, acc = 0.9416, f1 = 0.9397
[Test] 2017-05-20 20:51:48.581178: step 300000, acc = 0.9281, f1 = 0.9274
[Status] 2017-05-20 20:51:48.581275: step 300000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 20:52:56.604580: step 300020, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-20 20:54:02.898601: step 300040, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 20:55:09.869324: step 300060, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-20 20:56:16.890873: step 300080, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 20:57:23.788646: step 300100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 20:58:29.942495: step 300120, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 20:59:37.609495: step 300140, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-20 21:00:43.898511: step 300160, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 21:01:51.002461: step 300180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 21:02:57.335464: step 300200, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-20 21:04:04.988113: step 300220, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-20 21:05:13.146948: step 300240, loss = 0.0021, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 21:06:21.464592: step 300260, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-20 21:07:28.460383: step 300280, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 21:08:35.463914: step 300300, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 21:09:41.725460: step 300320, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 21:10:49.570385: step 300340, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-20 21:11:57.364996: step 300360, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 21:13:04.635278: step 300380, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.546 sec/batch)
2017-05-20 21:14:12.162759: step 300400, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 21:15:18.975101: step 300420, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 21:16:26.233552: step 300440, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-20 21:17:32.655622: step 300460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-20 21:18:38.654806: step 300480, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 21:19:45.706419: step 300500, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-20 21:20:51.648543: step 300520, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 21:21:57.994409: step 300540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-20 21:23:03.931049: step 300560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 21:24:10.625517: step 300580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 21:25:16.969412: step 300600, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 21:26:23.510794: step 300620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-20 21:27:30.116469: step 300640, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-20 21:28:36.152344: step 300660, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-20 21:29:41.851229: step 300680, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 21:30:49.017934: step 300700, loss = 0.0022, acc = 0.9980 (19.1 examples/sec; 3.347 sec/batch)
2017-05-20 21:31:55.687812: step 300720, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 21:33:02.256524: step 300740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 21:34:08.633485: step 300760, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 21:35:15.005776: step 300780, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-20 21:36:21.325780: step 300800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 21:37:27.834113: step 300820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 21:38:34.441075: step 300840, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-20 21:39:40.524172: step 300860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 21:40:47.026321: step 300880, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-20 21:41:54.019198: step 300900, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 21:43:01.873607: step 300920, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-20 21:44:09.613280: step 300940, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 21:45:15.890809: step 300960, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-20 21:46:23.019604: step 300980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 21:47:30.573341: step 301000, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
[Eval] 2017-05-20 21:47:45.418303: step 301000, acc = 0.9418, f1 = 0.9399
[Test] 2017-05-20 21:47:55.956834: step 301000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-20 21:47:55.956895: step 301000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 21:49:03.625984: step 301020, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 21:50:09.784941: step 301040, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 21:51:15.983453: step 301060, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-20 21:52:22.831313: step 301080, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-20 21:53:29.171222: step 301100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-20 21:54:35.465630: step 301120, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-20 21:55:42.129647: step 301140, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 21:56:48.909956: step 301160, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 21:57:56.512122: step 301180, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-20 21:59:02.845206: step 301200, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 22:00:09.389180: step 301220, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-20 22:01:17.199745: step 301240, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-20 22:02:24.918550: step 301260, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-20 22:03:31.355397: step 301280, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-20 22:04:38.574153: step 301300, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-20 22:05:45.541659: step 301320, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-20 22:06:52.654388: step 301340, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.561 sec/batch)
2017-05-20 22:07:59.394694: step 301360, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 22:09:06.201028: step 301380, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-20 22:10:14.042208: step 301400, loss = 0.0026, acc = 0.9980 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 22:11:21.231522: step 301420, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-20 22:12:27.629144: step 301440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 22:13:35.959279: step 301460, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-20 22:14:42.739997: step 301480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 22:15:48.773875: step 301500, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-20 22:16:55.974937: step 301520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-20 22:18:02.970772: step 301540, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-20 22:19:10.474117: step 301560, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-20 22:20:16.654850: step 301580, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 22:21:23.282665: step 301600, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-20 22:22:30.579674: step 301620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 22:23:36.917684: step 301640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-20 22:24:43.394015: step 301660, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 22:25:50.327465: step 301680, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 22:26:58.073412: step 301700, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 22:28:04.720943: step 301720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-20 22:29:11.850750: step 301740, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-20 22:30:18.355356: step 301760, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-20 22:31:25.838714: step 301780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 22:32:32.135196: step 301800, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.423 sec/batch)
2017-05-20 22:33:39.588200: step 301820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-20 22:34:46.998034: step 301840, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-20 22:35:53.804290: step 301860, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 22:37:00.877006: step 301880, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-20 22:38:08.134265: step 301900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 22:39:14.704910: step 301920, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-20 22:40:21.843468: step 301940, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-20 22:41:28.994591: step 301960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 22:42:37.121785: step 301980, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.537 sec/batch)
2017-05-20 22:43:43.866233: step 302000, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
[Eval] 2017-05-20 22:43:59.384788: step 302000, acc = 0.9416, f1 = 0.9396
[Test] 2017-05-20 22:44:10.002365: step 302000, acc = 0.9277, f1 = 0.9271
[Status] 2017-05-20 22:44:10.002488: step 302000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 22:45:17.398993: step 302020, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 22:46:24.048397: step 302040, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-20 22:47:30.073930: step 302060, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 22:48:36.223490: step 302080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-20 22:49:44.054257: step 302100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-20 22:50:50.827009: step 302120, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-20 22:51:57.686471: step 302140, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 22:53:05.260184: step 302160, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-20 22:54:11.741772: step 302180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 22:55:17.606030: step 302200, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 22:56:24.596165: step 302220, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-20 22:57:31.144650: step 302240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-20 22:58:37.714668: step 302260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-20 22:59:44.967783: step 302280, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-20 23:00:51.395717: step 302300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-20 23:01:58.027509: step 302320, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-20 23:03:04.998122: step 302340, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-20 23:04:11.631019: step 302360, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-20 23:05:17.723985: step 302380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-20 23:06:25.795189: step 302400, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 23:07:33.011031: step 302420, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-20 23:08:40.143326: step 302440, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 23:09:47.056732: step 302460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 23:10:53.882651: step 302480, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-20 23:12:01.799076: step 302500, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 23:13:09.121039: step 302520, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-20 23:14:15.110986: step 302540, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 23:15:22.315373: step 302560, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-20 23:16:31.045416: step 302580, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-20 23:17:37.437067: step 302600, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-20 23:18:44.797368: step 302620, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-20 23:19:51.346327: step 302640, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 23:20:58.410102: step 302660, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-20 23:22:04.864286: step 302680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-20 23:23:11.249859: step 302700, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-20 23:24:17.487498: step 302720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-20 23:25:25.710593: step 302740, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-20 23:26:32.887718: step 302760, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-20 23:27:39.565280: step 302780, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-20 23:28:46.808473: step 302800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 23:29:54.510121: step 302820, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-20 23:31:01.276862: step 302840, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-20 23:32:07.904090: step 302860, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-20 23:33:14.513632: step 302880, loss = 0.0007, acc = 1.0000 (19.8 examples/sec; 3.240 sec/batch)
2017-05-20 23:34:21.990214: step 302900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-20 23:35:29.320680: step 302920, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-20 23:36:35.556719: step 302940, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-20 23:37:42.270154: step 302960, loss = 0.0006, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-20 23:38:50.080866: step 302980, loss = 0.0012, acc = 1.0000 (18.6 examples/sec; 3.437 sec/batch)
2017-05-20 23:39:57.452206: step 303000, loss = 0.0022, acc = 0.9980 (19.3 examples/sec; 3.311 sec/batch)
[Eval] 2017-05-20 23:40:13.006921: step 303000, acc = 0.9417, f1 = 0.9398
[Test] 2017-05-20 23:40:23.654506: step 303000, acc = 0.9280, f1 = 0.9274
[Status] 2017-05-20 23:40:23.654633: step 303000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-20 23:41:31.319712: step 303020, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-20 23:42:38.981249: step 303040, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-20 23:43:45.410111: step 303060, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-20 23:44:54.161716: step 303080, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-20 23:46:02.232348: step 303100, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-20 23:47:10.221102: step 303120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-20 23:48:17.683929: step 303140, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-20 23:49:24.118595: step 303160, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-20 23:50:30.971034: step 303180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-20 23:51:38.178336: step 303200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-20 23:52:45.259637: step 303220, loss = 0.0021, acc = 0.9980 (18.9 examples/sec; 3.392 sec/batch)
2017-05-20 23:53:51.558677: step 303240, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-20 23:54:59.133425: step 303260, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-20 23:56:06.263960: step 303280, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-20 23:57:14.015510: step 303300, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-20 23:58:21.477699: step 303320, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-20 23:59:28.379638: step 303340, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 00:00:34.661610: step 303360, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-21 00:01:41.706942: step 303380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 00:02:48.410945: step 303400, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 00:03:56.404811: step 303420, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 00:05:03.929354: step 303440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-21 00:06:10.619840: step 303460, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 00:07:16.901082: step 303480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 00:08:23.349818: step 303500, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 00:09:31.027149: step 303520, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-21 00:10:37.395100: step 303540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 00:11:43.959913: step 303560, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 00:12:51.216628: step 303580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 00:13:57.453045: step 303600, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-21 00:15:04.519561: step 303620, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-21 00:16:11.231139: step 303640, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 00:17:18.240229: step 303660, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-21 00:18:26.009737: step 303680, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 00:19:33.106810: step 303700, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-21 00:20:41.045111: step 303720, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-21 00:21:47.676188: step 303740, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-21 00:22:54.359318: step 303760, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 00:24:01.371479: step 303780, loss = 0.0011, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-21 00:25:08.905857: step 303800, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 00:26:15.530301: step 303820, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-21 00:27:22.730073: step 303840, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-21 00:28:29.535283: step 303860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 00:29:36.185430: step 303880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 00:30:42.197284: step 303900, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 00:31:49.075986: step 303920, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 00:32:56.523838: step 303940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 00:34:03.129162: step 303960, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-21 00:35:09.924559: step 303980, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 00:36:16.146119: step 304000, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
[Eval] 2017-05-21 00:36:31.286664: step 304000, acc = 0.9411, f1 = 0.9391
[Test] 2017-05-21 00:36:41.475744: step 304000, acc = 0.9283, f1 = 0.9277
[Status] 2017-05-21 00:36:41.475821: step 304000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 00:37:50.245691: step 304020, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 00:38:56.350948: step 304040, loss = 0.0006, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-21 00:40:03.065473: step 304060, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-21 00:41:09.742746: step 304080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 00:42:16.364375: step 304100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 00:43:22.683109: step 304120, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 00:44:30.114799: step 304140, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-21 00:45:37.135115: step 304160, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 00:46:44.553314: step 304180, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 00:47:51.662375: step 304200, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 00:48:59.646358: step 304220, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-21 00:50:06.954156: step 304240, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-21 00:51:14.796910: step 304260, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-21 00:52:22.999508: step 304280, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.456 sec/batch)
2017-05-21 00:53:29.912176: step 304300, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 00:54:36.600797: step 304320, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-21 00:55:43.366536: step 304340, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 00:56:51.741926: step 304360, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-21 00:57:59.200081: step 304380, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-21 00:59:06.444937: step 304400, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 01:00:14.365409: step 304420, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-21 01:01:22.708936: step 304440, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-21 01:02:30.287865: step 304460, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-21 01:03:36.862273: step 304480, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-21 01:04:43.668777: step 304500, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 01:05:50.333232: step 304520, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 01:06:56.561862: step 304540, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 01:08:03.589486: step 304560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 01:09:10.520536: step 304580, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 01:10:17.350979: step 304600, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 01:11:24.430124: step 304620, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-21 01:12:31.278995: step 304640, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-21 01:13:37.224534: step 304660, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 01:14:44.666845: step 304680, loss = 0.0006, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 01:15:52.703299: step 304700, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-21 01:16:59.852609: step 304720, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 01:18:06.222344: step 304740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 01:19:12.926409: step 304760, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-21 01:20:20.571936: step 304780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 01:21:27.101528: step 304800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 01:22:33.795362: step 304820, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-21 01:23:40.228522: step 304840, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-21 01:24:46.600704: step 304860, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 01:25:53.609396: step 304880, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-21 01:26:59.506490: step 304900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 01:28:05.786843: step 304920, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 01:29:11.945141: step 304940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 01:30:19.034570: step 304960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 01:31:25.751210: step 304980, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-21 01:32:32.985255: step 305000, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
[Eval] 2017-05-21 01:32:48.420273: step 305000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-21 01:32:58.999339: step 305000, acc = 0.9281, f1 = 0.9275
[Status] 2017-05-21 01:32:58.999437: step 305000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 01:34:05.757134: step 305020, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 01:35:12.462032: step 305040, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 01:36:18.522063: step 305060, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 01:37:27.242612: step 305080, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-21 01:38:35.583040: step 305100, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-21 01:39:42.605009: step 305120, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 01:40:49.857877: step 305140, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-21 01:41:57.450398: step 305160, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-21 01:43:03.988783: step 305180, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-21 01:44:11.596249: step 305200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 01:45:18.666737: step 305220, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 01:46:25.540234: step 305240, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-21 01:47:32.691083: step 305260, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-21 01:48:40.216221: step 305280, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 01:49:47.277270: step 305300, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-21 01:50:53.833763: step 305320, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 01:52:00.579650: step 305340, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-21 01:53:07.970496: step 305360, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 01:54:14.705172: step 305380, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-21 01:55:21.364628: step 305400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 01:56:29.848542: step 305420, loss = 0.0007, acc = 1.0000 (17.4 examples/sec; 3.685 sec/batch)
2017-05-21 01:57:36.530315: step 305440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 01:58:43.803425: step 305460, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-21 01:59:50.714212: step 305480, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 02:00:59.205807: step 305500, loss = 0.0006, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-21 02:02:06.457767: step 305520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 02:03:14.214861: step 305540, loss = 0.0013, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-21 02:04:21.625117: step 305560, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 02:05:28.310385: step 305580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 02:06:36.677116: step 305600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 02:07:43.793212: step 305620, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 02:08:51.907434: step 305640, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-21 02:09:58.620684: step 305660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 02:11:04.832200: step 305680, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 02:12:11.777108: step 305700, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-21 02:13:17.830381: step 305720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 02:14:24.348632: step 305740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 02:15:30.496824: step 305760, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-21 02:16:37.150786: step 305780, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 02:17:44.513232: step 305800, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 02:18:51.426495: step 305820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 02:19:58.251195: step 305840, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-21 02:21:04.454177: step 305860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 02:22:10.551187: step 305880, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 02:23:17.941049: step 305900, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-21 02:24:24.004623: step 305920, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 02:25:30.506385: step 305940, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 02:26:37.252289: step 305960, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-21 02:27:44.630665: step 305980, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 02:28:52.347794: step 306000, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
[Eval] 2017-05-21 02:29:07.783483: step 306000, acc = 0.9413, f1 = 0.9395
[Test] 2017-05-21 02:29:18.501250: step 306000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-21 02:29:18.501345: step 306000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 02:30:24.924104: step 306020, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 02:31:31.493150: step 306040, loss = 0.0006, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-21 02:32:38.923145: step 306060, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 02:33:46.029474: step 306080, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 02:34:53.059223: step 306100, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-21 02:36:00.371539: step 306120, loss = 0.0021, acc = 0.9980 (18.3 examples/sec; 3.499 sec/batch)
2017-05-21 02:37:07.562497: step 306140, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-21 02:38:13.899795: step 306160, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 02:39:21.819255: step 306180, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.556 sec/batch)
2017-05-21 02:40:28.046802: step 306200, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 02:41:34.362475: step 306220, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 02:42:41.405956: step 306240, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-21 02:43:48.752043: step 306260, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-21 02:44:55.263677: step 306280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 02:46:02.151507: step 306300, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-21 02:47:09.740690: step 306320, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-21 02:48:16.258978: step 306340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-21 02:49:23.872504: step 306360, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 02:50:30.265078: step 306380, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-21 02:51:36.549774: step 306400, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 02:52:42.660977: step 306420, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-21 02:53:49.595770: step 306440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 02:54:56.510959: step 306460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 02:56:04.231928: step 306480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 02:57:10.733877: step 306500, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 02:58:18.558242: step 306520, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-21 02:59:24.758779: step 306540, loss = 0.0035, acc = 0.9980 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 03:00:31.399289: step 306560, loss = 0.0021, acc = 0.9980 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 03:01:39.636285: step 306580, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-21 03:02:49.007589: step 306600, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-21 03:03:56.004339: step 306620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 03:05:03.141866: step 306640, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 03:06:09.464379: step 306660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 03:07:15.610625: step 306680, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 03:08:21.674283: step 306700, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 03:09:28.833792: step 306720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 03:10:35.326773: step 306740, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 03:11:42.443874: step 306760, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.576 sec/batch)
2017-05-21 03:12:49.312244: step 306780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 03:13:56.087485: step 306800, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-21 03:15:02.929693: step 306820, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 03:16:11.064273: step 306840, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-21 03:17:17.315401: step 306860, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-21 03:18:23.361982: step 306880, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 03:19:29.872637: step 306900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 03:20:36.394936: step 306920, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 03:21:44.199898: step 306940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 03:22:50.376917: step 306960, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 03:23:56.515103: step 306980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 03:25:03.238499: step 307000, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
[Eval] 2017-05-21 03:25:18.736970: step 307000, acc = 0.9417, f1 = 0.9398
[Test] 2017-05-21 03:25:29.232654: step 307000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-21 03:25:29.232737: step 307000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 03:26:36.145498: step 307020, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 03:27:43.119100: step 307040, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 03:28:50.991237: step 307060, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-21 03:29:56.861911: step 307080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 03:31:03.190438: step 307100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 03:32:09.854324: step 307120, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 03:33:16.687264: step 307140, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 03:34:24.034556: step 307160, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 03:35:32.058734: step 307180, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-21 03:36:39.256916: step 307200, loss = 0.0033, acc = 0.9980 (18.2 examples/sec; 3.508 sec/batch)
2017-05-21 03:37:46.827116: step 307220, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-21 03:38:52.928677: step 307240, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 03:39:58.773648: step 307260, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 03:41:05.436192: step 307280, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-21 03:42:12.350408: step 307300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 03:43:18.428773: step 307320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 03:44:25.566712: step 307340, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 03:45:31.350876: step 307360, loss = 0.0021, acc = 0.9980 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 03:46:39.371664: step 307380, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.494 sec/batch)
2017-05-21 03:47:46.893186: step 307400, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 03:48:54.121900: step 307420, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.424 sec/batch)
2017-05-21 03:50:02.222119: step 307440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 03:51:09.529951: step 307460, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-21 03:52:15.539645: step 307480, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 03:53:21.984318: step 307500, loss = 0.0022, acc = 0.9980 (19.2 examples/sec; 3.342 sec/batch)
2017-05-21 03:54:29.783109: step 307520, loss = 0.0006, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-21 03:55:36.311701: step 307540, loss = 0.0006, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-21 03:56:42.910788: step 307560, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 03:57:49.990784: step 307580, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-21 03:58:57.242691: step 307600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 04:00:03.920317: step 307620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 04:01:10.880638: step 307640, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 04:02:17.444579: step 307660, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 04:03:23.562647: step 307680, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 04:04:30.957513: step 307700, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-21 04:05:37.296039: step 307720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 04:06:43.213430: step 307740, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 04:07:49.928636: step 307760, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-21 04:08:56.136918: step 307780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 04:10:03.620104: step 307800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 04:11:10.160624: step 307820, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 04:12:16.434178: step 307840, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 04:13:22.946847: step 307860, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 04:14:29.478188: step 307880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 04:15:35.878152: step 307900, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 04:16:44.042339: step 307920, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 04:17:51.383319: step 307940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 04:18:58.159507: step 307960, loss = 0.0006, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-21 04:20:04.109660: step 307980, loss = 0.0008, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-21 04:21:11.039261: step 308000, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
[Eval] 2017-05-21 04:21:26.643013: step 308000, acc = 0.9355, f1 = 0.9332
[Test] 2017-05-21 04:21:37.136303: step 308000, acc = 0.9211, f1 = 0.9203
[Status] 2017-05-21 04:21:37.136400: step 308000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 04:22:45.014489: step 308020, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 04:23:52.881397: step 308040, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 04:25:00.153130: step 308060, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-21 04:26:07.022302: step 308080, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-21 04:27:13.645757: step 308100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 04:28:20.843284: step 308120, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 04:29:27.650700: step 308140, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 04:30:34.283963: step 308160, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-21 04:31:41.063197: step 308180, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-21 04:32:48.317276: step 308200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 04:33:54.659097: step 308220, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 04:35:02.717633: step 308240, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 04:36:09.106859: step 308260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 04:37:15.686323: step 308280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 04:38:21.940024: step 308300, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-21 04:39:27.921486: step 308320, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 04:40:34.119855: step 308340, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 04:41:41.399700: step 308360, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-21 04:42:48.676895: step 308380, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-21 04:43:56.747440: step 308400, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-21 04:45:03.839864: step 308420, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-21 04:46:10.106524: step 308440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 04:47:18.165917: step 308460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 04:48:26.781439: step 308480, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-21 04:49:33.978122: step 308500, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 04:50:42.060335: step 308520, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.369 sec/batch)
2017-05-21 04:51:48.747869: step 308540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 04:52:55.369105: step 308560, loss = 0.0021, acc = 0.9980 (19.1 examples/sec; 3.350 sec/batch)
2017-05-21 04:54:02.995672: step 308580, loss = 0.0006, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-21 04:55:10.035533: step 308600, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-21 04:56:17.675035: step 308620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 04:57:25.376412: step 308640, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 04:58:31.639088: step 308660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 04:59:39.211391: step 308680, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-21 05:00:45.880912: step 308700, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 05:01:52.858315: step 308720, loss = 0.0006, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 05:02:59.320558: step 308740, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 05:04:06.240210: step 308760, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-21 05:05:13.098565: step 308780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 05:06:19.294501: step 308800, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-21 05:07:26.019736: step 308820, loss = 0.0021, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 05:08:32.682861: step 308840, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-21 05:09:40.580941: step 308860, loss = 0.0006, acc = 1.0000 (18.5 examples/sec; 3.457 sec/batch)
2017-05-21 05:10:47.820865: step 308880, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-21 05:11:54.375628: step 308900, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-21 05:13:01.300171: step 308920, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 05:14:08.752561: step 308940, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-21 05:15:15.244412: step 308960, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 05:16:22.772652: step 308980, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-21 05:17:29.801857: step 309000, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
[Eval] 2017-05-21 05:17:44.971915: step 309000, acc = 0.9381, f1 = 0.9360
[Test] 2017-05-21 05:17:55.515762: step 309000, acc = 0.9238, f1 = 0.9231
[Status] 2017-05-21 05:17:55.515882: step 309000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 05:19:03.738719: step 309020, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-21 05:20:10.327278: step 309040, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 05:21:17.956242: step 309060, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 05:22:23.954857: step 309080, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 05:23:30.876172: step 309100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 05:24:37.470561: step 309120, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 05:25:44.237504: step 309140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 05:26:51.924078: step 309160, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 05:27:59.060512: step 309180, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-21 05:29:05.831915: step 309200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 05:30:12.515211: step 309220, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-21 05:31:19.035415: step 309240, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-21 05:32:25.128349: step 309260, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 05:33:32.039693: step 309280, loss = 0.0006, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-21 05:34:39.052969: step 309300, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.508 sec/batch)
2017-05-21 05:35:45.849515: step 309320, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-21 05:36:52.266939: step 309340, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 05:37:58.552711: step 309360, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-21 05:39:04.548573: step 309380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 05:40:12.063885: step 309400, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-21 05:41:18.671509: step 309420, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 05:42:25.392923: step 309440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 05:43:33.482986: step 309460, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 05:44:40.821243: step 309480, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-21 05:45:47.728645: step 309500, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-21 05:46:55.058352: step 309520, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 05:48:02.136145: step 309540, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 05:49:08.607370: step 309560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 05:50:15.694386: step 309580, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-21 05:51:23.085252: step 309600, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-21 05:52:30.560344: step 309620, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-21 05:53:37.902967: step 309640, loss = 0.0006, acc = 1.0000 (18.7 examples/sec; 3.419 sec/batch)
2017-05-21 05:54:44.005699: step 309660, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 05:55:50.758401: step 309680, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-21 05:56:57.764243: step 309700, loss = 0.0006, acc = 1.0000 (18.0 examples/sec; 3.552 sec/batch)
2017-05-21 05:58:04.983754: step 309720, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-21 05:59:11.288750: step 309740, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 06:00:18.914241: step 309760, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-21 06:01:25.760491: step 309780, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-21 06:02:32.592488: step 309800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 06:03:39.381633: step 309820, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-21 06:04:46.455528: step 309840, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 06:05:53.751080: step 309860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 06:07:00.658778: step 309880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 06:08:07.108040: step 309900, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 06:09:13.103195: step 309920, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-21 06:10:19.505290: step 309940, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-21 06:11:25.529054: step 309960, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 06:12:33.633627: step 309980, loss = 0.0006, acc = 1.0000 (18.1 examples/sec; 3.541 sec/batch)
2017-05-21 06:13:40.611062: step 310000, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
[Eval] 2017-05-21 06:13:56.206924: step 310000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-21 06:14:06.967689: step 310000, acc = 0.9283, f1 = 0.9277
[Status] 2017-05-21 06:14:06.967846: step 310000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 06:15:13.338342: step 310020, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 06:16:20.721852: step 310040, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 06:17:27.543282: step 310060, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 06:18:34.867885: step 310080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 06:19:42.170783: step 310100, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-21 06:20:48.487360: step 310120, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 06:21:55.091495: step 310140, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 06:23:01.920356: step 310160, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-21 06:24:09.399417: step 310180, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 06:25:15.787597: step 310200, loss = 0.0016, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-21 06:26:22.397498: step 310220, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 06:27:28.903546: step 310240, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 06:28:35.806883: step 310260, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 06:29:42.848428: step 310280, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 06:30:49.700497: step 310300, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-21 06:31:57.093346: step 310320, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 06:33:05.165031: step 310340, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-21 06:34:11.233612: step 310360, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 06:35:17.495950: step 310380, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 06:36:23.992547: step 310400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 06:37:30.288984: step 310420, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 06:38:39.493157: step 310440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 06:39:47.313131: step 310460, loss = 0.0007, acc = 1.0000 (17.4 examples/sec; 3.677 sec/batch)
2017-05-21 06:40:54.017749: step 310480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 06:42:00.165951: step 310500, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-21 06:43:06.738718: step 310520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 06:44:14.369027: step 310540, loss = 0.0006, acc = 1.0000 (17.9 examples/sec; 3.572 sec/batch)
2017-05-21 06:45:21.913923: step 310560, loss = 0.0006, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-21 06:46:28.170557: step 310580, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 06:47:34.497426: step 310600, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 06:48:41.670008: step 310620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 06:49:47.774419: step 310640, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-21 06:50:54.209321: step 310660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 06:52:01.442739: step 310680, loss = 0.0006, acc = 1.0000 (19.0 examples/sec; 3.361 sec/batch)
2017-05-21 06:53:09.168570: step 310700, loss = 0.0006, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-21 06:54:15.499817: step 310720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 06:55:23.533693: step 310740, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 06:56:29.934589: step 310760, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 06:57:36.797468: step 310780, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-21 06:58:43.098536: step 310800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 06:59:50.230002: step 310820, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 07:00:56.779986: step 310840, loss = 0.0006, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-21 07:02:02.956738: step 310860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 07:03:10.498460: step 310880, loss = 0.0026, acc = 0.9980 (18.5 examples/sec; 3.450 sec/batch)
2017-05-21 07:04:16.777144: step 310900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 07:05:23.889365: step 310920, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-21 07:06:31.403292: step 310940, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-21 07:07:39.867927: step 310960, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-21 07:08:46.859385: step 310980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 07:09:53.972714: step 311000, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
[Eval] 2017-05-21 07:10:09.386837: step 311000, acc = 0.9408, f1 = 0.9389
[Test] 2017-05-21 07:10:20.031986: step 311000, acc = 0.9274, f1 = 0.9268
[Status] 2017-05-21 07:10:20.032122: step 311000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 07:11:27.203245: step 311020, loss = 0.0006, acc = 1.0000 (18.8 examples/sec; 3.409 sec/batch)
2017-05-21 07:12:33.608626: step 311040, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-21 07:13:41.728276: step 311060, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-21 07:14:49.360486: step 311080, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 07:15:56.748616: step 311100, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.433 sec/batch)
2017-05-21 07:17:04.045820: step 311120, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-21 07:18:11.569217: step 311140, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-21 07:19:18.400219: step 311160, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-21 07:20:24.954596: step 311180, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-21 07:21:33.094607: step 311200, loss = 0.0018, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 07:22:40.551284: step 311220, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.569 sec/batch)
2017-05-21 07:23:47.566968: step 311240, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-21 07:24:54.651491: step 311260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 07:26:02.787352: step 311280, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-21 07:27:09.456287: step 311300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 07:28:16.781338: step 311320, loss = 0.0006, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-21 07:29:24.258406: step 311340, loss = 0.0034, acc = 0.9980 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 07:30:31.266074: step 311360, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-21 07:31:38.539631: step 311380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 07:32:45.557865: step 311400, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-21 07:33:52.049988: step 311420, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 07:34:58.174853: step 311440, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 07:36:04.625844: step 311460, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 07:37:12.874746: step 311480, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-21 07:38:18.744768: step 311500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 07:39:24.961409: step 311520, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 07:40:31.088851: step 311540, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 07:41:37.101748: step 311560, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 07:42:44.205224: step 311580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 07:43:53.550195: step 311600, loss = 0.0006, acc = 1.0000 (17.2 examples/sec; 3.713 sec/batch)
2017-05-21 07:45:00.581935: step 311620, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 07:46:07.327528: step 311640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 07:47:14.933831: step 311660, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 07:48:23.945485: step 311680, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-21 07:49:29.942917: step 311700, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 07:50:36.308766: step 311720, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 07:51:42.839360: step 311740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 07:52:49.834435: step 311760, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 07:53:57.339465: step 311780, loss = 0.0021, acc = 0.9980 (18.4 examples/sec; 3.485 sec/batch)
2017-05-21 07:55:04.260299: step 311800, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 07:56:10.893043: step 311820, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 07:57:17.355816: step 311840, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-21 07:58:23.943004: step 311860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 07:59:30.956706: step 311880, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-21 08:00:37.090055: step 311900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 08:01:44.065144: step 311920, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-21 08:02:50.328575: step 311940, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 08:03:56.699309: step 311960, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 08:05:04.410670: step 311980, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-21 08:06:11.474731: step 312000, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
[Eval] 2017-05-21 08:06:26.313080: step 312000, acc = 0.9415, f1 = 0.9396
[Test] 2017-05-21 08:06:36.027633: step 312000, acc = 0.9287, f1 = 0.9281
[Status] 2017-05-21 08:06:36.027706: step 312000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 08:07:42.282909: step 312020, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 08:08:49.514260: step 312040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 08:09:56.805268: step 312060, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 08:11:04.356965: step 312080, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 08:12:10.517478: step 312100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 08:13:17.361873: step 312120, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 08:14:23.465115: step 312140, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-21 08:15:30.475462: step 312160, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 08:16:37.298828: step 312180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 08:17:43.771426: step 312200, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 08:18:51.838307: step 312220, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-21 08:19:58.419929: step 312240, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 08:21:04.678342: step 312260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 08:22:12.164638: step 312280, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 08:23:20.575942: step 312300, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 08:24:27.160533: step 312320, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 08:25:35.154498: step 312340, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 08:26:41.736019: step 312360, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-21 08:27:49.262743: step 312380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 08:28:56.935342: step 312400, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-21 08:30:04.635924: step 312420, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-21 08:31:11.493484: step 312440, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-21 08:32:19.318788: step 312460, loss = 0.0006, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-21 08:33:27.368448: step 312480, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-21 08:34:34.909390: step 312500, loss = 0.0006, acc = 1.0000 (17.7 examples/sec; 3.623 sec/batch)
2017-05-21 08:35:41.009250: step 312520, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-21 08:36:47.248532: step 312540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 08:37:55.551783: step 312560, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.502 sec/batch)
2017-05-21 08:39:03.027767: step 312580, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-21 08:40:10.119023: step 312600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 08:41:16.716078: step 312620, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 08:42:23.781661: step 312640, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-21 08:43:30.323563: step 312660, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.261 sec/batch)
2017-05-21 08:44:36.355467: step 312680, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 08:45:43.715997: step 312700, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 08:46:51.213206: step 312720, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-21 08:47:58.338248: step 312740, loss = 0.0022, acc = 0.9980 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 08:49:05.641410: step 312760, loss = 0.0008, acc = 1.0000 (18.1 examples/sec; 3.534 sec/batch)
2017-05-21 08:50:11.940895: step 312780, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 08:51:18.422289: step 312800, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 08:52:24.883595: step 312820, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 08:53:32.826971: step 312840, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 08:54:39.493991: step 312860, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 08:55:46.452106: step 312880, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-21 08:56:52.746444: step 312900, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 08:57:59.751424: step 312920, loss = 0.0021, acc = 0.9980 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 08:59:07.005264: step 312940, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 09:00:13.073206: step 312960, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 09:01:20.088488: step 312980, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-21 09:02:26.001823: step 313000, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
[Eval] 2017-05-21 09:02:40.926387: step 313000, acc = 0.9402, f1 = 0.9384
[Test] 2017-05-21 09:02:50.949550: step 313000, acc = 0.9271, f1 = 0.9264
[Status] 2017-05-21 09:02:50.949678: step 313000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 09:03:57.502056: step 313020, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 09:05:03.725946: step 313040, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 09:06:11.025664: step 313060, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 09:07:17.744670: step 313080, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 09:08:25.225065: step 313100, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.510 sec/batch)
2017-05-21 09:09:31.554707: step 313120, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 09:10:39.035065: step 313140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 09:11:45.631661: step 313160, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-21 09:12:52.830572: step 313180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 09:13:58.762183: step 313200, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 09:15:05.313965: step 313220, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 09:16:13.329711: step 313240, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-21 09:17:21.262065: step 313260, loss = 0.0010, acc = 1.0000 (17.7 examples/sec; 3.617 sec/batch)
2017-05-21 09:18:29.270033: step 313280, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-21 09:19:35.753465: step 313300, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 09:20:43.679298: step 313320, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-21 09:21:50.065281: step 313340, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-21 09:22:57.229315: step 313360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 09:24:04.018880: step 313380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 09:25:10.883142: step 313400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 09:26:18.873597: step 313420, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-21 09:27:25.582890: step 313440, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-21 09:28:31.908777: step 313460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 09:29:38.636900: step 313480, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-21 09:30:46.743158: step 313500, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-21 09:31:53.719569: step 313520, loss = 0.0089, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 09:33:00.367901: step 313540, loss = 0.1030, acc = 0.9920 (19.0 examples/sec; 3.376 sec/batch)
2017-05-21 09:34:07.790880: step 313560, loss = 0.0883, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-21 09:35:14.404007: step 313580, loss = 0.1139, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 09:36:22.540359: step 313600, loss = 0.1271, acc = 0.9980 (18.6 examples/sec; 3.441 sec/batch)
2017-05-21 09:37:29.664566: step 313620, loss = 0.1278, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 09:38:36.252989: step 313640, loss = 0.1653, acc = 0.9960 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 09:39:42.519330: step 313660, loss = 0.1290, acc = 0.9980 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 09:40:49.318023: step 313680, loss = 0.1189, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-21 09:41:57.102893: step 313700, loss = 0.1147, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-21 09:43:03.421037: step 313720, loss = 0.1109, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 09:44:09.850861: step 313740, loss = 0.1072, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 09:45:17.430898: step 313760, loss = 0.1038, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 09:46:23.897512: step 313780, loss = 0.1006, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 09:47:30.755074: step 313800, loss = 0.0975, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 09:48:36.665737: step 313820, loss = 0.0947, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 09:49:43.564093: step 313840, loss = 0.1076, acc = 0.9980 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 09:50:50.050527: step 313860, loss = 0.0894, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 09:51:56.581174: step 313880, loss = 0.0869, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-21 09:53:03.191263: step 313900, loss = 0.0846, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
2017-05-21 09:54:09.572818: step 313920, loss = 0.0825, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 09:55:16.345102: step 313940, loss = 0.0803, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-21 09:56:23.816152: step 313960, loss = 0.0783, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 09:57:30.726026: step 313980, loss = 0.0764, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 09:58:37.840421: step 314000, loss = 0.0745, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
[Eval] 2017-05-21 09:58:51.963524: step 314000, acc = 0.9321, f1 = 0.9298
[Test] 2017-05-21 09:59:02.310901: step 314000, acc = 0.9172, f1 = 0.9164
[Status] 2017-05-21 09:59:02.310987: step 314000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 10:00:09.018606: step 314020, loss = 0.0726, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 10:01:16.410502: step 314040, loss = 0.0708, acc = 1.0000 (18.7 examples/sec; 3.421 sec/batch)
2017-05-21 10:02:23.286707: step 314060, loss = 0.0704, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 10:03:30.843435: step 314080, loss = 0.0674, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 10:04:36.932397: step 314100, loss = 0.0659, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 10:05:43.817594: step 314120, loss = 0.0642, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-21 10:06:50.015170: step 314140, loss = 0.0627, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 10:07:57.039276: step 314160, loss = 0.0613, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-21 10:09:04.210681: step 314180, loss = 0.0599, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-21 10:10:12.010765: step 314200, loss = 0.0585, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-21 10:11:19.374149: step 314220, loss = 0.0571, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 10:12:25.642951: step 314240, loss = 0.0559, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 10:13:33.257223: step 314260, loss = 0.0546, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-21 10:14:40.319357: step 314280, loss = 0.0534, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-21 10:15:47.326866: step 314300, loss = 0.0522, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 10:16:54.539743: step 314320, loss = 0.0516, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 10:18:03.175071: step 314340, loss = 0.0498, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 10:19:09.817196: step 314360, loss = 0.0487, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 10:20:16.764830: step 314380, loss = 0.0476, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 10:21:24.062056: step 314400, loss = 0.0466, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
2017-05-21 10:22:30.820134: step 314420, loss = 0.0547, acc = 0.9980 (18.7 examples/sec; 3.418 sec/batch)
2017-05-21 10:23:37.195920: step 314440, loss = 0.0446, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 10:24:44.654989: step 314460, loss = 0.0745, acc = 0.9960 (19.0 examples/sec; 3.360 sec/batch)
2017-05-21 10:25:51.602728: step 314480, loss = 0.0430, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 10:26:58.903463: step 314500, loss = 0.0418, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 10:28:06.188547: step 314520, loss = 0.0409, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 10:29:13.635938: step 314540, loss = 0.0401, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 10:30:19.839737: step 314560, loss = 0.0393, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 10:31:27.623919: step 314580, loss = 0.0384, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 10:32:34.116099: step 314600, loss = 0.0376, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-21 10:33:40.187857: step 314620, loss = 0.0368, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 10:34:47.230373: step 314640, loss = 0.0360, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 10:35:53.360435: step 314660, loss = 0.0353, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 10:37:00.817392: step 314680, loss = 0.0345, acc = 1.0000 (18.3 examples/sec; 3.495 sec/batch)
2017-05-21 10:38:07.356158: step 314700, loss = 0.0338, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 10:39:13.585716: step 314720, loss = 0.0331, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-21 10:40:19.925968: step 314740, loss = 0.0325, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 10:41:26.198439: step 314760, loss = 0.0318, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 10:42:35.088837: step 314780, loss = 0.0311, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 10:43:41.358013: step 314800, loss = 0.0305, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 10:44:47.543465: step 314820, loss = 0.0299, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 10:45:55.962816: step 314840, loss = 0.0293, acc = 1.0000 (18.4 examples/sec; 3.484 sec/batch)
2017-05-21 10:47:02.042305: step 314860, loss = 0.0287, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 10:48:08.719173: step 314880, loss = 0.0282, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-21 10:49:14.898119: step 314900, loss = 0.0276, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 10:50:21.518797: step 314920, loss = 0.0278, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 10:51:28.114092: step 314940, loss = 0.0266, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 10:52:34.141749: step 314960, loss = 0.0272, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 10:53:41.568939: step 314980, loss = 0.0256, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-21 10:54:48.067655: step 315000, loss = 0.0251, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
[Eval] 2017-05-21 10:55:02.345210: step 315000, acc = 0.9277, f1 = 0.9251
[Test] 2017-05-21 10:55:12.788672: step 315000, acc = 0.9121, f1 = 0.9113
[Status] 2017-05-21 10:55:12.788769: step 315000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 10:56:18.929390: step 315020, loss = 0.0247, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 10:57:25.425887: step 315040, loss = 0.0256, acc = 0.9980 (19.1 examples/sec; 3.352 sec/batch)
2017-05-21 10:58:32.688238: step 315060, loss = 0.0236, acc = 1.0000 (18.5 examples/sec; 3.450 sec/batch)
2017-05-21 10:59:40.245335: step 315080, loss = 0.0236, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 11:00:46.161001: step 315100, loss = 0.0226, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 11:01:52.521102: step 315120, loss = 0.0229, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-21 11:03:00.071068: step 315140, loss = 0.0216, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-21 11:04:06.588343: step 315160, loss = 0.0212, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 11:05:13.454475: step 315180, loss = 0.0208, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-21 11:06:19.993320: step 315200, loss = 0.0204, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 11:07:27.021069: step 315220, loss = 0.0200, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-21 11:08:33.253266: step 315240, loss = 0.0195, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 11:09:40.805662: step 315260, loss = 0.0192, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 11:10:48.552540: step 315280, loss = 0.0188, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-21 11:11:55.955472: step 315300, loss = 0.0184, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-21 11:13:04.756943: step 315320, loss = 0.0180, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 11:14:11.423251: step 315340, loss = 0.0177, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-21 11:15:17.722210: step 315360, loss = 0.0173, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 11:16:24.504276: step 315380, loss = 0.0183, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-21 11:17:31.954415: step 315400, loss = 0.0166, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-21 11:18:39.323412: step 315420, loss = 0.0163, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 11:19:45.653772: step 315440, loss = 0.0234, acc = 0.9980 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 11:20:53.152584: step 315460, loss = 0.0157, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 11:21:59.463699: step 315480, loss = 0.0156, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 11:23:06.840550: step 315500, loss = 0.0150, acc = 1.0000 (18.0 examples/sec; 3.550 sec/batch)
2017-05-21 11:24:14.984145: step 315520, loss = 0.0159, acc = 1.0000 (17.5 examples/sec; 3.658 sec/batch)
2017-05-21 11:25:23.149492: step 315540, loss = 0.0144, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-21 11:26:30.206757: step 315560, loss = 0.0141, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-21 11:27:36.459775: step 315580, loss = 0.0138, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 11:28:42.954817: step 315600, loss = 0.0138, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 11:29:50.174824: step 315620, loss = 0.0132, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-21 11:30:57.042020: step 315640, loss = 0.0130, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-21 11:32:04.317724: step 315660, loss = 0.0127, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 11:33:10.497761: step 315680, loss = 0.0124, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 11:34:17.133573: step 315700, loss = 0.0122, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 11:35:23.754116: step 315720, loss = 0.0119, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 11:36:30.423726: step 315740, loss = 0.0117, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-21 11:37:37.260335: step 315760, loss = 0.0114, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 11:38:44.209039: step 315780, loss = 0.0112, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 11:39:50.240411: step 315800, loss = 0.0111, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 11:40:58.902302: step 315820, loss = 0.0108, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 11:42:05.588335: step 315840, loss = 0.0106, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 11:43:12.134515: step 315860, loss = 0.0105, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 11:44:18.785918: step 315880, loss = 0.0102, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 11:45:25.209490: step 315900, loss = 0.0099, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 11:46:31.684717: step 315920, loss = 0.0098, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 11:47:37.951382: step 315940, loss = 0.0096, acc = 1.0000 (19.6 examples/sec; 3.258 sec/batch)
2017-05-21 11:48:44.079564: step 315960, loss = 0.0095, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 11:49:51.587582: step 315980, loss = 0.0092, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 11:50:59.556639: step 316000, loss = 0.0091, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
[Eval] 2017-05-21 11:51:15.060411: step 316000, acc = 0.9240, f1 = 0.9212
[Test] 2017-05-21 11:51:25.745876: step 316000, acc = 0.9094, f1 = 0.9085
[Status] 2017-05-21 11:51:25.745980: step 316000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 11:52:33.239399: step 316020, loss = 0.0089, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 11:53:39.706448: step 316040, loss = 0.0086, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-21 11:54:46.449044: step 316060, loss = 0.0085, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 11:55:52.744898: step 316080, loss = 0.0089, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 11:56:59.827245: step 316100, loss = 0.0082, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 11:58:07.795220: step 316120, loss = 0.0080, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 11:59:14.695925: step 316140, loss = 0.0080, acc = 1.0000 (18.4 examples/sec; 3.488 sec/batch)
2017-05-21 12:00:21.134886: step 316160, loss = 0.0076, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 12:01:27.698610: step 316180, loss = 0.0075, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 12:02:34.216183: step 316200, loss = 0.0073, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-21 12:03:40.493365: step 316220, loss = 0.0087, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 12:04:48.813551: step 316240, loss = 0.0071, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 12:05:55.578723: step 316260, loss = 0.0070, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 12:07:02.498140: step 316280, loss = 0.0068, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 12:08:08.774049: step 316300, loss = 0.0071, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-21 12:09:15.609698: step 316320, loss = 0.0072, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 12:10:22.329975: step 316340, loss = 0.0065, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 12:11:28.945999: step 316360, loss = 0.0064, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-21 12:12:36.563838: step 316380, loss = 0.0063, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-21 12:13:42.907546: step 316400, loss = 0.0061, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 12:14:49.031021: step 316420, loss = 0.0061, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 12:15:56.038792: step 316440, loss = 0.0059, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-21 12:17:03.006093: step 316460, loss = 0.0057, acc = 1.0000 (18.4 examples/sec; 3.480 sec/batch)
2017-05-21 12:18:09.892909: step 316480, loss = 0.0057, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 12:19:16.634696: step 316500, loss = 0.0069, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-21 12:20:25.349769: step 316520, loss = 0.0056, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 12:21:33.399169: step 316540, loss = 0.0052, acc = 1.0000 (18.7 examples/sec; 3.432 sec/batch)
2017-05-21 12:22:40.008196: step 316560, loss = 0.0051, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 12:23:46.913096: step 316580, loss = 0.0050, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 12:24:53.073316: step 316600, loss = 0.0050, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 12:25:59.732157: step 316620, loss = 0.0074, acc = 0.9980 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 12:27:06.427178: step 316640, loss = 0.0048, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 12:28:14.027608: step 316660, loss = 0.0049, acc = 1.0000 (18.3 examples/sec; 3.493 sec/batch)
2017-05-21 12:29:20.969352: step 316680, loss = 0.0046, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-21 12:30:27.917711: step 316700, loss = 0.0045, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-21 12:31:35.077586: step 316720, loss = 0.0044, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 12:32:42.056034: step 316740, loss = 0.0043, acc = 1.0000 (18.3 examples/sec; 3.504 sec/batch)
2017-05-21 12:33:48.644363: step 316760, loss = 0.0042, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-21 12:34:55.228833: step 316780, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 12:36:02.259885: step 316800, loss = 0.0040, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 12:37:09.070281: step 316820, loss = 0.0069, acc = 0.9980 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 12:38:15.206746: step 316840, loss = 0.0039, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 12:39:21.758625: step 316860, loss = 0.0039, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-21 12:40:28.945778: step 316880, loss = 0.0037, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-21 12:41:36.348273: step 316900, loss = 0.0036, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 12:42:43.470875: step 316920, loss = 0.0039, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 12:43:51.143830: step 316940, loss = 0.0035, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-21 12:44:57.447916: step 316960, loss = 0.0034, acc = 1.0000 (19.6 examples/sec; 3.272 sec/batch)
2017-05-21 12:46:04.038715: step 316980, loss = 0.0034, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 12:47:10.703301: step 317000, loss = 0.0035, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
[Eval] 2017-05-21 12:47:26.380345: step 317000, acc = 0.9295, f1 = 0.9270
[Test] 2017-05-21 12:47:37.048728: step 317000, acc = 0.9154, f1 = 0.9146
[Status] 2017-05-21 12:47:37.048805: step 317000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 12:48:44.573773: step 317020, loss = 0.0041, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 12:49:52.089673: step 317040, loss = 0.0040, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-21 12:50:59.135830: step 317060, loss = 0.0031, acc = 1.0000 (19.0 examples/sec; 3.360 sec/batch)
2017-05-21 12:52:06.724811: step 317080, loss = 0.0030, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-21 12:53:13.072043: step 317100, loss = 0.0030, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-21 12:54:19.949421: step 317120, loss = 0.0030, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 12:55:27.705168: step 317140, loss = 0.0029, acc = 1.0000 (18.3 examples/sec; 3.498 sec/batch)
2017-05-21 12:56:34.515421: step 317160, loss = 0.0030, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-21 12:57:42.358928: step 317180, loss = 0.0028, acc = 1.0000 (17.7 examples/sec; 3.623 sec/batch)
2017-05-21 12:58:49.499358: step 317200, loss = 0.0028, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 12:59:56.831226: step 317220, loss = 0.0028, acc = 1.0000 (18.0 examples/sec; 3.549 sec/batch)
2017-05-21 13:01:03.817614: step 317240, loss = 0.0043, acc = 0.9980 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 13:02:11.049404: step 317260, loss = 0.0025, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-21 13:03:17.172534: step 317280, loss = 0.0026, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-21 13:04:23.630417: step 317300, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 13:05:29.506551: step 317320, loss = 0.0038, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 13:06:36.500390: step 317340, loss = 0.0026, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-21 13:07:42.614374: step 317360, loss = 0.0027, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 13:08:49.367615: step 317380, loss = 0.0024, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 13:09:56.774830: step 317400, loss = 0.0024, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 13:11:04.334101: step 317420, loss = 0.0024, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-21 13:12:11.359900: step 317440, loss = 0.0023, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-21 13:13:18.253904: step 317460, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-21 13:14:24.533071: step 317480, loss = 0.0023, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 13:15:30.803397: step 317500, loss = 0.0021, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-21 13:16:38.756843: step 317520, loss = 0.0030, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-21 13:17:45.469498: step 317540, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 13:18:51.724014: step 317560, loss = 0.0021, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 13:19:58.739531: step 317580, loss = 0.0032, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 13:21:04.693443: step 317600, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 13:22:11.368084: step 317620, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 13:23:18.412386: step 317640, loss = 0.0018, acc = 1.0000 (18.4 examples/sec; 3.481 sec/batch)
2017-05-21 13:24:24.734593: step 317660, loss = 0.0026, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 13:25:31.213421: step 317680, loss = 0.0023, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 13:26:37.140746: step 317700, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.264 sec/batch)
2017-05-21 13:27:43.371901: step 317720, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 13:28:50.132155: step 317740, loss = 0.0020, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-21 13:29:57.655650: step 317760, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 13:31:05.375443: step 317780, loss = 0.0018, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-21 13:32:12.984360: step 317800, loss = 0.0017, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-21 13:33:19.495074: step 317820, loss = 0.0018, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 13:34:27.384919: step 317840, loss = 0.0017, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 13:35:33.712991: step 317860, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 13:36:40.396435: step 317880, loss = 0.0018, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-21 13:37:47.777438: step 317900, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 13:38:54.622359: step 317920, loss = 0.0015, acc = 1.0000 (17.9 examples/sec; 3.572 sec/batch)
2017-05-21 13:40:00.941340: step 317940, loss = 0.0016, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-21 13:41:08.228950: step 317960, loss = 0.0015, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-21 13:42:14.540112: step 317980, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 13:43:20.947747: step 318000, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
[Eval] 2017-05-21 13:43:36.414975: step 318000, acc = 0.9370, f1 = 0.9349
[Test] 2017-05-21 13:43:47.072834: step 318000, acc = 0.9229, f1 = 0.9222
[Status] 2017-05-21 13:43:47.072904: step 318000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 13:44:54.083350: step 318020, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-21 13:46:00.982725: step 318040, loss = 0.0015, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-21 13:47:07.914718: step 318060, loss = 0.0014, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-21 13:48:15.226256: step 318080, loss = 0.0015, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-21 13:49:22.887429: step 318100, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-21 13:50:29.344906: step 318120, loss = 0.0017, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 13:51:36.236950: step 318140, loss = 0.0015, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-21 13:52:43.244358: step 318160, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 13:53:50.252580: step 318180, loss = 0.0019, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 13:54:57.145625: step 318200, loss = 0.0016, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 13:56:03.330414: step 318220, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 13:57:09.378392: step 318240, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 13:58:16.321483: step 318260, loss = 0.0016, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-21 13:59:23.730740: step 318280, loss = 0.0013, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 14:00:31.506531: step 318300, loss = 0.0012, acc = 1.0000 (19.6 examples/sec; 3.257 sec/batch)
2017-05-21 14:01:38.776988: step 318320, loss = 0.0014, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-21 14:02:45.442458: step 318340, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 14:03:54.142470: step 318360, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 14:05:01.411547: step 318380, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 14:06:09.036878: step 318400, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-21 14:07:16.369236: step 318420, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-21 14:08:23.270499: step 318440, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-21 14:09:29.538451: step 318460, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 14:10:36.099921: step 318480, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 14:11:43.016406: step 318500, loss = 0.0011, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-21 14:12:50.152536: step 318520, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 14:13:58.468908: step 318540, loss = 0.0011, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-21 14:15:06.297498: step 318560, loss = 0.0012, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-21 14:16:12.411016: step 318580, loss = 0.0011, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-21 14:17:18.782634: step 318600, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 14:18:26.433313: step 318620, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-21 14:19:34.357841: step 318640, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 14:20:41.723406: step 318660, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 14:21:48.928978: step 318680, loss = 0.0010, acc = 1.0000 (18.9 examples/sec; 3.378 sec/batch)
2017-05-21 14:22:55.911231: step 318700, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 14:24:02.235494: step 318720, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 14:25:08.977107: step 318740, loss = 0.0010, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-21 14:26:14.966424: step 318760, loss = 0.0019, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-21 14:27:21.235690: step 318780, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 14:28:27.315516: step 318800, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 14:29:35.275657: step 318820, loss = 0.0016, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 14:30:42.104378: step 318840, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 14:31:50.031672: step 318860, loss = 0.0011, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 14:32:56.678988: step 318880, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 14:34:03.982424: step 318900, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 14:35:10.932497: step 318920, loss = 0.0010, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-21 14:36:18.181396: step 318940, loss = 0.0014, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-21 14:37:25.837314: step 318960, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-21 14:38:32.613627: step 318980, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 14:39:39.797102: step 319000, loss = 0.0011, acc = 1.0000 (18.8 examples/sec; 3.404 sec/batch)
[Eval] 2017-05-21 14:39:55.420634: step 319000, acc = 0.9403, f1 = 0.9383
[Test] 2017-05-21 14:40:06.124018: step 319000, acc = 0.9267, f1 = 0.9261
[Status] 2017-05-21 14:40:06.124109: step 319000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 14:41:13.173617: step 319020, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-21 14:42:21.026635: step 319040, loss = 0.0013, acc = 1.0000 (18.1 examples/sec; 3.532 sec/batch)
2017-05-21 14:43:27.619509: step 319060, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 14:44:34.707040: step 319080, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-21 14:45:41.015727: step 319100, loss = 0.0013, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 14:46:47.676429: step 319120, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 14:47:54.238852: step 319140, loss = 0.0013, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-21 14:49:00.599965: step 319160, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-21 14:50:07.277058: step 319180, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-21 14:51:14.248291: step 319200, loss = 0.0010, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 14:52:20.937959: step 319220, loss = 0.0011, acc = 1.0000 (19.0 examples/sec; 3.374 sec/batch)
2017-05-21 14:53:27.904331: step 319240, loss = 0.0025, acc = 0.9980 (18.8 examples/sec; 3.399 sec/batch)
2017-05-21 14:54:34.362118: step 319260, loss = 0.0023, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 14:55:41.965827: step 319280, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.431 sec/batch)
2017-05-21 14:56:48.121466: step 319300, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 14:57:54.768580: step 319320, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 14:59:02.534977: step 319340, loss = 0.0025, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 15:00:09.192253: step 319360, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 15:01:15.978691: step 319380, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 15:02:22.389823: step 319400, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 15:03:30.596973: step 319420, loss = 0.0010, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-21 15:04:37.069876: step 319440, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 15:05:43.999834: step 319460, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 15:06:50.540951: step 319480, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 15:07:56.842206: step 319500, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-21 15:09:03.077076: step 319520, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 15:10:10.570807: step 319540, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-21 15:11:17.867939: step 319560, loss = 0.0010, acc = 1.0000 (18.6 examples/sec; 3.443 sec/batch)
2017-05-21 15:12:24.634768: step 319580, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 15:13:31.484866: step 319600, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 15:14:38.138224: step 319620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 15:15:44.678563: step 319640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 15:16:50.907842: step 319660, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 15:17:57.696296: step 319680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 15:19:05.087121: step 319700, loss = 0.0012, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 15:20:11.573853: step 319720, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 15:21:17.804426: step 319740, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 15:22:24.747507: step 319760, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 15:23:30.998660: step 319780, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 15:24:37.223958: step 319800, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 15:25:43.211703: step 319820, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 15:26:49.533441: step 319840, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 15:27:55.740023: step 319860, loss = 0.0016, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-21 15:29:02.254966: step 319880, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 15:30:08.987842: step 319900, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 15:31:15.754520: step 319920, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 15:32:22.142947: step 319940, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 15:33:29.434883: step 319960, loss = 0.0023, acc = 0.9980 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 15:34:38.570037: step 319980, loss = 0.0014, acc = 1.0000 (18.9 examples/sec; 3.395 sec/batch)
2017-05-21 15:35:44.710245: step 320000, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
[Eval] 2017-05-21 15:36:00.157116: step 320000, acc = 0.9409, f1 = 0.9390
[Test] 2017-05-21 15:36:10.950555: step 320000, acc = 0.9278, f1 = 0.9272
[Status] 2017-05-21 15:36:10.950686: step 320000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 15:37:17.402312: step 320020, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-21 15:38:23.612923: step 320040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 15:39:30.109462: step 320060, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 15:40:37.418639: step 320080, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 15:41:44.552371: step 320100, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-21 15:42:52.426053: step 320120, loss = 0.0012, acc = 1.0000 (18.1 examples/sec; 3.531 sec/batch)
2017-05-21 15:44:00.050185: step 320140, loss = 0.0011, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-21 15:45:06.625420: step 320160, loss = 0.0009, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-21 15:46:14.566128: step 320180, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.489 sec/batch)
2017-05-21 15:47:21.697379: step 320200, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-21 15:48:28.816783: step 320220, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 15:49:35.440403: step 320240, loss = 0.0009, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-21 15:50:42.089765: step 320260, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 15:51:48.338010: step 320280, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 15:52:55.195427: step 320300, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 15:54:01.242578: step 320320, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 15:55:07.190467: step 320340, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 15:56:13.792155: step 320360, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-21 15:57:20.471296: step 320380, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 15:58:27.864716: step 320400, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 15:59:35.417889: step 320420, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.358 sec/batch)
2017-05-21 16:00:43.377575: step 320440, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-21 16:01:49.631788: step 320460, loss = 0.0011, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 16:02:56.469110: step 320480, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 16:04:02.890049: step 320500, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-21 16:05:09.557603: step 320520, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 16:06:17.911719: step 320540, loss = 0.0011, acc = 1.0000 (18.2 examples/sec; 3.522 sec/batch)
2017-05-21 16:07:25.161585: step 320560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 16:08:31.436353: step 320580, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 16:09:38.182876: step 320600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 16:10:45.177061: step 320620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 16:11:52.599929: step 320640, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.441 sec/batch)
2017-05-21 16:12:59.944471: step 320660, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 16:14:07.123999: step 320680, loss = 0.0009, acc = 1.0000 (18.0 examples/sec; 3.546 sec/batch)
2017-05-21 16:15:14.380961: step 320700, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 16:16:21.920342: step 320720, loss = 0.0009, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-21 16:17:28.532852: step 320740, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-21 16:18:36.076904: step 320760, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 16:19:42.535702: step 320780, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 16:20:51.515367: step 320800, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-21 16:21:58.261852: step 320820, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-21 16:23:05.062866: step 320840, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 16:24:11.922738: step 320860, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-21 16:25:18.194241: step 320880, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 16:26:25.144872: step 320900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 16:27:32.009863: step 320920, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 16:28:38.984284: step 320940, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-21 16:29:46.210618: step 320960, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 16:30:53.877975: step 320980, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 16:32:00.248530: step 321000, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
[Eval] 2017-05-21 16:32:14.481693: step 321000, acc = 0.9411, f1 = 0.9392
[Test] 2017-05-21 16:32:24.997070: step 321000, acc = 0.9278, f1 = 0.9272
[Status] 2017-05-21 16:32:24.997155: step 321000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 16:33:31.684903: step 321020, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 16:34:38.836647: step 321040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 16:35:45.155660: step 321060, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 16:36:52.114690: step 321080, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-21 16:37:58.749966: step 321100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 16:39:07.038298: step 321120, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-21 16:40:14.904415: step 321140, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 16:41:21.716333: step 321160, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-21 16:42:28.609432: step 321180, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-21 16:43:35.611362: step 321200, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 16:44:42.122877: step 321220, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-21 16:45:48.783847: step 321240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 16:46:56.703461: step 321260, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.451 sec/batch)
2017-05-21 16:48:03.658704: step 321280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 16:49:10.021042: step 321300, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 16:50:16.821852: step 321320, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-21 16:51:23.229432: step 321340, loss = 0.0010, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-21 16:52:29.415057: step 321360, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 16:53:36.142860: step 321380, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 16:54:43.936432: step 321400, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 16:55:51.283052: step 321420, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-21 16:56:57.876276: step 321440, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-21 16:58:04.329166: step 321460, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 16:59:10.502590: step 321480, loss = 0.0024, acc = 0.9980 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 17:00:17.650233: step 321500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 17:01:24.640740: step 321520, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-21 17:02:30.733731: step 321540, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 17:03:39.656086: step 321560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 17:04:47.107608: step 321580, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 17:05:53.975560: step 321600, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-21 17:07:01.421212: step 321620, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 17:08:08.151389: step 321640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.325 sec/batch)
2017-05-21 17:09:14.205110: step 321660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 17:10:21.189713: step 321680, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-21 17:11:28.015136: step 321700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 17:12:35.002234: step 321720, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-21 17:13:42.200175: step 321740, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.442 sec/batch)
2017-05-21 17:14:49.669509: step 321760, loss = 0.0009, acc = 1.0000 (18.2 examples/sec; 3.517 sec/batch)
2017-05-21 17:15:56.203026: step 321780, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 17:17:02.856206: step 321800, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-21 17:18:10.337452: step 321820, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 17:19:17.460859: step 321840, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 17:20:24.194686: step 321860, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-21 17:21:31.589855: step 321880, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-21 17:22:38.064960: step 321900, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 17:23:44.067678: step 321920, loss = 0.0008, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-21 17:24:50.430526: step 321940, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 17:25:57.004694: step 321960, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 17:27:03.987421: step 321980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 17:28:10.674096: step 322000, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.307 sec/batch)
[Eval] 2017-05-21 17:28:25.962123: step 322000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-21 17:28:36.075314: step 322000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-21 17:28:36.075401: step 322000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 17:29:43.308676: step 322020, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-21 17:30:49.751815: step 322040, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 17:31:56.342087: step 322060, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-21 17:33:04.289101: step 322080, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-21 17:34:11.352610: step 322100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 17:35:17.362830: step 322120, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 17:36:24.989506: step 322140, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 17:37:32.838269: step 322160, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.513 sec/batch)
2017-05-21 17:38:39.764915: step 322180, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 17:39:46.830709: step 322200, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-21 17:40:54.503647: step 322220, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 17:42:00.762338: step 322240, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 17:43:07.920040: step 322260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 17:44:14.289823: step 322280, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.342 sec/batch)
2017-05-21 17:45:20.647702: step 322300, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 17:46:26.983001: step 322320, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 17:47:33.494459: step 322340, loss = 0.0023, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-21 17:48:40.977676: step 322360, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.526 sec/batch)
2017-05-21 17:49:48.164969: step 322380, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 17:50:55.740402: step 322400, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-21 17:52:03.130779: step 322420, loss = 0.0009, acc = 1.0000 (19.0 examples/sec; 3.372 sec/batch)
2017-05-21 17:53:09.645609: step 322440, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-21 17:54:16.766466: step 322460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 17:55:23.429259: step 322480, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 17:56:29.630884: step 322500, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 17:57:36.854427: step 322520, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-21 17:58:44.650446: step 322540, loss = 0.0008, acc = 1.0000 (19.0 examples/sec; 3.364 sec/batch)
2017-05-21 17:59:52.869769: step 322560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 18:01:00.171858: step 322580, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 18:02:07.106898: step 322600, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 18:03:14.758852: step 322620, loss = 0.0023, acc = 0.9980 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 18:04:22.127104: step 322640, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 18:05:28.606981: step 322660, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 18:06:35.194344: step 322680, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 18:07:43.130355: step 322700, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 18:08:49.704414: step 322720, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 18:09:55.976085: step 322740, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 18:11:02.919628: step 322760, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-21 18:12:10.390170: step 322780, loss = 0.0014, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 18:13:16.645324: step 322800, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 18:14:23.221046: step 322820, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 18:15:31.616650: step 322840, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.436 sec/batch)
2017-05-21 18:16:38.675690: step 322860, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-21 18:17:45.082953: step 322880, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-21 18:18:53.246509: step 322900, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.465 sec/batch)
2017-05-21 18:19:59.665509: step 322920, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 18:21:07.260618: step 322940, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 18:22:13.636001: step 322960, loss = 0.0017, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 18:23:20.618075: step 322980, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 18:24:28.692771: step 323000, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
[Eval] 2017-05-21 18:24:44.028596: step 323000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-21 18:24:54.893120: step 323000, acc = 0.9286, f1 = 0.9280
[Status] 2017-05-21 18:24:54.893230: step 323000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 18:26:02.458109: step 323020, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-21 18:27:09.038514: step 323040, loss = 0.0021, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 18:28:16.455542: step 323060, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.472 sec/batch)
2017-05-21 18:29:23.982931: step 323080, loss = 0.0013, acc = 1.0000 (17.9 examples/sec; 3.578 sec/batch)
2017-05-21 18:30:32.189653: step 323100, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-21 18:31:38.994809: step 323120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-21 18:32:45.511997: step 323140, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 18:33:52.360006: step 323160, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 18:34:59.363882: step 323180, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-21 18:36:06.168217: step 323200, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 18:37:13.267182: step 323220, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 18:38:20.533968: step 323240, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 18:39:27.032237: step 323260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 18:40:33.294920: step 323280, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 18:41:40.092011: step 323300, loss = 0.0010, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 18:42:47.719577: step 323320, loss = 0.0010, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 18:43:54.115532: step 323340, loss = 0.0012, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 18:45:01.241374: step 323360, loss = 0.0009, acc = 1.0000 (18.8 examples/sec; 3.400 sec/batch)
2017-05-21 18:46:07.851170: step 323380, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 18:47:14.416847: step 323400, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-21 18:48:21.293135: step 323420, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-21 18:49:29.100114: step 323440, loss = 0.0033, acc = 0.9980 (18.5 examples/sec; 3.460 sec/batch)
2017-05-21 18:50:35.545597: step 323460, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-21 18:51:41.681397: step 323480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 18:52:48.367293: step 323500, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 18:53:55.759348: step 323520, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 18:55:02.276090: step 323540, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.353 sec/batch)
2017-05-21 18:56:08.439636: step 323560, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 18:57:15.659164: step 323580, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-21 18:58:22.763680: step 323600, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-21 18:59:30.409361: step 323620, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.497 sec/batch)
2017-05-21 19:00:37.667710: step 323640, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-21 19:01:44.887342: step 323660, loss = 0.0015, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 19:02:52.573291: step 323680, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.391 sec/batch)
2017-05-21 19:04:00.044833: step 323700, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 19:05:06.505778: step 323720, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 19:06:13.261664: step 323740, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-21 19:07:19.583139: step 323760, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 19:08:26.034012: step 323780, loss = 0.0010, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 19:09:32.695486: step 323800, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-21 19:10:39.766800: step 323820, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.462 sec/batch)
2017-05-21 19:11:47.023191: step 323840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 19:12:54.868199: step 323860, loss = 0.0012, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-21 19:14:02.844246: step 323880, loss = 0.0018, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 19:15:09.680431: step 323900, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 19:16:16.254364: step 323920, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-21 19:17:22.665914: step 323940, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 19:18:29.421759: step 323960, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 19:19:35.483628: step 323980, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 19:20:41.972390: step 324000, loss = 0.0011, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
[Eval] 2017-05-21 19:20:57.327877: step 324000, acc = 0.9414, f1 = 0.9395
[Test] 2017-05-21 19:21:07.890740: step 324000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-21 19:21:07.890846: step 324000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 19:22:15.028869: step 324020, loss = 0.0019, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 19:23:22.172346: step 324040, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-21 19:24:28.368937: step 324060, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-21 19:25:34.717400: step 324080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 19:26:41.162862: step 324100, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 19:27:48.007420: step 324120, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 19:28:54.115545: step 324140, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 19:30:01.279832: step 324160, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-21 19:31:09.454748: step 324180, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-21 19:32:16.727967: step 324200, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-21 19:33:23.652574: step 324220, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 19:34:30.746666: step 324240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 19:35:37.216151: step 324260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 19:36:44.311104: step 324280, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.463 sec/batch)
2017-05-21 19:37:51.315365: step 324300, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-21 19:38:57.781541: step 324320, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-21 19:40:04.353171: step 324340, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 19:41:10.436588: step 324360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 19:42:17.333187: step 324380, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-21 19:43:23.179114: step 324400, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 19:44:30.026496: step 324420, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-21 19:45:36.886628: step 324440, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-21 19:46:43.716504: step 324460, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 19:47:50.216604: step 324480, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 19:48:56.689115: step 324500, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 19:50:03.443238: step 324520, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 19:51:10.671833: step 324540, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 19:52:17.469206: step 324560, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 19:53:25.543009: step 324580, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-21 19:54:32.434356: step 324600, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-21 19:55:39.509043: step 324620, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 19:56:46.261420: step 324640, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 19:57:52.787225: step 324660, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 19:59:00.702507: step 324680, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.485 sec/batch)
2017-05-21 20:00:07.014258: step 324700, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 20:01:14.073599: step 324720, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 20:02:21.925106: step 324740, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-21 20:03:28.227456: step 324760, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 20:04:34.389227: step 324780, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 20:05:41.823954: step 324800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 20:06:48.634772: step 324820, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 20:07:54.417293: step 324840, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-21 20:09:01.762266: step 324860, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.365 sec/batch)
2017-05-21 20:10:08.221205: step 324880, loss = 0.0009, acc = 1.0000 (18.7 examples/sec; 3.422 sec/batch)
2017-05-21 20:11:14.849512: step 324900, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 20:12:22.605799: step 324920, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-21 20:13:28.959287: step 324940, loss = 0.0008, acc = 1.0000 (19.7 examples/sec; 3.253 sec/batch)
2017-05-21 20:14:35.731128: step 324960, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-21 20:15:42.544431: step 324980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 20:16:50.196896: step 325000, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
[Eval] 2017-05-21 20:17:05.673783: step 325000, acc = 0.9413, f1 = 0.9394
[Test] 2017-05-21 20:17:16.403297: step 325000, acc = 0.9281, f1 = 0.9275
[Status] 2017-05-21 20:17:16.403401: step 325000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 20:18:22.968880: step 325020, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-21 20:19:30.439129: step 325040, loss = 0.0022, acc = 0.9980 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 20:20:36.828655: step 325060, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.401 sec/batch)
2017-05-21 20:21:43.156796: step 325080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 20:22:49.602086: step 325100, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-21 20:23:57.256816: step 325120, loss = 0.0016, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-21 20:25:04.485406: step 325140, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-21 20:26:11.393117: step 325160, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.342 sec/batch)
2017-05-21 20:27:17.741964: step 325180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 20:28:24.396785: step 325200, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 20:29:31.491552: step 325220, loss = 0.0012, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
2017-05-21 20:30:37.972657: step 325240, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 20:31:44.181134: step 325260, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 20:32:51.345474: step 325280, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-21 20:33:58.664402: step 325300, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-21 20:35:05.967554: step 325320, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 20:36:11.869508: step 325340, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-21 20:37:18.161536: step 325360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 20:38:25.275809: step 325380, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.439 sec/batch)
2017-05-21 20:39:32.131708: step 325400, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.483 sec/batch)
2017-05-21 20:40:38.967914: step 325420, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 20:41:46.463328: step 325440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 20:42:52.526714: step 325460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 20:43:59.151929: step 325480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-21 20:45:06.495143: step 325500, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-21 20:46:13.664271: step 325520, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.383 sec/batch)
2017-05-21 20:47:19.527280: step 325540, loss = 0.0022, acc = 0.9980 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 20:48:26.844705: step 325560, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.417 sec/batch)
2017-05-21 20:49:34.165497: step 325580, loss = 0.0023, acc = 0.9980 (18.0 examples/sec; 3.549 sec/batch)
2017-05-21 20:50:41.420815: step 325600, loss = 0.0039, acc = 0.9980 (19.1 examples/sec; 3.352 sec/batch)
2017-05-21 20:51:48.883717: step 325620, loss = 0.0013, acc = 1.0000 (18.9 examples/sec; 3.394 sec/batch)
2017-05-21 20:52:55.971055: step 325640, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 20:54:02.767783: step 325660, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-21 20:55:09.113037: step 325680, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 20:56:16.633866: step 325700, loss = 0.0014, acc = 1.0000 (18.4 examples/sec; 3.469 sec/batch)
2017-05-21 20:57:23.323890: step 325720, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.430 sec/batch)
2017-05-21 20:58:30.695882: step 325740, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-21 20:59:36.851624: step 325760, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 21:00:44.044355: step 325780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-21 21:01:50.881170: step 325800, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-21 21:02:58.067703: step 325820, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-21 21:04:04.455087: step 325840, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 21:05:11.746109: step 325860, loss = 0.0017, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 21:06:18.941088: step 325880, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.449 sec/batch)
2017-05-21 21:07:25.182625: step 325900, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 21:08:33.673200: step 325920, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 21:09:40.256013: step 325940, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-21 21:10:48.369221: step 325960, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-21 21:11:54.457528: step 325980, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 21:13:01.958156: step 326000, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
[Eval] 2017-05-21 21:13:17.445792: step 326000, acc = 0.9413, f1 = 0.9394
[Test] 2017-05-21 21:13:28.223527: step 326000, acc = 0.9282, f1 = 0.9276
[Status] 2017-05-21 21:13:28.223662: step 326000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 21:14:34.519407: step 326020, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 21:15:40.738656: step 326040, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 21:16:47.507007: step 326060, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.447 sec/batch)
2017-05-21 21:17:54.250693: step 326080, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 21:19:00.901553: step 326100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 21:20:08.663295: step 326120, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 21:21:15.135199: step 326140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 21:22:22.604210: step 326160, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 21:23:28.750854: step 326180, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 21:24:36.416073: step 326200, loss = 0.0012, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-21 21:25:44.214751: step 326220, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-21 21:26:51.565545: step 326240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 21:27:58.764302: step 326260, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-21 21:29:05.398028: step 326280, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.336 sec/batch)
2017-05-21 21:30:11.883213: step 326300, loss = 0.0022, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 21:31:18.508904: step 326320, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 21:32:24.935490: step 326340, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 21:33:32.055857: step 326360, loss = 0.0014, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 21:34:38.604054: step 326380, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-21 21:35:45.483681: step 326400, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-21 21:36:53.030602: step 326420, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
2017-05-21 21:37:59.734537: step 326440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-21 21:39:06.613408: step 326460, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.405 sec/batch)
2017-05-21 21:40:12.982012: step 326480, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-21 21:41:19.736532: step 326500, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-21 21:42:26.120863: step 326520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 21:43:33.548131: step 326540, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-21 21:44:39.679245: step 326560, loss = 0.0020, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 21:45:45.641738: step 326580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-21 21:46:52.544595: step 326600, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 21:48:00.102739: step 326620, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.473 sec/batch)
2017-05-21 21:49:07.659552: step 326640, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-21 21:50:14.951002: step 326660, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-21 21:51:22.480163: step 326680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-21 21:52:29.531874: step 326700, loss = 0.0008, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-21 21:53:36.073791: step 326720, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-21 21:54:44.712055: step 326740, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 21:55:51.230453: step 326760, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-21 21:56:57.300005: step 326780, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-21 21:58:05.715221: step 326800, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.448 sec/batch)
2017-05-21 21:59:12.047513: step 326820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-21 22:00:18.207182: step 326840, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 22:01:24.377683: step 326860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 22:02:31.200439: step 326880, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-21 22:03:37.785981: step 326900, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 22:04:44.187721: step 326920, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-21 22:05:51.755902: step 326940, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-21 22:06:57.747987: step 326960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-21 22:08:05.343485: step 326980, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-21 22:09:12.360445: step 327000, loss = 0.0010, acc = 1.0000 (19.6 examples/sec; 3.267 sec/batch)
[Eval] 2017-05-21 22:09:27.820119: step 327000, acc = 0.9411, f1 = 0.9392
[Test] 2017-05-21 22:09:38.536049: step 327000, acc = 0.9288, f1 = 0.9282
[Status] 2017-05-21 22:09:38.536132: step 327000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 22:10:45.434227: step 327020, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 22:11:53.368375: step 327040, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.486 sec/batch)
2017-05-21 22:13:00.533943: step 327060, loss = 0.0008, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-21 22:14:07.709359: step 327080, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-21 22:15:13.906537: step 327100, loss = 0.0015, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 22:16:21.700815: step 327120, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-21 22:17:27.880961: step 327140, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-21 22:18:36.203768: step 327160, loss = 0.0018, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-21 22:19:43.505747: step 327180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 22:20:50.139294: step 327200, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.402 sec/batch)
2017-05-21 22:21:56.295207: step 327220, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 22:23:02.581437: step 327240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 22:24:09.384541: step 327260, loss = 0.0030, acc = 0.9980 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 22:25:16.728361: step 327280, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 22:26:23.841381: step 327300, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.414 sec/batch)
2017-05-21 22:27:30.235187: step 327320, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 22:28:37.127605: step 327340, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 22:29:44.254808: step 327360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 22:30:50.711182: step 327380, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-21 22:31:57.126962: step 327400, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 22:33:04.767927: step 327420, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-21 22:34:11.402134: step 327440, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 22:35:18.143176: step 327460, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 22:36:25.299574: step 327480, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-21 22:37:31.595006: step 327500, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-21 22:38:38.213843: step 327520, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.396 sec/batch)
2017-05-21 22:39:45.251645: step 327540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 22:40:52.029839: step 327560, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-21 22:41:59.293292: step 327580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 22:43:06.582471: step 327600, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 22:44:13.430584: step 327620, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-21 22:45:20.901483: step 327640, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-21 22:46:28.999368: step 327660, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-21 22:47:36.659533: step 327680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 22:48:43.795409: step 327700, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-21 22:49:50.675302: step 327720, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-21 22:50:56.846129: step 327740, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.270 sec/batch)
2017-05-21 22:52:03.820772: step 327760, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.398 sec/batch)
2017-05-21 22:53:11.102426: step 327780, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-21 22:54:17.714744: step 327800, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-21 22:55:25.036959: step 327820, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.468 sec/batch)
2017-05-21 22:56:31.325254: step 327840, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-21 22:57:38.001896: step 327860, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.445 sec/batch)
2017-05-21 22:58:43.975524: step 327880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-21 22:59:51.619405: step 327900, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.503 sec/batch)
2017-05-21 23:00:58.265714: step 327920, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-21 23:02:05.263876: step 327940, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-21 23:03:11.421015: step 327960, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-21 23:04:17.968788: step 327980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 23:05:24.792640: step 328000, loss = 0.0020, acc = 1.0000 (18.7 examples/sec; 3.426 sec/batch)
[Eval] 2017-05-21 23:05:40.163636: step 328000, acc = 0.9413, f1 = 0.9394
[Test] 2017-05-21 23:05:50.800104: step 328000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-21 23:05:50.800204: step 328000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-21 23:06:57.491392: step 328020, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-21 23:08:04.915531: step 328040, loss = 0.0022, acc = 0.9980 (19.3 examples/sec; 3.314 sec/batch)
2017-05-21 23:09:12.326277: step 328060, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.452 sec/batch)
2017-05-21 23:10:20.108751: step 328080, loss = 0.0009, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-21 23:11:27.978203: step 328100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-21 23:12:34.177798: step 328120, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-21 23:13:41.425496: step 328140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-21 23:14:48.281227: step 328160, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-21 23:15:55.511048: step 328180, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.487 sec/batch)
2017-05-21 23:17:02.768184: step 328200, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-21 23:18:09.802281: step 328220, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.406 sec/batch)
2017-05-21 23:19:16.745939: step 328240, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-21 23:20:23.343126: step 328260, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-21 23:21:30.706967: step 328280, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.407 sec/batch)
2017-05-21 23:22:37.096935: step 328300, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-21 23:23:43.831346: step 328320, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-21 23:24:50.825461: step 328340, loss = 0.0014, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-21 23:25:57.686587: step 328360, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-21 23:27:05.982846: step 328380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-21 23:28:13.705483: step 328400, loss = 0.0008, acc = 1.0000 (18.3 examples/sec; 3.506 sec/batch)
2017-05-21 23:29:20.585251: step 328420, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 23:30:27.016875: step 328440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 23:31:33.811501: step 328460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-21 23:32:41.136798: step 328480, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-21 23:33:47.648249: step 328500, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-21 23:34:55.937792: step 328520, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.491 sec/batch)
2017-05-21 23:36:02.558930: step 328540, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-21 23:37:09.984052: step 328560, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.410 sec/batch)
2017-05-21 23:38:16.395523: step 328580, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-21 23:39:23.852791: step 328600, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-21 23:40:31.166550: step 328620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-21 23:41:39.247238: step 328640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-21 23:42:45.664959: step 328660, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-21 23:43:53.649016: step 328680, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.380 sec/batch)
2017-05-21 23:45:00.929878: step 328700, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-21 23:46:07.692497: step 328720, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-21 23:47:14.186181: step 328740, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-21 23:48:21.379038: step 328760, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.392 sec/batch)
2017-05-21 23:49:28.583578: step 328780, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.535 sec/batch)
2017-05-21 23:50:36.064884: step 328800, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-21 23:51:43.149864: step 328820, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-21 23:52:50.429136: step 328840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-21 23:53:59.199503: step 328860, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-21 23:55:06.860930: step 328880, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.459 sec/batch)
2017-05-21 23:56:14.131897: step 328900, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.348 sec/batch)
2017-05-21 23:57:21.523150: step 328920, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-21 23:58:28.949145: step 328940, loss = 0.0009, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-21 23:59:34.770040: step 328960, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-22 00:00:41.893348: step 328980, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-22 00:01:48.783891: step 329000, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.444 sec/batch)
[Eval] 2017-05-22 00:02:03.917191: step 329000, acc = 0.9413, f1 = 0.9394
[Test] 2017-05-22 00:02:14.681328: step 329000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-22 00:02:14.681435: step 329000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 00:03:20.938612: step 329020, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.332 sec/batch)
2017-05-22 00:04:27.999898: step 329040, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-22 00:05:34.322173: step 329060, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
2017-05-22 00:06:40.888862: step 329080, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-22 00:07:47.272382: step 329100, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.314 sec/batch)
2017-05-22 00:08:54.579948: step 329120, loss = 0.0020, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 00:10:01.164161: step 329140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-22 00:11:07.825244: step 329160, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-22 00:12:14.680537: step 329180, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.475 sec/batch)
2017-05-22 00:13:22.259602: step 329200, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.341 sec/batch)
2017-05-22 00:14:28.895528: step 329220, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.420 sec/batch)
2017-05-22 00:15:35.711284: step 329240, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-22 00:16:42.882065: step 329260, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-22 00:17:49.216225: step 329280, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-22 00:18:56.779596: step 329300, loss = 0.0022, acc = 0.9980 (19.3 examples/sec; 3.315 sec/batch)
2017-05-22 00:20:03.791938: step 329320, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-22 00:21:10.650028: step 329340, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 00:22:18.474932: step 329360, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-22 00:23:25.106675: step 329380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-22 00:24:33.473585: step 329400, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-22 00:25:40.359662: step 329420, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 00:26:46.735935: step 329440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.280 sec/batch)
2017-05-22 00:27:53.465943: step 329460, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.399 sec/batch)
2017-05-22 00:29:01.293605: step 329480, loss = 0.0009, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-22 00:30:08.490502: step 329500, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-22 00:31:15.032580: step 329520, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-22 00:32:21.623517: step 329540, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-22 00:33:28.001836: step 329560, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-22 00:34:35.771661: step 329580, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-22 00:35:42.978474: step 329600, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-22 00:36:50.305225: step 329620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-22 00:37:57.687703: step 329640, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.397 sec/batch)
2017-05-22 00:39:04.174544: step 329660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-22 00:40:11.743033: step 329680, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-22 00:41:20.321594: step 329700, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.556 sec/batch)
2017-05-22 00:42:27.437832: step 329720, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.470 sec/batch)
2017-05-22 00:43:34.176725: step 329740, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.390 sec/batch)
2017-05-22 00:44:42.206909: step 329760, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.490 sec/batch)
2017-05-22 00:45:48.642565: step 329780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 00:46:55.037018: step 329800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-22 00:48:01.631199: step 329820, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-22 00:49:08.292212: step 329840, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-22 00:50:14.823716: step 329860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-22 00:51:21.925437: step 329880, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.500 sec/batch)
2017-05-22 00:52:29.996546: step 329900, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.259 sec/batch)
2017-05-22 00:53:36.415434: step 329920, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-22 00:54:43.715560: step 329940, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-22 00:55:51.695144: step 329960, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.411 sec/batch)
2017-05-22 00:56:58.359545: step 329980, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-22 00:58:06.222873: step 330000, loss = 0.0007, acc = 1.0000 (18.1 examples/sec; 3.530 sec/batch)
[Eval] 2017-05-22 00:58:21.856612: step 330000, acc = 0.9411, f1 = 0.9391
[Test] 2017-05-22 00:58:32.326091: step 330000, acc = 0.9281, f1 = 0.9275
[Status] 2017-05-22 00:58:32.326190: step 330000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 00:59:38.607028: step 330020, loss = 0.0009, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-22 01:00:45.800089: step 330040, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-22 01:01:52.006386: step 330060, loss = 0.0009, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-22 01:03:01.184115: step 330080, loss = 0.0008, acc = 1.0000 (18.6 examples/sec; 3.440 sec/batch)
2017-05-22 01:04:08.482890: step 330100, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-22 01:05:15.710261: step 330120, loss = 0.0023, acc = 0.9980 (19.3 examples/sec; 3.322 sec/batch)
2017-05-22 01:06:21.991343: step 330140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-22 01:07:29.077224: step 330160, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-22 01:08:36.129584: step 330180, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-22 01:09:42.397595: step 330200, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-22 01:10:49.262400: step 330220, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-22 01:11:56.225863: step 330240, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-22 01:13:05.222367: step 330260, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-22 01:14:13.056629: step 330280, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.263 sec/batch)
2017-05-22 01:15:19.646470: step 330300, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-22 01:16:26.051708: step 330320, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-22 01:17:32.792318: step 330340, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.328 sec/batch)
2017-05-22 01:18:39.734947: step 330360, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.454 sec/batch)
2017-05-22 01:19:46.642370: step 330380, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-22 01:20:53.507176: step 330400, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.271 sec/batch)
2017-05-22 01:22:00.296697: step 330420, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-22 01:23:07.527266: step 330440, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-22 01:24:15.168305: step 330460, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.507 sec/batch)
2017-05-22 01:25:21.709304: step 330480, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-22 01:26:29.586786: step 330500, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.568 sec/batch)
2017-05-22 01:27:35.804347: step 330520, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-22 01:28:42.605040: step 330540, loss = 0.0015, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-22 01:29:48.905729: step 330560, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-22 01:30:56.055837: step 330580, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.370 sec/batch)
2017-05-22 01:32:02.698787: step 330600, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-22 01:33:10.316092: step 330620, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.501 sec/batch)
2017-05-22 01:34:16.637192: step 330640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-22 01:35:23.798466: step 330660, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.524 sec/batch)
2017-05-22 01:36:30.559777: step 330680, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.286 sec/batch)
2017-05-22 01:37:39.139344: step 330700, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.425 sec/batch)
2017-05-22 01:38:47.131984: step 330720, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-22 01:39:54.502090: step 330740, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 01:41:01.686583: step 330760, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-22 01:42:07.790078: step 330780, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-22 01:43:14.936030: step 330800, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.312 sec/batch)
2017-05-22 01:44:21.268415: step 330820, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-22 01:45:28.226621: step 330840, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.277 sec/batch)
2017-05-22 01:46:35.278484: step 330860, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-22 01:47:41.618878: step 330880, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.326 sec/batch)
2017-05-22 01:48:48.997699: step 330900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-22 01:49:55.835955: step 330920, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.346 sec/batch)
2017-05-22 01:51:01.888330: step 330940, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
2017-05-22 01:52:09.488032: step 330960, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-22 01:53:16.493059: step 330980, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 01:54:23.066890: step 331000, loss = 0.0018, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
[Eval] 2017-05-22 01:54:37.057198: step 331000, acc = 0.9413, f1 = 0.9393
[Test] 2017-05-22 01:54:46.706271: step 331000, acc = 0.9284, f1 = 0.9278
[Status] 2017-05-22 01:54:46.706354: step 331000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 01:55:53.216326: step 331020, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-22 01:57:00.537678: step 331040, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-22 01:58:07.687481: step 331060, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-22 01:59:14.888830: step 331080, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-22 02:00:22.458037: step 331100, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-22 02:01:30.056083: step 331120, loss = 0.0020, acc = 1.0000 (17.6 examples/sec; 3.643 sec/batch)
2017-05-22 02:02:36.856648: step 331140, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-22 02:03:43.371500: step 331160, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-22 02:04:50.168784: step 331180, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-22 02:05:56.862800: step 331200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-22 02:07:03.911402: step 331220, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-22 02:08:11.242910: step 331240, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.366 sec/batch)
2017-05-22 02:09:17.923060: step 331260, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-22 02:10:24.998074: step 331280, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-22 02:11:31.405219: step 331300, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 02:12:38.119198: step 331320, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-22 02:13:44.495935: step 331340, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-22 02:14:50.805140: step 331360, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-22 02:15:57.638751: step 331380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-22 02:17:05.067104: step 331400, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.373 sec/batch)
2017-05-22 02:18:11.359414: step 331420, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-22 02:19:17.978475: step 331440, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-22 02:20:24.305852: step 331460, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-22 02:21:30.484670: step 331480, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-22 02:22:36.783158: step 331500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-22 02:23:44.630240: step 331520, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.492 sec/batch)
2017-05-22 02:24:51.942861: step 331540, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-22 02:25:58.627322: step 331560, loss = 0.0008, acc = 1.0000 (19.3 examples/sec; 3.318 sec/batch)
2017-05-22 02:27:05.457813: step 331580, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-22 02:28:12.723140: step 331600, loss = 0.0009, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-22 02:29:19.786977: step 331620, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-22 02:30:28.506840: step 331640, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.379 sec/batch)
2017-05-22 02:31:35.707616: step 331660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.359 sec/batch)
2017-05-22 02:32:42.738865: step 331680, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-22 02:33:50.106952: step 331700, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.446 sec/batch)
2017-05-22 02:34:57.293335: step 331720, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-22 02:36:04.515425: step 331740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.316 sec/batch)
2017-05-22 02:37:11.111645: step 331760, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-22 02:38:19.066183: step 331780, loss = 0.0008, acc = 1.0000 (18.5 examples/sec; 3.461 sec/batch)
2017-05-22 02:39:26.090492: step 331800, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-22 02:40:32.629072: step 331820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-22 02:41:39.064254: step 331840, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.268 sec/batch)
2017-05-22 02:42:45.690815: step 331860, loss = 0.0006, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-22 02:43:52.952006: step 331880, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.354 sec/batch)
2017-05-22 02:45:00.118597: step 331900, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.416 sec/batch)
2017-05-22 02:46:06.991203: step 331920, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-22 02:47:13.162541: step 331940, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-22 02:48:20.053625: step 331960, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-22 02:49:26.451765: step 331980, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-22 02:50:33.441252: step 332000, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
[Eval] 2017-05-22 02:50:48.913515: step 332000, acc = 0.9418, f1 = 0.9398
[Test] 2017-05-22 02:50:59.623457: step 332000, acc = 0.9287, f1 = 0.9281
[Status] 2017-05-22 02:50:59.623576: step 332000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 02:52:07.491052: step 332020, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.350 sec/batch)
2017-05-22 02:53:13.792078: step 332040, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-22 02:54:20.563162: step 332060, loss = 0.0007, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-22 02:55:26.545190: step 332080, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-22 02:56:33.565438: step 332100, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-22 02:57:41.193510: step 332120, loss = 0.0006, acc = 1.0000 (17.9 examples/sec; 3.568 sec/batch)
2017-05-22 02:58:48.099347: step 332140, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.375 sec/batch)
2017-05-22 02:59:55.342543: step 332160, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.382 sec/batch)
2017-05-22 03:01:03.368183: step 332180, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.455 sec/batch)
2017-05-22 03:02:10.007801: step 332200, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-22 03:03:17.109441: step 332220, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-22 03:04:24.229855: step 332240, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-22 03:05:31.563520: step 332260, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.434 sec/batch)
2017-05-22 03:06:38.056037: step 332280, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-22 03:07:46.254992: step 332300, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.458 sec/batch)
2017-05-22 03:08:52.433188: step 332320, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-22 03:09:58.192070: step 332340, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-22 03:11:04.759611: step 332360, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.282 sec/batch)
2017-05-22 03:12:11.084011: step 332380, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.344 sec/batch)
2017-05-22 03:13:17.950152: step 332400, loss = 0.0006, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-22 03:14:24.883674: step 332420, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-22 03:15:32.142251: step 332440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 03:16:38.671926: step 332460, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-22 03:17:46.181911: step 332480, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.338 sec/batch)
2017-05-22 03:18:52.321132: step 332500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 03:19:59.256617: step 332520, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.453 sec/batch)
2017-05-22 03:21:07.402325: step 332540, loss = 0.0009, acc = 1.0000 (18.3 examples/sec; 3.496 sec/batch)
2017-05-22 03:22:13.716747: step 332560, loss = 0.0010, acc = 1.0000 (19.5 examples/sec; 3.283 sec/batch)
2017-05-22 03:23:21.047814: step 332580, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-22 03:24:27.578150: step 332600, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 03:25:36.235244: step 332620, loss = 0.0007, acc = 1.0000 (18.0 examples/sec; 3.555 sec/batch)
2017-05-22 03:26:43.030442: step 332640, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 03:27:49.971494: step 332660, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-22 03:28:57.924416: step 332680, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.347 sec/batch)
2017-05-22 03:30:04.415512: step 332700, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.432 sec/batch)
2017-05-22 03:31:11.081833: step 332720, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.385 sec/batch)
2017-05-22 03:32:18.100132: step 332740, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.368 sec/batch)
2017-05-22 03:33:25.532764: step 332760, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.343 sec/batch)
2017-05-22 03:34:32.871530: step 332780, loss = 0.0006, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-22 03:35:39.702163: step 332800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-22 03:36:46.221042: step 332820, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-22 03:37:52.939195: step 332840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-22 03:39:00.661618: step 332860, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.387 sec/batch)
2017-05-22 03:40:07.312914: step 332880, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.389 sec/batch)
2017-05-22 03:41:14.973135: step 332900, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-22 03:42:21.858441: step 332920, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.333 sec/batch)
2017-05-22 03:43:28.423506: step 332940, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-22 03:44:34.812621: step 332960, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-22 03:45:42.012082: step 332980, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-22 03:46:48.893055: step 333000, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.464 sec/batch)
[Eval] 2017-05-22 03:47:02.972194: step 333000, acc = 0.9412, f1 = 0.9393
[Test] 2017-05-22 03:47:13.390819: step 333000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-22 03:47:13.390908: step 333000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 03:48:20.050526: step 333020, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-22 03:49:26.174903: step 333040, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-22 03:50:32.667769: step 333060, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-22 03:51:39.616734: step 333080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.301 sec/batch)
2017-05-22 03:52:46.534181: step 333100, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-22 03:53:53.349083: step 333120, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-22 03:54:59.889042: step 333140, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-22 03:56:06.096497: step 333160, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-22 03:57:12.939964: step 333180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-22 03:58:19.496087: step 333200, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-22 03:59:25.856928: step 333220, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-22 04:00:32.492052: step 333240, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.302 sec/batch)
2017-05-22 04:01:38.960546: step 333260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-22 04:02:45.278306: step 333280, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-22 04:03:52.777815: step 333300, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.466 sec/batch)
2017-05-22 04:05:00.487034: step 333320, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.371 sec/batch)
2017-05-22 04:06:06.632411: step 333340, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-22 04:07:12.805357: step 333360, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.299 sec/batch)
2017-05-22 04:08:19.733619: step 333380, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-22 04:09:25.917804: step 333400, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-22 04:10:32.833302: step 333420, loss = 0.0007, acc = 1.0000 (19.7 examples/sec; 3.256 sec/batch)
2017-05-22 04:11:39.124944: step 333440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 04:12:46.306104: step 333460, loss = 0.0007, acc = 1.0000 (18.3 examples/sec; 3.488 sec/batch)
2017-05-22 04:13:52.893554: step 333480, loss = 0.0006, acc = 1.0000 (19.6 examples/sec; 3.273 sec/batch)
2017-05-22 04:14:59.454792: step 333500, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.308 sec/batch)
2017-05-22 04:16:07.401651: step 333520, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-22 04:17:14.283121: step 333540, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-22 04:18:20.913808: step 333560, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.321 sec/batch)
2017-05-22 04:19:27.969796: step 333580, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.467 sec/batch)
2017-05-22 04:20:35.169507: step 333600, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-22 04:21:41.577704: step 333620, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-22 04:22:48.079992: step 333640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-22 04:23:56.059579: step 333660, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.412 sec/batch)
2017-05-22 04:25:02.051571: step 333680, loss = 0.0026, acc = 0.9980 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 04:26:09.300032: step 333700, loss = 0.0006, acc = 1.0000 (18.5 examples/sec; 3.460 sec/batch)
2017-05-22 04:27:15.416982: step 333720, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.275 sec/batch)
2017-05-22 04:28:22.161464: step 333740, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.291 sec/batch)
2017-05-22 04:29:28.549381: step 333760, loss = 0.0020, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-22 04:30:34.755148: step 333780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-22 04:31:41.273368: step 333800, loss = 0.0006, acc = 1.0000 (18.9 examples/sec; 3.384 sec/batch)
2017-05-22 04:32:48.743640: step 333820, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.393 sec/batch)
2017-05-22 04:33:55.665398: step 333840, loss = 0.0012, acc = 1.0000 (19.0 examples/sec; 3.376 sec/batch)
2017-05-22 04:35:01.637889: step 333860, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.306 sec/batch)
2017-05-22 04:36:07.827189: step 333880, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 04:37:13.920154: step 333900, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.274 sec/batch)
2017-05-22 04:38:20.296445: step 333920, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.317 sec/batch)
2017-05-22 04:39:27.487675: step 333940, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-22 04:40:34.367679: step 333960, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-22 04:41:41.666434: step 333980, loss = 0.0006, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-22 04:42:47.824404: step 334000, loss = 0.0008, acc = 1.0000 (19.1 examples/sec; 3.357 sec/batch)
[Eval] 2017-05-22 04:43:03.289695: step 334000, acc = 0.9412, f1 = 0.9393
[Test] 2017-05-22 04:43:13.785687: step 334000, acc = 0.9285, f1 = 0.9279
[Status] 2017-05-22 04:43:13.785802: step 334000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 04:44:20.307094: step 334020, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.331 sec/batch)
2017-05-22 04:45:26.250527: step 334040, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.335 sec/batch)
2017-05-22 04:46:34.012327: step 334060, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-22 04:47:40.254917: step 334080, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-22 04:48:47.845518: step 334100, loss = 0.0006, acc = 1.0000 (18.9 examples/sec; 3.381 sec/batch)
2017-05-22 04:49:53.993669: step 334120, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-22 04:51:01.038015: step 334140, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.388 sec/batch)
2017-05-22 04:52:08.004862: step 334160, loss = 0.0007, acc = 1.0000 (18.9 examples/sec; 3.386 sec/batch)
2017-05-22 04:53:14.522359: step 334180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-22 04:54:21.053615: step 334200, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.362 sec/batch)
2017-05-22 04:55:29.204916: step 334220, loss = 0.0007, acc = 1.0000 (17.9 examples/sec; 3.569 sec/batch)
2017-05-22 04:56:36.374961: step 334240, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.413 sec/batch)
2017-05-22 04:57:42.597271: step 334260, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.288 sec/batch)
2017-05-22 04:58:48.646361: step 334280, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.305 sec/batch)
2017-05-22 04:59:54.734089: step 334300, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.287 sec/batch)
2017-05-22 05:01:03.142303: step 334320, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-22 05:02:09.030780: step 334340, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-22 05:03:15.850915: step 334360, loss = 0.0006, acc = 1.0000 (18.7 examples/sec; 3.418 sec/batch)
2017-05-22 05:04:22.938201: step 334380, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-22 05:05:30.813001: step 334400, loss = 0.0008, acc = 1.0000 (18.4 examples/sec; 3.477 sec/batch)
2017-05-22 05:06:37.304410: step 334420, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.356 sec/batch)
2017-05-22 05:07:43.754126: step 334440, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-22 05:08:50.576669: step 334460, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.307 sec/batch)
2017-05-22 05:09:56.890105: step 334480, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-22 05:11:03.049033: step 334500, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 05:12:09.806988: step 334520, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-22 05:13:16.318380: step 334540, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.345 sec/batch)
2017-05-22 05:14:22.863413: step 334560, loss = 0.0007, acc = 1.0000 (19.0 examples/sec; 3.367 sec/batch)
2017-05-22 05:15:29.180375: step 334580, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.278 sec/batch)
2017-05-22 05:16:36.430019: step 334600, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-22 05:17:43.558038: step 334620, loss = 0.0010, acc = 1.0000 (18.7 examples/sec; 3.429 sec/batch)
2017-05-22 05:18:50.364602: step 334640, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.309 sec/batch)
2017-05-22 05:19:57.467193: step 334660, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.320 sec/batch)
2017-05-22 05:21:05.852728: step 334680, loss = 0.0006, acc = 1.0000 (18.2 examples/sec; 3.519 sec/batch)
2017-05-22 05:22:11.936032: step 334700, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-22 05:23:18.427652: step 334720, loss = 0.0006, acc = 1.0000 (19.5 examples/sec; 3.290 sec/batch)
2017-05-22 05:24:24.839082: step 334740, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.322 sec/batch)
2017-05-22 05:25:33.110255: step 334760, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.438 sec/batch)
2017-05-22 05:26:39.460538: step 334780, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.281 sec/batch)
2017-05-22 05:27:46.611376: step 334800, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.298 sec/batch)
2017-05-22 05:28:53.089971: step 334820, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.313 sec/batch)
2017-05-22 05:30:01.201600: step 334840, loss = 0.0007, acc = 1.0000 (18.8 examples/sec; 3.408 sec/batch)
2017-05-22 05:31:08.847110: step 334860, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.265 sec/batch)
2017-05-22 05:32:16.788460: step 334880, loss = 0.0022, acc = 0.9980 (19.5 examples/sec; 3.280 sec/batch)
2017-05-22 05:33:23.841730: step 334900, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.329 sec/batch)
2017-05-22 05:34:30.526229: step 334920, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
2017-05-22 05:35:37.013821: step 334940, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.334 sec/batch)
2017-05-22 05:36:43.423657: step 334960, loss = 0.0007, acc = 1.0000 (19.6 examples/sec; 3.266 sec/batch)
2017-05-22 05:37:51.265587: step 334980, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.293 sec/batch)
2017-05-22 05:38:58.993144: step 335000, loss = 0.0012, acc = 1.0000 (19.3 examples/sec; 3.323 sec/batch)
[Eval] 2017-05-22 05:39:14.522906: step 335000, acc = 0.9400, f1 = 0.9382
[Test] 2017-05-22 05:39:25.116450: step 335000, acc = 0.9276, f1 = 0.9270
[Status] 2017-05-22 05:39:25.116541: step 335000, maxindex = 2000, maxdev = 0.9637, maxtst = 0.9536
2017-05-22 05:40:33.821096: step 335020, loss = 0.0007, acc = 1.0000 (18.4 examples/sec; 3.479 sec/batch)
2017-05-22 05:41:40.512151: step 335040, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 05:42:46.378938: step 335060, loss = 0.0006, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-22 05:43:53.430151: step 335080, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.319 sec/batch)
2017-05-22 05:45:01.647360: step 335100, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.300 sec/batch)
2017-05-22 05:46:08.668604: step 335120, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.428 sec/batch)
2017-05-22 05:47:14.982946: step 335140, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.304 sec/batch)
2017-05-22 05:48:21.267636: step 335160, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.310 sec/batch)
2017-05-22 05:49:27.526599: step 335180, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.292 sec/batch)
2017-05-22 05:50:33.793813: step 335200, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-22 05:51:39.774672: step 335220, loss = 0.0009, acc = 1.0000 (19.4 examples/sec; 3.297 sec/batch)
2017-05-22 05:52:48.086317: step 335240, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.276 sec/batch)
2017-05-22 05:53:55.525396: step 335260, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 05:55:01.766770: step 335280, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.337 sec/batch)
2017-05-22 05:56:08.724245: step 335300, loss = 0.0006, acc = 1.0000 (19.3 examples/sec; 3.315 sec/batch)
2017-05-22 05:57:15.308173: step 335320, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 05:58:22.352665: step 335340, loss = 0.0007, acc = 1.0000 (18.5 examples/sec; 3.469 sec/batch)
2017-05-22 05:59:29.903990: step 335360, loss = 0.0006, acc = 1.0000 (18.4 examples/sec; 3.474 sec/batch)
2017-05-22 06:00:36.290457: step 335380, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.279 sec/batch)
2017-05-22 06:01:43.214208: step 335400, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.294 sec/batch)
2017-05-22 06:02:49.338214: step 335420, loss = 0.0028, acc = 0.9980 (19.5 examples/sec; 3.278 sec/batch)
2017-05-22 06:03:56.218666: step 335440, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.295 sec/batch)
2017-05-22 06:05:03.344343: step 335460, loss = 0.0007, acc = 1.0000 (18.7 examples/sec; 3.427 sec/batch)
2017-05-22 06:06:10.582167: step 335480, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.349 sec/batch)
2017-05-22 06:07:17.600765: step 335500, loss = 0.0006, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-22 06:08:23.700247: step 335520, loss = 0.0007, acc = 1.0000 (19.4 examples/sec; 3.303 sec/batch)
2017-05-22 06:09:30.641481: step 335540, loss = 0.0008, acc = 1.0000 (19.5 examples/sec; 3.284 sec/batch)
2017-05-22 06:10:37.707266: step 335560, loss = 0.0007, acc = 1.0000 (18.6 examples/sec; 3.450 sec/batch)
2017-05-22 06:11:44.538843: step 335580, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.340 sec/batch)
2017-05-22 06:12:52.062298: step 335600, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.355 sec/batch)
2017-05-22 06:13:57.993077: step 335620, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.285 sec/batch)
2017-05-22 06:15:06.147014: step 335640, loss = 0.0007, acc = 1.0000 (18.2 examples/sec; 3.509 sec/batch)
2017-05-22 06:16:15.156226: step 335660, loss = 0.0007, acc = 1.0000 (16.6 examples/sec; 3.850 sec/batch)
2017-05-22 06:17:22.387688: step 335680, loss = 0.0008, acc = 1.0000 (19.4 examples/sec; 3.296 sec/batch)
2017-05-22 06:18:29.049893: step 335700, loss = 0.0006, acc = 1.0000 (19.6 examples/sec; 3.269 sec/batch)
2017-05-22 06:19:35.955890: step 335720, loss = 0.0007, acc = 1.0000 (19.1 examples/sec; 3.352 sec/batch)
2017-05-22 06:20:43.038200: step 335740, loss = 0.0006, acc = 1.0000 (19.1 examples/sec; 3.351 sec/batch)
2017-05-22 06:21:50.540488: step 335760, loss = 0.0006, acc = 1.0000 (18.4 examples/sec; 3.478 sec/batch)
2017-05-22 06:22:56.759802: step 335780, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 06:24:03.110932: step 335800, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.325 sec/batch)
2017-05-22 06:25:11.441601: step 335820, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.289 sec/batch)
2017-05-22 06:26:17.828734: step 335840, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.324 sec/batch)
2017-05-22 06:27:24.762076: step 335860, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.327 sec/batch)
2017-05-22 06:28:30.868572: step 335880, loss = 0.0007, acc = 1.0000 (19.5 examples/sec; 3.274 sec/batch)
2017-05-22 06:29:37.196990: step 335900, loss = 0.0007, acc = 1.0000 (19.3 examples/sec; 3.311 sec/batch)
2017-05-22 06:30:44.075850: step 335920, loss = 0.0008, acc = 1.0000 (18.8 examples/sec; 3.403 sec/batch)
2017-05-22 06:31:51.334189: step 335940, loss = 0.0008, acc = 1.0000 (19.2 examples/sec; 3.330 sec/batch)
2017-05-22 06:32:59.140396: step 335960, loss = 0.0008, acc = 1.0000 (18.2 examples/sec; 3.521 sec/batch)
2017-05-22 06:34:06.674821: step 335980, loss = 0.0007, acc = 1.0000 (19.2 examples/sec; 3.339 sec/batch)
